#+Title: Math
#+todo: TODO CHECK | DONE
#+setupfile: setup/ejerciciocomputacion.setup

** Header                                                                                                     :ignore:
#+latex_header: \usepackage{libertine}
#+latex_header: \usepackage[scale=0.85]{FiraMono}
#+latex_header: \usepackage{unicode-math}

#+latex_header: \usepackage[utf8x]{inputenc} 
#+latex_header: \setcounter{secnumdepth}{0}
#+latex_header: \setlength{\parindent}{0pt}
#+latex_header: \usepackage{physics}
#+latex_header: \usepackage{amsthm}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amssymb}
#+latex_header: \usepackage{stmaryrd}
#+latex_header: \usepackage{mathtools}
#+latex_header: \usepackage{mathabx}
#+latex_header: \usepackage{color}
#+latex_header: \usepackage{bussproofs}\EnableBpAbbreviations{}
#+latex_header: \usepackage{tikz}
#+latex_header: \usepackage{tikz-cd}
#+latex_header: \usepackage{bussproofs} \EnableBpAbbreviations{}
#+latex_header: \usepackage[makeroom]{cancel}

#+latex_header: \DeclareMathOperator{\im}{Im}
#+latex_header: \DeclareMathOperator{\coker}{Coker}
#+latex_header: \DeclareMathOperator{\spec}{Spec}
#+latex_header: \DeclarePairedDelimiter\bbk{\llbracket}{\rrbracket}
#+latex_header: \newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
#+latex_header: \newcommand{\nnorm}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}

#+latex_header: \newcommand\id{\mathrm{id}}
#+latex_header: \newcommand\Id{\mathrm{Id}}
#+latex_header: \newcommand\hom{\mathrm{hom}}
#+latex_header: \newcommand\Nat{\mathrm{Nat}}
#+latex_header: \newcommand\Grp{\mathsf{Grp}}
#+latex_header: \newcommand\Set{\mathsf{Set}}
#+latex_header: \newcommand\todot{\xrightarrow{.}}
#+latex_header: \usepackage{mathtools}
#+latex_header: \DeclarePairedDelimiter\pair{\langle}{\rangle}

#+latex_header: \DeclarePairedDelimiter\abs{\lvert}{\rvert}%
#+latex_header: \DeclarePairedDelimiter\norm{\lVert}{\rVert}%
#+latex_header: \DeclarePairedDelimiter\brck{\llbracket}{\rrbracket}%

*** Macros on HoTT                                                                                           :ignore:
#+latex_header: \newcommand\ap{\mathsf{ap}}
#+latex_header: \newcommand\apd{\mathsf{apd}}
#+latex_header: \newcommand\refl{\mathsf{refl}}
#+latex_header: \newcommand\id{\mathsf{id}}
#+latex_header: \newcommand\transport{\mathsf{transport}}
#+latex_header: \newcommand\happly{\mathsf{happly}}
#+latex_header: \newcommand\funext{\mathsf{funext}}
#+latex_header: \newcommand\proj{\mathsf{pr}}
#+latex_header: \newcommand\rec{\mathsf{rec}}
#+latex_header: \newcommand\pr{\mathsf{pr}}
#+latex_header: \newcommand\idtoeqv{\mathsf{idtoeqv}}
#+latex_header: \newcommand\ua{\mathsf{ua}}
#+latex_header: \newcommand\isSet{\mathsf{isSet}}
#+latex_header: \newcommand\isProp{\mathsf{isProp}}
#+latex_header: \newcommand\Set{\mathsf{Set}}
#+latex_header: \newcommand\Prop{\mathsf{Prop}}
#+latex_header: \newcommand\fnot{\mathsf{not}}
#+latex_header: \newcommand\LEM{\mathsf{LEM}}
#+latex_header: \newcommand\trunc[1]{\left\lVert#1\right\rVert}
#+latex_header: \newcommand\isContr{\mathsf{isContr}}
#+latex_header: \newcommand\ishae{\mathsf{ishae}}
#+latex_header: \newcommand\qinv{\mathsf{qinv}}
#+latex_header: \newcommand\fib{\mathsf{fib}}
#+latex_header: \newcommand\biinv{\mathsf{biinv}}
#+latex_header: \newcommand\linv{\mathsf{linv}}
#+latex_header: \newcommand\rinv{\mathsf{rinv}}
#+latex_header: \renewcommand\succ{\mathsf{succ}}
#+latex_header: \newcommand\isequiv{\mathsf{isequiv}}
#+latex_header: \newcommand\isHinit{\mathsf{isHinit}}
#+latex_header: \newcommand\isEmbedding{\mathsf{isEmbedding}}
#+latex_header: \newcommand\isSurjective{\mathsf{isSurjective}}
#+latex_header: \newcommand\pair{\mathsf{pair}}
#+latex_header: \newcommand\inl{\mathsf{inl}}
#+latex_header: \newcommand\inr{\mathsf{inr}}
#+latex_header: \newcommand\seg{\mathsf{seg}}
#+latex_header: \newcommand\base{\mathsf{base}}
#+latex_header: \newcommand\N{\mathsf{N}}
#+latex_header: \newcommand\conn{\mathsf{conn}}
#+latex_header: \newcommand\code{\mathsf{code}}
#+latex_header: \newcommand\encode{\mathsf{encode}}
#+latex_header: \newcommand\decode{\mathsf{decode}}
#+latex_header: \renewcommand\S{\mathsf{S}}
#+latex_header: \newcommand\merid{\mathsf{merid}}
#+latex_header: \newcommand\isCut{\mathsf{isCut}}
#+latex_header: \newcommand\apart{\mathbin{\#}}
#+latex_header: \newcommand\istype[1]{\mathop{\mbox{$\mathsf{is}$-$#1$-$\mathsf{type}$}}}
*** Logic macros                                                                                             :ignore:
#+latex_header: \newcommand\land{\wedge}
#+latex_header: \newcommand\lor{\vee}
#+latex_header: \newcommand\model{\mathfrak{M}}
#+latex_header: \newcommand\entail{\models}
#+latex_header: \newcommand\seq{\Rightarrow}
*** Analysis                                                                                                 :ignore:
#+latex_header: \newcommand\oy{\overline{y}}
#+latex_header: \newcommand\tf{\tilde{f}}
#+latex_header: %\newcommand\bV{\overset{\bullet}{V}}
#+latex_header: \newcommand\bV{\dot{V}}
*** Type theory macros                                                                                       :ignore:
#+latex_header: \newcommand\ap{\mathsf{ap}}
#+latex_header: \newcommand\apd{\mathsf{apd}}
#+latex_header: \newcommand\refl{\mathsf{refl}}
#+latex_header: \newcommand\id{\mathsf{id}}
#+latex_header: \newcommand\transport{\mathsf{transport}}
#+latex_header: \newcommand\happly{\mathsf{happly}}
#+latex_header: \newcommand\funext{\mathsf{funext}}
#+latex_header: \newcommand\proj{\mathsf{pr}}
#+latex_header: \newcommand\rec{\mathsf{rec}}
#+latex_header: \newcommand\pr{\mathsf{pr}}
#+latex_header: \newcommand\idtoeqv{\mathsf{idtoeqv}}
#+latex_header: \newcommand\ua{\mathsf{ua}}
#+latex_header: \newcommand\isSet{\mathsf{isSet}}
#+latex_header: \newcommand\isProp{\mathsf{isProp}}
#+latex_header: \newcommand\Set{\mathsf{Set}}
#+latex_header: \newcommand\Prop{\mathsf{Prop}}
#+latex_header: \newcommand\fnot{\mathsf{not}}
#+latex_header: \newcommand\LEM{\mathsf{LEM}}
#+latex_header: \newcommand\trunc[1]{\left\lVert#1\right\rVert}
#+latex_header: \newcommand\isContr{\mathsf{isContr}}
#+latex_header: \newcommand\ishae{\mathsf{ishae}}
#+latex_header: \newcommand\qinv{\mathsf{qinv}}
#+latex_header: \newcommand\fib{\mathsf{fib}}
#+latex_header: \newcommand\biinv{\mathsf{biinv}}
#+latex_header: \newcommand\linv{\mathsf{linv}}
#+latex_header: \newcommand\rinv{\mathsf{rinv}}
#+latex_header: \renewcommand\succ{\mathsf{succ}}
#+latex_header: \newcommand\isequiv{\mathsf{isequiv}}
#+latex_header: \newcommand\isHinit{\mathsf{isHinit}}
#+latex_header: \newcommand\isEmbedding{\mathsf{isEmbedding}}
#+latex_header: \newcommand\isSurjective{\mathsf{isSurjective}}
#+latex_header: \newcommand\pair{\mathsf{pair}}
#+latex_header: \newcommand\inl{\mathsf{inl}}
#+latex_header: \newcommand\inr{\mathsf{inr}}
#+latex_header: \newcommand\seg{\mathsf{seg}}
#+latex_header: \newcommand\base{\mathsf{base}}
#+latex_header: \newcommand\N{\mathsf{N}}
#+latex_header: \renewcommand\S{\mathsf{S}}
#+latex_header: \newcommand\merid{\mathsf{merid}}
#+latex_header: \newcommand\istype[1]{\mathop{\mbox{$\mathsf{is}$-$#1$-$\mathsf{type}$}}}
*** Category theory macros                                                                                   :ignore:
#+latex_header: \newcommand\hom{\mathrm{hom}}
#+latex_header: \newcommand\Sets{\mathsf{Sets}}
#+latex_header: \newcommand\Set{\mathsf{Set}}
#+latex_header: \newcommand\todot{\xrightarrow{.}}
* Topics
Resources on spaced repetition

 * [[http://www.gwern.net/Spaced-repetition][Gwern - spaced repetition]]
 * [[https://www.supermemo.com/en/articles/20rules][SuperMemo - twenty rules of formulating knowledge]]

A session should be 10-15 mins long.

** Category theory                                                                                        :categories:
*** Basic category theory
**** Universality and Yoneda
***** Universal arrow                                                                                       :drill:
SCHEDULED: <2019-10-27 Sun>
:PROPERTIES:
:ID:       f5c0c0db-7fd6-486d-9eab-d0f87b6f8d54
:DRILL_LAST_INTERVAL: 487.2209
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 9
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.444
:DRILL_EASE: 3.2
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:15]
:END:
Definition of *universal arrow* from $c$ to $S$.

****** Definition
For any other $f : c \to Sd$ there exists this unique diagram

\[\begin{tikzcd}
 & Sd & d \\
c \rar[swap]{u}\urar{f} & Sr \uar[swap, dashed]{S \widetilde f}
& r \uar[dashed,swap]{\exists! \widetilde{f}}
\end{tikzcd}\]

***** Yoneda Lemma                                                                                          :drill:
SCHEDULED: <2019-01-08 Tue>
:PROPERTIES:
:ID:       dbee06db-00f9-4f92-9e8f-ab855a4a5fc6
:DRILL_LAST_INTERVAL: 199.6489
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.2
:DRILL_EASE: 2.52
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:15]
:END:
Statement of the first Yoneda Lemma.

****** Statement
For any covariant $K\colon D \to \mathsf{Set}$ and $r \in D$, there is a bijection

\[
y \colon \mathrm{Nat}(\mathrm{hom}_{D}(r,-), K) \cong Kr
\]

sending any natural transformation $\alpha \colon \mathrm{hom}_{D}(r,-) \xrightarrow{.} K$
to its image on the identity, $\alpha_r(\mathsf{id}_r)$.

***** Yoneda functor                                                                                        :drill:
SCHEDULED: <2018-12-31 Mon>
:PROPERTIES:
:ID:       0d249757-6f4e-4635-a141-605ae14c0aaf
:DRILL_LAST_INTERVAL: 192.1774
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.2
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:
Definition of the *Yoneda functor*.

****** Definition
The *Yoneda functor*, $Y \colon D^{op} \to\Sets^{D}$, is a currying of the
hom-functor $X \mapsto \mathrm{hom}(X,-)$.

\[
\left(f \colon s \to r\right) \mapsto 
\Big(- \circ f \colon \hom_D(r,-) \to \hom_D(s,-)\Big).
\]

It can be also written as $Y' \colon D \to\Sets^{D^{op}}$.

**** Adjoints, monads and algebras
***** Unit of an adjunction                                                                                 :drill:
SCHEDULED: <2019-03-29 Fri>
:PROPERTIES:
:ID:       9048327b-599e-4d3d-8674-3384c97f18fb
:DRILL_LAST_INTERVAL: 279.9851
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.6
:DRILL_EASE: 2.76
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:09]
:END:
Given an adjunction $F \vdash G$, what signature does the *unit* have?

****** Answer
The unit is a natural transformation $\eta \colon \mathsf{Id} \to GF$.
***** Counit of an adjunction                                                                               :drill:
SCHEDULED: <2019-05-23 Thu>
:PROPERTIES:
:ID:       3d507be4-76aa-4852-9a79-842202789c3d
:DRILL_LAST_INTERVAL: 335.3332
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.6
:DRILL_EASE: 2.76
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:09]
:END:
Given an adjunction $F \vdash G$, what signature does the *counit* have?

****** Answer
The counit is $\epsilon : FG \to \mathrm{id}$.
***** Monad of an adjunction                                                                                :drill:
SCHEDULED: <2018-10-17 Wed>
:PROPERTIES:
:ID:       be30eb00-9dbd-4d91-8d58-a5e3c9cf9bff
:DRILL_LAST_INTERVAL: 157.7961
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 10
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 4.2
:DRILL_EASE: 3.2
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:47]
:END:
Given an adjunction $F \vdash G$, what monad does it give rise to?

****** Answer
The monad $T := G \circ F$.
**** Limits
***** Set is small-complete                                                                                 :drill:
SCHEDULED: <2019-03-06 Wed>
:PROPERTIES:
:ID:       b60b88db-63fd-40a4-aaa5-d591be15e290
:DRILL_LAST_INTERVAL: 251.5049
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:09]
:END:
A category is *small-complete* when all small diagrams have limits in it.
Why is Set small-complete? How can we construct small limits in it?

****** Constructing small limits on set
Given $F \colon J \to \mathsf{Set}$, we have that the set of cones from the terminal
object $\mathrm{Cone}(\ast,F)$ is the limit.

****** As an adjunction
Crucially,

\[
\mathrm{Cone}(X,F) \cong \hom(X, \mathrm{Cone}(\ast,F)).
\]

****** TODO Isn't this simpler using the cartesian product?
***** Creation of limits                                                                                    :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       a2541164-f873-495c-8cdd-a7ea1f8a389d
:DRILL_LAST_INTERVAL: 9.3475
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.572
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:14]
:END:
When does a functor $V \colon A \to X$ *create limits* for $F \colon J \to A$?

****** Answer
For any limiting cone $\tau\colon x \xrightarrow{.} VF$ exists $\sigma\colon a\todot F$ with $Va = x$ and $V\sigma = \tau$.
and this $\sigma$ is a limiting cone.

***** Limits from products and equalizers                                                                   :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       c4ec22a4-89bb-47a8-839c-25faa68f3766
:DRILL_LAST_INTERVAL: 3.6869
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.833
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:08]
:END:
Conditions on products and equalizers to 

 1) have all finite limits
 2) have all small limits.

****** Answer
Having all equalizers and

 1) all finite products;
 2) all small products.

**** Monoidal and enriched categories
***** Preadditive category                                                                                  :drill:
SCHEDULED: <2018-07-02 Mon>
:PROPERTIES:
:ID:       01815e4d-7bf6-473c-a818-f5a4f3568967
:DRILL_LAST_INTERVAL: 4.5907
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 11
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 3.818
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:07]
:END:
Definition of [[https://en.wikipedia.org/wiki/Preadditive_category][preadditive category]]

****** Definition
Every $hom(a,b)$ is an *abelian group* and composition is *bilinear*

\[
f \circ (g+h) = f \circ g+f \circ h \qquad (f+g)\circ h = f\circ h+g \circ h
\]

***** Monoidal category: isomorphisms                                                                       :drill:
SCHEDULED: <2018-09-26 Wed>
:PROPERTIES:
:ID:       b388bb90-8069-4b36-b655-acdea2c12f12
:DRILL_LAST_INTERVAL: 136.9062
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:44]
:END:
Isomorphisms defining a monoidal category.

****** Answer
A *monoidal category* ${\cal B}$ is a category with a bifunctor $\otimes \colon {\cal B} \times {\cal B} \to {\cal B}$
and three natural isomorphisms

 * $\alpha \colon a \otimes (b \otimes c) \cong (a \otimes b) \otimes c$

 * $\lambda \colon e \otimes a \cong a$

 * $\rho \colon a \otimes e \cong a$

***** Monoidal category: associativity diagram                                                              :drill:
SCHEDULED: <2018-07-22 Sun>
:PROPERTIES:
:ID:       72cb82e6-5bcf-4229-ae84-54bc0cb43c26
:DRILL_LAST_INTERVAL: 70.5114
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.6
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:40]
:END:
Diagram of a monoidal category for

 * $\alpha \colon a \otimes (b \otimes c) \cong (a \otimes b) \otimes c$

****** Diagram
\begin{tikzcd}
a \otimes (b \otimes (c \otimes d)) \dar{1 \otimes \alpha} \rar{\alpha}& 
(a \otimes b) \otimes (c \otimes d) \rar{\alpha}& 
((a \otimes b) \otimes c) \otimes d \dar{\alpha \otimes 1}\\
a \otimes ((b \otimes c) \otimes d) \ar[rr, "\alpha"] & & 
(a \otimes (b \otimes c)) \otimes d
\end{tikzcd}

***** Monoidal category: units diagram                                                                      :drill:
SCHEDULED: <2018-08-25 Sat>
:PROPERTIES:
:ID:       8c876951-43d8-4c15-b5bb-b5d659d70e1f
:DRILL_LAST_INTERVAL: 63.8954
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:20]
:END:
Diagrams of a monoidal category for

 * $\lambda \colon e \otimes a \cong a$

 * $\rho \colon a \otimes e \cong a$

****** Diagram
\[\begin{tikzcd}[row sep=tiny]
& a \otimes (e \otimes c) \ar[dd, "\alpha"]\dlar[swap]{1 \otimes \lambda}\\ 
a \otimes c &\\
& (a \otimes e) \otimes c \ular{\rho \otimes 1}
\end{tikzcd}\]

**** Categorical logic
***** Cartesian closed category                                                                             :drill:
SCHEDULED: <2018-12-01 Sat>
:PROPERTIES:
:ID:       1c544faf-0c54-4fc1-b2f2-839d6fdd8354
:DRILL_LAST_INTERVAL: 203.0545
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.4
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:46]
:END:
Define a *cartesian closed category* using three adjoints.

****** Definition
Any category ${\cal C}$ is cartesian closed if and only if there exist
*right adjoints* for the following functors

\begin{prooftree}
\AX$\ast\ \fCenter\to\ast$
\doubleLine
\UI$x\ \fCenter\to \top$
\AX$(x,x)\ \fCenter\to (y,z)$
\doubleLine
\UI$x\ \fCenter\to y \times z$
\AX$x \times a\ \fCenter\to y$
\doubleLine
\UI$x\ \fCenter\to y^a$
\noLine
\TIC{}
\end{prooftree}

***** Elementary topos                                                                                      :drill:
SCHEDULED: <2019-01-30 Wed>
:PROPERTIES:
:ID:       5e288a7b-486a-4d12-91b2-45834c6a0b47
:DRILL_LAST_INTERVAL: 221.7358
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.4
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:11]
:END:
Define *elementary topos* with three properties.

****** Definition

 1. is *cartesian closed*,
 2. has all *finite limits* (pullbacks and terminal),
 3. has a *subobject classifier*.

Note that /locally cartesian closed/ is implied from these properties

***** Pullback functor in objects                                                                           :drill:
SCHEDULED: <2019-03-10 Sun>
:PROPERTIES:
:ID:       fed94983-29d8-4580-97ba-0e0d06a7da46
:DRILL_LAST_INTERVAL: 260.7727
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:19]
:END:
Given $f : B \to A$, define the pullback functor $f^{\ast} : {\cal C}/A \to {\cal C}/B$
in objects.

****** Definition in objects
Given any $c : C \to A$, the pullback functor gives $f^{\ast}c : f^{\ast}C \to B$
with the following pullback square

\[\begin{tikzcd}
f^{\ast}C \rar{}\drar[phantom, near start, "\ulcorner"] \dar[swap]{f^{\ast}c} & C \dar{c} \\
B\rar{f} & A
\end{tikzcd}\]

***** Pullback functor in morphisms                                                                         :drill:
SCHEDULED: <2019-04-17 Wed>
:PROPERTIES:
:ID:       5bb637fe-4ec1-4cc1-9e8d-b973b81ebaff
:DRILL_LAST_INTERVAL: 299.3226
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:17]
:END:
Given $f : B \to A$, define the pullback functor $f^{\ast} : {\cal C}/A \to {\cal C}/B$
on the morphism $h : C \to D$ defined between $c : C \to A$ and $d : D \to A$.

****** Definition in morphisms
The morphism $f^{\ast}h : f^{\ast}C \to f^{\ast}D$ is defined by the universal property
of pullbacks

\[\begin{tikzcd}
f^{\ast}C \ar{rrr}\ar[dashed]{dr}{\exists! f^{\ast}h} \ar[bend right,swap]{ddr}{f^{\ast}c} &&&
C \ar{dl}{h}\ar[bend left=60]{ddl}{c} \\
& f^{\ast}D \dar[swap]{f^{\ast}d} \rar & D \dar{d} \\
& BÃ± \rar[swap]{f} & A \\
\end{tikzcd}\]
*** Categories for the working mathematician - MacLane
# Exports using config.setup, essay.setup

**** I. Categories, Functors and Natural Transformations
***** I.1. Axioms for categories
****** Metagraph
A *metagraph* consists of objects $a,b,c,\dots$ and arrows $f,g,h,\dots$, with two 
operations:

  - *domain*, $dom(f) = a$, and
  - *codomain*, $cod(f) = b$,

that we write as $f : a \to b$.

****** Metacategory
A *metacategory* is a metagraph with two additional operations:

  - *Identity*: $id_a : a \to a$
  - *Composition*: for a pair of arrows $dom(g) = cod(f)$, it defines a
    new arrow $g \circ f : dom(f) \to cod(g)$.

    \[\begin{tikzcd}
    & b \drar{g} & \\
    a \arrow{rr}{g\circ f} \urar{f} & & c
    \end{tikzcd}\]

Subject to the following axioms:

  - *Associativity*: Given objects and arrows in this configuration:
    
    $a \overset{f}\to b \overset{g}\to c \overset{k}\to d$

    We have the equality: $k \circ (g \circ f) = (k \circ g) \circ f$.

  - *Unit law*: composition with the identity arrow is neutral.

******* Metacategory of Sets
******* Metacategory of Groups
******* Arrow-only axioms
***** I.2. Categories
****** Category
An interpretation of a metacategory within set theory.

******* Diagram scheme (directed graphs)
A category is a graph with identity and composition functions.

****** Examples of categories
******* Basic examples
******** The empty category
******** 1, the category with one object and one identity
******** 2, the category a -> b with two objects
******** 3, a triangle category
******** Parallel arrows

******* Discrete categories
Where every arrow is an identity.

******* Monoids and groups
A category with one object.

******* Matrices
Objects: positive integers.
Arrows: $m\times n$ matrices.

******* Sets of sets
******* Preorder
******** Partial orders
******** Linear orders
******* Ordinal numbers
******* Simplicial category
******* Large categories
***** I.3. Functors
****** Functor
A morphism of categories. A functor $T : {\cal C} \to {\cal B}$ is given by:

  - The *object function*, $T : Obj({\cal C}) \to Obj({\cal B})$.
  - The *arrow function*, $T : (c \to c') \to (Tc \to Tc')$

With the axioms:

  - $T(1_c) = 1_{Tc}$
  - $T(f\circ g) = Tg \circ Tf$

****** Examples
******* The powerset functor
******* Homology groups
******* General linear group
******* Commutators
******* Forgetful functors
****** Composition of functors, the metacategory Cat
We can define composition of functors, and also functors as isomorphisms.

******* Isomorphisms
A functor is an isomorphism iff there is a functor $S : B \to C$ for 
which both composites are the identity $S \circ T = Id = T \circ S$.

******* Full functor
Every $g : c \to c'$ of $B$ is of the form $Tf : Tc \to Tc'$. In other words,
the arrow function (the map!) is injective.

******* Faithful functor
The equality $Tf_1 = Tf_2$ implies $f_1 = f_2$. In other words, the arrow
function is surjective.

******* Subcategories
A subcategory gives us an inclusion functor.

***** I.4. Natural transformations
****** Natural transformation
Given two functors $S,T : C \to B$, a natural transformation $\tau : S \xrightarrow{.} T$
is a function assigning every $c \in C$ an arrow $Sc \to Tc$ and yielding
a commutative diagram:

\[\begin{tikzcd}
c \dar{f} & & Sc \rar{\tau_c}\dar{Sf} & Tc \dar{Tf} \\
c' & & Sc' \rar{\tau_{c'}} & Tc'
\end{tikzcd}\]

We say that $\tau_c$ is natural in $c$.

******* Translation of pictures
A natural transformation translates a diagram from $S$ to $T$.

\[\begin{tikzcd}
a \arrow{dd}{h}\drar{f} &   & & S a \arrow{dd}{S h}\drar{S f} \arrow{rrr}{\tau a} &     & & T a \arrow{dd}{T h}\drar{T f} &     \\
  & b \dlar{g} & &     & S b \dlar{S g} \arrow{rrr}{\tau b} & &     & T b \dlar{T g} \\
c &   & & S c \arrow{rrr}{\tau c} &     & & T c &     \\
\end{tikzcd}\]

****** Natural isomorphisms
A natural transformation is a morphism of functors. We call 
*natural isomorphism* to a natural transformation where every 
component $\tau_c$ is invertible. We write it as:

$\tau : S \cong T$

The inverses form another natural transformation.

****** Examples
******* Determinant
Natural transformation between two functors
$GL, {\cal U}() : \mathtt{CRng} \longrightarrow \mathtt{Grp}$.

******* Identity and factor-commutator functor

******* Double character group
Defined as $D(G) = Hom(G, \mathbb{R}/\mathbb{Z})$, it is a contravariant functor when 
we define $(Df) t = t \circ f$. But the twice iterated functor $D^2$ is a 
covariant one.

There is a natural transformation between $Id$ and $D^2$.

\[
\tau_G(g) t = t(g)
\]

It is easy (with lambda calculus, for example) to check that this
diagram commutes.

******* Double dual of a finite vectorial space

******* Inclusion and cardinality of ordinals

***** I.5. Monics, epis and zeros
****** Isomorphism
An arrow $e : a \to b$ is an isomorphism if there is an arrow $e^{-1} : b \to a$
such that $e'e = 1$ and $ee' = 1$.

******* Isomorphic objects
Two objects are isomorphic if there is an isomorphism between them.

****** Monic and epi
An arrow is *monic* if it is left cancellable, it is *epi* if it is
right cancellable.

******* Retractions and sections
A left inverse is called a *retraction*, while a right inverse is
called a *section*.

****** Splits and idempotents
When $gh = 1$, $g$ is a split epi, $h$ is a split monic and the composite
$hg$ is an idempotent. An arrow is *idempotent* if $f^2 = f$, and it is said
to split when there exist arrows $g$ and $h$ such that $f = hg$ and $gh = 1$.

****** Terminal objects and initial objects
An object $t$ is terminal if for every $a$ there is exactly one arrow $a \to t$.
An object $s$ is initial if for every $a$ there is exactly one arrow $s \to a$.
An object $z$ is null if it is both initial and terminal.

******* Zero arrow
There is an unique arrow $a \to z \to b$ called the *zero* arrow. Any 
composite with it is itself a zero arrow.

****** Groupoids
A category where every arrow is invertible.

******* The fundamental groupoid
******* Group of homomorphisms
In a groupoid, each object determines a group, $hom(x,x)$.
If there is an arrow $f : x \to x'$, the groups $hom(x,x)$ and $hom(x',x')$
are isomorphic under conjugation $g \mapsto fgf^{-1}$.

******* Connected groupoid
A *connected groupoid* is deteremined by a group and the set of
all objects.

***** I.6. Foundations
****** Universe
An universe is a set $U$ with the closure properties:

  1. $x \in u \in U \implies x \in U$.
  2. e$u,v \in U \implies \{u,v\},(u,v),u\times v \in U$.
  3. $x \in U \implies {\cal P}x, \cup x \in U$.
  4. $\omega \in U$ where $\omega = \{0,1,2,\dots\}$.
  5. $f : a \to b$ surjective and $a \in U$, $b \subseteq U$ implies $b \in U$.

****** Small sets
Fixed an universe, we call a set $u \in U$ a *small set*. The universe is
the set of all small sets. $\mathtt{Set}$ denotes the category of small sets.

******* Small structures
A small group is a small set with a group structure. $\mathtt{Grp}$ denotes the
category of small groups.

****** Small categories
A category is small if the set of its arrows and objects are both small
sets. $\mathtt{Set}$ is not a small category.

***** I.7. Large categories
****** Abelian groups
****** Rings
****** Modules over a ring
****** Bimodules
****** Topological spaces with continuous maps
****** Topological spaces with homotopy classes
****** Pointed sets
****** Binary relations
****** Concrete categories
***** I.8. Hom-Sets
****** Hom-set
For objects $a,b \in C$, the *hom-set* is the set of all arrows between them:

\[
hom(a,b) = \{ f \mid f : a \to b \}
\]

******* Redefinition of a category
A small category is given by:

  1. Set of objects $a,b,\dots$
  2. Hom-set between two objects $hom(a,b)$
  3. Composition function $hom(b,c)\times hom(a,b) \to hom(a,c)$
  4. Identity function for every object, $id \in hom(a,a)$

The distributivity can be seen as commutativity of the following
diagram:

\[\begin{tikzcd}
hom(c,d)\times hom(b,c)\times hom(a,b) \rar\dar &
hom(b,d)\times hom(a,b) \dar \\
hom(c,d)\times hom(a,c) \rar &
hom(a,d)
\end{tikzcd}\]

****** Preadditive category
A category $A$ where each hom-set $hom(a,b)$ is an additive abelian group
for which composition is bilinear:

\[
(g+g')\circ(f+f') = g\circ f + g\circ f' + g' \circ f + g' \circ f'
\]

****** Preadditive category as an enriched category
A preadditive category is given by:

  1. Set of objects $a,b,\dots$
  2. Hom-set, an abelian group between two objects $hom(a,b)$
  3. A bilinear composition $hom(b,c) \otimes hom(a,b) \to hom(a,c)$
  4. Identity morphism for every object, $\mathbb{Z} \to hom(a,a)$

We can see the similitude with the definition of a category by hom-sets.
Categories created in this way are called *enriched categories*.

**** II. Constructions on categories
***** II.1. Duality
****** Elementary theory of an abstract category (ETAC)
A theory with statements involving:

  - Domains: $a = dom(f)$
  - Codomains: $b = codom(f)$
  - Composition: $h = g \circ f$

A *sentence* is a statement with all variables quantified, using
the logical connectives and quantifiers.

******* Dual of an statement
A dual is formed by making the following replacements:

  - $a = dom(f)$ changes to $a = codom(f)$
  - $h = g \circ f$ changes to $h = f \circ g$
  - Logic is unchanged.

******* Table of dualities
\begin{tabular}{l|r}
Statement & Dual statement \\
\hline
$f : a \to b$ & $f : b \to a$ \\
$a = \operatorname{dom} f$ & $a = \operatorname{cod} f$ \\
$h = g \circ f$ & $h = f \circ g$ \\
$f$ mono & $f$ epi
\end{tabular}

****** Duality principle
The dual of each of the axioms for a category is also an axiom. Therefore,
if $\Sigma$ is a consequence of the axioms, so is its dual statement.

****** Duality and functors
The elementary theory of one functor has the axioms of two categories
$A,B$, and the properties of a functor $T$ between them. The duality for
a statement involving several categories reverses the arrows in each
category, but does not reverse the functors.

***** II.2. Contravariance and opposites
****** Opposite category
The *opposite category*, $C^{op}$, is defined with the objects of $C$; for each 
arrow $f : a \to b$ of $C$, the corresponding $f^{op} : b \to a$ is defined with the 
composition

\[
f^{op} \circ g^{op} = (g\circ f)^{op}.
\]

******* Opposite functors
If $T : C \to B$ is a functor, we can create an opposite functor
$T^{op} : C^{op} \to B^{op}$. The assignment $C \to C^{op}$ defines a covariant functor
$\mathtt{Cat} \to \mathtt{Cat}$.

****** Contravariant functors
A functor $S : C^{op} \to B$ can be seen as a *contravariant* functor $S : C \to B$,
knowing that

\[
S(f^{op}\circ g^{op}) = (Sf^{op})(Sg^{op}) = S((g\circ f)^{op}).
\]

******* Examples: Hom-sets
For each object $a \in C$, the *contravariant hom-functor* $hom(a,-)$ and
the *covariant hom-functor* $hom(-,a)$ are defined.

******* Examples: Sheafs
Given $X$, a topological space. Its open sets define a category by 
inclusion, $\mathtt{Open}(X)$. Let ${\cal C}(U)$ denote the set of continuous functions
$h : U \to \mathbb{R}$, the assignment $h \mapsto h|_V$ is a function ${\cal C}(U)\to{\cal C}(V)$ for
each $V \subset U$; and this gives us a contravariant functor.

******* Examples: R-Modules
$\mathtt{ModR} : \mathtt{Rng} \to \mathtt{Cat}$ is a contravariant functor. If $\rho : R \longrightarrow S$ is a morphism
of rings, given $B$ an S-module, we can define $(Mod\rho) B = B\rho$ by pull-back.

***** II.3. Products of categories
****** Product of two categories
We define the *product* $B \times C$ as the category with:

  - pairs of objects $(b,c)$ as objects,
  - pairs of arrows $(f,g)$ as arrows,

with the composition given by the composites in $B$ and $C$

\[
(f',g') \circ (f,g) = (f' \circ f,g' \circ g).
\]

We can define projection functors $P : B\times C \to B$, $Q : B\times C \to C$. Given
any category $D$ and two functors $B \overset{R}\longleftarrow D \overset{T}\longrightarrow C$, there is an unique 
functor $F : D \to B\times C$ with $PF = R, QF = T$.

\[\begin{tikzcd}
& D \drar{T}\dlar[swap]{R} \dar[dashed]{\exists! F} & \\
B & B\times C \rar[swap]{Q}\lar{P} & C
\end{tikzcd}\]

****** Product of functors
Two functors $U : B \to B'$, $V : C\to C'$ have a product:

\[
U\times V : B\times C \to B'\times C'
\]

That can be described as the unique functor making the following
diagram commutative:

\[\begin{tikzcd}
B \dar{U} &
B \times C  \rar{Q}\lar[swap]{P} \dar[dashed]{U \times V}&
C \dar{V} \\
B' &
B' \times C' \rar[swap]{Q'}\lar{P'}&
C'
\end{tikzcd}\]

******* Product as a functor
The product of categories can be seen as a functor:

\[
\times : \mathtt{Cat} \times \mathtt{Cat} \to \mathtt{Cat}
\]

******* Bifunctors
Our definition of product category gives an automatic definition for
a *bifunctor*, a functor of two variables.

****** Bifunctor determined by its currifications
Let $B,C,D$ be categories. For all objects $c \in C, b \in B$, let $L_c : B \to D$
and $M_b : C \to D$ be functors such that $M_b(c) = L_c(b)$.

There exists a bifuctor $S : B\times C \to D$ such that $S(-,c) = L_c$
and $S(b,-) = M_b$ iff for every pair of arrows:

\[
M_{b'}g \circ L_c f = L_{c'}f \circ M_b g
\]

so we can define the value of $S(f,g)$ uniquely.

******* TODO Proof

****** Natural transformations between bifunctors
Given bifunctors $S,S'$, the function $\alpha$ is a natural transformation
$\alpha : S \Rightarrow S'$ iff $\alpha(b,c)$ is natural in $b$ and natural in $c$.

******* TODO Proof

***** II.4. Functor categories
****** Composition of natural transformations
Given functors $R,S,T : C \to B$ and natural transformations $\tau : S\to T$
and $\sigma : R \to S$, their compositions define composite arrows which are
the components of the composite natural transformation $\tau \cdot \sigma$.

\[\begin{tikzcd}
Rc \rar{Rf}\dar{\sigma_c}\arrow[dd,bend right=90]{(\tau \circ \sigma)_c} &
Rc' \dar{\sigma_{c'}} \arrow[dd,bend left=90]{(\tau\circ\sigma)_{c'}} \\
Sc \rar{Sf}\dar{\tau_c}  &
Sc' \dar{\tau_{c'}} \\
Tc \rar{Tf}  &  Tc' 
\end{tikzcd}\]

******* Proof
Naturality of the external square follows from the commutativity of the
two internal squares.

****** Functor category
Given categories $B,C$, let $B^C = \mathrm{Funct}(C,B)$ be the *functor category*,
with objects the functors $T\colon C \to B$ and morphisms the natural 
transformations

\[ \mathrm{Nat}(S,T) = \left\{ \tau\mid \tau\colon S \dot\to T \right\}.
\]

******* Example: Category of arrows
The category $\mathrm{Funct}(B,2)$ is called the *category of arrows* of $B$. Its
objects are arrows $f\colon a \to b$ and its morphisms are commutative squares

\[\begin{tikzcd}
a\rar{h} \dar[swap]{f} & a' \dar{f'} \\
b\rar{k} & b'
\end{tikzcd}\]

******* Example: Actions on a set
If $M$ is a monoid, $\mathrm{Funct}(M, \mathtt{Set})$ is the category with objects the actions
of $M$ on some sets and arrows the morphisms of such actions.

******* TODO Example: Group with operators
****** Example: Group representations
Given $K$ a commutative ring, the functor category $\mathrm{Funct}(G, K\mathtt{-mod})$ is the 
category of linear representations of $G$.

******* Objects
Every functor $T \colon G \to K\mathtt{-mod}$ is determined by a $K\text{-module}$ $V$ and
a morphism of groups $T\colon G \to \text{Aut}(V)$.

******* Natural transformations
A natural transformation $\sigma\colon T \to S$ is given by a single module
homomorphism $\sigma\colon V \to V'$, such that

\[\begin{tikzcd}
V\rar{\sigma} \dar[swap]{Tg} & V' \dar{T'g} \\
V\rar{\sigma} & V'
\end{tikzcd}\]

and it is called an *intertwining* operator.

***** II.5. The category of all categories
****** Identity natural transformations
The identity functor is the identity for the horizontal and vertical
compositions. We can use the symbol of the functor to denote it like
$S \colon S \dot\to S$.

****** Horizontal composition of natural transformations
Given functors $S,T \colon C \to B$ and $S',T'\colon B \to A$ and natural transformations

\[\begin{tikzcd}
C 
\rar[shift left=7pt, ""{name=UL, below}]{S} 
\rar[shift right=7pt, ""name=LL][swap]{T\vphantom{'}} &
B 
\rar[shift left=7pt, ""{name=UR, below}]{S'}
\rar[shift right=7pt, ""name=LR][swap]{T'} &
A\\
\ar[from=UL, to=LL, "\tau", shorten <= -2pt, shorten >= -2pt]
\ar[from=UR, to=LR, "\tau\smash{'}", shorten <= -2pt, shorten >= -2pt]
\end{tikzcd}\]

we can define the *horizontal composite* $\tau' \circ \tau \colon S'S \to T'T$ as the diagonal
of the commutative square

\[\begin{tikzcd}
S'Sc\rar{\tau'_{Sc}} \dar[swap]{S'\tau_c} & T'Sc \dar{T' \tau_c} \\
S'Tc\rar{\tau'_{Tc}} & T'Tc
\end{tikzcd}\]

explicitly, as $\tau' \circ \tau = T' \tau \circ \tau' = \tau' \circ S'\tau$. It is a natural transformation.

******* Alternative notation
With identity notation, we can then define the horizontal composition
as

\[
\tau'\circ \tau = 
(T' \circ \tau) \cdot (\tau'\circ S) =
(\tau' \circ T) \cdot (S'\circ \tau).
\]

******* Proof
It is natural as the following diagram is the composition of two
naturality squares

\[\begin{tikzcd}
S'Sc \rar{S'\tau} \dar{S'Sf} &
S'Tc \rar{\tau'}  \dar{S'Tf} &
T'Tc \dar{T'Tf} \\
S'Sb \rar{S'\tau} &
S'Tb \rar{\tau'} &
T'Tb
\end{tikzcd}\]

defined respectively by the naturality of $S'\tau$ and $\tau'$.

****** Interchange law
Given three categories and four transformations

\[\begin{tikzcd}
C 
\rar[shift left=15pt, ""{name=UL, below}]{} 
\rar[""name=L]
\rar[shift right=15pt, ""name=LL][swap]{} &
B 
\rar[shift left=15pt, ""{name=UR, below}]{}
\rar[""name=R]
\rar[shift right=15pt, ""name=LR][swap]{} &
A\\
\ar[from=UL, to=L, "\sigma", shorten <= -2pt, shorten >= -2pt]
\ar[from=L, to=LL, "\tau", shorten <= 2pt, shorten >= -2pt]
\ar[from=UR, to=R, "\sigma\smash{'}", shorten <= -2pt, shorten >= -2pt]
\ar[from=R, to=LR, "\tau\smash{'}", shorten <= 2pt, shorten >= -2pt]
\end{tikzcd}\]

the vertical and horizontal composites follow the interchange law

\[
(\tau'\cdot \sigma') \circ (\tau \cdot \sigma) = 
(\tau'\circ \tau) \cdot (\sigma'\circ \sigma).
\]

******* Proof
In this diagram, where every internal square is commutative by
naturality

\[\begin{tikzcd}
G_1F_1x \dar{G_1\sigma} \rar{\sigma'} \drar{\sigma'\circ\sigma} 
\ar[bend left]{rr}{\tau'\cdot\sigma'}
\ar[bend right=60,swap]{dd}{G_1(\tau \cdot \sigma)} & 
G_2F_1x \dar{G_2\sigma} \rar{\tau'} &
G_3F_1x \dar{G_3\sigma} \ar[bend left=60]{dd}{G_3(\tau \cdot \sigma)}\\
G_1F_2x \dar{G_1\tau} \rar{\sigma'} &
G_2F_2x \dar{G_2\tau} \rar{\tau'}  \drar{\tau'\circ\tau} &
G_3F_2x \dar{G_3\tau} \\
G_1F_3x \rar{\sigma'} \ar[bend right,swap]{rr}{\tau'\cdot\sigma'} &
G_2F_3x \rar{\tau'} &
G_3F_3x \\
\end{tikzcd}\]

we know, by definition, that the big diagonal must be $(\tau'\cdot \sigma') \circ (\tau\cdot\sigma)$.

****** Double category
A *double category* is a set of arrows for two different compositions
which together satisfy the [[*Interchange law][interchange law]].

****** 2-category
A *2-category* is a double category in thich every identity for the
first composition is also an identity for the second one.

******* Counterexample: commutative squares in Set
The category of commutative squares in Set is a double category but
not a 2-category.

****** Interchange law for operations
Two binary operations $\cdot,\circ$ are said to satisfy the interchange law when

\[
(a' \cdot b') \circ (a \cdot b) = (a' \circ a) \cdot (b'\circ b).
\]

******* Example: matrices
The usual product of matrices $\circ$ satisfies the interchange law with the
square-composition of matrices

\[
\tau \cdot \sigma = \begin{pmatrix}
\tau & 0\\
0 & \sigma
\end{pmatrix}.
\]

******** Proof
Trivial by definition.

****** Functor category as a functor
The functor category can be regarded as a functor

\[\mathrm{Func} \colon \mathtt{Cat}^{op}\times \mathtt{Cat} \to \mathtt{Cat}.
\]

sending an arrow, consisting of two functors $F,G$, to $F^{G}$, a functor
defined as

  * $F^GS = F \circ S \circ G$, on objects.
  * $F^{G}\tau = F \circ \tau \circ G$, on arrows.

******* Analogue with the hom-functor
Note that this is the categorical analogue of the hom-functor

\[\mathrm{Hom} \colon \mathtt{Set}^{op}\times \mathtt{Set} \to \mathtt{Set}.
\]

***** II.6. Comma categories
****** Category of objects under an object
Given $b \in C$, the category of *objects under* it, $(b \downarrow C)$, is the category
with objects all pairs $(f\colon b \to c, c)$, and arrows $h\colon (f,c) \to (f',c')$ for those
arrows such that $h\circ f = f'$.

******* Displayed notation
Objects are of the form

\[\begin{tikzcd}
b \rar{f} & c
\end{tikzcd}\]

morphisms are of the form

\[\begin{tikzcd}
& c \arrow[dd, "h"] \\[-15pt]
b \urar{f}\drar[swap]{f'}& \\[-15pt]
& c'
\end{tikzcd}\]

and the composition is the composition of two commutative triangles
in that form.

****** Category of objects over an object
The category of *objects over* $a$, $(C \downarrow a)$ can be defined similarly in
displayed notation as objects of the form

\[\begin{tikzcd}
c \rar{f} & a
\end{tikzcd}\]

and morphisms

\[\begin{tikzcd}
c \arrow[dd, "h"] \drar{f} &  \\[-15pt]
& b \\[-15pt]
c' \urar[swap]{f'} &
\end{tikzcd}\]

****** Comma category
Given categories and functors $E \overset{T}\longrightarrow C \overset{S}\longleftarrow D$, the comma category
$(T\downarrow S)$ has objects

\[\begin{tikzcd}
Te \rar{f} & Sd
\end{tikzcd}\]

and arrows commutative squares

\[\begin{tikzcd}
Te\rar{Tk} \dar[swap]{f} & Te' \dar{f'} \\
Sd\rar{Sh} & Sd'
\end{tikzcd}\]

The composite is the yuxtaposition of squares.

******* Categories of objects under/over and object
Those are particular cases when the object is seen as a functor
from the unital category $b \colon 1 \to C$.

***** II.7. Graphs and free categories
****** TODO Forgetful functor from categories to graphs
****** Free category
Let $G$ be a small graph. There is a small category $C$ called its free
category satisfying the following universal property

\[\begin{tikzcd}
G \rar{P} \drar[swap]{D} & UC\dar[dashed]{\exists! UD'} \\
& UB
\end{tikzcd}\]

which is equivalent to $P\colon G \to UC$ being initial in $(G \downarrow U)$.

***** TODO II.8. Quotient categories
**** III. Universals and Limits
***** III.1. Universal arrows
****** Universal arrow
A *universal arrow* from $c$ to $S$ is an arrow $u \colon c \to Sr$ such that
for every $c \to Sd$ exists a unique $r \to d$ making this diagram commute

\[\begin{tikzcd}
& Sd & d \\
c \rar[swap]{u}\urar{g} & Sr \uar[swap,dashed]{Sf} & r \uar[dashed]{\exists! f} &.
\end{tikzcd}\]

***** III.2. The Yoneda lemma
****** Universal arrows as natural bijections
The arrow $u \colon c \to Sr$ is universal iff $f \mapsto Sf \circ u$ is a bijection
$\mathrm{hom}(r,d) \cong \mathrm{hom}(c,Sd)$ natural in $d$. Any natural bijection of this
kind is determined by a unique universal arrow.

******* Proof
Bijection follows from the definition of [[*Universal arrow][universal arrow]], and
naturality follows from $S(gf)\circ u = Sg \circ Sf \circ u$.

Given a bijection $\varphi$, we define $u = \varphi(\mathrm{id}_r)$. By naturality we have
the bijection $\varphi(f) = Sf \circ u$, every arrow is written in this way.

****** Representation
A *representation* of $K \colon D \to \mathtt{Set}$ is a natural isomorphism

\[
\psi\colon \mathrm{hom}_{D}(r,-) \cong K.
\]

A functor is /representable/ if it has a representation. An object $r$ is
called a /representing object/. $D$ must have small hom-sets.

****** Representations from universal arrows
If $u \colon \ast \to Kr$ is a universal arrow for a functor $K\colon D \to \mathtt{Set}$, then
$f \mapsto K(f)(u\ast)$ is a representation. Every representation is obtained
in this way.

******* Proof
We know that $\mathrm{hom}(\ast, X) \xrightarrow{.} X$ is a natural isomorphism in $X$; in particular
$\mathrm{hom}(\ast, K-) \xrightarrow{.} K-$. Every representation is built then as

\[ \mathrm{hom}_{D}(r,-) \cong \mathrm{hom}(\ast,K-) \cong K, \]

for every natural isomorphism $D(r,-) \cong \mathtt{Set}(\ast,K-)$. But every natural
isomorphism of this kind is an [[*Universal arrows as natural bijections][universal arrow]].

****** Yoneda Lemma
For any $K\colon D \to \mathtt{Set}$ and $r \in D$, there is a bijection

\[
y \colon \mathrm{Nat}(\mathrm{hom}_{D}(r,-), K) \cong Kr
\]

sending the natural transformation $\alpha \colon \mathrm{hom}_{D}(r,-) \xrightarrow{.} K$ to the image of
the identity, $\alpha_r1_r$.

******* TODO Proof

****** Corollary to Yoneda Lemma
Given $r,s \in D$, any natural transformation $\mathrm{hom}(r,-) \xrightarrow{.} \mathrm{hom}(s,-)$ has
the form $h_{\ast}$ for a unique $h\colon s \to r$.

******* Proof
Using Yoneda Lemma, we know that

\[ \mathrm{Nat}(\mathrm{hom}_D(r,-), \mathrm{hom}_D(s,-)) \cong \mathrm{hom}_D(s,r),
\]

sending the natural transformation to a morphism $\alpha(id_r) = h \colon s \to r$. The
rest of the natural transformation is determined as $h_{\ast}$ by naturality.

****** Addendum to the Yoneda Lemma
The bijection on the [[*Yoneda Lemma][Yoneda Lemma]] is a natural isomorphism between
two $\mathtt{Set}^D \times D \to \mathtt{Set}$ functors.

****** Yoneda functor
In the conditions of [[*Yoneda Lemma][Yoneda Lemma]], the *Yoneda functor* is a full and
faithful functor $Y \colon D^{op} \to \mathtt{Set}^{D}$ defined with the arrow function

\[
\left(f \colon s \to r\right) \mapsto 
\Big(D(f,-) \colon D(r,-) \to D(s,-)\Big).
\]

******* TODO Proof
***** III.3. Coproducts and colimits
****** Diagonal functor
The *diagonal functor* $\Delta \colon C \to C \times C$ is defined by $\Delta(c) = (c,c)$ on objects
and by $\Delta(f) = (f,f)$ on arrows.

****** Coproduct diagram
A *coproduct diagram* of a pair $(a,b)$ is a universal arrow from $(a,b)$ to
the diagonal functor $\Delta$. The object defining this universal arrow up to
isomorphism is called the *coproduct object* and written as $a \amalg b$.

\[\begin{tikzcd}
 & (d,d) \\
(a,b) \rar[swap]{(i_a,i_b)} \urar{(f,g)} & 
(a\amalg b, a\amalg b) \uar[swap]{(f \amalg g, f \amalg g)} & .
\end{tikzcd}\]

******* Diagram rewriting
This universal property can be rewritten as

\[\begin{tikzcd}
a \rar{i}\drar[swap]{f} & a \amalg b \dar[dashed]{\exists! h} & b \lar{j}\dlar{g} \\
& d &  & .
\end{tikzcd}\]

******* Bijection rewriting
This universal arrow is a bijection $\mathrm{hom}(a,d) \times \mathrm{hom}(b,d) \cong \mathrm{hom}(a \amalg b,d)$
natural in $d$.

****** Coproduct functor
In a category $C$ where every pair has a coproduct, the coproduct bifunctor
$\amalg \colon C \times C \to C$ is defined on arrows as the unique arrow

\[\begin{tikzcd}
a \dar[swap]{f}\rar{i} & a \amalg b \dar[dashed]{\exists! f\amalg g} & b \dar{g}\lar[swap]{j} \\
a' \rar[swap]{i'} & a' \amalg b' & b \lar{j'}
\end{tikzcd}\]

making this diagram commute.

****** Infinite coproducts
A *coproduct* of objects of $C$ indexed by a discrete category $X$ is a
universal arrow to the indexed diagonal functor $\Delta \colon C \to C^{X}$. We write
the coproduct object as $\coprod_{x \in X} a_x$.

******* Bijection rewriting
This universal arrow is a bijection

\[\mathrm{hom}\left( \coprod_{x \in X}a_x, c \right)
\cong \prod_{x \in X} \mathrm{hom}(a_x,c)
\]

natural in $c$.

****** Copowers
A *copower* is a coproduct where all the factors are equal. It is written
as 

\[\coprod_{x\in X} b \cong X \cdot b.\]

****** Coequalizers
A *coequalizer* of two arrows $f,g \colon a \to b$ is $u \colon b \to e$ such that $uf = ug$; and
for every other $u'$ with the same property factorizes through it.

****** Pushout
***** III.5. Categories with finite products
****** Category with finite products
A category *has finite products* if to any finite number of objects exists
a product diagram. In particular, it has a terminal object.

****** Binary products define finite products
A category having binary products has finite products. There is then a
product bifunctor, an isomorphism

\[
\alpha \colon a \times (b \times c) \cong (a \times b) \times c
\]

natural in $a,b,c$; and isomorphisms

\[
\lambda \colon t \times a \cong a,
\qquad
\varrho\colon a \times t \cong a,
\]

both natural in $a$.

******* TODO Proof
We will prove that $a \times (b \times c)$ is a product of $a,b,c$; due to its universal
property we know it has the universal property of the product of $a,b,c$

\[\begin{tikzcd}[row sep=small]
& & b \\
a & a \times (b \times c) \rar\lar\urar\drar & b \times c \dar \uar \\
& & c
\end{tikzcd}\]

and, by the same reasoning, $(a \times b) \times c$ is also a product. They are
isomorphic due to universality.

# Why is it natural?

***** III.6. Groups in categories
****** A group has a group hom functor
In a category with finite products, $c$ is a group iff $\mathrm{hom}(c,-)$ is a group
in the functor category $\mathtt{Set}^{C^{op}}$.

******* TODO Proof

***** III.7. Colimits of representable functors
****** Representation as colimits
**** IV. Adjoints
***** IV.1. Adjunctions
****** IV.1.0. Adjunction
An *adjunction* from categories $X$ to $A$ is a pair of functors
$F\colon X \to A$, $G\colon A \to X$ with a natural bijection

\[
\varphi \colon \mathrm{hom}(Fx,a) \cong \mathrm{hom}(x,Ga),
\] 

natural in both $x\in X$ and $a \in A$. We write it as $F \dashv G$.

****** IV.1.1. Unit and counit
An adjunction determines a *unit* and a *counit*

 1) A natural transformation made with universal arrows $\eta\colon I \xrightarrow{.} GF$, where
    the right adjoint of each $f \colon Fx \to a$ is

    \[
    \varphi f = Gf \circ \eta_x \colon x \to Ga.
    \]

 2) A natural transformation made with universal arrows $\varepsilon \colon FG \xrightarrow{.} I$, where
    the left adjoint of each $g \colon x \to Ga$ is

    \[\varphi^{-1}g = \varepsilon \circ Fg \colon Fx \to a.\]

that follow the /triangle identities/ $\eta G \circ G \varepsilon = \mathrm{id}$ and $F\eta \circ \varepsilon F = \mathrm{id}$.

****** IV.1.2. Characterization of adjunctions
Each adjunction is completely determined by any of

 1) functors $F,G$ and $\eta\colon 1 \xrightarrow{.} GF$ where $\eta_x\colon x \to GFx$ is universal to $G$.
 2) functor $G$ and universals $\eta_x \colon x \to GF_0 x$, creating a functor $F$.
 3) functors $F,G$ and $\varepsilon\colon FG \xrightarrow{.} 1$ where $\varepsilon_a\colon FGa \to a$ is universal from $F$.
 4) functor $F$ and universals $\varepsilon_a\colon FG_0a \to a$, creating a functor $G$.
 5) functors $F,G$, with units and counits satisfiying the triangle
    identities $\eta G \circ G \varepsilon = \mathrm{id}$ and $F\eta \circ \varepsilon F = \mathrm{id}$.

***** IV.3. Reflective subcategories
***** IV.4. Equivalence of categories
****** Equivalence of categories
A *equivalence of categories* is a pair of functors $S,T$ which are
inverses, $S \circ T = I$. Their domain and codomain are called /equivalent/ 
/categories/.

****** Skeleton of a category
A *skeleton* is a full subcategory where every isomorphism class has
exactly one representative.

***** IV.5. Adjoints for preorders
****** Galois connections are adjoint pairs
Let $L\colon P \to Q^{op}$ and $R\colon Q^{op} \to P$ two order preserving functions. They
are adjoints iff for every two objects

\[
Lp \geq q \iff p \leq Rq.
\]

The adjunction is unique and

\[
Lp \geq LRLp \geq Lp, \qquad Rq \leq RLRq \leq Rq.
\]

***** IV.6. Cartesian closed categories
****** Cartesian closed category
A *cartesian closed category* is a category with all finite products where
the functors

\[\begin{tabular}{ccc}
$C \to 1$, & 
$C \to C \times C$, & 
$C \to C$, \\
$c \mapsto 0$, &
$c \mapsto (c,c)$, &
$c \mapsto c \times b$;
\end{aligned}\]

have specified adjoints

\[\begin{tabular}{ccc}
$0 \mapsto t$, &
$(a,b) \mapsto a \times b$, &
$c \mapsto c^b$.
\end{aligned}\]

****** TODO Evaluation map

***** IV.7. Transformations of adjoints
****** Map of adjunctions
Given two adjunctions $\varphi\colon F \dashv G$ and $\varphi'\colon F' \dashv G'$; we define a *map of adjunctions*
as two functors $K \colon A \to A'$ and $L \colon X \to X'$ such that

\[\begin{tikzcd}
A \dar{K}\rar{G} & X \dar{L} \\
A' \rar{G'} & X'
\end{tikzcd}
\quad
\begin{tikzcd}
X \dar{L}\rar{F} & A \dar{K} \\
X' \rar{F'} & A' &
\end{tikzcd}\]

commute and such that, knowing that $KF = F'L$ and $LG = G'K$,

\[\begin{tikzcd}
\mathrm{hom}(Fx,a) \rar{\varphi} \dar[swap]{K} & \mathrm{hom}(x,Ga) \dar{L} \\
\mathrm{hom}(F'Lx,Ka) \rar{\varphi'} & \mathrm{hom}(Lx,G'Ka)
\end{tikzcd}\]

commutes.

******* Equivalence for units and counits
Given the first property, the condition on hom-sets is equivalent
to $L\eta = \eta'L$ and $\varepsilon'K = K\varepsilon$.

******** TODO Proof

****** TODO Conjugate natural transformations

***** IV.9. Subsets and characteristic functions
****** Subobject classifier
A *subobject classifier* for ${\cal C}$ with a terminal object $1$ is a monomorphism
$t \colon 1 \to \Omega$ such that for every monomorphism $m$ exists a unique square

\[\begin{tikzcd}
S\rar{} \dar[swap,hook]{m} & 1 \dar[hook]{t} \\
X\rar[dashed]{\Psi_{S}} & \Omega &,
\end{tikzcd}\]

which is at the same time a pullback.

***** IV.10. Categories like sets
****** Elementary topos
An *elementary topos* is a category $E$ which

 1) has all finite limits,
 2) has a subobject classifier,
 3) is cartesian closed.

****** Presheaves
A *presheaf* is a set-valued contravariant functor on a small category.

**** V. Limits
***** V.1. Creation of limits
****** V.1.0. Small-complete category
A category is *small-complete* when all small diagrams have limits in it.

****** V.1.1. Completeness of Set
$\mathsf{Set}$ has all small limits. Given $J$ small, $F \colon J \to \mathtt{Set}$ has limit $\mathrm{Cone}(\ast,F)$,
the set of all cones $\sigma\colon \ast \to F$; with the functions $v_j\colon \sigma \mapsto \sigma_j(\ast)$.

******* Proof
As $J$ is small, $\mathrm{Cone}(\ast,F)$ is an object of $\mathsf{Set}$. We prove that it is a
cone; for any given $u \colon j \to k$ in $J$,

\[
(Fu)v_j(\sigma) = (Fu)\sigma_j = \sigma_k = v_k(\sigma).
\]

And we prove that it is universal. Given any cone $\tau \colon X \todot F$, for any
$x \in X$ we have a $\mathrm{Cone}(\ast,F)$, so there is a unique function
$h \colon X \to \mathrm{Cone}(\ast,F)$.

******* Adjunction
This can be written as the following adjunction

\[
\Nat(\Delta X,F) \cong \hom(X, \mathrm{Cone}(\ast,F)).
\]

****** TODO V.1.1. Example: p-adic numbers
****** V.1.2. Creation of limits in Grp
If a small $H \colon J \to \Grp$ can be forgot into $\Set$ with a limiting cone $v \colon L \todot UH$,
there is exactly one group structure on $L$ making each $v_i$ a homomorphism;
$L$ is a limit of $H$ with this structure.

******* Proof
We take $L = \mathrm{Cone}(\ast,UH)$ and define a group structure with $(\sigma\tau)_j = \sigma_j\tau_j$
and $(\sigma^{-1})_j = (\sigma_j)^{-1}$; making each $v$ a homomorphism. These are cones because
the functions $UHj$ are homomorphisms. Any other structure making each $v$ a
homomorphism should in particular satisfy these equations; so this structure
is unique.

Given any other group cone $\lambda \colon G \todot H$, then $U\lambda \colon UG \todot UH$ is a set cone
and there exists a unique $h \colon UG \to L$ making the diagram commute; this is
a homomorphism of groups

\[
(h(g_1g_2))_j = \lambda_j(g_1g_2) = \lambda_j(g_1)\lambda_j(g_2) = (hg_1)_j(hg_2)_j.
\]

****** V.1.2. Creation of limits
The functor $V\colon A \to X$ *creates limits* for $F\colon J \to A$ when

 1) for any con $\tau\colon x \xrightarrow{.} VF$ exists $\sigma\colon a \to F$ with $Va = x$ and $V\sigma = \tau$.
 2) this $\sigma$ is a limiting cone.

****** V.1.3. The forgetful Groups to Sets functor creates limits
The forgetful functor $U \colon \Grp\to \Set$ creates limits.

******* Proof
Rereading of the [[*V.1.2. Creation of limits in Grp][previous theorem]].

***** V.2. Limits by products and equalizers
****** V.2.1. Existence of limits from products and equalizers
If ${\cal C}$ has all equalizers and all products indexed by the sets of objects
and arrows of $J$, it has every limit $F\colon J \to {\cal C}$.

******* TODO Proof

****** TODO V.2.2. Limits from products and equalizers
****** TODO V.2.2. Finite limits from products and equalizers
If a category ${\cal C}$ has all finite products and all equalizers, it
has all finite limits.

***** TODO V.3. Limits with parameters
***** V.4. Preservation of limits
****** V.4.0. Preservation of limits
A functor $H\colon C \to D$ *preserves limits* $J \to C$ when every limiting
cone $v$ in $C$ yields a limiting cone $Hv$ in $D$.

****** V.4.0. Continuous functor
A functor is *continuous* if it preserves all small limits.

****** V.4.1. Hom-functors preserve all limits
In ${\cal C}$ with small hom-functors, each $\hom(c,-)$ preserves all limits.

******* TODO Proof

***** TODO V.5. Adjoints on limits
****** V.5.1. Right adjointa preserve all limits
If $G \colon A \to X$ has a left adjoint and the diagram $T\colon J \to A$ has a
limiting cone $\tau \colon a\todot T$, then $GT$ has the limiting cone $G\tau \colon Ga \todot GT$
in $X$.

******* Proof
As $G$ is a functor, $G\tau$ is already a cone. Given any other cone $\sigma \colon x \todot GT$,
we can take adjoints on every component and, by naturality of the adjoint,
we have a cone $\sigma_{\ast} \colon Fx \todot T$. This cone uniquely factorizes throught $\tau$ with
$h \colon Fx \to a$, naturality preserves this unique factorization in $h_{\ast} \colon x \to Ga$,

\begin{prooftree}
\AXC{\begin{tikzcd}[fragile,ampersand replacement=\&] 
x \rar{h_\ast}\& Ga \rar{G\tau} \& Gi  \end{tikzcd}}
\doubleLine
\UIC{\begin{tikzcd}[fragile,ampersand replacement=\&]
Fx \rar{h}\& a \rar{\tau} \& i \end{tikzcd}}
\end{prooftree}

so $G\tau$ is universal.

***** V.6. Freyd's adjoint functor theorem
****** V.6.1. Existence of an initial object
In ${\cal D}$ small-complete with small hom-sets, there exists an initial object
if and only if it satisfies

 - solution set condition :: there exists a small family $\left\{ k_i \right\}_{i \in I}$ such that
      for each $d \in D$ exists an arrow $k_i \to d$.

******* Proof
If it has an initial object, it trivially satisfies the condition with that
object.

We take $w = \prod k_i$ and we have at least one arrow $w \to d$ for each $d$, given
by projections. We can construct the equalizer of all $\hom(w,w)$, which is
small

\begin{tikzcd}
v \rar{e} & w \rar[bend left] \rar[bend right] & w
\end{tikzcd}

For each $d$ there exists at least one $v \to d$, if there were two $f,g \colon v \to d$,
we take the equalizer of both, $u$, and

\[\begin{tikzcd}
u \rar{e_1} & v \dlar[swap]{e}\dar{e}\rar[bend left]{f}\rar[bend right]{g} & d \\
w \uar[dashed]{s}\rar{ee_1s} & w \rar & k_i \uar
\end{tikzcd}\]

we know that a $s \colon w \to u$ must exist; as $ee_1se = e$ and $e$ is an equalizer
(and thus, a monomorphism), $e_1se =\id$. Then, $e_1$ has a right inverse and
it is also a monomorphism, so it has to be an isomorphism, $f = g$.

****** V.6.2. Lemma to Freyd I: creation of products by adjoints
Given $G \colon A \to X$ preserving all small products, the projection
$Q \colon (x \downarrow G) \to A$, $(x \to Ga) \mapsto a$ from the comma category creates
all small products.

******* Proof
Given $\left\{ f_j\colon x \to Ga_j \right\}$ an indexed family of objects in $(x \downarrow G)$ such
that $p_k \colon \prod a_j \to a_k$ exists in $A$; we know that $Gp_k \colon G\prod a_j \to Ga_j$
is a product, so there is a unique $f \colon x \to G\prod a_j$ with $(Gp_j)f=f_j$,

\[\begin{tikzcd}
\Pi a_j \dar{p_j} & & G\Pi a_j \dar{Gp_j} \\
a_j & x \rar[swap]{f_j}\urar[dashed]{f} & Ga_j \\
\end{tikzcd}\]

We can now verify that these $p_j$ create a product in the comma category.

****** TODO V.6.2. Lemma to Freyd II: creation of equalizers by adjoints
****** V.6.2. The Freyd adjoint functor theorem
A functor $G \colon A \to X$ from a locally small category has
left-adjoint iff it preserves all small limits and satisfies the

 - solution set condition :: for each object $x \in X$ there is a small set $I$
      and a family of arrows $\left\{ f_i \colon x \to Ga \right\}_{i \in I}$ such that every $h \colon x \to Ga$
      can be written as $h = Gt \circ f_i$ for some $i$ with some $t \colon a_i \to a$.

******* Proof
******** First implication
If $G$ has a left-adjoint, it preserves all limits; the unit 
$\eta \colon x \to GFx$ satisfies the solution set condition.

******** Second implication
By [[*IV.1.2. Characterization of adjunctions][characterization of adjoints]], it suffices to create a universal
arrow $x \to Ga$ for each $x$; that is, a initial object in the comma
category $(x \downarrow G)$; the solution set condition gives us the conditions
of [[*V.6.1. Existence of an initial object][existence of an initial object]]. We only need to prove that $(x \downarrow G)$
is small-complete; but $G$ preserving small products makes the projection
create all [[*V.6.2. Lemma to Freyd I: creation of products by adjoints][products]] and create all [[*V.6.2. Lemma to Freyd II: creation of equalizers by adjoints][equalizers]].

****** V.6.3. The representability theorem

***** TODO V.7. Subobjects and generators
***** TODO V.8. The special adjoint functor theorem
***** TODO V.9. Adjoints in topology
**** VI. Monads and algebras
***** VI.1. Monads in a category
****** Monad
A *monad* is a functor $T\colon X \to X$ with natural transformations

 * $\eta\colon I \xrightarrow{.} T$, called /unit/
 * $\mu \colon T^2 \xrightarrow{.} T$, called /multiplication/

such that

\[\begin{tikzcd}
T^3 \rar{T\mu}\dar{\mu T} & T^2\dar{\mu} \\
T^2 \rar{\mu} & T
\end{tikzcd}
\qquad
\begin{tikzcd}
IT \rar{\eta T}\drar[swap]{\cong} & T^2\dar{\mu} & \lar[swap]{T\eta}\dlar{\cong} TI \\
& T & &.
\end{tikzcd}\]

****** Each adjunction defines a monad
Given $F \dashv G$, $GF$ is a monad.

******* Proof
We take the unit of the adjunction as the monad unit. We define the
product as $\mu = G\varepsilon F$. Associativity follows from these diagrams

\[\begin{tikzcd}
FGFG\rar{FG\varepsilon} \dar[swap]{\varepsilon FG} & FG \dar{\varepsilon} \\
FG\rar{\varepsilon} & I
\end{tikzcd}
\qquad
\begin{tikzcd}
GFGFGF\rar{GFG\varepsilonF} \dar[swap]{G\varepsilon FGF} & GFGF \dar{G\varepsilonF} \\
GFGF\rar{G\varepsilonF} & GF &,
\end{tikzcd}\]

where the first is commutative by the [[*Interchange law][interchange law]] and the second
is obtained by applying functors $G$ and $F$. Unit laws follow from
the [[*Unit and counit][triangular identities]] after applying $F$ and $G$.

****** Comonad
A *comonad* is a functor $L\colon X \to X$ with natural transformations

 * $\varepsilon\colon L\to I$, called /counit/
 * $\delta\colon L \to L^2$, called /comultiplication/

such that

\[\begin{tikzcd}
L\rar{\delta} \dar[swap]{\delta} & L^{2} \dar{L\delta} \\
L^{2}\rar{\delta L} & L^{3}
\end{tikzcd}
\qquad
\begin{tikzcd}
& L \dar{\delta} \dlar[swap]{\cong} \drar{\cong} & \\
IL & 
L^2 \lar{\varepsilon L}\rar[swap]{L \varepsilon} & 
LI
&.
\end{tikzcd}\]

****** TODO Each adjunction defines a comonad
***** VI.2. Algebras for a monad
****** T-algebra
For a monad $T$, a $T\text{-algebra}$ is an object $x$ with an arrow $h \colon Tx \to x$ called 
/structure map/ making these diagrams commute

\[\begin{tikzcd}
T^{2}x \rar{Th}\dar[swap]{\mu} & Tx \dar{h} \\
Tx\rar{h} & x &.
\end{tikzcd}\]

****** Morphisms of T-algebras
A morphism of T-algebras is an arrow $f\colon x \to x'$ making the following square
commute

\[\begin{tikzcd}
Tx \dar[swap]{Tf}\rar{h} & Tx \dar{f} \\
Tx' \rar[swap]{h'} & Tx' &.
\end{tikzcd}\]

****** Category of T-algebras
The set of all $T\text{-algebras}$ and their morphisms form a category $X^{T}$.

******* Proof
Given $f\colon x \to x'$ and $g\colon x'\to x''$, $T\text{-algebra}$ morphisms, their composition
is also a $T\text{-algebra}$ morphism, due to the fact that this diagram

\[\begin{tikzcd}
Tx \rar{h}\dar[swap]{Tf} & 
x \dar{f}\\
Tx' \dar[swap]{Tg} \rar{h'} &
x' \dar{g}\\
Tx'' \rar{h''}&
x''
\end{tikzcd}\]

commutes.

****** Every monad is defined by its T-algebras
The monad defined by the adjunction $F^{T} \dashv G^{T}$ is $T$, where the functors
are defined as

\[F^{T}\colon \begin{tikzcd}
x \dar{f} \\
x'
\end{tikzcd} \mapsto \begin{tikzcd}
(Tx,\mu) \dar{Tf} \\
(Tx',\mu)
\end{tikzcd}
\qquad
G^{T}\colon  \begin{tikzcd}
(x,h) \dar{f} \\
(x',h')
\end{tikzcd} \mapsto
\begin{tikzcd}
x \dar{Tf} \\
x' &
\end{tikzcd}\]

knowing that $(Tx,\mu)$ is always a $T\text{-algebra}$.

******* TODO Proof

***** VI.3. The comparison with algebras
****** Comparison of adjunctions with algebras
Given $F \dashv G$ with the monad $T$, there is a unique $K\colon A \to X^T$ with
$G^TK = G$ and $KF = F^T$.

******* TODO Proof

***** TODO VI.4. Words and free semigroups
***** TODO VI.5. Free algebras for a monad
**** VII. Monoids
***** VII.1. Monoidal categories
****** VII.1.0. Strict monoidal category
A *strict monoidal category* ${\cal B}$ is a category with a bifunctor $\otimes \colon {\cal B} \times {\cal B} \to {\cal B}$
which is associative

\[
\otimes(\otimes \times 1) = \otimes (1 \times \otimes) \colon {\cal B} \times {\cal B}\times {\cal B} \to {\cal B}
\]

and with a *unit* object,

\[
\otimes(e \times \Id) = \Id_{{\cal B}} =\otimes(\Id \times e).
\]

****** VII.1.0. Monoidal category
A *monoidal category* ${\cal B}$ is a category with a bifunctor $\otimes \colon {\cal B} \times {\cal B} \to {\cal B}$
and three natural isomorphisms

 * $\alpha \colon a \otimes (b \otimes c) \cong (a \otimes b) \otimes c$

 * $\lambda \colon e \otimes a \cong a$

 * $\rho \colon a \otimes e \cong a$

such that the pentagonal diagram commutes

\begin{tikzcd}
a \otimes (b \otimes (c \otimes d)) \dar{1 \otimes \alpha} \rar{\alpha}& 
(a \otimes b) \otimes (c \otimes d) \rar{\alpha}& 
((a \otimes b) \otimes c) \otimes d \dar{\alpha \otimes 1}\\
a \otimes ((b \otimes c) \otimes d) \ar[rr, "\alpha"] & & 
(a \otimes (b \otimes c)) \otimes d
\end{tikzcd}

and the following triangular diagram commutes

\[\begin{tikzcd}[row sep=tiny]
& a \otimes (e \otimes c) \ar[dd, "\alpha"]\dlar[swap]{1 \otimes \lambda}\\ 
a \otimes c &\\
& (a \otimes e) \otimes \ular{\rho \otimes 1}
\end{tikzcd}\]

**** IX. Special limits
***** IX.1. Filtered limits
****** Directed preorder
A preorder $P$ is *directed* when any two elements have an upper bound.

****** Filtered category
A category ${\cal C}$ is *filtered* when

 1) for any $a,b \in {\cal C}$ there exists a $c$ with arrows $a \to c$, $b \to c$.
 2) for any $u,v \colon a \to b$ exists a $w\colon b \to c$ such that $wu=wv$.

In a filtered category, every finite diagram is base of at least one cone.

**** Exercises [35/52]
***** I. Categories, functors and natural transformations [19/23]
****** I.3. Functors
******* CHECK Exercise I.3.1
#+begin_statement
Show how each of the following constructions can be regarded as a functor:

 - the field of quotients of an integral domain.
 - the Lie algebra of a Lie group.
#+end_statement

******** Field of quotients
Given two integral domains and a ring homomorphism $f : R \to S$, we
define an homomorphism between its fields of quotients as:

\[
\widetilde{f}\left(\frac{a}{b}\right)
=
\frac{f(a)}{f(b)}
\]

We can prove it is well-defined using that $ab=cd$ implies
$f(a)f(b) = f(c)f(d)$, and then:

\[
\widetilde{f}\left(\frac{a}{b}\right)
=
\frac{f(a)}{f(b)}
=
\frac{f(c)}{f(d)}
=
\widetilde{f}\left(\frac{c}{d}\right)
\]

And it respects sums and products:

\[
\widetilde{f}\left(\frac{a}{b} + \frac{c}{d}\right)
=
\widetilde{f}\left(\frac{ad+cb}{bd}\right)
=
\frac{f(a)f(d)+f(c)f(d)}{f(b)f(d)}
=
\frac{f(a)}{f(b)}+\frac{f(c)}{f(d)}
\]

\[
\widetilde{f}\left(\frac{ac}{bd}\right) =
\frac{f(a)f(c)}{f(b)f(d)}
\]

So it is a field homomorphism.

******** TODO Lie algebra
Given $\phi : G \to H$, a Lie group homomorphism, we can compute its first
derivative at the identity $\phi^*$.
******* DONE Exercise I.3.2
#+begin_statement
Show that functors $1 \to C$, $2 \to C$, and $3 \to C$ correspond respectively to
objects, arrows, and composable pairs of arrows in $C$.
#+end_statement

A functor $F\colon 1 \to C$ is determined by $F1$. A functor $F\colon 2 \to C$ is determined
by $F(1\leq 2)\colon F1 \to F2$. A functor $F\colon 3 \to C$ is determined by $F(1\leq 2)$ and
$F(2\leq 3)$, which must be composable in $F2$.

******* DONE Exercise I.3.3
#+begin_statement
Interpret "functor" in the following special types of categories:

  1. A functor between two preorders is a function $T$ which is monotonic
     (i.e. $p \leq p'$ implies $Tp \leq Tp'$).
  2. A functor between two groups (one-object categories) is a morphism
     of groups.
  3. If $G$ is a group, a functor $G \to \mathtt{Set}$ is a permutation representation
     of $G$, while $G \to Matr_K$ is a matrix representation of $G$.
#+end_statement

******** First statement
A functor must be a monotonic function, as it has to send $(p\leq p')$ into
a morphism between $Tp$ and $Tp'$. This morphism exists if and only if 
$Tp \leq Tp'$.

******** Second statement
It respects the identity and the group operation, as functors respect
the identity and the composition.

******** Third statement
A functor $F\colon G \to \mathtt{Set}$ is determined by $FG$ and the assignment of every
element of $G$ to a set automorphism, that is, an element of the permutation
group of the set.

The functor $F\colon G \to \mathtt{Matr}_K$ selects a dimension $n$, and sends every element
of the group to an invertible matrix $M_{n\times n}$.

******* DONE Exercise I.3.4
#+begin_statement
Prove that there is no functor $\mathtt{Grp} \to \mathtt{Ab}$ sending each group $G$ to its
center. (Consider $S_2 \to S_3 \to S_2$, the symmetric groups).
#+end_statement

A functor must preserve identities and composition. We have the following
diagram in $\mathtt{Grp}$,

\[\begin{tikzcd}
S_2 \rar[hook]\arrow[rr,bend left, "id"] & S_3 \rar & S_2
\end{tikzcd}\]

that cannot be translated into $\mathtt{Ab}$ by this functor

\[\begin{tikzcd}
S_2 \rar & \{id\} \rar &  S_2
\end{tikzcd}\]

as we know that the identity is not the zero morphism.

******* DONE Exercise I.3.5
#+begin_statement
Find two different functors $T : \mathtt{Grp} \to \mathtt{Grp}$ with object function $T(G) = G$
the identity for every group $G$.
#+end_statement

The identity functor and a functor sending every morphism to the zero 
morphism.

****** I.4. Natural transformations
******* DONE Exercise I.4.1
#+begin_statement
Let $S$ be a fixed set, and $X^S$ the set of all functions $h : S \longrightarrow X$.
Show that $X \mapsto X^S$ is the object function of a functor $\mathtt{Set} \longrightarrow \mathtt{Set}$,
and that evaluation $e_X : X^S \times S \longrightarrow X$ defined by $e(h,s) = h(s)$, the
value of the function $h$ at $s \in S$ is a natural transformation.
#+end_statement

We define the functor $\_^S$ on arrows as follows. Given a $f : X \to Y$
and a $g : S \longrightarrow X$:

\[
f^S(g) = f \circ g
\]

And it follows the functor laws.

We can see that evaluation is a natural transformation with the
naturality square:

\[\begin{tikzcd}
X^S \times S \dar{e_X}\rar{f^S,id} & Y^S \times S \dar{e_Y}\\
X \rar{f} & Y
\end{tikzcd}\]

Which commutes on its elements:

\[\begin{tikzcd}
(f,s) \dar{e_X}\rar{g^S,id} & (g\circ f, s) \dar{e_Y}\\
f(s) \rar{f} & g(f(s))
\end{tikzcd}\]

******* DONE Exercise I.4.2
#+begin_statement
If $H$ is a fixed group, show that $G \mapsto H \times G$ defines a functor
$H \times - \colon \mathtt{Grp}\to \mathtt{Grp}$ and that each morphism $f \colon H \to K$ of groups defines
a natural transformation $H \times -\; \dot\to \; K \times -$.
#+end_statement

The functor will send a morphism $f \colon G \to G'$ to $\mathrm{id}\times f\colon H\times G \to H \times G'$.
The naturality condition is satisfied if the following diagram commutes

\[\begin{tikzcd}
H \times G \dar{\mathrm{id}\times f} \rar{g\times\mathrm{id}} & 
K \times G \dar{\mathrm{id}\times f} \\
H \times G' \rar{g\times\mathrm{id}} & 
K \times G'
\end{tikzcd}\]

but it is trivial to check commutativity.

******* DONE Exercise I.4.3
#+begin_statement
If $B$ and $C$ are groups (regarded as categories with one object each) and
$S,T \colon B \to C$ are functors (homomorphisms of groups), show that there is a
natural transformation $S \dot\to T$ if and only if $S$ and $T$ are conjugate; i.e.
if and only if there is an element $h \in C$ with $Tg = h(Sg)h^{-1}$ for all
$g \in B$.
#+end_statement

If the only object in $B$ is $b$, then $Sb = Tb = c$ must be the only object
in $c$. Naturality gives us

\[\begin{tikzcd}
c\rar{\varphi} \dar[swap]{Sf} & c \dar{Tf} \\
c\rar{\varphi} & c
\end{tikzcd}\]

so we know that $Sf \circ \varphi = \varphi \circ Tf$, and this is the conjugate condition.

******* DONE Exercise I.4.4
#+begin_statement
For functors $S,T \colon C \to P$ where $C$ is a category and $P$ a preorder, show
that there is a natural transformation $S \dot\to T$ (which is then unique) if
and only if $Sc \leq Tc$ for every object $c \in C$.
#+end_statement

To define a natural transformation, we must have a familiy of morphisms

\[
Sc \to Tc \quad\forall c \in C,
\]

but this morphism exists if and only if $Sc \leq Tc$ for every object. If the
morphisms exist, the naturality condition is trivial, as there will be
an unique morphism between two objects and all squares will commute.

******* DONE Exercise I.4.5
#+begin_statement
Show that every natural transformation $\tau\colon S \dot\to T$ defines a function
(also called $\tau$) which sends each arrow $f\colon c \to c'$ of $C$ to an arrow
$\tau f\colon Sc \to Tc'$ of $B$ in such a way that $Tg \circ \tau f = \tau(gf) = \tau g\circ Sf$ for
each composable pair $(g,f)$. Conversely, show that every such function $\tau$ 
comes from a unique natural transformation with $\tau_c = \tau(1_c)$. (This gives 
an arrows only description of a natural transformation.)
#+end_statement

Given $f\colon c \to c'$, we apply the naturality condition and take $\tau$ to be the
diagonal

\[\begin{tikzcd}
Sc \drar{\tau f} \rar{\tau} \dar[swap]{Sf} & Tc \dar{Tf} \\
Sc'\rar{\tau} & Tc'
\end{tikzcd}\]

i.e. we have defined $\tau f = Tf\circ \tau_c = \tau_{c'} \circ Sf$. The condition holds now 
trivially, as we know that

\[
Tg \circ \tau f = Tg\circ Tf\circ \tau_c = \tau(gf) = \tau_{c'}\circ Sg\circ Sf
= \tau g \circ Sf.
\]

******* TODO Exercise I.4.6
#+begin_statement
Let $F$ be a field. Show that te category of all finite-dimensional vector
spaces over $F$ (with morphisms all lineal transformations) is equivalent
to the category $\mathtt{Matr}$.
#+end_statement
****** I.5. Monics, epis and zeros
******* DONE Exercise I.5.1
#+begin_statement
Find a category with an arrow which is both epi and monic, but not 
invertible (e.g., dense subset of a topological space).
#+end_statement

In the $\mathtt{Top}$ category of topological spaces with continuous functions,
we can include a dense subset in its base space. This inclusion will
be a monomorphism (as it is injective) and an epimorphism as we know
that, if $i \colon U \subset V$ is our inclusion,

\[ f\circ i = i \circ g \implies f|_{U} = g|_{U},\]

and because it is a dense subset, by continuity, $f = g$.

But it has not to be an isomorphism. In fact, it won't be if $U$ is a
proper subset.

******* DONE Exercise I.5.2
#+begin_statement
Prove that the composite of monics is monic, and likewise for epis.
#+end_statement

If $f,g$ are monics, we can apply the definition twice to get

\[
f \circ g \circ a = f \circ g \circ b \implies
g \circ a = g\circ b \implies
a = b.
\]

The same proof can be applied in reverse.

******* DONE Exercise I.5.3
#+begin_statement
If a composite $g\circ f$ is monic, so is $f$. Is this true of $g$?
#+end_statement

No, $f$ could be a zero morphism and $g$ could still give $g\circ h = g\circ h'$ for
two $h \neq h'$.

******* DONE Exercise I.5.4
#+begin_statement
Show that the inclusion $\mathbb{Z} \to \mathbb{Q}$ is epi in the category $\mathtt{Rng}$.
#+end_statement

If $f \circ i = g \circ i$, then $f|_{\mathbb{Z}} = g|_{\mathbb{Z}$, and we can extend the morphisms uniquely
to the ring $\mathbb{Q}$, as the ring morphisms have to preserve inverses.

******* TODO Exercise I.5.5
#+begin_statement
In $\mathtt{Grp}$ prove that every epi is surjective (Hint. If $\varphi\colon G\to H$ has image
$M$ not $H$, use the factor group $H/M$ if $M$ has index 2. Otherwise, let
$\mathrm{Perm}\ H$ be the group of all permutations of the set $H$, choose three
different cosets $M,Mu$ and $Mv$ of $M$, define $\sigma \in \mathrm{Perm}\ H$ by
$\sigma(xu) = xv$, $\sigma(xv) = xu$ for $x \in M$, and $\sigma$ otherwise the identity.
Let $\psi\colon H \to \mathrm{Perm}\ H$ send each $h$ to left multiplication $\psi_h$ by $h$, while
$\psi'_h = \sigma^{-1}\psi_h\sigma$. Then $\psi\varphi = \psi'\varphi$, but $\psi \neq \psi'$).
#+end_statement

******* DONE Exercise I.5.6
#+begin_statement
In $\mathtt{Set}$, show that all idempotents split.
#+end_statement

Given $f \colon A \to A$ idempotent, we can define the set $\mathrm{img}\ f$, and two functions
$g \colon A \to \mathrm{img}\ f$, $h\colon \mathrm{img}\ f \to A$ defined naturally and satisfiying the conditions.
Notice that $g$ is an epimorphism and $h$ a monomorphism.

******* DONE Exercise I.5.7
#+begin_statement
An arrow $f \colon a \to b$ in a category $C$ is /regular/ when there exists an arrow
$g\colon b \to a$ such that $fgf = f$. Show that $f$ is regular if it has either a left or
a right inverse, and prove that every arrow in $\mathtt{Set}$ with $a \neq \varnothing$ is regular.
#+end_statement

If $f$ has either a right or a left inverse, it is trivial that it is regular.

If $a \neq \varnothing$, we can take $x_y \in \left\{ x \in a \mid f(x) = y \right\}$, a representative of the class
of elements that are mapped onto $y$, and $u \in a$ an arbitrary element, and define

\[
g(y) = \left\{\begin{array}{ll} 
x_y & \mbox{if } y \in \mathrm{img}(f)  \\
u & \mbox{otherwise }
\end{array} 
\right.
\]

and this morphism makes the $f$ regular. (Have we used the choice axiom to define
the representatives?)

******* DONE Exercise I.5.8
#+begin_statement
Consider the category with objects $\left\langle X,e,t \right\rangle$, where $X$ is a set, $e \in X$, and
$t \colon X \to X$, and with arrows $f\colon \left\langle X,e,t \right\rangle \to \left\langle X',e',t' \right\rangle$ the functions $f$ on $X$ to $X'$
with $fe=e'$ and $ft = tf'$. Prove that this category has an initial object in
which $X$ is the set of natural numbers, $e=0$, and $t$ is the successor function.
#+end_statement

We will prove that $\left\langle \mathbb{N},0,S \right\rangle$ is the initial object. For every object $\left\langle X,e,t \right\rangle$,
we only can define a function $f$, its image on zero is determined by the
first arrow condition $f(0) = e$, and its image in every other natural is
determined as

\[
f(n) = f(S \circ \overset{n}\dots \circ S(0)) = t\circ \overset{n}\dots \circ t(f(0)).
\]

******* DONE Exercise I.5.9
#+begin_statement
If the functor $T \colon C \to B$ is faithful and $Tf$ is monic, prove $f$ monic.
#+end_statement

If $f\circ g = f \circ h$, we have $Tf\circ Tg = Tf\circ Th$. $Tf$ is monic and $T$ is faithful, so
we get $g = h$.

****** I.6. Foundations
******* CHECK Exercise I.6.1
#+begin_statement
Given a universe $U$ and a function $f \colon I \to b$ with domain $I \in U$ and
with every value $f_i$ an element of $U$, for $i \in I$, prove that the usual
cartesian product $\prod_i f_i$ is an element of $U$.
#+end_statement

An element of the cartesian product will be a function $g \colon I \to \bigcup f_i$,
where $g(i) \in f_i$ for every $i \in I$. Our cartesian product is defined as

\[
\left\{ x \in {\cal P}\left( I \times \bigcup f_i \right) \;\middle|\; 
\left( \forall (i,b) \in x \colon b \in f_i \right) \wedge
\left( \forall j \in I\colon \exists! (i,b) \in x\colon j = i \right)
\right\}.
\]

Notice that the powerset and the union of elements in $U$ are elements
in $U$.

******* TODO Exercise I.6.2
#+begin_statement
(a) Given a universe $U$ and a function $f \colon I \to b$ with domain $I \in U$, show
that the usual union $\bigcup_i f_i$ is a set of $U$.

(b) Show that this one closue property of $U$ may replace condition (v) and the
condition $x \in U$ implies $\bigcup x \in U$ in the definition of a universe.
#+end_statement
***** II. Constructions on categories [6/11]
****** II.3. Products of categories
******* DONE Exercise II.3.1
#+begin_statement
Show that the product of categories includes the following known special
cases: The product of monoids (categories with one object), of groups,
of sets (discrete categories).
#+end_statement

The product of two monoids or groups has only one object, and so it is
a monoid or group. Every pair of isomorphisms has an inverse given by
the pair of inverses, and because of that, we know that the product of
two groups is in fact a group.

The product of two sets has no nontrivial morphisms and only pairs of
objects as objects.

******* DONE Exercise II.3.2
#+begin_statement
Show that the product of two preorders is a preorder.
#+end_statement

The product of two preorders gives the product preorder, where $(a,b) \leq (a',b')$
if and only if $a \leq a'$ and $b\leq b'$. That pair of morphisms is witness of the 
order.

******* DONE Exercise II.3.3
#+begin_statement
If $\left\{ C_i \mid i \in I \right\}$ is a family of categories indexed by a set $I$, describe the
product $C= \prod_iC_i$, its projections $P_i\colon C \to C_i$, and establish the universal
property of these projections.
#+end_statement

An object in this category is a function assigning $i \mapsto c_i \in C_i$. A morphism
between it and $i \mapsto d_i \in C_i$ is a function assigning $i \mapsto (f_i \colon c_i \to d_i)$ and
componentwise composition. Its projections are only the functors given by
evaluation on any element in $I$.

Given any other family of functors $D \to C_i$, we can define a functor to $C$
componentwise, being the unique functor making a product diagram commute.

******* TODO Exercise II.3.4
#+begin_statement
Describe the opposite of the category $\mathtt{Matr}_K$.
#+end_statement

******* DONE Exercise II.3.5
#+begin_statement
Show that the ring of continuous real-valued functions on a topological
space is the object function of a contravariant functor on $\mathtt{Top}$ to $\mathtt{Rng}$.
#+end_statement

We are going to define the functor $T \colon \mathtt{Top}\to \mathtt{Rng}$ taking $Tf$ to be
$(\circ f) \colon \mathrm{hom}_{\mathtt{Top}}(Y,\mathbb{R}) \to \mathrm{hom}_{\mathtt{Top}}(X,\mathbb{R})$ for any given $f \colon X \to Y$. It is 
trivially a functor, as it preserves composition.

We know that the composition of two continuous functions is
continuous, and we have to prove that $(\circ f)$ is a ring homomorphism,
but it is trivial that

\[\begin{aligned}
(g\cdot h) \circ f &= (g \circ f) \cdot (h \circ f) \\
(g + h) \circ f &= (g \circ f) + (h \circ f). \\
\end{aligned}
\]

****** II.4. Functor categories
******* TODO Exercise II.4.1
#+begin_statement
For $R$ ring, describe $R\text{-Mod}$ as a full subcategory of the functor category
$\mathtt{Ab}^R$.
#+end_statement

******* TODO Exercise II.4.2
#+begin_statement
Describe $B^X$, for $X$ a finite set (a finite discrete category).
#+end_statement

******* TODO Exercise II.4.3
#+begin_statement
Let $\mathbb{N}$ be the discrete category of natural numbers. Describe the functor
category $\mathtt{Ab}^{\mathbb{N}}$ (commonly known as the category of graded abelian groups). 
#+end_statement

****** II.5. The category of all categories
******* TODO Exercise II.5.1
#+begin_statement
For small categories $A$, $B$ and $C$ establish a bijection

\[ \mathrm{hom}(A \times B, C) \cong \mathrm{hom}(A, C^B),
\]

and show it natural in $A$, $B$ and $C$. Hence show that $-\times B \colon \mathtt{Cat}\to \mathtt{Cat}$ has
a right adjoint.
#+end_statement

****** II.6. Comma categories
******* DONE Exercise II.6.1
#+begin_statement
If $K$ is a commutative ring, show that the comma category $(K \downarrow \mathtt{CRng})$ is
the usual category of all small commutative $K\text{-algebras}$.
#+end_statement

A $K\text{-algebra}$ can be defined as an inclusion from $K$ on a ring, morphisms
of algebras must preserve this inclusion.

# A more detailed proof would be interesting.

******* DONE Exercise II.6.2
#+begin_statement
If $t$ is a terminal object in $C$, prove that $(C \downarrow t)$ is isomorphic to $C$.
#+end_statement

By definition of terminal object, there will be only an arrow 
$\ast \colon u \to t$ for any $u \in C$. Every morphism will create a commutative 
diagram because of the unicity of the morphisms.

***** III. Universals and limits [9/16]
****** III.1. Universal arrows
******* DONE Exercise III.1.1
#+begin_statement
Show how each of the following familiar constructions can be interpreted
as a universal arrow:

 * The integral group ring of a group (better, of a monoid).
 * The tensor algebra of a vector space.
 * The exterior algebra of a vector space.
#+end_statement

******** The integral group ring (monoid)
The integral group ring $\mathbb{Z}G$ is defined as the initial object of $(G \downarrow {\cal U})$,
where ${\cal U}$ is the functor that takes a ring and returns its group of units.

******** The tensor algebra of a vector space
The tensor algebra $T(V)$ is defined as the initial object of $(V \downarrow U)$,
where $U\colon \mathtt{Alg}_k \to \mathtt{Vect}_k$ is the forgetful functor.

******** The exterior algebra of a vector space
The same construction as above can be performed on the full subcategory of
external algebras.

******* DONE Exercise III.1.2
#+begin_statement
Find a universal element for the contravariant power set functor
${\cal P} \colon \mathtt{Set}^{op} \to \mathtt{Set}$.
#+end_statement

The universal arrow for $A$ will be its inclusion in ${\cal P}{\cal P}A$ as

\[
i(a) = \left\{ U \in {\cal P}A \mid a \in U \right\}.
\]

Given a morphism $f \colon A \to {\cal P}B$, we define $g \colon B \to {\cal P}A$ as

\[
g(b) = \left\{ a \in A \mid b \in f(a) \right\}.
\]

And its image when the functor is applied is

\[
{\cal P}g(\mathbb{A}) = g^{-1}(\mathbb{A})
= \left\big\{ b \in B \mid \left\{ a \mid b \in f(a) \right\} \in \mathbb{A} \right\big\}.
\]

The $g$ is unique, as it is defined by $x \in f(a) \iff a \in g(x)$.

******* DONE Exercise III.1.3
#+begin_statement
Find (from any given object) universal arrows to the following forgetful
functors: $\mathtt{Ab} \to \mathtt{Grp}$, $\mathtt{Rng}\to \mathtt{Ab}$ (forget the multiplication), $\mathtt{Top} \to \mathtt{Set}$,
$\mathtt{Set}_{\ast} \to \mathtt{Set}$.
#+end_statement

******** Abelian groups to groups
The universal arrow defines the [[https://en.wikipedia.org/wiki/Commutator_subgroup][abelianization]] of the group. A morphism
from a group to an abelian group must have the commutator subgroup in 
its kernel

\[f(aba^{-1}b^{-1}) = f(a)f(b)f(b)^{-1}f(a)^{-1} = 1.\]

Thus, every morphism can be factorized in $G/[G,G]$, giving

\[\begin{tikzcd}
g \rar[hook]{i}\drar[swap]{f} & G/[G,G]\dar[dashed]{\widetilde f} \\
  & h & .
\end{tikzcd}\]

******** TODO Rings to abelian groups
A tensor Z-algebra over the abelian group.

******** Topological spaces to sets
The inclusion on the discrete topology over the set is an universal
arrow. If we define any application $f\colon A \to O$, there is an unique 
continuous function $\widetilde f \colon (A,\tau_d) \to (O,\tau)$ defined by $\widetilde f(x) = f(x)$. It is
trivially continuous, as $\tau_d$ is the discrete topology.

******** Pointed sets to sets
Trivially, the inclusion on $(S \cup \left\{ \ast \right\}, \ast)$ defines an universal arrow.

******* TODO Exercise III.1.4
#+begin_statement
Use only universality (of projections) to prove the following isomorphisms of
group theory:

 1) For normal subgroups $M,N$ of $G$ with $M \subset N$, $(G/M)(N/M) \cong (G/M)$.
 2) For subgroups $S$ and $N$ of $G$, $N$ normal, with join $SN$, $SN/N \cong S/S\cap N$.
#+end_statement

******* TODO Exercise III.1.5
#+begin_statement
Show that the quotient $K\text{-module}$ $A/S$ ($S$ a submodule of $A$) has a description
by universality. Derive isomorphism theorems.
#+end_statement

******* TODO Exercise III.1.6
#+begin_statement
Describe quotients of a ring by a two-sided ideal by universality.
#+end_statement

******* TODO Exercise III.1.7
#+begin_statement
Show that the construction of the polynomial ring $K[x]$ in a indeterminate $x$
over a commutative ring $K$ is a universal construction.
#+end_statement

****** III.2. Yoneda Lemma
******* DONE Exercise III.2.1
#+begin_statement
Let functors $K,K' \colon D \to \mathtt{Set}$ have representations $\left\langle r,\psi \right\rangle$ and $\left\langle r',\psi' \right\rangle$, 
respectively. Prove that to each natural transformation $\tau\colon K \overset{\cdot}\to K'$,
there is a unique morphism $h \colon r' \to r$ of $D$ such that

\[
\tau\circ \psi = \psi' \circ D(h,-) \colon D(r,-) \overset{\cdot}\to K'.
\]
#+end_statement

The following natural transformation

\[\begin{tikzcd} 
\mathrm{hom}(r,-) \rar{\psi}&
K \rar{\tau}&
K' \rar{\psi'^{-1}}& 
\mathrm{hom}(r',-),
\end{tikzcd}\]

[[*Corollary to Yoneda Lemma][must]] have the form $h_{\ast}$ for a unique $h\colon r' \to r$, so $\tau \circ \psi = \psi' \circ h_{\ast}$.

******* TODO Exercise III.2.2
#+begin_statement
State the dual of the Yoneda Lemma ($D$ replaced by $D^{op}$).
#+end_statement

****** III.3. Coproducts and colimits
******* DONE Exercise III.3.1
#+begin_statement
In the category of commutative rings, show that $R \to R \otimes S \gets S$, with maps
$r \mapsto r \otimes 1$, $s \mapsto 1\otimes s$, is a coproduct diagram.
#+end_statement

Given $\alpha\colon R \to T$ and $\beta\colon S \to T$; the unique homomorphism making the diagram
commute must be defined by

\[f(r\otimes s) = f(r\otimes 1)f(1 \otimes s) = \alpha(r) \beta(s).\]
****** III.4. Products and limits
******* DONE Exercise III.4.1
#+begin_statement
In $\mathtt{Set}$, show that the pullback of $f\colon X \to Z$ and $g \colon Y \to Z$ is given
by the set of pairs $\left\{ (x,y) \mid x \in X, y \in Y, fx = gy \right\}$. Describe pullbacks
in $\mathtt{Top}$.
#+end_statement

Let $h_x,h_y$ be two functions such that $f(h_x(c)) = g(h_y(c))$. Then the only
possible $\phi \colon C \to P = \left\{ (x,y) \mid fx =gy \right\}$ is $\Phi(c) = (h_xc,h_yc) \in P$.

In $\mathtt{Top}$, the pullback is similar: it is a subspace of the product given
by $\left\{ (x,y) \mid fx = gy \right\}$.

******* DONE Exercise III.4.4
#+begin_statement
In any category, prove that $f\colon a \to b$ is epi if and only if the following
square is a pushout

\[\begin{tikzcd}
a\rar{f} \dar[swap]{f} & b \dar{1} \\
b\rar{1} & b &.
\end{tikzcd}\]
#+end_statement

The usual definition of epimorphism is equivalent to the universal
property.

******* DONE Exercise III.4.5
#+begin_statement
In a pullback square, show that $f$ monic implies $q$ monic.
#+end_statement

If $n,m \colon c \to b \times d$, with $n\pi_2 = m\pi_{2}$; as we know that $n\pi_1 f = m\pi_1 f$, we
can use monicity to get $n\pi_1 = n\pi_2$. By universal property of the pullback
square, $n = m$.

****** III.5. Categories with Finite Products
******* DONE Exercise III.5.1
#+begin_statement
Prove that the diagonal $\delta_c \colon c \to c \times c$ is natural in $c$.
#+end_statement

We use the identity $\pi_1\delta_c = 1_c = \pi_2\delta_c$ to create the four commutative
triangles that we use in this diagram. The commutativity of the diagram
follows from the universal property of the product.

\[\begin{tikzcd}
& c\dar{\delta}\dlar[swap,bend right]{id}\drar[bend left]{id} & \\
c \dar{f} & c \times c\lar[swap]{\pi}\rar{\pi} \dar{f \times f} & c \dar{f} \\
d\drar[bend right]{\delta} & d \times d \lar[swap]{\pi}\rar{\pi}\dar{id} & 
d\dlar[bend left,swap]{\delta} \\
& d\times d \\
\end{tikzcd}\]

******* TODO Exercise III.5.5
#+begin_statement
If $B$ has (finite) products show that any functor category $B^C$ also has (finite)
products (calculated "pointwise").
#+end_statement
***** IV. Adjoints [1/2]
****** IV.1. Adjunctions
******* DONE Exercise IV.1.2
#+begin_statement
Given functors $G\colon A \to X$ and $F \colon X \to A$, show that each adjunction $\left\langle F,G,\varphi \right\rangle$ can
be described as an isomorphism $\theta$ of comma categories such that the following diagram
commutes

\begin{tikzcd}[column sep=none]
\theta\colon & (F \downarrow I_{A})\dar & \cong & (I_x \downarrow G)\dar \\
& X \times A & = & X \times A & &.
\end{tikzcd}

Here the vertical maps have components the projection functors $P$ and
$Q$ of II.6(5).
#+end_statement

Given an adjunction $\varphi \colon \mathrm{hom}(Fx,y) \cong \mathrm{hom}(x,Gy)$, we can define the isomorphism
in objects as $\theta(f \colon Fx \to y) = (\varphi f \colon x \to Gy)$. And the isomorphism on morphisms
as

\begin{tabular}{ccc}
\begin{tikzcd}
Fx \dar{f}\rar{Fk} & Fx' \dar{f'} \\
y \rar{h} & y'
\end{tikzcd} 
& \Longrightarrow &
\begin{tikzcd}
x \dar{\varphi f}\rar{k} & x' \dar{\varphi f'} \\
Gy \rar{Gh} & Gy'
\end{tikzcd}
\end{tabular}

where the commutativity of the second diagram follows from the commutativity of the
first one if we apply naturality of $\varphi$. This $\theta$ is bijective and respects composition
trivially. The given diagram commutes.

# Are all isomorphisms on this form adjunctions?

******* TODO Exercise IV.1.3
#+begin_statement
For the adjunction $\left\langle \Delta,\times,\varphi \right\rangle$, show that the unit $\delta_c : c \to c \times c$ for each object
$c \in C$ is the unique arrow such that the diagram

\[\begin{tikzcd}
& C\drar{1}\dlar[swap]{1}\dar[dashed]{\delta_c} & \\
c & c \times c \rar[swap]{q}\lar{p} & c
\end{tikzcd}\]

commutes. This arrow is often called the /diagonal arrow/ of $c$. If $C = \mathtt{Set}$, show
that $\delta_cx = \left\langle x,x \right\rangle$ for $x \in c$.
#+end_statement

*** Category theory - Awodey
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/awodey_category_theory.pdf
:END:
**** Preface
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: 4
:END:
**** 1. Categories
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: 11
:END:
***** 1.1. Introduction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (11 . 0.29894089511445165)
:END:

1945 - Eilenberg, Mac Lane's "General theory of natural equivalences".
1940s - Algebraic topology and abstract algebra.
1950s - Grothendieck et al. in algebraic geometry.
1960s - Lawvere et al. in logic.
1970s - Computer science, linguistics, philosophy.

**** 8. Categories of diagrams
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: 169
:END:

***** 8.1. Set-valued functor categories
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (169 . 0.4065596173556542)
:END:

***** 8.2. The Yoneda embedding
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (170 . 0.4543901605739665)
:END:

****** Definition 8.1. Yoneda embedding                                                             :drill:yoneda:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (171 . 0.17936453706867098)
:ID:       75e75540-0bbc-4316-a943-67a3d0d514fb
:DRILL_LAST_INTERVAL: 3.9254
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:42]
:END:
Define the Yoneda embedding. What does it mean to be an embedding?

******* Definition
The Yoneda embedding is defined by

\[
yC = \hom_{\mathbb{C}}(-,C) \colon \mathbb{C}^{op} \to \mathsf{Set}
\]

and, for each $f \colon C \to D$,

\[
yf = (f \circ -) \colon \mathrm{hom}(-,C) \to \mathrm{hom}(-,D).
\]

******* Embedding
It is an /embedding/ because it is full, faithful and injective on
objects.  This is a consequence of the Yoneda Lemma.

****** Yoneda as a representation                                                                         :yoneda:
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (172 . 0.2750256235052955)
:END:

In this sense, the Yoneda embedding y represents the objects and
arrows of C as certain âstructured setsâ and (all of) their
"homomorphisms".

***** 8.3. The Yoneda Lemma
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (172 . 0.3467714383327639)
:END:

****** Yoneda lemma                                                                                        :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (172 . 0.3826443457464981)
:ID:       2c7b9f5c-4064-4abe-be4f-953e7b83c626
:DRILL_LAST_INTERVAL: 3.474
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:39]
:END:

Statement of the Yoneda lemma.

******* Answer
Let $\mathbb{C}$ be locally small, there is an isomorphism

\[
\mathrm{hom}(yC,F) \cong FC
\]

natural in each object $C \in \mathbb{C}$ and any functor $F \in \mathsf{Set}^{\mathbb{C}^{op}}$.

******* Extra
Note that these are homomorphisms between functors (natural transformations).
We are not making the naturality conditions explicit.

***** 8.4. Applications of the Yoneda Lemma
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (176 . 0.3467714383327639)
:END:

****** Embedding is full and faithful                                                                      :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (176 . 0.4902630679877007)
:ID:       8903d3bd-7725-42ab-a047-80170fc13440
:DRILL_LAST_INTERVAL: 3.8708
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:40]
:END:
Why in any small category $\mathbb{C}$ an isomorphism $yA \cong yB$ implies $A \cong B$?

******* Answer
The Yoneda embedding is full and faithful.

****** Exponential rule in cartesian closed categories                                                     :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (176 . 0.5739665186197471)
:ID:       3581b5dc-2d1e-4830-accf-991868186c52
:DRILL_LAST_INTERVAL: 5.3678
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:41]
:END:

Prove that $(A^B)^C \cong A^{(B \times C)}$ in any cartesian category using the
Yoneda lemma.

******* Answer
It is full and faithful, so we only have to show that $y((A^B)^C) \cong y(A^{(B \times C)})$.

The following is a chain of natural isomorphisms given by adjunctions
and commutativity.

\[\begin{aligned}
\mathrm{hom}(X,(A^B)^{C})
&\cong \mathrm{hom}(X \times C,A^B) \\
&\cong \mathrm{hom}((X \times C) \times B,A) \\
&\cong \mathrm{hom}(X \times (C \times B),A) \\
&\cong \mathrm{hom}(X, A^{B \times C}) \\
\end{aligned}\]

***** 8.5 Limits in categories of diagrams
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (177 . 0.5978817902289033)
:END:

****** Presheaf categories are complete                                                           :drill:presheaf:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (177 . 0.7413734198838401)
:ID:       887fd9dc-cff2-473b-aef6-25ed2c15cb87
:DRILL_LAST_INTERVAL: 3.677
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:43]
:END:
Existence of limits on presheaf categories.

******* Answer
For any small $\mathbb{C}$, the category $\mathsf{Sets}^{\mathbb{C}^{op}}$ is complete; it has all small limits.
Moreover, the evaluation functor $\mathrm{ev}_C \colon \mathsf{Sets}^{\mathbb{C}^{op}} \to \mathsf{Sets}$ is preserves all limits.

***** 8.8 Topoi
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (184 . 0.29894089511445165)
:END:

****** 8.17. Categories of diagrams are topoi                                                              :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (185 . 0.37068670994192005)
:ID:       122fd7e7-8879-47d0-98b7-e0242e3b6e45
:DRILL_LAST_INTERVAL: 5.3361
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:41]
:END:

Relation between presheaf categories and topoi.

******* Answer
For any small $\mathbb{C}$, the category $\mathsf{Sets}^{\mathbb{C}^{op}}$ is a topos.
*** Category theory foundations talk - Awodey
Following this [[https://www.youtube.com/playlist?list=PLGCr8P_YncjVjwAxrifKgcQYtbZ3zuPlb][video series]].

**** Category theory foundations 1.0
Category theory is the abstract algebra of abstract functions.

***** Functions on sets
In set theory, a function $f \colon A \to B$ is a subset of the cartesian
product $f \subset A \times B$, such that for all $a \in A$, there exists a unique
$b \in B$ such that $f(a) = b$.

Composition of functions is distributive and it has an identity.
Categories arise as an abstraction of this setting.

***** Definition of category
A category is defined by a class of objects and a class of arrows.
A distributive composition with identity. The axioms of a category
are the distributivity and the identity as a neutral element.
***** Isomorphisms
***** Examples
****** Finite categories
****** Posets
****** Monoids
****** Category of all posets
****** Relations on sets
****** Category of proofs
The objects are logic formulas and arrows are deductions on the formal
system, from the assumptions to the conclusions, that can be composed.
**** Category theory foundations 1.1
***** Constructions on categories
****** Product category
****** Arrow category
${\cal C}^{\to}$, where objects are arrows on ${\cal C}$ and arrows
are commutative squares.
****** Slice category
******* Cat, the category of categories
***** Functors
Preserve all the structure of a category.

****** Exponential functor on sets
Fix an object $A$, then $B \mapsto B^{A}$ is a functor on sets.

***** Duality
****** Contravariant functors
******* Example: Hom-Functors
***** Hom-sets
Assuming ${\cal C}$ small, $\mathrm{hom}(A,B)$ is the set of arrows from $A$ to $B$. If we fix $A$,
we get a functor $\mathrm{hom}(A,-)$; if we fix $B$, we get a contravariant functor $\mathrm{hom}(-,B)$.

**** Category theory foundations 1.2
***** Representable functors
A functor is representable if it can be written as an hom-set.

The currying of the $Hom$ functor is the Yoneda embedding. (!)

**** Category theory foundations 2.0
***** Universal mappings
****** Products (MacLane, 1949)
Products on categories, on posets...
****** Coproducts and duality
****** Exponentials
Suppose we have all products. An exponential for $A,B$ is
an object $B^{A}$ such that

\[\begin{tikzcd}
B^{A} & B^{A} \rar{e} \times A & B \\
X \uar[dashed]{\exists! f} & X \times A \uar{f \times 1} \urar[swap]{g} &
\end{tikzcd}\]

**** Category theory foundations 2.1
In groups, the homomorphisms are not an exponential. Groupoids are!
They form a cartesian closed category.

***** Cartesian closed category
A category is *cartesian closed* if it has

  * all products.
  * all exponentials.
  * the terminal object.
 
***** Adjunction Product-Exponential
Observe that the universal mapping property of the exponent
implies that every time we have a map like $X \times A \to B$,
we can define $X \to B^{A}$. It exactly says that the two functors
are adjoints.

***** Universal properties as rules of inference
Universal properties are like rules of inference

  * Product

    \begin{prooftree}
    \AxiomC{$X \to A$}
    \AxiomC{$X \to B$}
    \BinaryInfC{$X \to A \times  B$}
    \end{prooftree}

  * Coproduct

    \begin{prooftree}
    \AxiomC{$A \to X$}
    \AxiomC{$B \to X$}
    \BinaryInfC{$A + B \to X$}
    \end{prooftree}

  * Exponential

    \begin{prooftree}
    \AxiomC{$X \times A \to B$}
    \UnaryInfC{$X \to B^A$}
    \end{prooftree}

In the case where $X=1$, we get a particular case of the inference rules.

***** Exponentials in a poset
Exponentials in a poset are implications on intuitionistic
propositional calculus. Rules of logic can be written as universal
properties.

****** Proof relation
A natural deduction logic is a category where deductions are morphisms
and propositions are objects. We can prove the adjunction on the rules.

****** Category of proofs
If we take the category of proofs, we get the lambda calculus.
This is a richer structure.

This is called *category of types* or *category of proofs*.

**** Category theory foundations 2.2
***** Category of types on \lambda-calculus is CCC
To make this a CCC, we need to identify certain proofs. We have to identify
some proofs to make this a CCC

  * $\mathrm{fst}(a,b) = a$
  * $\mathrm{snd}(a,b) = b$
  * $(\lambda x.b)a = b[a/x]$

This is simplification of proofs. 

***** Theory in the \lambda-calculus
We have some basic types $A,B,\dots$, basic terms $a:X,b:Y,\dots$ and equations
between terms of the same type. That defines a theory.

You could formulate the theory of groups or any algebraic theory in
this way as a lambda theory. The theory of the reflexive domain is an
example of higher order theory that can be formulated as a lambda theory.

We can then define a model of such a theory in a CCC as an assignment of
objects to basic types and morphisms as functions.

  * $\llbracket A \rrbracket \in {\cal C}$, for any basic type
  * $\bbk{a} \colon 1 \to \bbk{X}$, for any basic term

and it can be extended to all terms as

  * $\bbk{t} \colon 1 \to A$, for any term $t \colon A$,
  * in particular, $f \colon A \to B$, $\bbk{f} \colon \bbk{A} \to \bbk{B}$.

That is, you can interpret this on any algebraic theory and a model of that
theory will arise.

***** Completeness theorem of CCC for \lambda-calculus
For any theory $\mathbb{T}$ in \lambda-calculus

  1) for any closed terms $a,b\colon X$, $\mathbb{T} \vdash a = b$ if and only if $\bbk{a} = \bbk{b}$ in
     any CCC.
  2) there exists a closed term $t : X$ if and only if in every $\mathbb{T}$ model of in 
     a CCC, $1 \to \bbk{X}$.


This says that lambda calculus is really equivalent to the notion of a CCC.
\[
\lambda^{x,\to} \simeq CCC
\]

Propositional logic itself is exactly equivalent to the notion of a CCC Poset.
If we add the disjunction, we get a Heyting algebra.

Kripke models are a specialization to some special cartesian closed categories.

**** Category theory foundations 3.0
***** Arithmetic on a Cartesian Closed Category
All the usual arithmetic can be expressed on cartesian closed categories.
***** Lambda calculus with sums on a CCC
Lambda calculus with sums can be expressed on a CCC with coproducts and
you can use it to prove equations on a CCC because of its soundness.
***** Natural transformation
Represents the concept of a morphism independent of the choice
of objects. This is the notion of parametricity or uniformity.
****** Definition of natural transformation
****** Example of naturality: sets through time
Discrete time parametrized on the natural numbers. A function
between two sets through time should be a natural transformation
between the indexed family of sets.

If we see $\omega$ as the natural preorder, the category of sets through
time is $\mathtt{Set}^{\omega}$, the category of functors from $\omega$ to $\mathtt{Set}$.

***** Definition of functor categories
****** Functor categories make Cat a CCC
****** Example: the arrow category
Can be seen as a functor category from a category with only
one morphism to the base category.
****** Example: product category
$\mathbb{C}^2 \cong \mathbb{C} \times \mathbb{C}$ where $2$ is a discrete category of two objects.
****** Example: graph category
****** Example: simplicial sets
**** Category theory foundations 3.1
***** The Yoneda embedding
Presheaves of the form $\mathtt{Sets}^{{\cal C}^{op}}$. If we assume ${\cal C}$ locally small, the functor
hom can be defined. The functor $\mathrm{Hom}$ can be curried to get a functor
$y \colon {\cal C} \to \mathtt{Sets}^{{\cal C}^{op}}$.

This functor takes a morphism to a natural transformation between two functors.
***** Yoneda Lemma
Given any $c \in {\cal C}$ and $F \colon {\cal C}^{op}\to \mathtt{Sets}$, there is an isomorphism
\[
\mathrm{Hom}(yC,F) \cong FC.
\]

****** Proof of the Yoneda Lemma

****** Corollary
The isomorphism is natural on both arguments.

****** Utility of Yoneda
The category $\mathtt{Sets}^{{\cal C}^{op}}$ is a topos, has a nice logical structure; while the
category was ${\cal C}$ any category. If the Yoneda embedding preserves the structure,
that gives us a nice structure on ${\cal C}$.

***** Examples of usage of the Yoneda Lemma
****** Product
Propositionally, the universal mapping property of the product
can be written as

\begin{prooftree}
\AxiomC{$X \to A$}
\AxiomC{$X \to B$}
\doubleLine
\BinaryInfC{$X \to A \times  B$}
\end{prooftree}

but this rule of inference goes both ways. And this is also a
bijection between the two sides;

\[
\mathrm{hom}(X,A) \times \mathrm{hom}(X,B) \cong \mathrm{hom}(X, A \times B).
\]

****** Example
Consider $\mathrm{hom}(X,-)$ and apply it to the product diagram.
We get

\[\begin{tikzcd}
& \mathrm{hom}(X,A \times B) \drar\dlar\dar[dashed]{\cong} & \\
\mathrm{hom}(X,A) & \mathrm{hom}(X,A) \times \mathrm{hom}(X,B) \lar{\pi}\rar{\pi} & \mathrm{hom}(X,B)
\end{tikzcd}\]

The universal properties can be expressed using hom-sets.

****** Claim
$yC \cong yD \implies C \cong D$

We can prove some equations using this technique.

\[
y({\cal C}^{A+B}) \cong y({\cal C}^A \times {\cal C}^B)
\]

******* Proof
$\mathrm{hom}(X\times (A+B), C) \cong \mathrm{hom}(X \times A,C) \times \mathrm{hom}(X \times B,C)$

This reduces the proof of ${\cal C}^{A+B} \cong {\cal C}^A \times {\cal C}^B$ to a much simpler algebraic
manipulation.

**** Category theory foundations 3.2
***** Laws in any CCC
Products distribute over coproducts. Exponentials make products distribute
over coproducts.

****** Proof
Using Yoneda.

****** Yoneda embedding preserves all the cartesian closed structure

**** Category theory foundations 4.0
***** Sets^C^op is a cartesian closed category
They are some kind of "variable sets"; sets that vary on a parameter
coming from the index category. These things have products and coproducts
defined pointwise. Exponentials could be also defined pointwise, but it has
a covariant and a contravariant component, so it is not useful to get a
definition of exponentials on morphisms. This does not work for exponentials.

***** Yoneda to the rescue
We are trying to define a functor $B^A(X)$. If it exists, it has to be like
\[
B^A(X) \cong \mathrm{hom}(yX,B^{A}) \cong \mathrm{hom}(yX \times A, B).
\]

If we take this as the definition, $\mathrm{hom}(y- \times A,B)$ is a contravariant
function.

****** Corollary
As a corollary, $y \colon {\cal C} \to \mathtt{Set}^{{\cal C}^{op}}$ preserves CC structure.

******* Proof
We can prove using Yoneda lemma that $y(B^A) = y(B)^{y(A)}$.

***** Completeness of \lambda-calculus
Every cartesian closed category can be embedded into a $\mathtt{Sets}^{{\cal C}^{op}}$.

#+begin_theorem
Completeness of \lambda-calculus with respect to variable sets.
Given any \lambda theory $\mathbb{T}$, and $s,t {:} X$
\[
\mathbb{T} \vdash s=t \iff \text{ for any interpretation}, \bbk{s} = \bbk{t}
\]

There is a $t \colon X$ iff in every model $\bbk{t} \colon 1 \to \bbk{X}$.
#+end_theorem

***** Awodey's Theorem
Kripke completeness.

#+begin_theorem
The same thing happens to respect to $\mathtt{Sets}^{P}$, where $P$ is a poset.
#+end_theorem

***** Adjointness
The following are examples of adjoints

  1. Products
  2. Exponentials
  3. Coproduct
  4. Induction
  5. Recursion
  6. Natural numbers
  7. Inductively defined types
  8. Logical connectives
  9. Quantifiers
  10. Sigma and Pi types

****** Product-exponential
The functors $F(X) = X \times A$ and $G(X) = Y^{A}$ are adjoints. There is a natural
isomorphism between $\mathrm{hom}(X \times A,Y) \cong \mathrm{hom}(X,Y^{A})$.

****** Product-diagonal
\[
\mathtt{hom}((X,X), (A,B)) \cong \mathtt{hom}(X,A \times B)
\]

****** Chain of adjoints

\[ + \dashv \Delta \dashv \times \dashv \mathrm{exp}
\]

**** Category theory foundations 4.1
***** Uniqueness of adjoints
Every universal property and every adjoint determine an object up
to isomorphism. Adjoints are unique up to isomorphism.

****** Proof of the uniqueness of adjoints
Using the Yoneda Lemma

***** Unit and counit of and adjunction
****** Example: product-exponent
Evaluation is the counit.
****** Example: diagonal-product
The diagonal map is the unit.
***** Example: adjoints in logic
In Propositional calculus, conjunction is an adjoint.
Entailment is reflexive and transitive, so proofs form a category.

\begin{prooftree}
\AxiomC{$\theta \vdash \varphi$}
\AxiomC{$\theta \vdash \rho$}
\doubleLine
\BinaryInfC{$\theta \vdash \varphi \times \rho$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\varphi \vdash \theta$}
\AxiomC{$\rho \vdash \theta$}
\doubleLine
\BinaryInfC{$\rho + \varphi \vdash \theta$}
\end{prooftree}

Terminal and initial objects can be written as adjoints. Implications
can be written as adjoints. This work is due to Lawvere. 

/Adjointness in Foundations/, W. Lawvere.

***** Example: quantifiers
We will write formulas with individual variables; and we build up formulas
involving some variables in the usual way. We have a deduction system with
entailment and rules of inference.

There is an operation of adding a new variable to a context. If I have,
\[
x_1,\dots,x_n \vdash \varphi(x_1,\dots,x_n),
\]

I can get
\[
x_1,\dots,x_n,y \vdash \varphi(x_1,\dots,x_n) \vdash \psi(x_1,\dots,x_n,y).
\]


This operation has adjoints. Its right adjoint is $\ast \vdash \forall$

\begin{prooftree}
\AxiomC{$^{y\text{ in context}}\varphi(x_1,\dots,x_n) \vdash \psi(x_1,\dots,x_n,y)$}
\doubleLine
\UnaryInfC{$\varphi(x_1,\dots,x_n) \vdash \forall_y \psi(x_1,\dots,x_n,y)$} 
\end{prooftree}

and its left adjoint is $\exists \vdash \ast$

\begin{prooftree}
\AxiomC{$\varphi(x_1,\dots,x_n,y) \vdash ^{y\text{ in context}}\psi(x_1,\dots,x_n)$}
\doubleLine
\UnaryInfC{$\exists_y\varphi(x_1,\dots,x_n) \vdash \psi(x_1,\dots,x_n,y)$} 
\end{prooftree}

Unit and counit look like provable formulas.

***** Proof theory of first order logic
The proof theory of propositional logic was the lambda calculus. The proof
theory of first order logic is dependent type theory (??).

****** Propositional logic

\begin{tabular}{c|c|c}
Propositional logic & Provability & Proof/type theory \\
\hline
Logical Point of view & Intuitionistic PL $\wedge,\vee$ & STLC with simple types $\times,+$ \\
CT & Poset CCC / Heyting algebra & CCC with sums \\
\hline
& Poset & Category
\end{tabular}

****** First order logic

\begin{tabular}{c|cc}
FOL & Provability & Proof/type theory \\
\hline
Logic & Intuitionistic FOL $\forall,\exists$ & Dependent Type theory $\Pi,\Sigma$ \\
CT & Heyting category & Locally cartesian closed category \\
\hline
& Poset & Category
\end{tabular}

A locally cartesian closed category is a category such that any slice is CCC.

****** Yoneda and CCC
If ${\cal C}$ is a CCC, the Yoneda embedding is also cartesian closed. It preserves the
interpretation of dependent type structure of a category.

We have the same Kripke completeness theorem for the full system of Martin-Lof
type theory.

*** The Catsters
**** Adjunctions
Serie de [[https://www.youtube.com/playlist?list=PL54B49729E5102248][vÃ­deos]] sobre funtores adjuntos.

***** Adjuntions 1
Tenemos varias nociones de igualdad entre categorÃ­as.

#+begin_definition
*Isomorfismo de categorÃ­as*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que $1_C = GF$ y $FG = 1_D$.
#+end_definition

#+begin_definition
*Equivalencia de categorÃ­as*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que $1_C \cong GF$ y $FG \cong 1_D$. Entendiendo la isomorfÃ­a en la 
categorÃ­a de funtores, es decir, una [[https://ncatlab.org/nlab/show/natural+isomorphism][isomorfÃ­a natural]].
#+end_definition

#+begin_definition
*AdjunciÃ³n*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que tenemos transformaciones naturales $1_C \overset{\eta}\Longrightarrow GF$ y 
$FG \overset{\epsilon}\Longrightarrow 1_D$ que cumplen las dos identidades triangulares siguientes:
 
\[ \begin{tikzcd}
F \arrow{r}{\eta} \arrow{dr}{id} & FGF \arrow{d}{\epsilon} \\
 & F
\end{tikzcd}   
\]     \[ \begin{tikzcd}
G \arrow{r}{\eta} \arrow{dr}{id} & GFG \arrow{d}{\epsilon} \\
 & G
\end{tikzcd}
\]
#+end_definition

En este caso escribimos $F \dashv G$, y $F$ es funtor adjunto de $G$.

***** Adjuntions 2
Damos una definiciÃ³n equivalente de funtores adjuntos.

#+begin_definition
*AdjunciÃ³n*. Una adjunciÃ³n es un isomorfismo natural:

\[Hom_D(FX,Y) \cong Hom_C(X,GY)\]

Natural sobre $X$ fijado cualquier $Y$ y natural sobre $Y$ fijado 
cualquier $X$. Entendiendo que usamos los funtores contravariantes $Hom(F-,Y)$,
$Hom(-,GY)$ por un lado y los funtores covariantes $Hom(FX,-)$ y $Hom(X,G-)$;
que nos dan los siguientes cuadrados de naturalidad:

\[ \begin{tikzcd}
Hom_D(FX',Y) \arrow{d}[swap]{Hom_D(Ff,Y)} \arrow{r}{\alpha_{X'}} & Hom_C(X',GY) \arrow{d}{Hom_C(f,GY)}\\
Hom_D(FX, Y) \arrow{r}{\alpha_{X}}& Hom_C(X,GY)
\end{tikzcd}
\] 

\[ \begin{tikzcd}
Hom_D(FX,Y) \arrow{d}[swap]{Hom_D(FX,g)} \arrow{r}{\beta_{Y}} & Hom_C(X,GY) \arrow{d}{Hom_C(X,Gf)}\\
Hom_D(FX,Y') \arrow{r}{\beta_{Y'}}& Hom_C(X,GY')
\end{tikzcd}
\] 
#+end_definition

Esta definiciÃ³n es equivalente intuitivamente a la anterior porque
podemos crear $\eta$ y $\epsilon$ desde las identidades usando las
siguientes transformaciones naturales:

\[Hom_D(FX,FX) \cong Hom_C(X,GFX)\]

\[Hom_D(FGY,Y) \cong Hom_C(GY,GY)\]

***** Adjuntions 3
Podemos presentar ejemplos de adjunciones.
Los *funtores libres y de olvido* suelen ser adjuntos. Entre $Set$ y $Monoid$ tenemos:

\[ \begin{tikzcd}
{Set} \arrow[bend left]{r}{Free} & {Monoid} \arrow[bend left]{l}{Forget}
\end{tikzcd}
\]

Con la adjunciÃ³n $Free \dashv Forget$. 

#+begin_theorem
*MÃ³nada de una adjunciÃ³n*. Cada adjunciÃ³n da lugar a una mÃ³nada.
#+end_theorem

Tenemos un funtor $T = GF : {\cal C}  \longrightarrow {\cal C}$. Podemos definir la unidad de
la mÃ³nada como la unidad de la adjunciÃ³n $\eta : 1_C \Longrightarrow T$ y la
multiplicaciÃ³n podemos definirla usando $id \ast \epsilon \ast id : GFGF \Longrightarrow GF$.

Ahora debemos comprobar que cumple los axiomas de mÃ³nada. El primero
se obtiene directamente desde los triÃ¡ngulos de la adjunciÃ³n:

\[ \begin{tikzcd}
T \arrow{r}{T\eta} \arrow{dr}{id} & T^2 \arrow{d}{\mu} \\
 & T
\end{tikzcd}   
\]   \[ \begin{tikzcd}
GF \arrow{r}{GF\eta} \arrow{dr}{id} & GFGF \arrow{d}{G \epsilon F} \\
 & GF
\end{tikzcd}   
\]

Donde el segundo es resultado de aplicar el funtor $G$ a uno de los triÃ¡ngulos conmutativos
de la adjunciÃ³n. Comprobamos el segundo axioma:

\[ \begin{tikzcd}
T^2 \arrow{d}{\mu} & T \arrow{dl}{id} \arrow{l}[swap]{\eta T} \\
T
\end{tikzcd}   
\]   \[ \begin{tikzcd}
GFGF \arrow{d}{G \epsilon F} & GF \arrow{dl}{id} \arrow{l}[swap]{\eta GF} \\
GF
\end{tikzcd}   
\]

Donde tenemos el resultado de aplicar $F$ por la derecha al otro triÃ¡ngulo conmutativo.

Y finalmente el axioma de conmutatividad de la mÃ³nada se comprueba como:

\[ \begin{tikzcd}
T^3 \arrow{d}{T \mu} \arrow{r}{\mu T} & T^2 \arrow{d}{\mu} \\
T^2 \arrow{r}{\mu} & T
\end{tikzcd} \]  \[ \begin{tikzcd}
GFGFGF \arrow{d}{GFG \epsilon F} \arrow{r}{G \epsilon FGF} & GFGF \arrow{d}{G\epsilon F} \\
GFGF \arrow{r}{G \epsilon F} & GF
\end{tikzcd} \] 

Donde el segundo diagrama se obtiene desde la naturalidad de $\epsilon$ aplicando funtores.

***** Adjuntions 4
Vamos a probar la igualdad entre las dos definiciones de adjunciÃ³n.
Supongamos primero que tenemos el isomorfismo natural entre los dos 
conjuntos de morfismos, es decir, tenemos:

\[ (-) : Hom_D(FX,Y) \cong Hom_C(X,GY) \]

Si tomamos ahora los dos cuadrados naturales que tenÃ­amos por este 
isomorfismo y tomamos en ellos los casos particulares $Y = FX$ primero,
y $X = GY$ despuÃ©s:

\[ \begin{tikzcd}
Hom_D(FX,FX) \arrow{d}[swap]{\_ \circ Ff} \arrow{r}{(-)} & Hom_C(X,GFX) \arrow{d}{\_\circ f}\\
Hom_D(FX', FX) \arrow{r}{(-)}& Hom_C(X',GFX)
\end{tikzcd}
\]

Si tomamos la identidad $1_{FX}$ y llamamos $\eta_X = \overline{1_{FX}}$, tenemos que
\(\eta \circ f = \overline{Ff}\). Ahora, si damos la vuelta al isomorfismo $(-)$ en este 
diagrama a la vez que hacemos $X = GY$:

\[ \begin{tikzcd}
Hom_D(FGY,Y) \arrow{d}[swap]{\_ \circ Ff}  & Hom_C(GY,GY) \arrow{l}[swap]{(-)} \arrow{d}{\_\circ f}\\
Hom_D(FGY',Y) & Hom_C(GY',GY) \arrow{l}[swap]{(-)}
\end{tikzcd}
\]

Volviendo a tomar la identidad $1_{GY}$ y llamando $\epsilon_Y = \overline{1_{GY}}$, tenemos
$\epsilon \circ Ff = \overline{f}$.

Ahora tomamos el segundo cuadrado natural, y repetimos el mismo
proceso.

\[ \begin{tikzcd}
Hom_D(FX,FX) \arrow{d}[swap]{g \circ \_} \arrow{r}{(-)} & Hom_C(X,GFX) \arrow{d}{Gg\circ \_}\\
Hom_D(FX,FX') \arrow{r}{(-)}& Hom_C(X,GFX')
\end{tikzcd}
\] 

Obteniendo desde la identidad en $FX$ la ecuaciÃ³n $\overline{g} = Gg \circ \eta$. Y volviendo
a dar la vuelta a los isomorfimos llegamos a:

\[ \begin{tikzcd}
Hom_D(FGY,Y) \arrow{d}[swap]{g \circ \_}  & Hom_C(GY,GY) \arrow{l}[swap]{(-)} \arrow{d}{Gg \circ \_}\\
Hom_D(FGY,Y') & \arrow{l}[swap]{(-)} Hom_C(GY,GY')
\end{tikzcd}
\]

Obteniendo finalmente $\overline{Gg} = g \circ \epsilon$. De este proceso hemos obtenido finalmente
las siguientes ecuaciones:

\[ \begin{aligned}
\eta \circ f &= \overline{Ff} \\
\epsilon \circ Ff &= \overline{f} \\
Gg \circ \eta &= \overline{g} \\
g \circ  \epsilon &= \overline{Gg} 
\end{aligned} \]

Con ellas podemos probar la naturalidad de $\eta$ y la naturalidad de
$\epsilon$:

\[ \begin{tikzcd}
GFX  \arrow{r}{GFf} & GFY \\
X \arrow{u}[swap]{\eta_X} \arrow{r}[swap]{f} & Y \arrow{u}{\eta_Y}
\end{tikzcd}
\]   \[ \begin{tikzcd}
FGX \arrow{d}[swap]{\epsilon_X} \arrow{r}{FGg} & FGY \arrow{d}{\epsilon_Y}\\
X \arrow{r}[swap]{g} & Y
\end{tikzcd}
\]

Ya que $\eta \circ f = \overline{Ff} = GFf \circ \eta$ y $f \circ \epsilon = \overline{Gf} = \epsilon \circ FGf$. Y ademÃ¡s podemos probar
los dos triÃ¡ngulos de naturalidad.

\[ \begin{tikzcd}
F \arrow{r}{F \eta_X} \arrow{dr}{id} & FGF \arrow{d}{\epsilon_{FX}} \\
 & F
\end{tikzcd}   
\]     \[ \begin{tikzcd}
G \arrow{r}{\eta_{GX}} \arrow{dr}{id} & GFG \arrow{d}{G\epsilon_X} \\
 & G
\end{tikzcd}
\]

Teniendo finalmente que:

\[ \begin{aligned}
\epsilon \circ F\eta &= \overline{\eta} = 1 \\
G\epsilon \circ \eta &= \overline{\epsilon} = 1
\end{aligned} \]

El otro sentido de la demostraciÃ³n se tiene llegando primero a las
cuatro ecuaciones, y usÃ¡ndolas para definir el isomorfismo
$(-)$. Falta entonces demostrar su naturalidad.

*** What is an Operad? Part I - Math3ma ([[http://www.math3ma.com/mathema/2017/10/23/what-is-an-operad-part-1][link]])
An *operad* is a sequence ${\cal O}(1),{\cal O}(2),\dots$ together with compositions

\[
\circ_i \colon {\cal O}(n) \times {\cal O}(m) \to {\cal O}(m+n-1)
\]

satisfying that

 1. /composition is associative/; for each $f \in {\cal O}(n), g \in {\cal O}(m), h \in {\cal O}(p)$,

    \[
    (f \circ_j g) \circ_i h = \left\{\begin{array}{ll}
    (f \circ_i h) \circ_{j+p-1} g & \mbox{ if } 1 \leq i \leq j-1 \\
    f \circ_j (g \circ_{i-j+1} h) & \mbox{ if } j \leq i \leq n+j-1 \\
    (f \circ_{i-m+1} h) \circ_{j} g & \mbox{ if } i \geq n+j \\
    \end{array}\right.
    \]

 2. /arguments can be permuted/; for each ${\cal O}(n)$, we have an action of the
    symmetric group $S_n$ on the operations, $S_n \times {\cal O}(n) \to {\cal O}(n)$ such that
    for all $f \in {\cal O}(n)$, $g \in {\cal O}(m)$, and for all $\sigma \in S_n$, $\tau \in S_m$,

    \[
    \sigma f \circ_i \tau g = (\sigma \circ_i \tau)(f \circ_i g)
    \]

 3. /there exists a unit/; $1 \in {\cal O}(1)$ such that for every $n$ and every $f \in {\cal O}(n)$,
    we have $1 \circ_1 f = f \circ_i 1 = f$.

We are defining operads over sets, but we could define them over a
different category and ask $\circ_i$ and $S_n$ to be endomorphisms of
the category.

**** Operad axioms                                                                                           :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       b74b26b5-d74c-46c5-8c2e-7a08d7d1201e
:DRILL_LAST_INTERVAL: 4.1325
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:58]
:END:
Definition of operad. Axioms for defining an operad.

***** Answer
An *operad* is a sequence ${\cal O}(1),{\cal O}(2),\dots$ together with compositions

\[
\circ_i \colon {\cal O}(n) \times {\cal O}(m) \to {\cal O}(m+n-1)
\]

satisfying that

 1. composition is associative,
 2. arguments can be permuted,
 3. there exists a unit.
**** Endomorphism operad                                                                                     :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       f7950629-51ba-4655-a2ab-705532a9e002
:DRILL_LAST_INTERVAL: 4.6732
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:19]
:END:
What is an example of an operad?

***** Answer
The endomorphism operad over a vector space $V$, with ${\cal O}(n) = \mathrm{hom}(V^n,V)$.

**** Algebra over an operad                                                                                  :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       9d225db0-ce9b-44a2-ac3e-e16fcf194dbe
:DRILL_LAST_INTERVAL: 4.3932
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:15]
:END:
Definition of algebra over an operad.

***** Answer
Given a vector space $V$, an *O-algebra* is an assignment $\varphi_n \colon {\cal O}(n) \to \mathrm{End}_{V}(n)$
compatible with $\circ_i$ and $S_n$, compositions and permutations of arguments.

*** What is an Operad? Part II - Math3ma ([[http://www.math3ma.com/mathema/2017/10/30/what-is-an-operad-part-2][link]])
**** Associative operad                                                                                      :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       9f18443b-0ce6-4a15-b3af-987d37d4affd
:DRILL_LAST_INTERVAL: 4.3754
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:31]
:END:
Definition of the associative operad.

***** Answer
Given $V$ a vector space over a field $k$, we define $\mathsf{Assoc}(n)$ as the 1-dimensional
vector space generated by a tree with $n$ leaves. The associative operad is given
by the only possible linear composition and the obvious identity.

**** Algebra over the associative operad                                                                     :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       6c04a468-3844-49d1-808c-4c82cc693453
:DRILL_LAST_INTERVAL: 5.1672
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:25]
:END:
What name receives an algebra over the associative operad?

***** Answer
It is determined by compatibility and the image of $\varphi(\mathsf{Y})$, the image of the only
2-to-1 operation.  If we write $\varphi(\mathsf{Y})$ as multiplication, by compatibility, we have

\[
(v_1 \cdot v_2) \cdot v_3 = v_1 \cdot (v_2 \cdot v_3),
\]

an *associative algebra*.

***** Extra
If we consider the symmetric group action to be trivial, we have $v_1 \cdot v_2 = v_2 \cdot v_{1}$,
a *commutative algebra*.
**** [[https://www.youtube.com/watch?v=N7wNWQ4aTLQ&t=][Associahedra: the shapes of multiplication - Infinite Series]]
**** TODO Associahedra operad
**** TODO The little k-cubes operad
**** TODO The simplex operad

** Categorical logic                                                                                :categories:logic:
*** Sheaves in geometry and logic - MacLane, Moerdijk
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/mac_lane_moerdijk_sheaves_in_geometry_and_logic.pdf
:END:
**** Old notes
***** IV. First properties of Elementary topoi
****** IV.1. Definition of a topos
******* Elementary topos
******** Definition
A *topos* ${\cal E}$ is a category with

  * an object $\Omega$, called the *subobject classifier*;
  * a function ${\cal P}$ on objects, called the *powerset*;
  * two natural isomorphisms $\mathrm{Sub}_{{\cal E}}A \cong \mathrm{hom}_{{\cal E}}(A,\Omega)$ and
    $\mathrm{hom}_{{\cal E}}(B \times A,\Omega) \cong \mathrm{hom}_{{\cal E}}(A,{\cal P}B)$.
******** Alternative definition
A *topos* is a category with finite limits and a function on its
objects ${\cal P}$ such that

\[
\mathrm{Sub}_{{\cal E}}(B \times A) \cong \mathrm{Hom}_{{\cal E}}(A,{\cal P}B)
\]

is an natural isomorphism in $A$.

******** Definition: Elementary form
A *topos* is a category ${\cal E}$ such that

  1) pullbacks exist for every diagram of the form

     \[\begin{tikzcd}[column sep=small, row sep=small]
     X \drar  & & Y\dlar \\
        & B &
     \end{tikzcd}\]

  2) it has a terminal object $1$;

  3) it has a *subobject classifier* $\Omega$ with a monic arrow $\mathtt{true} \colon 1 \to \Omega$ such that
     for every monomorphism $m \colon S \to B$, there is a unique arrow $\mathrm{char}\ m$, called
     the *classifying map* of $m$ such that the following diagram is a pullback

     \[\begin{tikzcd}
     S \dar[swap]{m}\rar & 1 \dar{\mathrm{true}} \\
     B \rar[swap]{\mathrm{char}\ m} & \Omega
     \end{tikzcd}\]

  4) it has a function on objects ${\cal P}$ and an morphism $\in_B\colon B \times {\cal P}B \to \Omega$ such
     that for every $f \colon B \times A \to \Omega$, an unique $g \colon A \to {\cal P}B$ making the following
     diagram commute

     \[\begin{tikzcd}[row sep=small]
     A \ar[dashed]{dd}{g} & B \times A \drar{f}\ar[dashed,swap]{dd}{1 \times g} \\
     & & \Omega \\
     {\cal P}B & B \times {\cal P}B \urar[swap]{\in_B} &
     \end{tikzcd}\]

# Membership map is dinatural in B.

******* Generalized elements
******** Generalized and global elements
An arrow $b \colon X \to B$ is a *generalized element* of $B$ defined over $X$. The generalized
elements defined over the terminal object $1$ are the *global elements* of $B$.

******** Predicate
A morphism $\theta \colon B \to \Omega$ is called a *predicate*.

******** Subobjects
A subobject of $A$ has the following three descriptions

  1) $m \colon S \to A$ monomorphism, as an equivalence class of monics;

  2) $\phi\colon A \to \Omega$, as a predicate;

  3) $s \colon 1 \to {\cal P}A$, as a global element of the powerset.

we call $S = \left\{ a \mid \phi \right\}$ the *extension* of $\phi$; $\phi = \mathrm{char}\ S$ the *characteristic function*
of $S$ and $s = \lceil\phi\rceil$ the *name* of $\phi$.

******** Kronecker delta

******** Monicity of the transpose of the Kronecker delta
For all objects $B$ in a topos, $\left\{ \cdot \right\}$ is monic.

******** Bimorphisms are isomorphisms in a topos
In a topos, every monomorphism is an equalizer and every bimorphism
is an isomorphism.

******* Exponentials
******** Every topos is cartesian closed
Every topos has exponentials.

****** IV.8. Lattice and Heyting algebra objects in a topos
******* Internal lattice
An internal lattice in a category is an object $L$ with morphisms
\[
\bigwedge \colon L \times L \to L,
\qquad
\bigvee \colon L \times L \to L
\]

and commutative diagrams expressint the identities of a lattice

  * associativity,
  * commutativity,
  * idempotent laws,
  * absorption law.

Such a lattice object has a zero and a one if there are arrows
\[
\top \colon 1 \to L,
\qquad
\bot \colon 1 \to L,
\]
with the appropiate identities.

******* Internal Heyting algebra
Exists an implication $\Rightarrow \colon L \times L \to L$ satisfiying the diagrammatic version
of the identities of the implication.

******* Partial order on an internal lattice
We can define a subobject $\leq_L$ on the category as the following equalizer
\[\begin{tikzcd}
\leq_{L}\rar & 
L \times L \rar[shift left=.75ex]{\wedge}\rar[swap,shift right=.75ex]{\pi_1} & 
L.
\end{tikzcd}\]

******** Equivalent definition of internal Heyting algebra

******* External Heyting algebra
Given $A$ in a topos ${\cal E}$, $\mathrm{Sub}\ A$ is a Heyting algebra. The structure
is natural in $A$, in the sense that the pullback along any morphism
$k\colon A \to B$ induces a map $k^{-1}$ of Heyting algebras.

******** TODO Proof
******* Internal Heyting algebra
Given $A$ in a topos ${\cal E}$, ${\cal P}A$ is an internal Heyting algebra. Th structure
is natural in $A$, in the sense that that the any morphism $k\colon A \to B$
induces a map ${\cal P}k \colon {\cal P}A \to {\cal P}B$ of Heyting algebras.

The internal structure of ${\cal P}A$ makes $\mathrm{hom}(X,{\cal P}A)$ an external Heyting
algebra with the following isomorphism of Heyting algebras
\[
\mathrm{Sub}(A \times X) \cong \mathrm{Hom}(X,{\cal P}A).
\]

******** TODO Proof
***** VI.1. The topos of sets
****** Natural numbers object
In an arbitrary topos ${\cal E}$, a *natural numbers object* is the object $\mathbb{N}$
with arrows

\[\begin{tikzcd}
1\rar{0} & \mathbb{N}\rar{s} & \mathbb{N}
\end{tikzcd}\]

which is universal in the sense that for any other object with the same
arrows, we can define a commutative diagram

e\[\begin{tikzcd}
1\rar{0}\dar{\cong} & \mathbb{N}\rar{s}\dar[dashed]{h} & \mathbb{N} \dar[dashed]{h}\\
1\rar{x} & X\rar{f} & X &.\\
\end{tikzcd}\]

******* Recursion as a universal property
Definition by recursion assumes the existence of this universal object.

****** Natural numbers by adjunction
Given ${\cal E}$ with a natural numbers object $\mathbb{N}$ and adjoints $g^{\ast} \dashv g_{\ast}$, the diagram

\[\begin{tikzcd}
1 \cong g^{\ast}(1) \rar{g^{\ast}(0)} &
g^{\ast}(\mathbb{N}) \rar{g^{\ast}(s)} &
g^{\ast}(\mathbb{N})
\end{tikzcd}\]

is a natural numbers object.

******* TODO Proof

****** Boolean topos
A topos ${\cal E}$ is *boolean* when the internal Heyting algebra $\Omega$ is an internal
boolean algebra.

**** Notes for page 33
:PROPERTIES:
:interleave_page_note: 33
:END:

***** Examples of topoi
****** Sets
Small sets with functions between them.

****** n-Sets
Given $n \in \mathbb{N}$, the $n\text{-tuples}$ of sets with $n\text{-tuples}$ of functions between them.

****** G-Sets
Representations of a fixed group $\mu\colon X\times G \to X$, with morphisms preserving
the group action $f(x\cdot g) = f(x) \cdot g$.

****** M-Sets
Representations of a fixed monoid.

****** 2-Sets
Category of functions betweeen sets $\sigma\colon X \to X'$ and commutative squares
between them.

****** N-Sets
Category of sequences $X_0 \to X_1 \to X_2 \to \dots$ of sets and commutative squares
as morphisms.

****** Preseaves
Objects are presheaves $P\colon C^{op} \to \mathtt{Sets}$, and arrows natural transformations
$\theta\colon P \to P'$.

******* Yoneda embedding
$y \colon C \to \Set^{C^{op}}$ is full and faithful.

****** Comma category
Objects are functions $h \colon X \to J$ for a fixed $J$ and arrows
commuting triangles. Each object determines a family of sets

\[
H_j = h^{-1}(j) = \left\{ x \mid x \in X, h(x) = j \right\}
\]

and each function determines a family $f_j \colon H_j \to H'_j$. If we take
$J$ as a discrete category, each object of the comma category is a
functor from it and each morphism is a natural transformation.

That is, we can see the comma category inside a presheaf category
with this assignment

\[
L\colon \Set/J \to \Set^{J}
\]

while in the other direction, each functor provides an object $\bigsqcup H_j$
in the comma category; thus providing a complete equivalence of
categories.

/The comma category is equivalent to a presheaf category./

It is *not* an isomorphism because the composition of both
constructions may not be the identity (the disjoint union is only
unique up to isomorphism).

****** Sheaves over a topological space
Category of sheaves over a topological space.

****** Continuous G-Sets
Given $G$ a topological group, the continuous representations of the group
are the objects with the continuous homomorphisms of actions.

****** Simplicial sets
A *simplicial object* is a family of objects $\left\{ S_n  \right\} \in C$ satisfiying the
simplicial identities

******* TODO Simplicial identities
****** Finite sets
Finite sets and functions between them.

****** Presheaves to finite sets

**** Notes for page 38
:PROPERTIES:
:interleave_page_note: 38
:END:

A *pullback* of $X \to B \gets Y$ is the universal object and
projections making this diagram commute

\[\begin{tikzcd}
P\rar[dashed]{f'} \dar[dashed,swap]{g'} & Y \dar{g} \\
X\rar{f} & B &.
\end{tikzcd}\]

In Sets, we have the set of pairs $\pair{x,y}$ such that $f(x)=g(y)$.

If we have that $g$ is an inclusion, $P$ is the inverse image
$f^{-1}(Y)$, included in $X$. If both $f,g$ are inclusions, we have
the intersection of two subsets.

**** Notes for page 39
:PROPERTIES:
:interleave_page_note: 39
:END:

***** Pullbacks
The pullback of any two presheaves exists on a preseaf category.

The pullback along a monic is always monic.

A category with a terminal object and all pullbacks has all finite limits.

In presheaves, all finite limits are constructed pointwise. 

**** Pullback of group actions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (40 . 0.503994442514762)
:END:

**** Notes for page 41
:PROPERTIES:
:interleave_page_note: 41
:END:

***** Subobject classifier
A *subobject classifier* is a monic $t\colon 1 \rightarrowtail \Omega$ such that for every monic $S \rightarrowtail X$
exists a unique $\Phi \colon X \to \Omega$ creating a pullback square

\[\begin{tikzcd}
S\rar{1} \dar[swap]{i} & 1 \dar{t} \\
X\rar[dashed]{\Phi} & \Omega
\end{tikzcd}\]

where the morphism $t$ is called /true/.

***** Well-powered category
A category is *well-powered* when $\mathrm{Sub}_C(X)$ is isomorphic to a small set
for any $X$.

**** Notes for page 42
:PROPERTIES:
:interleave_page_note: 42
:END:

***** Proposition 1. Existence of a subobject classifier in well-behaved categories
A category $C$ with finite limits and small hom-sets has a subobject classifier
if and only if there is a natural isomorphism

\[
\theta \colon \mathrm{Sub}_C(-) \cong \mathrm{Hom}_C(-,\Omega),
\]

and then, $C$ is also well-powered.

****** Proof
******* Given a subobject classifier
There exists a unique characteristic function for each equivalence
class of monics; an this is a bijection (from the characteristic
function, the monic can be recovered up to isomorphism).

To show that this is natural, given any $f \colon Y \to X$, we have to
prove that

\[\begin{tikzcd}
\mathrm{Sub}(X) \rar{\theta} \dar[swap]{f^{-1}} & \mathrm{hom}(X,\Omega) \dar{\circ f} \\
\mathrm{Sub}(Y) \rar{\theta} & \mathrm{hom}(Y,\Omega)
\end{tikzcd}\]

and this can be proved by the pullback theorem, by which two pullback
squares can be joint in a outer pullback square

\[\begin{tikzcd}
S'\rar{} \dar[swap]{} & S \dar{}\rar & 1\dar{\mathrm{true}} \\
Y\rar{f} & X \rar{\phi} & \Omega
\end{tikzcd}\]

${\cal C}$ is well-powered because all hom-sets are small

******* Given a natural bijection
The identity corresponds to some subobject $t_0 \colon \Omega_0 \to \Omega$. By naturality,
$S = \mathrm{Sub}(\phi)(\Omega_0)$ and every subobject is the pullback of some $\Omega_0$.

Now, we show that $\Omega_0$ is in fact the terminal object. The following
two squares are pullbacks because $t_0$ is monic

\[\begin{tikzcd}
X \rar[bend left]{\phi'} \rar[bend right]{\phi''} \dar[swap]{\mathrm{id}} & 
\Omega_{0} \dar{t_{0}} \\
X \rar[bend left]{t_{0}\phi'} \rar[bend right]{t_{0}\phi''} & \Omega
\end{tikzcd}\]

and by unicity, we have $t_0\phi' = t_0\phi''$ and $\phi' = \phi''$.

**** Notes for page 44
:PROPERTIES:
:interleave_page_note: 44
:END:

***** Classifier for Sets and Finite sets
The classifier $1 \to 2$.

***** Classifier for n-Sets
The classifier is a n-tuple of $1 \to 2$ functions. There are $2^n$
truth values.

***** Classifier for BG-Sets
The same classifier $1 \to 2$ works, $G$ acts trivially on both sets.

***** Classifier for BM-Sets
The same classifier for groups does not work, because complements need not
to be closed under the action of a monoid.

**** Notes for page 50
:PROPERTIES:
:interleave_page_note: 50
:END:

***** Every object is a colimit of representable objects
In a functor category $\mathtt{Sets}^{C^{op}}$, every object $P$ is
the colimit of a diagram of representable objects.

**** Notes for page 55
:PROPERTIES:
:interleave_page_note: 55
:END:

For any small category ${\cal C}$, the functor category $\mathtt{Sets}^{\mathcal{C}^{op}}$ is cartesian closed.

**** Notes for page 57
:PROPERTIES:
:interleave_page_note: 57
:END:

Boolean algebras are algebraic correlates of classical propositional calculus.
Heyting algebras are algebraic correlates of intuitionistic propositional calculus.
**** Notes for page 58
:PROPERTIES:
:interleave_page_note: 58
:END:

***** Lattice
A *lattice* is a partially ordered set, which, interpreted as a category,
has all binary products and coproducts.

That is, $x \leq y$ if and only if $x \to y$.

A lattice with $0 \leq x \leq 1$ are the initial and terminal objects, and has all
finite limits and colimits.

***** Lattice                                                                            :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       91ba55a2-38e0-431b-bfe5-56c721bc7836
:DRILL_LAST_INTERVAL: 4.3514
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:44]
:END:
A lattice is a set with two binary operations â¨ : A Ã A â A, and â§ : A Ã A â A,
following which rules?

****** Answer

 * associativity:    x â§ (y â§ z) = (x â§ y) â§ z
                     x â¨ (y â¨ z) = (x â¨ y) â¨ z

 * commutativity:    x â§ y = y â§ x
                     x â¨ y = y â¨ x

 * absorption law:   x â§ (y â¨ x) = x = (x â§ y) â¨ x

**** Notes for page 59
:PROPERTIES:
:interleave_page_note: 59
:END:

***** Heyting algebra                                                                    :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       d2716609-a6b5-47aa-860f-7e516bccab3a
:DRILL_LAST_INTERVAL: 3.6061
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:43]
:END:
Heyting algebra, definition.

****** Answer
A Heyting algebra is a bicartesian closed poset.

In other words, it has all finite limits, all finite colimits and it
is cartesian closed.

**** Notes for page 64
:PROPERTIES:
:interleave_page_note: 64
:END:
***** Proposition 4.                                                                                        :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       34b1e843-690f-48cf-b370-2217437aa465
:DRILL_LAST_INTERVAL: 4.9606
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 18:42]
:END:
When is a Heyting algebra also a Boolean algebra?

****** Answer
A Heyting algebra is a Boolean algebra

 * if and only if $\neg\neg x = x$ for all elements;
 * if and only if $x \vee \neg x = 1$ for all elements.
**** Notes for page 75
:PROPERTIES:
:interleave_page_note: 75
:END:

***** Sheaf of sets                                                                      :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       9f420f7c-d5e9-4eea-9c8e-8505129e5f02
:DRILL_LAST_INTERVAL: 3.8386
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:34]
:END:
*Sheaf of sets*

****** Answer
A functor $F \colon {\cal O}(X)^{op} \to \mathsf{Set}$ such that each open covering $U = \bigcup_i U_i$ yields
an equalizer diagram

\[\begin{tikzcd}
FU \rar[dashed]{e} &
\prod_i FU_i \rar[yshift=0.5ex]{p}\rar[yshift=-0.5ex,swap]{q} &
\prod_{i,j} F(U_i \cap U_j)
\end{tikzcd}\]

where for $t \in FU$, we have $e(t) = (t_{|U_i})_{i\in I}$ and for a family $t_i \in FU_i$ we
have $p(t_i)_{i \in I} = (t_{i|U_i \cap U_j})_{i,j \in I}$ and $q(t_i)_{i \in I} = (t_{j|U_i \cap U_j})_{i,j\in I}$.

**** Notes for page 76
:PROPERTIES:
:interleave_page_note: 76
:END:

***** Morphism of sheaves                                                                                   :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       84046292-d887-4b2e-a603-72ec55322c48
:DRILL_LAST_INTERVAL: 4.3175
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:58]
:END:
What is a morphism between sheaves $F \to G$.

****** Answer
A natural transformation between them; they are functors.

**** Notes for page 170
:PROPERTIES:
:interleave_page_note: 170
:END:

The change of base functor preserves all topos structure.

**** Notes for page 171
:PROPERTIES:
:interleave_page_note: 171
:END:

**** Notes for page 172
:PROPERTIES:
:interleave_page_note: 172
:END:

***** Elementary form                                                                                       :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       25e51781-23db-44db-b36c-79904e44a812
:DRILL_LAST_INTERVAL: 4.0675
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:34]
:END:
Elementary definition of a topos.

****** Answer
A category with

 * all pullbacks

 * a terminal object

 * a subobject classifier

 * a powerset for every object.

**** Notes for page 175
:PROPERTIES:
:interleave_page_note: 175
:END:

**** Notes for page 176
:PROPERTIES:
:interleave_page_note: 176
:END:

***** TODO Every topos has exponentials                                                                   :theorem:

***** Topos                                                                                                 :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       0af09c4d-0a39-4034-97c8-95bb2c2479cc
:DRILL_LAST_INTERVAL: 5.0251
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:23]
:END:
Definition from cartesian closed categories.

****** Answer
A topos is a cartesian closed category with equalizers and a subobject
classifier.

**** Notes for page 185
:PROPERTIES:
:interleave_page_note: 185
:END:

***** Topos, finite limits and finite colimits                                                              :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       31d80b9e-40a7-403a-a4cd-b4f25462a68d
:DRILL_LAST_INTERVAL: 5.2485
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:25]
:END:
Has a topos finite limits? why? has it finite colimits? why?

****** Answer
*A topos has all finite limits and finite colimits* 

It has finite limits by definition (cartesian closed with equalizers).
It has finite colimits; this is not trivial and must be proven using
monads.

**** Notes for page 187
:PROPERTIES:
:interleave_page_note: 187
:END:

***** Comparison functor                                                                                    :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       170873b3-03ec-45ed-87c1-b30c9298aa3a
:DRILL_LAST_INTERVAL: 4.9561
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:31]
:END:
Construct the comparison functor $K$ over a $G \colon A \to C$ with a
left adjoint $F \colon C \to A$.

****** Answer
We construct a monad $GF$ and the following construction where
$C^T$ is the category of T-algebras over $GF$.

\[\begin{tikzcd}
A \rar{K}\dar[bend left]{G} & C^T\dar[bend left]{F^T} \\
C \rar{1}\uar[bend left]{F} & C\uar[bend left]{G^T}
\end{tikzcd}\]

The comparison functor $K \colon A \to C^T$ is given by $(GA, G\epsilon_A \colon GFGA \to GA)$.

***** Monadic functor                                                                                       :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       a5ca3d08-dfeb-498a-b2a7-419f30c05272
:DRILL_LAST_INTERVAL: 4.0224
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:32]
:END:
When is $G \colon A \to C$ monadic?

****** Answer
Has a left adjoint $F \dashv G$ and the comparison functor $K \colon A \to C^T$
given by $(GA, G\epsilon_A \colon GFGA \to GA)$ is an equivalence of categories.

***** Property of monadic functors                                                                          :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       5ba579f3-e5c2-40a0-87a1-e833366be045
:DRILL_LAST_INTERVAL: 5.1775
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:59]
:END:
How do monadic functors and limits relate?

****** Answer
Monadic functors create all limits.

**** Notes for page 189
:PROPERTIES:
:interleave_page_note: 189
:END:

***** Left adjoint of the powerset functor                                                                  :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       8b55fb4c-ba74-4720-bece-fd229260205f
:DRILL_LAST_INTERVAL: 5.6307
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:29]
:END:
What is the left adjoint of the powerset functor $P \colon {\cal E}^{op} \to {\cal E}$?

****** Answer
$P^{op} \colon {\cal E} \to {\cal E}^{op}$, that is, the same functor
but acting on the opposite category.

**** Notes for page 190
:PROPERTIES:
:interleave_page_note: 190
:END:

***** Left adjoint of the powerset functor
Why is the left adjoint of the powerset functor itself?

****** Answer
The product is commutative, so

\[
{\cal E}(A,PB) \cong
{\cal E}(A \times B,\Omega) \cong
{\cal E}(B \times A,\Omega) \cong
{\cal E}(B,PA) =
{\cal E}^{op}(PA,B).
\]

**** Notes for page 199
:PROPERTIES:
:interleave_page_note: 199
:END:

***** The slice of a topos                                                                                  :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       df04a625-5b2f-4579-8b5e-6a90430849c0
:DRILL_LAST_INTERVAL: 4.5982
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:31]
:END:
Structure of the slice of a topos ${\cal E}/B$.

****** Answer
It is again a topos.

**** Notes for page 207
:PROPERTIES:
:interleave_page_note: 207
:END:

***** Internal lattice
An *internal lattice* in a category is an object $L$ with morphisms

\[
\wedge \colon L \times L \to L,
\qquad
\vee \colon L \times L \to L
\]

and commutative diagrams expressing the identities of a lattice

  * associativity,
  * commutativity,
  * idempotent laws,
  * absorption law.

Such a lattice object has a zero and a one if there are arrows

\[
\top \colon 1 \to L,
\qquad
\bot \colon 1 \to L,
\]

with the appropiate identities.

**** Notes for page 208
:PROPERTIES:
:interleave_page_note: 208
:END:

Heyting algebra objects from lattices. ([[id:d2716609-a6b5-47aa-860f-7e516bccab3a][Heyting algebra]])

*** Higher order categorical logic - Lambek, Scott
***** 0. Introduction to category theory
****** 0.1. Categories and functors
****** 0.2. Natural transformations
****** 0.6. Triples
****** 0.7. Examples of cartesian closed categories
***** I. Cartesian closed categories and \lambda-calculus
***** II. Type theory and toposes
***** III. Representing numerical functions in various categories
*** Introduction to categorical logic (2017) - Bauer, Awodey
**** II. Type theories
***** II.1. Algebraic theories
******* Signatures
A *signature* $\left\{ \Sigma_k \right\}$ is a family of sets of k-ary operations. Its *terms*
are constructed inductively knowing that

 * variables are terms.
 * given $\left( x_1,\dots,x_{k} \right)$ a k-uple of terms and $f \in \Sigma_k$, $f(x_1,\dots,x_{k})$ is a term.

******* Algebraic theories
An *algebraic theory* $\mathbb{A} = (\Sigma,A)$ is a signature and a set of equation
between its terms.

They are also called /equational theories/ and /Lawvere theories/.

******** Examples
********* Theory of groups
********* Theory of unital commutative rings
********* Theory of sets
********* Theory of pointed sets
********* Theory of R-modules
********* Counterexample: theory of fields
********* Theory of inductive datatypes

****** II.1.1. Many-sorted algebraic theories
******* Examples
******** Theory of left modules
******** Theory of graphs
******** Theory of symmetric graphs
******** Theory of a RAM
****** II.1.2. Models of algebraic theories
The motivation is to generalize the classical notions of algebraic
structures to morphisms and commutative diagrams.

******* Interpretation
An *interpretation* of a theory $\mathbb{A}$ on a category ${\cal C}$ with finite products
is given by an object $I\mathbb{A} \in \mathrm{obj}({\cal C})$ and a morphism for each operation of 
arity $k$,

\[
\forall f \in \Sigma_k,\qquad If : (I\mathbb{A})^k \to I\mathbb{A}.
\]

The interpretation of a term with a *context* of variables, $x_1,\dots,x_n \mid t$,
is given inductively by

 1. the interpretation of $x_i$ is the projection $\pi_i$.
 2. a term $f(t_1,\dots,t_k)$ is interpreted as the composition of the 
    interpretation of every subterm with the interpretation of $f$ as

    \[\begin{tikzcd}[column sep=huge]
    (I\mathbb{A})^n  \rar{(It_1,\dots,It_k)}  &
    (I\mathbb{A})^k  \rar{If}  &
    \mathbb{A}.
    \end{tikzcd}\]
    
Note that the interpretation of a term depends on the context.

******* Satisfacibility of equations
An equation $u=v$ of two terms in the same context is *satisfied*
by the interpretation $I$ if $Iu = Iv$ as morphisms.

******* Models of algebraic theories
A *model* is an interpretation that satisfies all the axioms of the
theory.

****** II.1.3. Algebraic theories as categories
The motivation is to have a general theory of what is a group,
independent from the choice of basic constants, operations and axioms.

******* Category of an algebraic theory
Given $\mathbb{A}$, we take as objects all the possible contexts $\left[ x_1,\dots,x_n \right]$ and
tuples $\langle t_1,\dots,t_n \rangle : [x_1,\dots,x_m] \to [x_1,\dots,x_n]$ of terms with context the
domain as morphisms.

Two morphisms are equal iff the axioms imply $t_k = t_{k}'$ on every $k$; and
the composition of morphisms $v = u \circ t$ is done by substitution

\[
v_i = u_i \left[ t_1,\dots,t_m / x_1,\dots,x_m \right].
\]

******** Closed to products
The product of $\left[ x_1,\dots,x_n \right]$ and $\left[x_1,\dots,x_m \right]$ exists as $\left[ x_1,\dots,x_{n+m} \right]$
in this category. Every object is a product of finitely many instances
of $[x_1]$.

******* Algebraic theory (alternative definition)
An *algebraic theory* is a small category with finite products whose
objects are $A^0,A^1,A^2,\dots$ such that $A^m\times A^n = A^{m+n}$.

******** Algebraic theory in the former sense
The basic operations of $\Sigma_k$ are the morphisms $A^k \to A$. An equation
$u = v$ is an axiom if the canonical interpretations of each morphism
being interpreted by itself coincide.

******* Examples
******** Algebraic theory of smooth maps
******** Algebraic theory of total recursive functions
******** Algebraic theory of an object
****** II.1.4. Models of algebraic theories as functors
******* Interpretations as functors
Given a model $M$ of $\mathbb{A}$ in ${\cal C}$, the interpretation is a functor $M : \mathbb{A} \to {\cal C}$
defined by

\[
M[x_1,\dots,x_k] = (M\mathbb{A})^k
\]

on objects and by the following rules on morphisms

 1) the morphism $\left\langle x_i \right\rangle : \left[ x_1,\dots,x_k \right] \to [x_1]$ is mapped to $\pi_i : (M\mathbb{A})^k \to M\mathbb{A}$.
 2) the morphism $\left\langle f(t_1,\dots,t_m) \right\rangle : [x_1,\dots,x_k] \to [x_{1}]$ is mapped into the
    composition

    \[\begin{tikzcd}[column sep=huge]
    (M\mathbb{A})^m \rar{(Mt_1,\dots,Mt_m)} &
    (M\mathbb{A})^k \rar{Mf} &
    \mathbb{A}
    \end{tikzcd}\]

 3) the morphism $\left\langle t_1,\dots,t_m \right\rangle : [x_1,\dots,x_k] \to [x_1,\dots,x_{m}]$ is mapped to the
    morphism $\left\langle Mt_1,\dots,Mt_m \right\rangle$, where $Mt_i$ is the value of $M\left\langle t_i \right\rangle$.

This interpretation is a in fact a functor.

******** The interpretation is a functor 
As $M$ is a model, all the equations of the theory are satisfied by
it. This preserves the identities given by composition of morphisms.

******* Model (alternative definition)
A *model* of $\mathbb{A}$ in ${\cal C}$ is a functor preserving finite products.

******* Category of models
The category $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ of models of the theory $\mathbb{A}$ in ${\cal C}$ has models
$M : \mathbb{A} \to {\cal C}$ as objects and natural transformations as morphisms.

******* Algebraic categories
An algebraic category is a category that is equivalent to a
category of models of an algebraic theory.

******** Examples
********* Category of groups
********* Category of C-rings
****** II.1.5. Completeness and universal models
******* Categorical logic
Categorical logic has two sides, the logical and the categorical.
The logic consists of

  1) A *type theory*, a calculus of types and terms. In the case
     of algebraic theories, there is only one type.
  2) A *logic*. In the case of algebraic theories, the logic only
     involves equations.
  3) A *theory* given by basic types, terms and axioms.
  4) *Interpretations and models*. The type theory and logic are
     interpreted denotationally in a category with enough structure.
     In the case of algebraic theories, those are categories with
     finite products.

There are special cases of simple logics

  * a /single-sorted logic/ if there is only one type.
  * a /type theory/ if there is only a very simple type system.
  * in /ML-type systems/, logic and types are identified.

And complementary to a logical system, we have its categorical
semantics

  1) Theories are categories. The structure of a category hides
     sintactic details and reflects types and logic.
  2) Models are functors. They go from theories to categories with
     richer structure, preserving the structure of the theory.
  3) Homomorphisms are natural transformations.
  4) Completeness and universal models. It is desirable for a
     categorical semantics to be complete or to have universal
     models.

******* Semantic completeness
The property that gives that if every model of $\mathbb{A}$ satisfies
an equation, the equation can be proved on the algebraic theory is
called *semantic completeness*.

******* Completeness for algebraic theories
Given $\mathbb{A}$ algebraic theory, exists a model $U \in \mathtt{Mod}_{{\cal A}}(\mathbb{A})$ called the
*universal model* for $\mathbb{A}$, such that,

\[
U \text{ satisfies } u = v
\iff
\mathbb{A} \text{ proves } u = v.
\]

As a corollary, categorical semantics of algebraic theories is
complete.

******** Proof
We can simply take $U = 1_{\mathbb{A}} : \mathbb{A} \to \mathbb{A}$ as a model, which clearly
identifies $1(f)=1(g)$ if and only if $f = g$.

******* Universal model on generalized sets
The Yoneda embedding $y : \mathbb{A} \to \widehat{\mathbb{A}}$ is a universal model for $\mathbb{A}$.

******** Proof
The embedding $y$ preserves limits and therefore, finite products.
It is a functor and a faithful one, which makes $\widehat{\mathbb{A}}$ an universal
model.

***** II.2. Cartesian closed categories
****** II.2.1. Exponentials
******* Exponentials
In a category with binary products ${\cal C}$, an *exponential* is an object $B^A$
with an evaluation morphism $e : B^A \times A \to B$ such that for every $f : C \times A \to B$
exists a unique $\widetilde f : C \to B^A$ for which this diagram commutes

\[\begin{tikzcd}
B^A &
B^A \times A \drar{e} &
\\
C \uar[dashed]{\widetilde f} &
C \times A \uar{{\widetilde f} \times 1_A} \rar[swap]{f} &
B
\end{tikzcd}\]

This is the universal property of exponentials.

******* Exponentiable object
An object $A \in \mathrm{obj}({\cal C})$ is exponentiable if $B^A$ exists for every $B$.

******* Characterization of exponentiable objects
An object $A$ is exponentiable iff the functor $- \times A$ has a right
adjoint $-^{A}$.

******** TODO Proofs

******* Examples
******** Propositional calculus
****** II.2.2. Cartesian closed categories
******* Cartesian category
A *cartesian category* is a category that has finite products.

******* Cartesian closed category
A *cartesian closed category* is a cartesian category with
exponentials.

******* Characterization of cartesian closed categories by adjoints
The category ${\cal C}$ is cartesian closed if the following functors
have adjoints

 * the functor $! : {\cal C} \to 1$ to the terminal category.
 * the diagonal functor $\Delta : {\cal C} \to {\cal C} \times {\cal C}$.
 * the product by every object $-\times A : {\cal C} \to {\cal C}$.

******** TODO Proof

******* Characterization of cartesian closed categories by equations
A category ${\cal C}$ is cartesian closed if and only if

  1) $1 \in {\cal C}$ and exists a morphism $! : A \to 1$ for every $A \in {\cal C}$.
  2) a product $A \times B$ with projections and the universal property
     of the product.
  3) an exponential $B^A$ with an evaluation map and the universal
     property of the exponential.

We write the projections as $\pi_0,\pi_1$; the unique function from the product
as $\left\langle f,g \right\rangle$ and $f \times g = \left\langle f \circ \pi_0, g \circ \pi_1 \right\rangle$. The types satisfy

  1) for every $f : A \to 1$, $f = !$.
  2) for all functions $\pi_0 \circ \left\langle f,g \right\rangle = f$, $\pi_1 \circ \left\langle f,g \right\rangle = g$ and $\left\langle \pi_0\circ h, \pi_1\circ h \right\rangle = h$.
  3) for all functions $e \circ (\widetilde{f} \times 1) = f$ and $(e \circ (g \times 1))^{\sim} = g$.

******* Examples of cartesian closed categories
******** Sets with hom-objects
******** Categories with functor categories
******** Presheaf category of a small category
****** II.2.3. Frames
******* Complete poset is cocomplete poset
A poset is complete iff it is cocomplete.

******** TODO Proof
******* TODO Infinite distributive law
******* Frames
A *frame* is a complete and cartesian closed poset. That is, it
is a complete poset with the distributive law

\[ x \wedge \bigvee_{i\in I} y_i = \bigvee_{i\in I} x \wedge y_i.
\]

******* Frame morphisms
A *frame morphism* is a $f : L \to M$ between frames preserving finite
infima and arbitrary suprema.

****** II.2.4. Heyting algebras
******* Lattices
A *lattice* is a poset with finite limits and colimits.

******* Lattice homomorphisms

** Type theory                                                                                                 :types:
*** Basic type theory
**** Canonicity                                                                                              :drill:
SCHEDULED: <2019-03-05 Tue>
:PROPERTIES:
:ID:       767fa380-22e3-4907-810a-a22a137c7808
:DRILL_LAST_INTERVAL: 247.7323
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:55]
:END:
Definition of *canonicity*.

***** Definition
A type theory enjoys canonicity if every closed term can be written
in a canonical form, only using constructors of the type. In particular,
if every term of type $\mathbb{N}$ is a numeral of the form $S(S(\dots(SZ)\dots))$.

***** Canonicity and axioms
Canonicity can be lost when we add axioms. If we add LEM,
$\mathrm{case}(\mathrm{lem}(P), 0, 1)$ is a non-canonical natural.

*** Mikrokosmos - Mario RomÃ¡n                                                            :lambda_calculus:functional:
**** CÃ¡lculo lambda sin tipos
Una expresiÃ³n lambda es

 * una variable,
 * una aplicaciÃ³n de dos expresiones $M\ N$,
 * una abstracciÃ³n $(\lambda x.M)$, donde $M$ es un tÃ©rmino que depende de $x$.

Una abstracciÃ³n aplicada a otro tÃ©rmino se puede reducir como

\[
(\lambda x.M)\ N \longrightarrow_{\beta} M[N/x]
\]

y las aplicaciones asocian a izquierda: $M\ N\ P$ se lee como $(M\ N)\ P$
en vez de $M\ (N\ P)$.

\[

\]

**** El intÃ©rprete
PodÃ©is instalarlo desde github si tenÃ©is Haskell y si no, podÃ©is usarlo
directamente desde la pÃ¡gina web

 * https://github.com/m42/mikrokosmos
 * https://m42.github.io/mikrokosmos/tutorial.html

En Mikrokosmos, las lambdas se escriben como una *barra invertida*, y
el programa responde con la expresiÃ³n lambda y una lista de posibles
nombres que tiene esa expresiÃ³n.

#+BEGIN_SRC haskell
mikro> (\x.x)
Î»a.a â I, ifelse, id
#+END_SRC

Para ver cÃ³mo funciona, se pueden probar algunas expresiones aritmÃ©ticas
simples

#+BEGIN_SRC haskell
mult 2 3
plus 3 4
and true false
sum (take 5 naturals)
#+END_SRC

CaracterÃ­sticas:
 
 * los argumentos van separados por espacios,
 * se entiende asociatividad a izquierda, y
 * se permite aplicaciÃ³n parcial.

Es un pequeÃ±o lenguaje de programaciÃ³n y estÃ¡ completamente basado en el 
cÃ¡lculo lambda. Quiero explicaros cÃ³mo se puede obtener un lenguaje de
programaciÃ³n desde el cÃ¡lculo lambda.

**** Primeras definiciones
Vamos a usar cÃ¡lculo lambda. Las expresiones lambdas se leen
como 

#+BEGIN_SRC haskell
(\x.\y.plus x y)
plus 3 4
(\e.plus e e) 3
#+END_SRC

Diciendo: esta es una funciÃ³n que toma =x= e =y= y devuelve =x+y=.
Lo que vamos a aprender es cÃ³mo funcionan por dentro los nÃºmeros
o la funciÃ³n =plus=.

La funciÃ³n *identidad* y la funciÃ³n *constantemente*.

#+BEGIN_SRC haskell
id = \x.x
const = \x.\y.x

id id
id const
id 3
id 5
const 4 2
const 4 3
const 4 (id (const id id))

devuelvecuatro = const 4
devuelvecuatro 5
#+END_SRC

**** TÃ©cnica de Church
Queremos usar estructuras de datos. Tenemos primero que escribir
la estructura de datos como constructores y hacer depender de ellos
a los tÃ©rminos.

#+BEGIN_SRC haskell
true = \t.\f.t
false = \t.\f.f

0 = \s.\z.z
1 = \s.\z.s z

cons = \h.\t.\c.\n.c h (t c n)
nil = \c.\n.n
#+END_SRC

**** LibrerÃ­a
***** BÃ¡sica
#+BEGIN_SRC haskell
id = \x.x
const = \x.\y.x
compose = \g.\f.\x.g (f x)
#+END_SRC

***** Booleanos
#+BEGIN_SRC haskell
true = \x.\y.x
false = \x.\y.y

not = \p.p false true

and = \p.\q.p q false
and = \p.\q.p q p

or = \p.\q.p true q
or = \p.\q.p p q
#+END_SRC

***** AritmÃ©tica bÃ¡sica
#+BEGIN_SRC haskell
0 = \f.\x.x
succ = \n.\f.\x.f (n f x)

plus = \m.\n.n succ m
plus = \m.\n.(\f.\x.n f (m f x))

mult = \m.\n.compose m n
mult = \m.\n.\f.\x.m (n f) x

iseven = \n.n not true
iszero = \n.n (const false) true
#+END_SRC

***** Tuplas
#+BEGIN_SRC haskell
tuple = \x.\y.\z.z x y

first = \p.p true
second = \p.p false

pred = \n.first (n (\t.tuple (first t) (succ (first t))) (tuple 0 0))
minus = \m.\n.n pred m
leq = \m.\n.iszero (minus m n)
geq = \m.\n.iszero (minus n m)
#+END_SRC

***** Listas
#+BEGIN_SRC haskell
nil = \c.\n.n
cons = \h.\l.(\c.\n.c h (l c n))

fold = \o.\n.\l.l o n

sum = fold plus 0
prod = fold mult 1
all = fold and true
any = fold or false
length = fold (\h.\t.)

map = \f.fold (\h.\t.cons (f h) t) nil
filter = \p.fold (\h.t.(p h) (cons h t) t) nil

head = fold const nil
tail = \l.first (l (\a.\t.tuple (second t) (cons a (second t))) (tuple nil nil))
take = \n.\l.first (n (\t.tuple (cons (head (second t)) (first t)) (tail (second t))) (tuple nil l))
#+END_SRC

***** Ãrboles
#+BEGIN_SRC haskell
nil = \d.\n.n
node = \x.\l.\r.\d.\n.(d x (l d n) (r d n))
#+END_SRC

***** RecursiÃ³n
#+BEGIN_SRC haskell
omega := (\x.x x)(\x.x x)
fix := (\f.(\x.f (x x)) (\x.\f (x x)))

fact := fix (\f.\n.iszero n 1 (mult n (f (pred n))))
fib :=  fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n)))))

infinity := fix succ
naturals := fix (compose (cons 0) (map succ))
#+END_SRC

***** Tipos
#+BEGIN_SRC haskell

#+END_SRC
*** Lecture notes on the lambda calculus - Salinger                                                 :lambda_calculus:
**** 1. Introduction
**** 2. The untyped lambda calculus
***** 2.5. Formal definitions of \beta-reduction and \beta-equivalence
****** \beta-equivalence
The \beta-equivalence $M =_{\beta} M'$ is the symmetric transitive closure
of the \beta-reduction $\rightarrow_{\beta}$.

**** 3. Programming in the untyped lambda calculus
**** 4. The Church-Rosser Theorem
***** 4.1. Extensionality, \eta-equivalence, and \eta-reduction
****** Extensionality principle
The *principle of extensionality* is defined as the following rule

\[\begin{prooftree}
\LeftLabel{($ext_{\forall}$)}
\AxiomC{$\forall A. M A = M' A$}
\UnaryInfC{$M = M'$}
\end{prooftree}\]

****** Single step \eta-reduction
****** Single step \beta-reduction

***** 4.2. Statement of the Church-Rosser Theorem
Let $\twoheadrightarrow$ be $\twoheadrightarrow_{\beta}$ or $\twoheadrightarrow_{\beta\eta}$; and lambda terms such that $M\twoheadrightarrow N$ and $M \twoheadrightarrow P$; then
there exists a term $Z$ such that $N \twoheadrightarrow Z$ and $P \twoheadrightarrow Z$.

\[\begin{tikzcd}[column sep=small]
& M \drar[two heads] \dlar[two heads] & \\
N \drar[two heads,dashed] & & P \dlar[two heads,dashed] \\
& Z & \\
\end{tikzcd}\]

***** 4.3. Preliminary remarks on the proof of the Church-Rosser theorem
****** Church-Rosser property

\[\begin{tikzcd}[column sep=small]
& M \drar[two heads] \dlar[two heads] & \\
N \drar[two heads,dashed] & & P \dlar[two heads,dashed] \\
& Z & \\
\end{tikzcd}\]

****** Semidiamond property

\[\begin{tikzcd}[column sep=small]
& M \drar[] \dlar[] & \\
N \drar[two heads,dashed] & & P \dlar[two heads,dashed] \\
& Z & \\
\end{tikzcd}\]

****** Diamond property

\[\begin{tikzcd}[column sep=small]
& M \drar[] \dlar[] & \\
N \drar[dashed] & & P \dlar[dashed] \\
& Z & \\
\end{tikzcd}\]

****** Relationship between properties
***** 4.4. Proof of the Church-Rosser Theorem (Tait & Martin-LÃ¶f)
****** Parallel one-step reduction
We define the *parallel one-step reduction* as the smallest relation
satisfying

\[\begin{prooftree}
\LeftLabel{(1)}
\AxiomC{$a$}
\UnaryInfC{$x \rhd x$}
\end{prooftree}\]

\[\begin{prooftree}
\LeftLabel{(2)}
\AxiomC{$P \rhd P'$}
\AxiomC{$N \rhd N'$}
\BinaryInfC{$PN \rhd P'N'$}
\end{prooftree}\]

\[\begin{prooftree}
\LeftLabel{(3)}
\AxiomC{$N \rhd N'$}
\UnaryInfC{$\lambda x. N \rhd \lambda x.N'$}
\end{prooftree}\]

\[\begin{prooftree}
\LeftLabel{(4)}
\AxiomC{$Q \rhd Q'$}
\AxiomC{$N \rhd N'$}
\BinaryInfC{$(\lambda x. Q) N \rhd Q'[N' / x]$}
\end{prooftree}\]

\[\begin{prooftree}
\LeftLabel{(5)}
\AxiomC{$P \rhd P'$, where $x \notin \mathrm{FV}(P)$}
\AxiomC{$N \rhd N'$}
\BinaryInfC{$(\lambda x. Q) N \rhd Q'[N' / x]$}
\end{prooftree}\]

****** TODO Lemmas on the parallel one-step reduction

****** Proof of the Church-Rosser Theorem
We know that $\rhd$ satisfies the diamond property, so its reflexive transitive
closure $\rhd^{\ast}$ also satisfies it. We use now that $\rhd^{\ast}$ is the same as $\twoheadrightarrow_{\beta\eta}$ and
that the diamond property for $\twoheadrightarrow_{\beta\eta}$ is the Church-Rosser property for $\twoheadrightarrow$.

**** 5. Combinatory algebras
***** 5.1. Applicative structures
****** Applicative structure
An *applicative structure* $(\mathbf{A},\cdot)$ is a set with a binary operation, that
can be non-associative.

****** Polynomials of applicative structures
A *polynomial* on an applicative structure $(\mathbf{A},\cdot)$ is a formal expression built
with the binary operation on variables and coefficients. It is the set of
expressions built from the grammar

\[
t,s ::= x \mid a \mid ts,
\]

where $x$ is a variable and $a \in A$.

***** 5.2. Combinatory completness
****** Combinatory completness
****** SK characterization of combinatory completness
***** 5.5. Lambda algebras

**** 6. Simply-typed lambda calculus, propositional logic, and the Curry-Howard isomorphism
***** 6.1. Simple types and simply-typed terms
****** Basic types
We assume a set of *basic types* to exist.

****** Simple types
The set of *simple types* is given by the BNF

\[
A,B ::= \iota\mid A \to B \mid A \times B \mid 1
\]

where $\iota$ is a [[*Basic types][basic type]] and $1$ is a one-element type.

****** Raw types lambda terms
The set of *typed lambda terms* is given by the BNF

\[ \mathtt{Term} ::=
\ast \mid
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term} \mid
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2\mathtt{Term}
\]

where $\ast$ will be the unique element of type $1$. Besides the
previously considered term application, we now introduce a typed
lambda abstraction and an explicit construction of the pair element with
its projections.

****** Typing rules for the simply-typed lambda calculus
***** 6.2. Connections to propositional logic
**** 7. Weak and strong normalization
***** 7.1. Definitions
***** 7.2. Weak and strong normalization in typed lambda calculus
**** 8. Polymorphism
System F is obtained extending the typed lambda calculus with the quantifier $\forall$.

***** 8.4. Church-Rosser property
**** 9. Type inference
A /type inference algorithm/ decides, given a term, whether it is typable or
not, and outputs a type if it is.

**** 10. Denotational semantics
Denotational semantics give an interpretation of the lambda calculus using
mathematical objects.

**** 11. The language PCF
**** 12. Complete partial orders
**** 13. Denotational semantics of PCF
*** Types and programming languages - Pierce
**** Preface
***** 1. Introduction
***** 2. Mathematical preliminaries
**** I. Untyped Systems
***** 3. Untyped arithmetic expressions
***** 4. An ML implementation of arithmetic expressions
***** 5. The untyped lambda calculus
***** 6. Nameless representation of terms
***** 7. An ML implementation of the lambda-calculus
**** II. Simple typesa
*** Dependent types at work - Ana Bove, Peter Dybjer
**** 1. What are dependent types?
**** 2. Simply Typed Functional Programming in Agda
***** 2.1. Truth Values
***** 2.2. Natural numbers
****** Notion of Inductive type
      /Recursive types/ in Haskell are *inductive types* in constructive type
      theory.
****** Notion of Canonical form
      Elements on canonical form are built up by constructors only. They do not
      contain defined functions. Martin-LÃ¶f considers /lazy canonical forms/, where
      it suffices to begin with a constructor:

      #+BEGIN_SRC haskell
      Zero * Zero        -- Not a canonical form
      Succ (Zero + Zero) -- Lazy canonical form
      Succ (Succ Zero)   -- Canonical form
      #+END_SRC
      
***** 2.3. Lambda Notation and Polymorphism
     In Agda we have no type variables, we have families of functions:

     #+BEGIN_SRC 
     id : (A : Set) -> A -> A
     id = \(A : Set) -> \(x : A) -> x
     #+END_SRC

***** 2.4. Implicit Arguments
     Implicit arguments are declared by enclosing their typings within curly 
     braces.

***** 2.5. GÃ¶del System T
     GÃ¶del System T is a system of primitive recursive functionals. All typable
     programs in GÃ¶del System T terminate. We can only use Î²-reduction and the
     definitions of:

     #+BEGIN_SRC 
     true
     false
     zero
     succ
     if_then_else
     natrec
     #+END_SRC

     We can define all primitive recursive functions, but also others such as the
     Ackermann fuction.

***** 2.6. Parametrised Types
***** 2.7. Termination-checking
     In M-L Type Theory, all recursion is *primitive recursion*; a structural
     recursion on the well-founded data types.

     As the Agda's termination-checker has not yet been documented, if Agda will
     be used as a system for formalising mathematics rigorously, it is advisable to
     stay within a well-specified subset such as Martin-LÃ¶f type theory.

     In fact, the termination checker will not recognize calls to non-constructors
     as smaller arguments. =(m-n)= will not be recognized as smaller than =m=,
     for example.

**** 3. Dependent Types
***** 3.1. Vectors of a given length
     We have to alternatives to define vectors of a given length:
     
     - *As a Recursive Family*:
       
       #+BEGIN_SRC 
       Vec : Set -> Nat -> Set
       Vec A zero = Unit
       Vec A (succ n) = A X Vec A n
       #+END_SRC

       Functions must be written by induction on the length of the vector.

     - *As an Inductive Family*:

       #+BEGIN_SRC 
       data Vec (A : Set) : Nat -> Set where
         [] : Vec A zero
	 _::_ : {n : Nat} -> A -> Vec A n -> Vec A (succ n)
       #+END_SRC
       
     We can use type-checking to define functions that work only over non-empty
     vectors, such as =tail= or =head=.

***** 3.2. Finite Sets
     This data type is useful when we want to access the element at a certain
     position in a vector.

***** 3.3. More Inductive Families
**** TODO 4. Propositions as Types
*** The derivative of a regular type is its type of one-hole contexts - Connor McBride                  :type_algebra:
Presented by [[https://www.youtube.com/watch?v=K7tQsKxC2I8][Erik Hinton]].

**** Types and fixed points
Empty type, unit type, product and other basic types.
We use parametric types with type variables.

# We need inductive types and the W from ML-theory?
Fixed points are used to define types. Naturals are
the fixed point of $Z + S x$. We write the fixed point
of a formula $F$ over a variable $x$ as $\mu x.F$.

\[
\mathtt{Nat} = \mu x. 1 + x
\]

**** Zippers and holes
One-hole contexts with respect to some interior type. A zipper is a
one-hole context of a type and the value that was removed.

**** Derivatives
To find the type of a context of type $T$ with a hole in place of some
$x$, take the partial derivative of $T$ with respect to $x$.

Partial derivatives with respect of a type variable work directly.

Product and sum rules can be proved. Chain rule can be proved.

**** Recursive derivatives

**** Questions
Negative and fractional types. Algebraic types and the field of rationals.
Computing on the field of rationals.
*** Semantics for type theory                                                                                 :types:
# Notes on paper.

**** Formal languages and semantics
Traditional definition of formal languages. An alphabet and rules to
inductively construct words. We call $L$ the set of valid strings.

***** Example: boolean algebra
 * symbols $\Sigma = \left\{ p_1,\dots,p_n, \wedge,\neg,\implies,(,),\top,\bot,\dots \right\}$ 
   atomic propositions and logical symbols.
 * valid strings are defined inductively as
   * singletons $p_i$, $\top$ or $\bot$
   * connectors: if $a, b$ are valid, so are $a \wedge b, a \vee b, \neg a, \dots$

***** Semantics
The semantics assigns to each string a number $0$ or $1$,

\[
s \colon L \to \left\{ 0,1 \right\}
\]

and this evaluation will depend on the evaluation of atomic propositions.
If we call $\Omega = \left\{ p_1,\dots,p_n \right\}$, it depends on a valuation $v \colon \Omega \to \left\{ 0,1 \right\}$.
Connectives are interpreted naturally.

***** Observation
Sometimes $s_v(a) = s_v(b)$ regardless of $v$; in particular, $s_v(a) = 1$ regardless
of $v$ for some formulae (tautologies and absurds).

\[
s_v(A \wedge B) = s_v(B \wedge A)
\]

**** Simply typed lambda calculus
***** Definition
Taking a inductively defined set of types

 * given type $I$
 * function types $\alpha \to \beta$

The set $\Sigma$ of symbols has countably many variables of each type and the symbols of
lambda calculus $(,),\lambda, O,+,r$. We call a pair $<s,\alpha>$ a typed string; the set of
valid strings is our set of valid strings; we write $s : \alpha$ instead of $<s,\alpha> \in L$.

We have

 * $0 : I$
 * $^+ \colon I \to I$
 * $r \colon I \to (I \to I) \to I \to I$

and the usual typing rules for application and abstraction

 * if $s : \alpha, l : \alpha \to \beta$ then $sl : \beta$,
 * if $s : \beta$ and $x$ var of type $\beta$, $\lambda x.s \colon \alpha \to \beta$.

***** Semantics
We associate a set to each type, with $M(I) := \mathbb{N}$ and $M(\alpha \to \beta) := M(\beta)^{M(\alpha)}$;
and to each constant a correspondant element on the set, for example,
$M(r)$ is the unique function defined by induction.

Based on this, we construct an interpretation for each lambda term with
$M(s) \in M(\alpha)$. We give first an interpretation of variables $v = (v_{\alpha})_{\alpha \in Tp}$.
Application is interpreted as application of functions and abstraction
is interpreted as the interpretation of the body of the lambda under
a valuation that takes the bounded variable to the argument.

We can define a set of free variables of a string. Note that an
interpretation $M_{v}(x)$ with a valuation $v$ only depends on the
values of $v$ for the elements of $FV(x)$.

***** Observations
We have alpha-equivalence, beta-reduction and eta-reduction
inside the interpretation.

**** Semantics of MLTT
***** Set-theoretical semantics
It is more difficult to write a complete formalization of mltt.

We want to interpret $x_1:A_1,\dots,x_n:A_n \vdash A\ \mathrm{type}$ after interpreting

$A_1,\dots,A_n$ as sets. The interpretation will be a tuple
\[
a = \left\langle a_1,\dots,a_n \right\rangle
\]
where $a_i \in M(A_i)$, this is called the *realization of the context*.

To create an element $x_1:A_1,\dots,x_n:A_n \vdash t : A$ we should get
$M_v(t) \in M_v(A)$. We want an interpretation such that for every
definitional equality $A \equiv B$, we have the same sets $M_v(A) = M_v(B)$.

\[
M(s = t) = \left\{ \ast \right\} \mbox{ if } M(s)=M(t) \mbox{, and } \varnothing \mbox{ otherwise}
\]

But these semantics satisfy UIP.
***** Topological semantics
We interpret each type as a topological space; and each function type
as the set of continuous functions. We use simplicial sets in MLTT.

UIP does not hold under this interpretation.

*** From sets to types to categories to sets - Steve Awodey                                                   :paper:
**** 1. Sets to Types
**** 2. Types to Categories
***** Syntactic topos
**** 3. Categories to Sets
***** How to extract an elementary set theory from a topos
***** At least BIST
**** TODO 4. Composites
*** Profunctor Optics - Bartosz Milewski                                                             :optics:haskell:
#+BEGIN_SRC haskell
type Lens s t a b  = forall p. Strong p => p a b -> p s t
type Prism s t a b = forall p. Choice p => p a b -> p s t

class Profunctor p => Strong p where
  first' :: p a b -> p (a,c) (b,c)

class Profunctor p => Choice p where
  left' :: p a b -> p (Either a c) (Either b c)

class Profunctor p where
  dimap :: (a -> b) -> (c -> d) -> (p b c -> p a d)
#+END_SRC

A profunctor is a bifunctor of the form ${\cal C}^{op} \times {\cal C} \to \mathsf{Set}$.
In principle, we are not constrained to a single category,
the important notion is that the functor must be contravariant
on the first argument and covariant on the second.

#+BEGIN_SRC haskell
type f ~> g = forall x. f x -> g x
#+END_SRC

Parametricity implies naturality (?).

We have defined natural transformations as polymorphic functions.
Is this equivalent to the usual definition of natural transformation using naturality squares?

But the usual definition of natural transformations talks about naturality squares
and naturlity conditions. Are these two definitions equivalent?

Is this equivalent to the usual definition of natural transformation?
That is, does every polymorphic function satisfy the naturality condition?


**** Yoneda Lemma
#+BEGIN_SRC haskell
type Reader a x = a -> x
type Yo f a = Functor f => Reader a ~> f
-- Yo f a ~ f a
-- forall x. (a -> x) -> f x -> f a

toYo :: Functor f => f a -> Yo f a
toYo fa = \atox -> fmap atox fa

fromYo :: Functor f => Yo f a -> f a
fromYo alpha = alpha id
#+END_SRC

The Yoneda embedding

#+BEGIN_SRC haskell
forall x. (a -> x) -> (b -> x) ~ (b -> a)
#+END_SRC
*** Monad transformers - Snoyman                                                                            :haskell:
Concurrency with IO a and IO b.

*** Seemingly impossible functional programs - EscardÃ³                                                     :topology:
#+BEGIN_SRC haskell :results output
data Bit = I | O deriving (Eq)
type Nats = Integer
type Cantor = Nats -> Bit

(#) :: Bit -> Cantor -> Cantor
x # a = \i -> if i == 0 then x else a(i-1)

forsome :: (Cantor -> Bit) -> Bit
find :: (Cantor -> Bit) -> Bit
forsome = undefined
find = undefined

main :: IO ()
main = putStrLn "hello!"
#+END_SRC

*** EUTypes Summer School
**** Introduction to type theory
***** Bibliography
HP. Barendregt. Lambda calculus: syntax and semantics.
F. Cardone, JR. Hindley. History of lambda-calculus and combinatory logic.
Statman. Lambda calculus with types.
Benjamin Pierce. Types and programming languages.
JL Krivine. Lambda calculus, types and models.

***** Introduction to type theory I
****** Introduction
******* Gentzen
Gentzen: natural deduction/sequent calculus/axiomatic system.
******* Functions
We can give multiple notions of function

 * functions as black boxes.
 * set-theoretical definition.

We can define a domain and codomain for functions. This notion leads
to the notion of the type of a function.

****** Untyped lambda calculus
******* Informal syntax
Lambda terms are divided in

 * variables, which can be bound or free. There is a countable set of
   variables.
 * application of terms, function application.
 * lambda-abstractions, function generation by binding a variable.

An example is $\lambda x. x+42$. We see the application as left-associative.

\[
M ::= x \mid MM \mid \lambda x.M
\]

******* Examples
Those are examples of lambda combinators.
 
 * $I = \lambda x.x$
 * $K = \lambda xy. x$
 * $\Delta = \lambda x . xx$
 * $Y = \lambda f.(\lambda x. f(xx))(\lambda x. f(xx))$
 * $\Omega = (\lambda x.xx)(\lambda x.xx)$.

******* Free variables and closed terms
We define the set of free variables recursively. A closed term or
*combinator* has no free variables.

******* \alpha-conversion
Renaming of bound variables. This could also be done by using *De
Bruijn* notation. We apply the Barendregt's convention of renaming
variables that would be bound after a \beta-reduction.

\[
\lambda x.M \longrightarrow_{\alpha} \lambda y.M[y/x]
\]

******* \beta-reduction
It represents function application of functions in lambda calculus.

\[
(\lambda x.M)N \longrightarrow_{\beta} M[N/x]
\]

******** Substitution as a meta notion
Substitution is an implicit meta notion that can be defined
recursively over terms

 * $x[M/x] := M$
 * $\dots$

******* \eta-conversion
It represents function extensionality

\[
\lambda x.(Mx) \longrightarrow_{\eta} M
\]

******* Normal form
A term is in *normal form* if beta-reduction cannot be applied.
For example

 * $I$ is in normal form.
 * 4$KI(KII)$ is strongly normalizing (SN) to $I$.
 * $KI\Omega$ normalizing term.
 * $Y$ is only *head-normalizable*, or solvable.

Evaluation order is important; $KI\Omega$ stops or enters an infinite loop
depending on the evaluation order; this is a normalizing but not strongly
normalizing term.

******* Confluence and the Church-Rosser theorem
If $M \to N$ and $M \to P$, then there exists $S$ such that $N \longrightarrow S$
and $P \longrightarrow S$. The proof is not trivial.

******** Corollaries
The order of applied reductions is arbitrary. The Normal form is
unique if it exists.

******* Normalisation therem
A term is in head-normal form if its head is a lambda abstraction.
A term is in normal form if there are no $\beta$ nor $\eta$ redexes.

The normalisation theorem says that the leftmost strategy results
in the normal form of $M$ if and only if it has a normal form.

******* Fixed-point theorem
There is a fixed point combinator

\[
Y \equiv \lambda f. (\lambda x.f(xx))(\lambda x.f(xx))
\]

such that $\forall F. YF \equiv F(YF)$.

******* Church encoding
Logic and arithmetic can be encoded in lambda calculus via
Church numerals.

\[
n :\equiv \lambda fx. f^n x
\]

******* Expressiveness of lambda calculus
In the 1930s

 * Kleene: it is equivalent to recursive funtions.
 * Church
 * Curry

****** Typed lambda calculus
****** Intersection types
***** Introduction to type theory II
****** Disadvantages of untyped lambda calculus

 * There exist lambda terms without normal form.
 * Meaningless expressions.

This motivates two typing paradigms

 * Implicit type assignment: lambda calculus with types.
 * Explicit type assignment: typed lambda calculus.
****** Sintatic definition of typed lambda calculus
Type assignments $M : \sigma$, declarations $x : \sigma$ and environments
$\Gamma = \left\{ x_1:\sigma_1,\dots, x_s:\sigma_s \right\}$. With rules

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma, x:\sigma \vdash x:\sigma$ }
\end{prooftree}


\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \sigma \to \tau$}
\AxiomC{$\Gamma \vdash N : \sigma$}
\BinaryInfC{$\Gamma \vdash \lambda x . M : \sigma \to \tau$ } 
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma, x:\sigma\vdash y:\tau$}
\UnaryInfC{$\Gamma \vdash \lambda x.y : \sigma \to \tau$}
\end{prooftree}

It can be defined as a natural deduction system with introduction
and elimination rules.

****** Typing example
There are non-typable normal forms.

 * $I : \sigma \to \sigma$
 * $K : \sigma \to \tau \to \sigma$
 * $\Delta$, $Y$ or $\Omega$ can not be typed

****** Type preservation
If $M \longrightarrow P$ and $M:\sigma$, then $P:\sigma$.

****** Generation and substitution lemmas
If $\Gamma \vdash \lambda x. M: \varphi$, then $\varphi = \sigma \to \tau$ and $\Gamma, x:\sigma \vdash M : \tau$.
If $\Gamma, x:\sigma \vdash M : \tau$ and $\Gamma \vdash N:\tau$, then $\Gamma \vdash M[x/N]:\tau$.

****** Strong normalization
If $M : \sigma$, then $M$ is strongly normalizing. This was proven by
Tait in 1967.
****** Typability and inhabitation
Questions on lambda calculus

 * *Typability:* iven a term, find a type for it.
 * *Inhavitation:* given a type, construct a term of that type.
 * *Type checking:* check the type of a term.

Typability is decidable in simply typed lambda calculus. It is
decidable in second order lambda calculus with the Hindley-Milner
algoritm.

Inhabitation is equivalent to the intuitionistic logic of Gentzen's
natural deduction. The rules of typed lambda calculus are the rules
of natural deduction if we do not use the terms.

****** Curry-Howard correspondence
A formula is provable in IL iff it is inhabited in simply-typed lambda
calculus. This is also the language of Cartesian Closed Categories
(Lambek, 1970).

BHK interpretation of logical connectives is formalized by the 
Curry-Howard correspondence.

****** Consistency/Completeness/Decidability
Intuitionistic propositional logic (IL) is consistent, complete and
decidable. Due to Curry-Howard, inhabitation is decidable in STLC.
****** Lambda cube
If any $M$ is typable, $M$ is strongly normalizing.
The [[https://en.wikipedia.org/wiki/Lambda_cube][lambda cube]] represent multiple type systems.

\[\begin{tikzcd}
& & & \\
\lambda 2 & & \lambda P2 & \\
& \lambda & & \\
\lambda_{\to}& & & & \\
\end{tikzcd}\]
****** Intersection types
In our current system, $\Delta$ is not typeable. We are going to introduce
intersection types with elimination rules

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \sigma \cap \tau$}
\UnaryInfC{$\Gamma \vdash M : \sigma$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \sigma \cap \tau$}
\UnaryInfC{$\Gamma \vdash M : \tau$}
\end{prooftree}

and a introduction rule

\begin{prooftree}
\AxiomC{$M:\sigma$}
\AxiomC{$M:\tau$}
\BinaryInfC{$M:\sigma\cap\tau$}
\end{prooftree}

In general, the Curry-Howard correspondance is lost here. We create
a new system $\lambda\cap$, but in this system, $\sigma \to \tau \to \sigma \cap \tau$ is provable while
it is not inhabited.

******* Now self application is typable
The self application can be of type $\lambda x.xx :((\sigma \to \tau) \cap \sigma) \to \tau$.

****** Characterization of strong normalization on intersection types
A term is typable iff it is strongly normalizing.

For example, $KI\Omega$ is not typable, even if $I$ is.

****** Typability and inhabitation are undecidable with intersection types
****** Models of lambda-calculus
We can prove completeness of type assignment. It is a theorem that

\[
\Gamma \vdash M:\sigma \iff \Gamma \models M : \sigma
\]
***** Pure type systems I
*alx@minuw.edu.pl*

****** Simple type systems
Differences between logics and type systems:

 * focus on computation instead of consistency.
 * it is meaningful to have two assumptions of the same type.
 * it is meaningful to use wh same assumption twice.

Combinatory logic, with combinators S,K, defines /minimal logic/.
With them, deduction theorem is provable. But we can add other
combinators such as B,C or W.
****** More complex type systems
We can add polymorphism with type variables. We get SystemF (aka
$\lambda 2$). We need more complex typing rules and beta reduction for types.
$\lambda P$ was proposed by deBruijn, Harper, Longo and Moggi.

****** Properties of interest of a pure type system

 1. Church-Rosser property. Values are computed deterministically.
 2. Subject reduction property. Types are invariants of the reduction.
 3. Strong normalisation property. Computation terminates.

Those properties prove consistency of the logical system.
****** Examples of PTSs
$\lambda_{\to}, \lambda 2, \lambda P, \lambda \omega, \lambda C, \lambda \ast, \lambda U$

****** Lemmas for PTSs
******* Free variables
******* Transitivity of contexts
******* Substitution
******* Weakening
******* Generation lemma
******* Condensing lemma
****** Properties of PTSs
******* Church-Rosser property
There is a PTS extended with a number of axioms which does not have
the Church-Rosser property.

******* Geuvers theorem
All functional strongly normalizing PTSs have the Church-Rosser
property.

******* Functionality
A PTS (S,A,R) is functional when A is a function from S to S and
R is a function from $S \times S$ to $S$.

******* Uniqueness of types lemma
***** Pure type systems II
****** The type inhabitation problem
**** Dependently typed programming
***** Milner's coincidence
Milner's Coincidence on Hindley-Milner's type systems

|-----------------------+--------------------------------------|
| Terms                 | Types                                |
|-----------------------+--------------------------------------|
| what we write         | we don't write these                 |
| what we read          | invisible (except errors)            |
| what gets compiled    | what gets erased                     |
| non dependent \lambda | polymorphism over types with \Lambda |
|-----------------------+--------------------------------------|

In the late 90s, this was the accepted unquestioned way of thinking
about types.
**** Homotopy type theory
The Tao of Types - Thorsten Altenkirch

 - A topological model of HoTT.

***** HoTT 1
There are multiple implementations of type theory (Coq, Agda, ...)

****** Extensionality vs intensionality
****** Set theory vs type theory
In set theory, we would write $3 \in \mathbb{N}$, and this is a proposition of the
language; we can write things like $x \in A \longrightarrow x \in B$. In type theory,
$3 : \mathbb{N}$ is instead a judgement. Statements such as $\mathbb{B}\cap \mathbb{N}$ are intensional:
they depend on the encoding.

Sometimes, we want to talk about intensional properties

****** Univalence
Two types in a one-to-one correspondence are equal.

****** Propositions as types explanation/Curry-Howard equivalence
******* Example
We will prove that $P \times Q \to R \iff P \to (Q \to R)$. We will define two
functions from and to the types

#+BEGIN_SRC haskell
f :: ((a,b) -> c) -> (a -> b -> c)
f h x y = h (x,y)

g :: (a -> b -> c) -> ((a,b) -> c)
g h (x,y) = h x y
#+END_SRC

these are curry/uncurry functions.

****** Products and sums
Products are created as

\begin{prooftree}
\AxiomC{$a:A$}
\AxiomC{$b:B$}
\BinaryInfC{$(a,b) : A \times B$}
\end{prooftree}

and sums as

\begin{prooftree}
\AxiomC{$a:A$}
\UnaryInfC{$left(a) : A +B$}
\AxiomC{$b:B$}
\UnaryInfC{$right(b):A+B$}
\noLine
\BinaryInfC{}
\end{prooftree}
 
****** Definitional equality
Equality given by the definition of the terms. These equalities are
static.

$\equiv$
****** Recursor
The recursor is a non-dependent eliminator. It gives us the ability
of doing pattern-matching on types. For example, if we want to define
a function from a pair type using the recursor for the product

\[ \mathtt{rec}^{\times} :
(A \to B \to C) \to (A \times B) \to C
\]

or a recursor for the sum type

\[ \mathtt{rec}^+ :
(A \to C) \to (B \to C) \to (A + B \to C)
\]

the recursor for the empty type

\[ \mathrm{rec}^\bot : \bot \to C
\]

is implemented without using anything because of the nature of the
empty type.
***** HoTT 2
****** What is a type?
We allow the following judgements,

 * $a:A$, type declarations.
 * $a \equiv_{A} b$, definitional equality.

and we define a universe of types ${\cal U}$, a type whose elements are types.

******* Is type a type?
If we set $Type : Type$, we can encode a version of Russell's paradox
using trees of types. Being of a type is a judgement, so we can not
encode the traditional Russell paradox.

******* Type universes
We are going to use type universes $Type_0: Type_1: Type_2 : \dots$,
constructing a *predicative* hierarchy. It is a cumulative hierarchy,
where we can lift a type $A : Type_{i}$ to $\lceil A\rceil : Type_{i+1}$ and any function
$A \to B$ to $\lceil A\rceil \to \lceil B\rceil$. 

****** Dependent types
A *dependent type* depends on a term. An example is $Fin : \mathbb{N} \to Type$.
Another example is $Vec : Type \to \mathbb{N} \to Type$ or $Prime : \mathbb{N} \to Type$.
****** Pi-types
Pi types are a generalization of function types allowing the codomain
to depend on the domain.

******* Example: zeroes

\[
zeroes : \prod_{n:\mathbb{N}} Vec\ \mathbb{N}\ n
\]

where

\[
zeroes\ n = (0,0,\dots,0)
\]

******* Example: theorems on naturals

\[
pluszero : \prod_{n : \mathbb{N}} n+0 =_{\mathbb{N}} n
\]

****** Sigma-types
Sigma tupes generalize product types to the case where the type of
second depends on the first. They work as dependent pairs.

******* Example: lists

\[
\sum_{n:\mathbb{N}} Vec\ A\ n
\]

****** Particular cases
The function type is a particular case of a pi-type, while the 
product type is a particular case of a sigma-type.

 * $\prod_{a:A} B \text{ is } A \to B$
 * $\sum_{a:A} B \text{ is } A \times B$

****** Example of predicate logic
We have the following logic equivalence in predicate logic

\[
\left(\sum_{x:A}  P\ x\right)\to Q \iff \prod_{x:A} P\ x \to Q
\]

and the proof is similar to that of $((P,Q) \to R) \to (P \to Q \to R)$.
Yesterday we talked only about propositional logic.
****** Numerical interpretation
If $f : n \to \mathbb{N}$, and we take $\overline{n}$ to be a type with $n$ elements

\[
\sum_{i:\overline{n}} \overline{f(i)}
=
\overline{\sum_{i=0}^{n-1} f(i)
\]

and the same is true for pi-types, they are related to a product.
****** Sum as a sigma type
We can define $A + B$ using $\sum_{x:2}\text{if x then } A \text{ else } B$; and the same trick
can be used for products, taking $A \times B$ to be $\prod_{x:2}\text{if x then } A \text{ else } B$.

\[\begin{tikzcd}
    & $\sum$ &          & $\prod$ &       \\
$+$  &        & $\times$ &         & $\to$
\end{tikzcd}\]

****** Eliminators of dependent types
The eliminator of the sum type is

\[
R^+ : (A \to B) \to (A \to C) \to A + B \to C
\]

and we can define a dependent version of the eliminator

\[
R^+ : \left( \prod_{x:A} C(inl(x)) \right) \to 
\left(\prod_{y:B} C(inr(y))\right) \to
\prod_{z: A+B} C(z)
\]

of which the first is a particular case.
***** HoTT 3
****** Intensional equality
****** Uniqueness of equality proofs

\[
uep : \prod_{x,y:A}\prod_{p,q: x=y} p=q
\]

******* Proof and the need for K
It has been proved that this does not depend on J using
countermodels. We need to add another eliminator called K.
If we have

\[
C : \prod_{x:A} x =x \to Type
\]

then

\[
K_C : \prod_{x:A} C\ x\ (refl\ x) \to \prod_{x:A}\prod_{p:x=x} C\ x\ p
\]

******* What can we proof without K?
The groupoid structure of paths can be proven wihtout K.

\[
\prod_{x,y:A}\prod_{p : x=y}
trans\ p\ refl = p
\]

****** Extensionality
We need extensionality to prove

\[
\lambda x. x+0 = \lambda x.0+x
\]

using that $f x = g x$ for all $x$ implies $f = g$.

******* Product
The equality of a product is the product of two equalities;
the equality of a coproduct is a coproduct, and so on.

******* Equality of types
Equivalence or isomorphism of types can be defined with two
mutually inverse functions between them. They give us a one-to-one
correspondence between types. This is written as $A \simeq B$.

We would need

\[
\eta : \prod_{x:A} g(f(x)) = x
\]

and

\[
\varepsilon : \prod_{y:B} f(g(y)) = y
\]

We could use J to prove

\[
\prod_{A,B: {\cal U}} A=B \to A \simeq B
\]

but this is not provable.

******* Automorphisms of Bool
There are two proofs of equality of Bool to Bool.

There are two ways of proving $f(g(f(x))) = f(x)$ with the previous
definition.

We fix that with

\[
\tau : \prod_{x:A} f(\eta (x)) = \varepsilon f(x)
\]

******* Definition of equivalence
This definition of isomorphism 

\[
isequiv(f) = \prod_{b:B} iscontractible \left( \sum_{a:A} f(a) = b \right)
\]

is equivalent to our previous definition of equivalence.
******* Isomorphisms and equivalence
There are more isomorphisms than equivalences, but for every
isomorphism, we can build an equivalence

\[
A \simeq B \iff A\cong B
\]

The previous definition of univalence was unsound because it
made isomorphisms and equivalences equal.
***** HoTT 4
****** What is a proposition?
****** Axiom of choice
Diaconescu; from the set-theoretical axiom of choice, we get that,
for all propositions the LEM holds, $\prod_{P:Prop} P \vee \neg P$.
****** Sets and propositions

\[ \mathtt{isSet}\ A \equiv
\prod_{x,y:A} \mathtt{isProp}(x =_{A} y)
\]

$Type_0$ is an example of something that is not a set. There are two
different proofs of the equality $Bool = Bool$.

****** n-Types

\[
isntype(A) \equiv
\prod_{x,y:A} is(n-1)type(x = y)
\]

An n-type is also an (n+1)-type.

|  -2 | Contractible type |
|  -1 | Proposition       |
|   0 | Set               |
|   1 | Groupoid          |
|   2 | 2-Groupoid        |
| ... | ...               |

The sphere $\mathbb{S}^2$ is not an n-type for any n.

****** Hedberg's theorem
Given $A:Type$ with decidable equality

\[
d : \forall x,y : A.\quad x=y \vee x \neq y
\]

it is a set, $\mathtt{isSet}(A)$.
***** HoTT 5
****** Negative translation of classical logic

 * $A \vee B \mapsto \neg (\neg P \wedge \neg Q)$
 * $\exists_{x:A}B(x) \mapsto \neg \prod_{x:A} \neg B(x)$

*** Oregon Programming Languages Summer School 2017
**** Programming Languages Background 1
A programming language is defined by

 * *statics*, how it is written;
 * *dynamics*, how it is executed;

and they should be coherent in some way.

***** Statics
 1. Concrete syntax. Linear representations, usually. Psycology of
    programming languages determines how they are written.
 2. Abstract syntax.
 3. Context-sensitive conditions and well-formation.

The practical foundation are ABT (abstract binding trees); the syntax
is generated by operators with arguments.

[7:27]
*** School on Univalent Mathematics - Birmingham
**** Spartan type theory - Andrej Bauer
**** Univalent foundations - Martin Escardo
**** Semantics for type theory - Helfer
# Notes on paper.

***** Formal languages and semantics
Traditional definition of formal languages. An alphabet and rules to
inductively construct words. We call $L$ the set of valid strings.

****** Example: boolean algebra
 * symbols $\Sigma = \left\{ p_1,\dots,p_n, \wedge,\neg,\implies,(,),\top,\bot,\dots \right\}$ 
   atomic propositions and logical symbols.
 * valid strings are defined inductively as
   * singletons $p_i$, $\top$ or $\bot$
   * connectors: if $a, b$ are valid, so are $a \wedge b, a \vee b, \neg a, \dots$

****** Semantics
The semantics assigns to each string a number $0$ or $1$,

\[
s \colon L \to \left\{ 0,1 \right\}
\]

and this evaluation will depend on the evaluation of atomic propositions.
If we call $\Omega = \left\{ p_1,\dots,p_n \right\}$, it depends on a valuation $v \colon \Omega \to \left\{ 0,1 \right\}$.
Connectives are interpreted naturally.

****** Observation
Sometimes $s_v(a) = s_v(b)$ regardless of $v$; in particular, $s_v(a) = 1$ regardless
of $v$ for some formulae (tautologies and absurds).

\[
s_v(A \wedge B) = s_v(B \wedge A)
\]

***** Simply typed lambda calculus
****** Definition
Taking a inductively defined set of types

 * given type $I$
 * function types $\alpha \to \beta$

The set $\Sigma$ of symbols has countably many variables of each type and the symbols of
lambda calculus $(,),\lambda, O,+,r$. We call a pair $<s,\alpha>$ a typed string; the set of
valid strings is our set of valid strings; we write $s : \alpha$ instead of $<s,\alpha> \in L$.

We have

 * $0 : I$
 * $^+ \colon I \to I$
 * $r \colon I \to (I \to I) \to I \to I$

and the usual typing rules for application and abstraction

 * if $s : \alpha, l : \alpha \to \beta$ then $sl : \beta$,
 * if $s : \beta$ and $x$ var of type $\beta$, $\lambda x.s \colon \alpha \to \beta$.

****** Semantics
We associate a set to each type, with $M(I) := \mathbb{N}$ and $M(\alpha \to \beta) := M(\beta)^{M(\alpha)}$;
and to each constant a correspondant element on the set, for example,
$M(r)$ is the unique function defined by induction.

Based on this, we construct an interpretation for each lambda term with
$M(s) \in M(\alpha)$. We give first an interpretation of variables $v = (v_{\alpha})_{\alpha \in Tp}$.
Application is interpreted as application of functions and abstraction
is interpreted as the interpretation of the body of the lambda under
a valuation that takes the bounded variable to the argument.

We can define a set of free variables of a string. Note that an
interpretation $M_{v}(x)$ with a valuation $v$ only depends on the
values of $v$ for the elements of $FV(x)$.

****** Observations
We have alpha-equivalence, beta-reduction and eta-reduction
inside the interpretation.

***** Semantics of MLTT
****** Set-theoretical semantics
It is more difficult to write a complete formalization of mltt.

We want to interpret $x_1:A_1,\dots,x_n:A_n \vdash A\ \mathrm{type}$ after interpreting

$A_1,\dots,A_n$ as sets. The interpretation will be a tuple
\[
a = \left\langle a_1,\dots,a_n \right\rangle
\]
where $a_i \in M(A_i)$, this is called the *realization of the context*.

To create an element $x_1:A_1,\dots,x_n:A_n \vdash t : A$ we should get
$M_v(t) \in M_v(A)$. We want an interpretation such that for every
definitional equality $A \equiv B$, we have the same sets $M_v(A) = M_v(B)$.

\[
M(s = t) = \left\{ \ast \right\} \mbox{ if } M(s)=M(t) \mbox{, and } \varnothing \mbox{ otherwise}
\]

But these semantics satisfy UIP.
****** Topological semantics
We interpret each type as a topological space; and each function type
as the set of continuous functions. We use simplicial sets in MLTT.

UIP does not hold under this interpretation.
**** Set-level mathematics - Helfer
***** Motivation
Given $X$ topological space, we say it is n-truncated if

\[
\pi_m(X) = 0 \text{ for all } m > n.
\]

 1) We know from homotopy theory that if $X$ is n-truncated, the
    space of paths between any two points is (n-1)-truncated.

 2) If $X$ is 0-truncated, it is homotopy equivalent to a discrete
    space; that is, a set.

A similar phenomenon occurs in category theory; a category has
objects, morphisms between them, morphisms between morphisms and so
on. The connection between categories and homotopical spaces is known
as the *[[https://ncatlab.org/nlab/show/homotopy+hypothesis][homotopy hypothesis]]*.

There are some definitions of â-groupoids for which the homotopy
hypothesis is a proven theorem.

***** H-levels
\[
\mathsf{isofhlevel} : \mathbb{N} \to {\cal U} \to {\cal U}
\]

defined as

 * $\mathsf{isoflevel}(0,X) :\equiv \mathsf{iscontr}(X)$
 * $\mathsf{isoflevel}(S(n),X) :\equiv \prod_{x,x' : X} \mathsf{isofhlevel}(n,x=x')$

****** Sets
A set is a type of h-level 2.

 * Dependent pair of sets is a set.
 * Binary product of sets is a set.
 * Dependent function from a set to a family of sets is a set.
 * Function space to a set is a set.

***** How to show that something is not a set
****** Decidable types
A type $A$ is *decidable* if $A + \neg A$.

A type $A$ has *decidable path-equality* if all path types are
decidable

\[
\prod_{x,x' : A} (x = x') + \neg (x = x')
\]

****** Hedberg's theorem
Any type with decidable equality is a set.

****** Are all types sets?
 * In spartan type theory, there are types that cannot be shown to be
   sets. It is consistent with spartan type theory to assume that all
   types are sets.
 * In univalent type theory, some types are not sets.

****** Another set

\[
\mathsf{hProp} :\equiv \sum_{X:U} \mathsf{isaprop}(X)
\]

is a set. This can be generalized: the type of types of n-level is
of (n+1)-level.

****** The universe is not a set
The booleans have a non-trivial automorphism.

****** Sets and propositions

\[
\mathsf{isInjective}(f) :\equiv \prod_{x,x' : X}f(x) = f(x') \to x = x'
\]

is a proposition when $X,Y$ as sets.

***** Set-level quotient
****** Quotient
A map compatible with a relation is a map from the quotient;
the equivalence is given by precomposition with the projection.

\[
\sum_{f \colon X \to Y} \mathsf{isCompatible}(f) \simeq X/R \to Y
\]

****** Subtype
A subtype of a type is a map from the subtype to hProp.
# Note that this definition is similar to that of the dependent sum.

******* The type of binary relations is a set

****** Defining quotients
Equivalence classes

\[
\mathsf{iseqclass}(A) = \| \mathsf{carrier}(A) \| \times 
\left( \prod_{x,y:A} Rxy \to Ax \to Ay \right)
\times 
\left( \prod_{x,y:A} Ax \to Ay \to Rxy \right)
\]

Quotients

\[
X/R :\equiv \sum_{A : X \to \mathsf{hProp}} \mathsf{iseqclass}(A)
\]

***** Set-level mathematics
****** Groups in type theory
We want the proofs of the axioms of a group to be elements of a
proposition; having this, any two groups with the same data are
equivalent.

A group isomorphism is a bijective function compatible with the
group structure; we can show that the type of equality between
of groups is the type of isomorphisms.

# Identity is isomorphism for groups!  Transport along the path given
# by univalence for a equivalence is conjugation by that equivalence.
**** Category theory - Lumsdaine
Definition of a category in the univalent setting is different than
that from a classical setting. What works the same in this
formulation and what works differently?

***** Difference
****** Definition
A *category* consists of 

 * a type of objects $\mathrm{ob}({\cal C})$;
 * for each pair of objects, a set of morphisms $\mathrm{hom}(a,b)$;

and the axioms, with are all about morphisms and they behave well with
objects not being a set.

******* Terminology
This is terminology in UniMath, in the HoTT book this is called
*precategory*. A precategory here would be just types
$\mathsf{hom}(x,y)$ but this is not discussed in the HoTT book.

******* Set-category
A set category is a category where $\mathrm{ob}({\cal C})$ is a set.

******* Carrier set
It is useful to take the carrier to be a set; in other case, we
would get a more general algebraic structure where equalities do
not work as well as in the particular case.

****** Examples
******* Sets
Sets is a category in the way we expect; but it is *not* a set
category; the set of h-sets is not an h-set. This is why we don't
want to take set-categories as our definition.

******* Simplicial set
$\Delta$ can be constructed with objects $\mathbb{N}$ and maps $m \to n$ 
as order-preserving maps $f \colon [m] \to [n]$. It is a set
category.

Small cats, combinatorially constructed ones, are set-cats in
practice.

****** Univalent categories and Rezk completion
A category is *univalent* if the canonical map (defined with identity)
$x = y \to \mathrm{Iso}(x,y)$ is an equivalence for each pair $x,y$. This is like
an internal version of univalence.

This is called /saturated category/ in the HoTT book.

******* Example: sets
Sets is univalent. Most categories in practice are univalent.
Tops, Grps, and algebraic constructions in general. Products of
univalent categories and algebras for a monad in a univalent
category are univalent.

/Heuristic:/ if the category is "the category of its objects" it will
be univalent.

******* Counterexample
The homotopy category of topological spaces where

 * objects are topological spaces,
 * and maps are continuous maps up to homotopy.

There are non-equal objects here that are homotopically-equal. It is not
the category of topological spaces but a category of homotopy types, and
we are using topological spaces only as representatives.

******* Counterexample
A preorder (category where each hom-set is a prop) is univalent iff
$\mathrm{ob}({\cal C})$ is a set and $x \leq y, y \leq x$ implies $x = y$.

******* Fact: categories have an univalent completion operation
The *Rezk completion* (Ahrens, Kapulkin, Shulman) is a sort of
univalent completion.

\[\begin{tikzcd}
& RC({\cal C}) \drar[dashed] & \\
{\cal C} \ar{rr} \urar & & {\cal E}
\end{tikzcd}\]

Given any univalent ${\cal E}$, any map factors through the completion.
The functor ${\cal C} \to RC({\cal C})$ is fully faithfull and essentially surjective.

***** What works the same?
Most things that don't mention equality of objects; modulo being
careful about existence vs chosen structure and not having the
axiom of choice.

****** Example: having products
If they exist, they are unique up to isomorphism.

****** In univalent categories
In an univalent category, they are actually unique! existence
of products is equivalent to chosen products and this is a
proposition.

****** Unimath

- Functors, natural transformations.
- Monads.
- Functor categories.
- Colimits, limits.

***** What works a little differently?
****** Displayed categories
Examples of equality on objects

 * *fibrations of categories*; for example, if ${\cal C}$ has pullbacks, the
   arrow category ${\cal C}^{\to}$ with the projection to the second object is a
   fibration.

   Typically, fibers are

   \[
   \mathrm{ob}({\cal E}) :\equiv \sum_{x : {\cal E}} \text{objects of ${\cal E}$ over ${\cal C}$}
   \]

****** Definition of displayed category
A displayed category over ${\cal C}$ has

 - for $c : \mathrm{ob}({\cal C})$, a type $\mathrm{ob}_c({\cal D})$;
 - for 

   \[\begin{tikzcd}
   d' & d \\
   c'\rar{f} & c
   \end{tikzcd}\]
 
   a set $\mathrm{hom}(d',d)$.

They 

****** Utility of displayed categories
Useful for building up categories of multi-component structures and
reasoning about them one cat at a time.

****** Example: groups
Groups are a displayed category over sets. The objects are group
structures over set; and the $\mathrm{hom}(g,g')$ is a function
between the sets together with the assumption that it preserves the
structure.
****** !
***** What works very differently?
Unexplored territory

****** 2-categories, higher categories
In univalent categories, we don't have enough strict 2-categories; in
particular, the category Cat is not a strict 2-category in the univalent
setting.
**** How to implement type theory in an hour - Andrej Bauer

 * Unicode input
 * Pretty printing
 * Loading filesv
 
***** Type checking
Bidirectional type checking.

***** Equality of types
Not every theory has an algorithm checking equality of types.

 -> We use an algorithm due to Bob Harper.
    It uses the extensionality rules to check for equality of terms.

     * Every two elements of the unit are equal.
     * We can apply a judgmental eta rule with a fresh variable.

    We have *extensional rules* for free.

It does not work with primitive symbols. Then we switch to normalization
phase and structurally compare the two weak normal form of the types.
**** Future of unimath - Ahrens
**** Presentations
***** Nominal sets
****** First definition
Given a set with decidable equality (eg. naturals), $X$ is
a nominal set...

\[
\forall x \in X, \exists A \subset \mathbb{N}, \forall u \in \mathrm{Perm}(\mathbb{N}), u|_{A} = \mathrm{id}: u(x) = x
\]
****** Presheaf definition
Ä¹et $N$ be the subcategory of finite subsets of $\mathbb{N}$,
morphisms injections. A nominal set is a functor $\mathbb{N} \to \mathrm{Set}$.

** Constructivism                                                                               :constructivism:logic:
*** Intuitionistic mathematics and realizability in the physical world - Andrej Bauer                         :drill:
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/bauer_intuitionistic_mathematics_and_realizability_in_the_physical_world.pdf
:ID:       6c678fe5-6875-41a1-a9f9-0e5343c31d35
:END:
**** Notes for page 4
:PROPERTIES:
:interleave_page_note: 4
:END:

***** Principle of micro-affinity                                                                           :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       4b085376-84d9-4589-aab5-31196a37346f
:DRILL_LAST_INTERVAL: 3.7488
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:13]
:END:
Principle of micro-affinity in synthetic differential geometry.

****** Answer
An infinitesimal change in the independent variable causes an affine
(linear) change in the dependent variable.

For any $f \colon \mathbb{R} \to \mathbb{R}$, there exists a unique $f'(x)$ such that

\[
f(x + dx) = f(x) + f'(x)dx
\]

for all nilpotent *infinitesimals* ($dx$ such that $dx^2 = 0$).

***** Non-standard analysis                                                                                 :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       88710dd8-2f57-4ec3-b686-fa5e9dbf4ffd
:DRILL_LAST_INTERVAL: 4.2705
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:21]
:END:
In which fundamental sense are Non-Standard Analysis and Synthetic
Differential Geometry different?

****** Answer

- Non-Standard Analysis is classical.
- Synthetic Differential Geometry has nilpotents.
**** Notes for page 5
:PROPERTIES:
:interleave_page_note: 5
:END:
***** Law of cancellation                                                                                   :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       c90276ae-709a-4ace-a332-fd03ba1ddbaa
:DRILL_LAST_INTERVAL: 5.5088
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:53]
:END:
Law of cancellation in synthetic differential geometry.

****** Answer
If $a\,dx = b\, dx$ for all infinitesimals $dx$, then $a = b$.

**** Notes for page 11
:PROPERTIES:
:interleave_page_note: 11
:END:

***** Unbounded Turing computable binary trees                                                              :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       b1e4c220-e3a8-426d-a2a7-44794ab561ea
:DRILL_LAST_INTERVAL: 3.9343
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:19]
:END:
Can an unbounded computable binary tree have no computable infinite
paths?

****** Answer
Yes, Kleene constructed one. See [[id:7e1cbeaa-95c1-44f2-a514-83b0a33d695f][KÃ¶nig's lemma and Kleene Tree by Andrej Bauer]].

*** TODO KÃ¶nig's lemma and Kleene Tree - Andrej Bauer
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/bauer_kÃ¶nig's_lemma_and_kleene_tree.pdf
:ID:       7e1cbeaa-95c1-44f2-a514-83b0a33d695f
:END:

*** TODO Realizability as the connection between computable and constructive mathematics - Andrej Bauer
*** TODO The realizability approach to computable analysis and topology - Andrej Bauer
*** TODO First steps in synthetic computability theory - Andrej Bauer
**** 2.1. Type I computability
In Type I computability, computations are performed by partial
recursive functions on natural numbers.

**** 2.2. Type II computability
In Type II computability, computations are performed by computable
partial functions on the Baire space $\mathbb{B} = \mathbb{N}^{\mathbb{N}}$.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       2bf3de9d-49f5-4393-bf32-24c4fabba4e0
:DRILL_LAST_INTERVAL: 8.1517
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:10]
:END:
Difference beeween Type I computability and Type II computability.

****** Description

 - *Type I*
   Computations are performed by partial recursive functions on natural
   numbers.

 - *Type II*
   Computations are performed by Turing machines writing infinite outputs
   on a write-only tape.  The usual domain is the Baire space $\mathbb{B}= \mathbb{N}^{\mathbb{N}}$.

**** 5. Examples from computable mathematics
Computable mathematics is the realizability interpretation of
constructive mathematics.

** Homotopy type theory                                                                                         :hott:
*** Basic homotopy type theory
**** 3. Sets and logic
***** Sets                                                                                                  :drill:
SCHEDULED: <2019-03-28 Thu>
:PROPERTIES:
:ID:       642db2e3-5cf0-4bfc-8d34-0bd7a3723374
:DRILL_LAST_INTERVAL: 317.2681
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-15 Tue 23:14]
:END:
Definition of *sets* in type theory.

****** Definition
\[
\textsf{isSet}(A) \equiv \prod_{(x,y : A)} \prod_{(p,q : x = y)} p = q.
\]

***** Subtypes                                                                                              :drill:
SCHEDULED: <2019-06-13 Thu>
:PROPERTIES:
:ID:       64ba6bc2-b0cc-4863-809e-baf0200c59d3
:DRILL_LAST_INTERVAL: 350.165
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.125
:DRILL_EASE: 2.96
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:09]
:END:
Definition of *subtype* in HoTT.

****** Definition
For a family of *mere propositions* $P : A \to {\cal U}$, we write

\[
\left\{ x : A\mid P(x) \right\} 
 \equiv
\sum_{x:A} P(x)
\]

and call this a subtype of $A$.

***** Axiom of choice                                                                                       :drill:
SCHEDULED: <2019-01-27 Sun>
:PROPERTIES:
:ID:       f8e99cbf-4821-4cb3-b622-dfd84d4d4215
:DRILL_LAST_INTERVAL: 219.3012
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.875
:DRILL_EASE: 2.76
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:07]
:END:
*Axiom of choice* in type theory.

****** Statement

\[
\left( \prod_{x:X} \trunc{\sum_{a:A(x)} R(x,a)} \right)
\to
\trunc{\sum_{(g: \prod_{x:X}A(x))} \prod_{(x:X)} R(x,g(x))}.
\]

for a given  $R : \prod_{x:X} (A(x) \to {\cal U})$.

***** Decidable type                                                                                        :drill:
SCHEDULED: <2019-03-17 Sun>
:PROPERTIES:
:ID:       b7080b88-8de1-4c7d-b804-73dc6ff6d702
:DRILL_LAST_INTERVAL: 268.2915
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:
Definition of *decidable type*.

****** Definition

$A + \neg A$

***** Decidable equality                                                                                    :drill:
SCHEDULED: <2019-03-10 Sun>
:PROPERTIES:
:ID:       0784e9e9-14c0-42c7-ae4a-25aa5e192c4b
:DRILL_LAST_INTERVAL: 261.2567
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:
Definition of *decidable equality*.

****** Definition

\[
\prod_{a,b : A} (a = b) + \neg (a = b)
\]

***** Decidable type family                                                                                 :drill:
SCHEDULED: <2019-05-02 Thu>
:PROPERTIES:
:ID:       dc617b60-30a0-40c0-b631-b6277f138150
:DRILL_LAST_INTERVAL: 313.7072
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:
Definition of *decidable type family*.

****** Definition

\[
\prod_{a : A} B(a) + \neg B(a)
\]

***** Contractible type                                                                                     :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       24050684-5b1f-48bc-859b-5169e66d0198
:DRILL_LAST_INTERVAL: 4.0884
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:08]
:END:
Definition of *contractible type*.

****** Definition
\[
\isContr(A) :\equiv \sum_{a:A}\prod_{x:A}(a = x)
\]

$A$ is contractible if there exists a center of contraction $a$.

***** Mere propositions and contractibility                                                                 :drill:
SCHEDULED: <2019-04-30 Tue>
:PROPERTIES:
:ID:       8be8a4a1-667a-4a64-b96a-f07d9d46ca61
:DRILL_LAST_INTERVAL: 312.4413
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.429
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:21]
:END:
Define *mere proposition* in terms of contractibility.

****** Definition
$A$ is a mere proposition iff for all $x,y : A$, the type $x = y$ is
contractible.

***** Hedberg's theorem                                                                                     :drill:
SCHEDULED: <2019-03-02 Sat>
:PROPERTIES:
:ID:       2bde59f5-b573-44cc-9aa0-319cafd7916c
:DRILL_LAST_INTERVAL: 253.0514
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:08]
:END:
Statement of *Hedberg's theorem*.

****** Statement
If a type has decidable equality, it is a set.

**** 4. Equivalences
***** Type-theoretical fiber                                                                                :drill:
SCHEDULED: <2019-04-02 Tue>
:PROPERTIES:
:ID:       f92e8b0d-a516-41ab-af1c-47a714f0b557
:DRILL_LAST_INTERVAL: 284.1624
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:09]
:END:
Definition of *fiber* of $f : A \to B$ over $b : B$ in HoTT.

****** Definition

\[
\fib_f(b) :\equiv \sum_{a:A}(f(a) = b).
\]

**** 11. Reals
***** Notions of constructive compactness                                                                   :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       4245c70a-41e5-4c32-8c1b-0fecf44d5f7d
:DRILL_LAST_INTERVAL: 11.8024
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:13]
:END:
Give the three constructive notions of compactness inside the
reals in Homotopy Type theory.

****** Three notions

 * *Metrically compact:* Cauchy complete and totally bounded.
 * *Bolzano-Weierstrass:* every sequence has a convergent subsequence.
 * *Heine-Borel:* every open cover has a finite subcover.

These are equivalent in classical mathematics.

**** Other
***** UIP is not derivable                                                                                  :drill:
SCHEDULED: <2019-03-03 Sun>
:PROPERTIES:
:ID:       59132f0b-aa24-454c-9397-622348e5b16d
:DRILL_LAST_INTERVAL: 253.5467
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:18]
:END:
Uniqueness of Identity Proofs is not derivable in MTLL. Why?

****** Reference
In the groupoid model by Hoffmann, elements of the identity type
different from reflexivity can be built.

*** Homotopy type theory book - Univalent Foundations
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/the_univalent_foundations_program_homotopy_type_theory.pdf
:END:

**** Preface
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (5 . 0.083235)
:END:

**** Table of Contents
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (9 . 0.083235)
:END:

**** Introduction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (15 . 0.318161)
:END:

**** I Foundations
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (34 . 0.083235)
:END:

***** 1 Type theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (37 . 0.083235)
:END:

****** 1.1 Type theory versus set theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (37 . 0.379617)
:END:
******* Judgements and rules
Set theory is not only about seta but also about the interplay between /sets/
and /propositions/ of first-order logic, the system where sets are formulated.
In contrast, type theory does not need to be formulated inside any 
superstructure such as first-order logic. It is its own deductive system.

First-order logic is based on only one kind of judgment: whether any
given proposition as a proof; but in type theory, the basic judgment
is $a : A$, where $a$ is an element of the type $A$. Although it could
be seen as an analogous to $a \in A$ in set theory, the difference
resides in that $a \colon A$ is not a proposition but a judgment of
the theory. In particular, we cannot disprove those judgements and we
cannot talk about an element $a$ without specifying its type.

******* Propositional equality
Equality here is not a proposition but a type. Given $a,b : A$, we can define
the type $a =_A b$; we say that $a$ and $b$ are *propositionally equal* when this
type is unhabited.

******* Judgmental equality
*Judgmental equality* or *definitional equality* is an equality judgment
given by definitions: it can be decided expanding out the definitions. 
We write it as $a \equiv b$ and we introduce definitions as $a :\equiv b$.

******* Judgments of type theory
Type theory will be a system based on two forms of judgement

 * $a : A$, meaning $a$ has type $A$.
 * $a \equiv b : A$, meaning that $a$ and $b$ are definitionally equal.

******* Contexts
A *context* is a collection of assumptions in which a judgment may depend on.

# It can be thougt as a parameter space (?)
# https://en.wikipedia.org/wiki/Parameter_space

******* Rules and axioms of type theory
Rules of type theory can be grouped into type formers, procedural ways
to construct types. Usually, no axioms are necessary in type theory.

****** 1.2 Function types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (42 . 0.984877)
:END:
******* Functions
Given types $A,B$, $A \to B$ is the type of *maps* or *functions* between them.
Functions are a primitive concept of type theory; given $f : A \to B$, it can
be applied to $a \colon A$ to obtain $f a : B$.

******** Constructing functions
Given $\Phi$, an expression of type $B$ assuming $x : A$; we can define a function
as

\[
f(x) :\equiv \Phi,
\]

and also as a \lambda-expression, written as

\[
(\lambda (x:A) . \Phi) : A \to B,
\quad
\text{ or even }
\quad
(x \mapsto \Phi) : A \to B.
\]

******* \beta-reduction
*\beta-reduction* is a computation rule defined by

\[
(\lambda x. \Phi) (a) \equiv \Phi',
\]

where every ocurrence of $x$ in $\Phi$ has been replaced by $a$ in $\Phi'$, in a way that
the binding structure is preserved; maybe renaming variables.

******* \eta-reduction
*\eta-reduction*, often called /uniqueness principle for function types/
is the computation rule defined by

\[
f \equiv (\lambda x. f(x)).
\]

******* Currying
*Currying* is a way to define multiple-input functions as functions returning
partially aplied functions. For example, $f : A \to (B \to C)$ can be applied
to two arguments as $(f\ a)\ b : C$.

****** 1.3 Universes and families
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (46 . 0.516406)
:END:
******* Universes
A *universe* is a type whose elements are types.

******** Russell's paradox
As in set theory, a universe of all types including itself, ${\cal U}_{\infty} : {\cal U}_{\infty}$, is
unsound.

******** Hierarchy of universes
A cumulative hierarchy of universes is defined, where every universe
is an elemtn of the next universe, ${\cal U}_i : {\cal U}_{i+1}$; and all the elements of
a universe are elements of all the higher universes.

\[
{\cal U}_0 : {\cal U}_1 : {\cal U}_2 : \dots
\]

*Typical ambiguity* is the writing style where we omit the level unless
it is necessary.

******* Families of types
A *family of types*, is a collection of types varying over a type
variable $A$. They are functions whose codomain is a universe, $f : A \to {\cal U}$.

****** 1.4 Dependent function types (Î -types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (47 . 0.768555)
:END:
******* Dependent function types
Given $A : {\cal U}$ and $B : A \to {\cal U}$, we construct the type of *dependent functions*
as $\prod_{(x:A)}B(x) : {\cal U}$.

******** Constructing dependent functions
Given $\Phi : B(x)$, a expression assuming $x : A$, we can use \lambda-abstraction
to write

\[
\lambda x . \Phi(x) : \prod_{(x:A)} B(x).
\]

******** Reductions
\beta and \eta-reductions still hold on dependent functions.

******* Polymorphic functions
A *polymorphic function* takes a type as one of its arguments, and acts on
elements of that type.

******** The identity function
The polymorphic identity function $\mathrm{id} : \prod_{(A:{\cal U})} A \to A}$ is defined as
$\mathrm{id} =& \lambda (A:{\cal U}) . \lambda (x:A) . x$.

****** 1.5 Product types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (49 . 0.861916)
:END:
******* Cartesian product
Given $A,B : {\cal U}$, the *cartesian product type* $A \times B : {\cal U}$ contains pairs
$(a,b) : A \times B$, where $a:A$ and $b:B$. A function on a product type is
defined by

\[
f((a,b)) :\equiv g(a)(b),
\]

where $g : A \to B \to C$.

******* Unit type
The *unit type* $1$ has a unique element $\star : 1$.

******* TODO Introducing new types
******* Product type recursor
The *recursor* for product types symbolizes the fact that we can define a 
function on a product type only by giving its value on pairs,

\[ \mathtt{rec}_{A \times B}(C,g,(a,b)) = g(a)(b),
\]

where it has type

\[ \mathtt{rec}_{A \times B} : \prod_{C : {\cal U}} (A \to B \to C) \to A \times B \to C.
\]

******* TODO Unit type recursor

******* TODO Product type dependent recursor
******* TODO Propositional uniqueness principle
******* Induction principle on product types
The induction principle on product types has type

\[ \mathtt{ind}_{A \times B} :
\prod_{C : A \times B \to {\cal U}}
\left( \prod_{(x:A)} \prod_{(y:B)} C((x,y)) \right) \to
\prod_{(x:A \times B)} C(x)
\]

and defining equation $\mathtt{ind}_{A \times B} (C,g,(a,b)) :\equiv g(a)(b)$.

******* TODO Induction principle on unit types

****** 1.6 Dependent pair types (Î£-types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (54 . 0.861155)
:END:
******* TODO Type-theoretic axiom of choice
******* Example: Magmas
We can define a *magma* as

\[ \mathtt{magma} :\equiv
\sum_{A : {\cal U}} A \to A \to A.
\]

****** 1.7 Coproduct types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (59 . 0.149731)
:END:
******* TODO Coproduct type
******* TODO Empty type

****** 1.8 The type of booleans
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (60 . 0.67055)
:END:
******* TODO if-then-else
******* TODO Coproducts as dependent types
****** 1.9 The natural numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (62 . 0.984877)
:END:
******* TODO Natural numbers
******* Addition
We define $\mathsf{add} : \mathbb{N} \to \mathbb{N} \to \mathbb{N}$ as

 * $\mathsf{add}(0,n) \equiv n$,
 * $\mathsf{add}(\succ(m),n) \equiv \succ(\mathsf{add}(m,n))$.

******* TODO Associativity
****** 1.10 Pattern matching and recursion
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (66 . 0.793771)
:END:
We would like to define a function only writing its /defining equations/.
An example of this is this =double= function

\[\begin{aligned} 
\mathtt{double}(0) &:\equiv 0 \\ 
\mathtt{double}( \mathtt{succ}(n) ) &:\equiv \mathtt{succ} (\mathtt{succ} (\mathtt{double} (n))).
\end{aligned}\]

This style is called *pattern matching*; it is similar to recursion but
it is limited in the recursive calls it can use. Explicitly, it can be used
only as a shorthand for writing a definition using the recursor. Given

\[\begin{aligned} 
f(0) &:\equiv \Phi_0 \\ 
f( \mathtt{succ}(n) ) &:\equiv \Phi_{s},
\end{aligned}\]

we need $\Phi_s$ to depend on $f$ only via $f(n)$ in order to be well-defined as

\[
f :\equiv \mathtt{rec}_{\mathbb{N}} (C,\Phi_0,\lambda n. \lambda r. \Phi'_{s}).
\]

****** 1.11 Propositions as types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (68 . 0.66391)
:END:
An element of the type corresponding to a proposition is a *witness* or 
a *proof* of the truth of that proposition. From this perspective, proofs
are mathematical objects per se.

******* Constructive logic
The natural interpretation of propositions-as-types is /constructive/,
meaning that certain tautologies on classical logic, such as the 
*law of excluded middle* (LEM) do not hold.

The logic is still compatible with the presence of the LEM as an axiom.

****** 1.12 Identity types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (76 . 0.503446)
:END:

\[ \mathsf{refl} : \prod_{a:A} (a =_A a) \]

******* 1.12.1 Path induction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (78 . 0.567577)
:END:

******* 1.12.2 Equivalence of path induction and based path induction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (82 . 0.695977)
:END:

******* 1.12.3 Disequality
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (85 . 0.354787)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (85 . 0.684066)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (88 . 0.572359)
:END:
******* DONE Exercise 1.1
#+begin_statement
Given functions $f : A \to B$ and $g : B \to C$, define their composite
$g \circ f : A \to C$. Show that we have $h \circ (g \circ f) \equiv (h \circ g) \circ f$.
#+end_statement

We define

\[
g \circ f :\equiv \lambda x.g(f(x))
\]

and thus

\[\begin{aligned}
h \circ (g \circ f) &:\equiv \lambda x. h((g \circ f)(x)) \\ 
&\equiv \lambda x. h((\lambda y.g(f(y)))(x)) \\
&\equiv \lambda x. h(g(f(x))) \\
&\equiv \lambda x. (\lambda y. h(g(y)))(f(x)) \\
&\equiv (h \circ g) \circ f.
\end{aligned}\]

******* TODO Exercise 1.2
******* TODO Exercise 1.3
******* TODO Exercise 1.4
#+begin_statement
Assuming as given only the iterator for natural numbers

\[
\mathsf{iter} : \prod_{C:{\cal U}} C \to (C \to C) \to \mathbb{N} \to C.
\]

with the defining equations

 * $\mathsf{iter}(C,c_0,c_s,0) :\equiv c_{0}$,
 * $\mathsf{iter}(C,c_0,c_s,\succ(n)) :\equiv c_s(\mathsf{iter}(C,c_0,c_s,n))$,

....
#+end_statement

******* TODO Exercise 1.5
******* DONE Exercise 1.10
#+begin_statement
Show that the Ackermann function $\mathsf{ack} : \mathbb{N} \to \mathbb{N} \to \mathbb{N}$ is definable using
only $\mathsf{rec}_{\mathbb{N}}$ satisfying the following equations

 * $\mathsf{ack}(0,n) \equiv \mathsf{succ}(n)$,

 * $\mathsf{ack}( \mathsf{succ}(m),0) \equiv \mathsf{ack}(m,1)$,

 * $\mathsf{ack}(\mathsf{succ}(m), \mathsf{succ}(n)) \equiv \mathsf{ack}(m, \mathsf{ack}(\succ(m),n)$.
#+end_statement

We can define

\[
\rec_{\mathbb{N}}\ \succ\ 
(\lambda m. \lambda a_m. 
\rec_{\mathbb{N}}\ (a_m\ 1)\ (\lambda n. \lambda a_{mn}. a_m\ a_{mn})
)
\]

where we can take $a_{m}$ to mean the $\mathsf{ack}$ function partially applied to $m$,
whereas we can take $a_{mn}$ to mean $\mathsf{ack}(m,n)$. With these definitions, we
have the base and successor equalities judgmentally.

******* DONE Exercise 1.11
#+begin_statement
Show that for any type $A$ we have $\neg\neg\neg A \to \neg A$.
#+end_statement

We write the function 

\[
\lambda f. \lambda a. f (\lambda h. h(a)) : \neg\neg\neg A \to \neg A
\]

where $f : \neg\neg\neg A$, $h : \neg A$ and $(\lambda h. h(a)) : \neg\neg A$.

******* DONE Exercise 1.12
#+begin_statement
Using the propositions as types interpretation, derive the
following tautologies

 1) if A, then (if B then A);
 2) if A, then not (not A);
 3) if (not A or not B), then not (A and B).
#+end_statement

We define the following terms

 * $\lambda a.\lambda b.a : A \to (B \to A)$;
 * $\lambda a.\lambda f.f(a) : A \to \neg\neg A$;
 * $\lambda u. \rec(u, \lambda f.\lambda (a,b).f(a), \lambda g.\lambda (a,b). g(b))$;

with the desired types.

******* DONE Exercise 1.13
#+begin_statement
Using propositions-as-types, derive the double negation of the principle
of excluded middle, i.e., prove /not (not (P or not P))/.
#+end_statement

We can define a function

\[
(\lambda f. f (\inr (\lambda p. f (\inl (p))))) 
\]

whose type is $\neg (\neg (P \vee \neg P))$ for any given $P$.

******* TODO Exercise 1.15
******* TODO Exercise 1.16
#+begin_statement
Show that addition of natural numbers is commutative,

\[
\prod_{i,j : \mathbb{N}} i + j = j + i.
\]
#+end_statement

We proceed by induction on $i$. In the first case, we have to
prove $\prod_{j : \mathbb{N}} 0 + j = j + 0$; and this can be done by induction
on $j$. In fact,

 * $\mathsf{commzero}(0) = \refl_0$,
 * $\mathsf{commzero}(S(n)) = \ap_{\succ}(\mathsf{commzero}(n))$.

***** 2 Homotopy type theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (91 . 0.083235)
:END:

In homotopy type theory, each type has the structure of an
$\infty\text{-groupoid}$, arising from the induction principle for
identity types.

Homotopy type theory provides a /synthetic/ description of the spaces,
in contrast with the usual analytic approach of topology.

****** 2.1 Types are higher groupoids
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (95 . 0.682291)
:END:

******* 2.1.1. Path inverse
Given $x,y : A$, there is a function called *inverse*

\[
(-)^{-1} : (x = y) \to (y = x)
\]

such that $\refl^{-1} = \refl$.

******** Proof
Given $p : x = y$, we apply path induction and then provide $\refl : x = x$.

******* 2.1.2. Path composition
Given $x,y,z : A$, there is a function called *concatenation*

\[
\cdot : (x = y) \to (y = z) \to (x = z)
\]

such that $\refl \cdot \refl = \refl$.

******** First proof
Given $p \cdot q$, we apply path induction on $p$ and $q$. Definitionally,
we can provide an element of $x = x$,

\[
\refl \cdot \refl = \refl
\]

******** Second proof
We apply path induction over $p$, and provide $q$ as an element
of $x = z$. We have $\refl \cdot q \equiv q$.

******** Third proof
We apply path induction over $q$, and provide $p$ as an element
of $x = y$. We have $p \cdot \refl \equiv p$.

******** Proof-relevance and definitional equalities
These three proofs are not definitionally equal, and they provide
different functions with sightly different definitions. In particular,
we get three different definitional equalities

 1) $\refl \cdot \refl \equiv \refl$,

 2) $p \cdot \refl \equiv p$,

 3) $\refl \cdot q \equiv q$;

and, while doing informal mathematics, we will prefer the symmetry of
the first one.

******* TODO 2.1.4. Path operation properties

******* TODO 2.1.6. Eckmann-Hilton
******* 2.1.7. Pointed type
A *pointed type* is a type with a basepoint of that type. That is,
${\cal U}_{\bullet} :\equiv \sum_{A:{\cal U}} A$ is the type of pointed types.

******* 2.1.8. Loop spaces
Given a pointed type $(A,a)$, we define the *loop space* as

\[
\Omega(A,a) :\equiv ((a=a),\refl_a)
\]

and the *n-fold iterated loop space* recursively as

 * $\Omega^0(A,a) :\equiv (A,a)$,

 * $\Omega^{n+1}(A,a) :\equiv \Omega^n(\Omega(A,a))$.

****** 2.2 Functions are functors
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (106 . 0.600341)
:END:

******* 2.2.1. Definition of ap
Given $f: A \to B$, there is an operation

\[
\ap_f : x=y \to f(x) = f(y)
\]

such that $\ap_f(\refl) \equiv \refl_{f(x)}$.

******** Notation
We write $\ap_f(p)$ as $f(p)$.

******** Proof
Trivially defined by path induction.

******* 2.2.2. Functoriality of ap
Given $f : A \to B$ and $g : B \to C$ and paths $p : x = y$ and
$q : y = z$, we have

 1) $\ap_f(p \cdot q) = \ap_f(p) \cdot \ap_f(q)$
 2) $\ap_f(p^{-1}) = ap_f(p)^{-1}$
 3) $\ap_g(\ap_f(p)) = \ap_{g \circ f}(p)$
 4) $\ap_{id_A}(p) = p$

******** Proof
Trivial by path induction on $p$.

****** 2.3 Type families are fibrations
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (107 . 0.773135)
:END:
******* 2.3.1. Transport
Given $P : A \to {\cal U}$ and $p : x = y$, there exists a function

\[
p_{\ast} : P(x) \to P(y),
\]

such that $\refl_{\ast}$ is the identity.

******** Notation
Sometimes we notate transport as

\[
p_{\ast} \equiv \transport^P(p,-) : P(x) \to P(y).
\]

******** Proof
Applying path induction over $p$, $x \equiv y$ and $\id : P(x) \to P(x)$
is an inhabitant of the type.

******* 2.3.2. Path lifting property
Given $P : A \to {\cal U}$ and $u : P(x)$, for any $p : x = y$,

\[
\mathsf{lift}(u,p) : (x,u) = (y, p_{\ast}(u));
\]

in $\sum_{x:A}P(x)$ such that $\mathsf{pr}_1(\mathsf{lift}(u,p)) = p$.

******** Proof
The first component is given by $p$, the second one can be defined
applying path induction over $p$ and, knowing that $x \equiv y$ and thus,
$u \equiv p_{\ast}(u)$.

******* 2.3.4. Dependent map
Given $f : \prod_{x:A} P(x)$ there exists a map

\[
\apd_f : \prod_{p : x=y} p_{\ast}(f(x)) =_{P(y)} f(y)
\]

******** Proof
Path induction.

******* TODO 2.3.5. Constant transport
******* TODO 2.3.8. Constant plus dependent transport
******* 2.3.9. Transport composition lemma
Given $P : A \to {\cal U}$, $p : x = y$ and $q : y = z$, for $u : P(x)$ we have

\[
q_{\ast}(p_{\ast}(u)) = (p \cdot q)_{\ast} (u).
\]

******** Proof
Double path induction.

******* TODO 2.3.10. Transport precomposition lemma
******* TODO 2.3.11. Naturality of transport
****** 2.4 Homotopies and equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (113 . 0.304028)
:END:
******* 2.4.1. Homotopy
A *homotopy* between $f, g : \prod_{x:A} P(x)$ is a dependent function
of type

\[
(f \sim g) :\equiv \prod_{x:A} f(x) = g(x).
\]

******* 2.4.2. Homotopy is an equivalence relation
Homotopy is an equivalence relation on each dependent function
type $\prod_{x:A} P(x)$. We have elements of

 1) reflexivity

    \[
    \prod_{f:\prod_{x:A} P(x)} (f \sim f)
    \]

 2) symmetry

    \[
    \prod_{f,g : \prod_{x:A}P(x)} (f \sim g) \to (g \sim f)
    \]

 3) transitivity

    \[
    \prod_{f,g,h : \prod_{x:A} P(x)} (f \sim g) \to (g \sim h) \to (f \sim h)
    \]

******** Proof
Given any $f$ and $x$, $\refl$ is of type $f(x) = f(x)$.

Given any $f,g$ such that $f \sim g$, for every $x$, we have an inhabitant of
$f \sim g$. By path induction, it must be $\refl$, so $\refl : g(x) = f(x)$.
 
Given any $f,g,h$ such that $f \sim g$ and $g \sim h$, for every $x$, we
have $f(x) = g(x) = h(x)$, and, in particular $f(x) = h(x)$.

******* 2.4.3. Naturality of homotopies
Given $H : f \sim g$ and $p : x = y$, 

\[
H(x) \cdot g(p) = f(p) \cdot H(y).
\]

As a commutative diagram,

\[\begin{tikzcd}
f(x)\rar[equal]{f(p)} \dar[swap,equal]{H(x)} & 
f(y)\dar[equal]{H(y)} \\
g(x)\rar[equal]{g(p)} &
g(y)
\end{tikzcd}\]


******** Proof
By path induction, $p = \refl$, and $\ap$ computes on reflexivity.

******* 2.4.4. Endonaturality of homotopies
Given $H : f \sim \id_{A}$, for any $x : A$,

\[
H(f(x)) = f(H(x))
\]

******** Proof
By naturality, and knowing that $H(x) : f(x) = x$, 

\[\begin{tikzcd}
f(x)\rar[equal]{f(H(x))} \dar[swap,equal]{H(x)} & 
f(f(x))\dar[equal]{H(f(x))} \\
x\rar[equal]{H(x)} &
f(x)
\end{tikzcd}\]

thus,

\[
f(H(x)) \cdot H(x) = H(f(x)) \cdot H(x),
\]

and then $f(H(x)) = H(f(x))$.

******* 2.4.6. Quasi-inverse
A *quasi-inverse* of $f : A \to B$ is a triple $(g,\alpha,\beta)$ with homotopies
$\alpha : f \circ g \sim \id_B$ and $\beta : g \circ f \sim \id_A$.

\[
\mathsf{qinv}(f) = \sum_{g:B \to A} (f \circ g \sim \id) \times (g \circ f \sim \id).
\]

******* 2.4.9. Transport has a quasi-inverse
The transport $p : x = y$ for $P \colon A \to {\cal U}$,

\[
\mathsf{transport}^P(p,-) : P(x) \to P(y)
\]

has a quasiinverse $\transport^P(p^{-1},-)$.

****** 2.5 The higher groupoid structure of type formers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (117 . 0.979938)
:END:

****** 2.6 Cartesian product types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (119 . 0.680777)
:END:
******* 2.6.2. Cartesian product equalities
For any $x,y$, the function

\[
x = y \to (\proj_1(x) = \proj_1(y)) \times (\proj_2(x) = \proj_2(y))
\]

given by applying projections to the equality, is an equivalence.
We denote the quasiinverse as

\[
\mathsf{pair}^{=} :
(\proj_1(x) = \proj_1(y)) \times (\proj_2(x) = \proj_2(y))
\to
x = y.
\]

******** Proof
We will define a function in the other direction. By induction,
we assume $x \equiv (a,b)$ and $y \equiv (a',b')$; thus we have $a = a'$
and $b = b'$. We apply path induction to both paths and we
get that $(a,b) \equiv (a',b')$.

Now we have to prove that it is a quasiinverse. In one direction,
if we have $r : x = y$, we apply path induction and we get the
pair $(\refl_{\proj_1(x)}, \refl_{\proj_2(x)})$. If we apply induction to $x$, we
get $(\refl_a,\refl_{b})$; our inverse takes this to $\refl_{(a,b)}$.

In the other direction, if we have $p : a = a'$ and $q : b = b'$,
we apply induction to get $\refl_{(a,b)}$; applying a function to
reflexivity gives again $(\refl_a, \refl_b)$.

******* 2.6.4. Cartesian product transport
Given two type families $A,B : Z \to {\cal U}$ and a path $p : z = w$,
for every $x : A(z) \times B(z)$,

\[
p_{\ast}(x) = (\transport^A(p,\proj_1(x)), \transport^B(p,\proj_2(x)))
\]

******** Proof
By path induction, it remains to prove

\[
x = (\proj_1(x), \proj_2(x)),
\]

which is definitionally equal.

******* 2.6.5. Functoriality under cartesian products
Given $x,y : A \times B$, $p,q$ path between components. For every function
defined as $f(x) :\equiv (g(\proj_1(x)), h(\proj_2(x)))$, it holds that

\[
f(\mathsf{pair}^{=}(p,q)) = \mathsf{pair}^{ =}(g(p),h(q)).
\]

******** Proof
We first apply induction over $x$, and then path induction over
$p,q$. We get reflexivity in both sides.

****** 2.7 Î£-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (123 . 0.273722)
:END:
******* 2.7.2. Sigma type equalities
Given a type family $P : A \to {\cal U}$, there is an equivalence

\[
(w = w') \simeq \sum_{p : \proj_1(w) = \proj_1(w')} 
p_{\ast}(\proj_2(w)) = \proj_2(w'). 
\]

This can be seen as an introduction $\pair^{=}$ and elimination rules
for equalities between dependent pairs.

******** Proof
********* First component of the equivalence
We define the first part of the equivalence depending on
$w,w' : \sum_{x:A}P(x)$, of type

\[
f : \prod_{w,w' : \sum_{x:A}P(x)} 
\left(
(w=w') \to
\sum_{p:\proj_1(w) = \proj_1(w')} p_{\ast}(\proj_2(w)) = \proj_2(w')
\right)
\]

by induction on the path $w = w'$ as

\[
f(w,w,\refl) = (\refl_{\pr_1(w)}, \refl_{\pr_2(w)}).
\]

********* Second component of the equivalence
And we define the second part of the equivalence depending
again on both $w,w'$, of type

\[
g : \prod_{w,w' : \sum_{x:A}P(x)}
\left( \left(
\sum_{p:\pr_1(w) = \pr_1(w')}  
p_{\ast}(\pr_2(w)) = \pr_2(w')
\right)
\to (w = w')
\right)
\]

defined by induction on $w = (x,y)$ and $w' = (x',y')$ first and then on
$p : x = x'$ and $p_{\ast}(y) = y'$, to get

\[
g((x,y),(x,y),\refl,\refl) = \refl_{(x,y)}.
\]

********* First homotopy
Finally, we have to show that they form an equivalence. Given any $w,w'$ and

\[
r : \sum_{p:\pr_1(w) = \pr_1(w')} p_{\ast}(\pr_2(w)) = \pr_2(w'),
\]

we can apply induction over both $w = (x,y)$ and $w' = (x',y')$, and then over
$r$ to get paths $p : x = y$ and $p_{\ast}(y) = y'$. By path induction and the definition
of $f$ and $g$, we get the desired result, $f(g(r)) = r$.

********* Second homotopy
On the other hand, if we have $p : w = w'$, we can directly apply path induction
and use the definitions to get $g(f(p)) = p$.

******* 2.7.3. Sigma equality to its parts
For any $z : \sum_{x:A}P(x)$, we have $z = (\pr_1(z),\pr_2(z))$.

******** Proof
By induction on $z = (x,y)$, we trivially arrive at an
identity path.

# HoTT MAILING LIST !

******** Proof in HoTT book
Applying the [[*2.7.2. Sigma type equalities][previous lemma]], we only have to provide evidence
for the equality of both projections. We trivially have

\[
\pr_1(z) = \pr_1(\pr_1(z),\pr_2(z))
\]

and by judgmental equality, it is trivial that

\[
(\refl_{\pr_1(z)})_{\ast}(\pr_2(z)) = \pr_2(z) = \pr_2(\pr_1(z),\pr_2(z)).
\]

******* 2.7.4. Transport over sigma equalities
Given $P : A \to {\cal U}$ and

\[
Q : \left( \sum_{x:A} P(x) \right) \to {\cal U},
\]

for any path $p : x = y$, and $(u,z) : \sum_{u:P(x)} Q(x,u)$ we have

\[
p_{\ast}(u,z) = 
(p_{\ast}(u), \pair^{=}(p,\refl_{p_{\ast}(u)})_{\ast} (z)).
\]

****** 2.8 The unit type
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (125 . 0.984877)
:END:
******* Unit type equality
Given $x,y:1$, we have $(x = y) \simeq 1$.

******** Proof
A function $(x = y) \to 1$ is defined trivially; and given any $x, y : 1$
we now by induction that $x \equiv y$ and we can write a constant function
to $\refl_{\star}$.

Given an element $u : 1$, it is trivial that the composite is an element
of $1$, and therefore both are equal to $\star$. Given an element $p : x = y$,
we can apply path induction to get $p = \refl_{x}$ and induction over $x$ to
get $\refl_{\star}$. As a consequence, $p$ goes to $\refl_{\star}$.

****** 2.9 Î -types and the function extensionality axiom
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (126 . 0.733191)
:END:
******* 2.9.2. happly
There exists a function

\[
\happly : (f = g) \to \prod_{x:A} f(x) = g(x)
\]

defined by path induction.

******* 2.9.3. Function extensionality axiom
The function $\happly$ is an equivalence. It has a quasi-inverse given
by

\[
\funext : \left(\prod_{x:A} f(x) = g(x)\right) \to (f = g).
\]

such that, for any $h : \prod_{x:A} f(x) = g(x)$,

\[
\happly(\funext(h), x) = h(x).
\]

******* TODO 2.9.4. Dependent identity, inverses and composition

******* 2.9.4. Rules for dependent transport
Given $f : A(x) \to B(x)$ and $p : x = y$,

\[
p_{\ast}(f) = p_{\ast}\circ f \circ p^{-1}_{\ast}.
\]

******** Proof
Path induction.

******* 2.9.6. Equivalence for the dependent function equality
Given $A,B : X \to {\cal U}$, $p : x = y$ and two functions $f : A(x) \to B(x)$
and $g : A(y) \to B(y)$, we have an equivalence

\[
(p_{\ast}(f) = g) \simeq \prod_{a:A(x)} p_{\ast}(f(a)) = g(p_{\ast}(a)).
\]

Moreover, given $q : p_{\ast}(f) = g$, we have

\[
\happly(q,p_{\ast}(a)) : (p_{\ast}(f))(p_{\ast}(a)) = g(p_{\ast}(a))
\]

equal to the composite

\[
p_{\ast}(f)(p_{\ast}(a)) = p_{\ast}(f(p^{-1}_{\ast}(p_{\ast}(a))))
= p_{\ast}(f(a)) = g(p_{\ast}(a)).
\]

******** Proof
By path induction on $p$, we arrive to function extensionality.
Computation rule for function extensionality gives us the value
of $\happly$.

******* TODO 2.9.7. Transport equivalence between families

****** 2.10 Universes and the univalence axiom
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (129 . 0.774211)
:END:
******* 2.10.1. idtoeqv
Given any types $A,B : {\cal U}$, there is a function

\[
\idtoeqv : (A = B) \to (A \simeq B).
\]

******** TODO Proof

******* 2.10.3. Voevodsky's Univalence Axiom
A universe is univalent if for any $A,B : {\cal U}$, $\idtoeqv$ is an equivalence.
All universes are univalent. There exists

\[\ua : (A \simeq B) \to (A = B),
\]

such that 

\[
\transport(\ua(f), x) = f(x).
\]

******* TODO 2.10.5. Transport and idtoeqv
****** 2.11 Identity type
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (131 . 0.796127)
:END:
******* 2.11.1. Aplication of equivalences is an equivalence
If $f : A \to B$ is an equivalence, so is

\[
\mathsf{ap}_f : (a = a') \to (f(a) = f(a')).
\]

******** Proof
Let $f^{-1}$ be a quasiinverse with homotopies

\[
\alpha : \prod_{b:B} f(f^{-1}(b)) = b
\quad\mbox{ and }\quad
\beta : \prod_{a:A}f^{-1}(f(a)) = a.
\]

the quasiinverse of $\ap_f$ will be $\ap_{f^{-1}}$ concatenated with $\beta^{-1}$ and $\beta$.
We will show that this is a quasiinverse. On one direction,

\[
\beta_a^{-1} \cdot \ap_{f^{-1}}(\ap_f(p)) \cdot \beta_{a'} = p
\]

is true by [[*2.4.4. Endonaturality of homotopies][endonaturality of the homotopy]] $\beta$ and functoriality
of the application $\ap_{f^{-1}} \circ \ap_f = \ap_{f^{-1} \circ f}$.

******* 2.11.2. Path transport
Given any $a : A$ with $p : x_1 = x_2$,

 1) for $q : a = x_1$, we have $\transport^{x \mapsto a=x}(p,q) = p_{\ast}(q) = q \cdot p$;
 2) for $q : x_1 = a$, we have $\transport^{x\mapsto x=a}(p,q) = p^{-1} \cdot q$;
 3) for $q : x_1 = x_1$, we have $\transport^{x\mapsto (x=x)}(p,q) = p^{-1} \cdot q \cdot p$.

******** Proof
By path induction on $p$, we get the composition rules for
reflexivity.

****** 2.12 Coproducts
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (134 . 0.228504)
:END:
******* 2.12.1. Characterization of equalities for coproducts
Given a coproduct type $A + B$, 

 * $(\inl(a_1) = \inl(a_2)) \simeq (a_1 = a_{2})$,
 * $(\inr(b_1) = \inr(b_2)) \simeq (b_1 = b_2)$,
 * $(\inl(a) = \inr(b)) \simeq 0$.

******** Proof
Given $a_0 : A$ we will characterize the family

\[
(x \mapsto (\inl(a_0) = x)) : A + B \to {\cal U},
\]

using the following type family

 * $\code(\inl(a)) :\equiv (a_0 = a)$,
 * $\code(\inr(a)) :\equiv 0$.

and proving that $(\inl(a_0) = x) \simeq \code(x)$ in the following
[[*2.12.5. Code for coproducts][lemma]]. An analogous family $(x \mapsto (\inr(b_0) = x))$ can be also
characterized.

******* 2.12.5. Code for coproducts
Given $a_0 : A$, for all $x:A+B$, we have $(\inl(a_0) = x) \simeq \code(x)$;
with the definition presented in the previous [[*2.12.1. Characterization of equalities for coproducts][proof]].

******** Proof
We first define a function

\[
\encode : \prod_{(x:A+B)} \prod_{(p : \inl(a_0) = x)} \code(x)
\]

using transport, as $\encode(\inl(a), p) = p_{\ast}(\refl_{a_0})$. Next, we define
a function

\[
\decode : \prod_{(x : A+B)}\prod_{(c : \code(x))} (\inl(a_0) = x)
\]

by induction on $x$ as

 * $\decode(\inl(a), c) :\equiv \ap_{\inl}(c)$,
 * $\decode(\inr(b),c) :\equiv \mathsf{abort}(c)$.

Now, we must prove that they form an equivalence. On the one hand, given
$x : A +B$ and $p : \inl(a_0) = x$, we must show that

\[
\decode(x,\encode(x,p)) = p;
\]

and this can be done by path induction on $p$. On the other hand, given
any $c : \code(x)$, we want to prove that

\[
\encode(x,\decode(x,c)) = c;
\]

and we can proceed by induction on $x$; if $x \equiv \inr(b)$, then we arrive at
a contradiction in $c$; in other case, $x \equiv \inl(a)$ so we can apply path
induction on $c$.

****** 2.13 Natural numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (137 . 0.857941)
:END:
******* 2.13.0. Codes for identities on natural numbers
We define $\mathsf{code} \colon \mathbb{N} \to \mathbb{N} \to {\cal U}$ by double recursion as

 * $\code(0,0) :\equiv 1$,
 * $\code(\succ(m),0) :\equiv 0$,
 * $\code(0,\succ(n)) :\equiv 0$,
 * $\code(\succ(m),\succ(n)) :\equiv \code(m,n)$,

and trivially, a diagonal function $r : \prod_{n:\mathbb{N}} \code(n,n)$ by induction.

******* 2.13.1. Equivalence code-identity
We have $(n = m) \simeq \code(m,n)$.

******** Proof
********* Encode function
We define a function $\prod_{m,n \colon \mathbb{N}} (n = m) \to \code(m,n)$ by transport
and using the diagonal $r : \prod_{n:\mathbb{N}} \code(n,n)$.

********* Decode function
We define a function $\prod_{m,n\colon \mathbb{N}} \code(m,n) \to (n = m)$ by double induction
on $n$ and $m$.

 * On the case $n=m=0$, we define a function to $\refl_0$.
 * On the cases where only one of them is zero, we arrive a contradiction.
 * On the case were both are successors, we have an element $\code(m,n)$, so
   we can recursively apply the decode function to it to get $m = n$. Now
   it suffices to use $\ap_{\succ}$.

********* Quasiinverses I
We will show first that given any $p : m = n$,

\[
\decode(n,n,\encode(n,n,\refl)) = \refl,
\]

which is to show $\decode(n,n,r(n)) = \refl$. This can be done by induction
on $n$, where in the case $0$, we get reflexivity and in the successor case,
we use that $\ap(\refl) = \refl$.

********* Quasiinverses II
Given any $c : \code(m,n)$, we can apply double induction.

 * In the zero case, we have a unit type that remains the same after
   encoding.
 * In the only one successor case, we arrive a contradiction.
 * In the both successor cases, 
   \[\begin{aligned}
   \encode&(\succ(m),\succ(n),\decode(\succ(m),\succ(n),c)) \\
     &= \encode(\succ(m),\succ(n),\ap_{\succ}(\decode(m,n,c))) \\
     &= (\ap_{\succ}(\decode(m,n,c)))_{\ast} (r(\succ(m))) \\
     &= (\decode(m,n,c))_{\ast} (r(m)) \\
     &= \encode(m,n,\decode(m,n,c)) \\
     &= c
   \end{aligned}\]
   by induction.

In other words, we can prove that each code is a diagonal and then
apply induction over $\decode(m,n,c)$.

******* 2.13.2. Zero is not a successor
We have that zero is not the successor of any natural number,
in particular

\[
\encode(\succ(m),0) : (\succ(m) = 0) \to 0.
\]

******** Proof
Applying [[*2.13.1. Equivalence code-identity][decode-encode]] directly.

******* 2.13.3. Successor is injective
The sucessor function is injective, in particular

\[
(\succ(m) = \succ(n)) \to (m = n).
\]

******** Proof
We can apply $\encode$ to the equality and get a new code
to which apply $\decode$. Note that

$\encode(\succ(m),\succ(n)) : \code(m,n)$

is well-typed.

****** 2.14 Example: equality of structures
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (139 . 0.984877)
:END:

******* 2.14.1 Lifting equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (140 . 0.879418)
:END:
******** 2.14.1. Semigroup structures
The type of *semigroup structures* on $A$ is defined as

\[
\mathsf{SemigroupStr}(A) :\equiv \sum_{(m : A \to A \to A)} \prod_{(x,y,z : A)} m(x,m(y,z)) = m(m(x,y),z)
\]

and a *semigroup* is defined in general as

\[
\mathsf{Semigroup} :\equiv \sum_{A : {\cal U}} \mathsf{SemigroupStr}(A).
\]

******** 2.14.1. Induced structures
Given an equivalence $e : A \simeq B$, we can transport semigroup
structures

\[
(\ua(e))_{\ast} : \mathsf{SemigroupStr}(A) \to \mathsf{SemigroupStr}(B).
\]

Given $(m,a) : \mathsf{SemigroupStr}(A)$, we want to compute

\[
\ua(e)_{\ast} (m,a) : \mathsf{SemigroupStr}(B)
\]

and transporting over a [[*2.7.4. Transport over sigma equalities][coproduct]] is the same as transporting over its
components. We will get some $(m',a')$ where

 * $m'(b_1,b_2) :\equiv (\ua(e)_{\ast}(m))(b_1,b_2)$;

 * $a' :\equiv (\pair^{=}(\ua(e), \refl))_{\ast}\ a$.

By function extensionality, we only have to check the behaviour of
$m'$ given a pair of arguments. We have,
# UA is quasi-inverse to transport^(X \to X)

\[\begin{aligned}
m'(b_1,b_2) &=
\ua(e)_{\ast} (m (\ua(e)_{\ast}^{-1} b_1, \ua(e)_{\ast}^{-1} b_2)) \\
&= e(m(e^{-1}b_1,e^{-1}b_2))
\end{aligned}\]

It can be proved that the transported $a'$ works by algebraic
manipulation using this fact.

******* 2.14.2 Equality of semigroups
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (142 . 0.612707)
:END:

****** 2.15 Universal properties
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (143 . 0.779086)
:END:
******* 2.15.2. Universal property of the product
There is an equivalence

\[
(X \to A \times B) \simeq (X \to A) \times (X \to B);
\]

given by $f \mapsto (\pr_1 \circ f, \pr_2 \circ f)$.

******** TODO Proof

******* 2.15.5. Dependent universal property of the product
There is an equivalence

\[
\left( \prod_{x:X} A(x) \times B(x) \right) \simeq
\left( \prod_{x:X} A(x) \right) \times 
\left( \prod_{x:X} B(x) \right)
\]

given by $f \mapsto (\pr_1 \circ f, \pr_2 \circ f)$.

******** TODO Proof

******* 2.15.7. Theorem of choice
There is an equivalence

\[
\left( \prod_{x:X}\sum_{(a : A(x))} P(x,a) \right) \simeq
\left( \sum_{g : \prod_{x:X}A(x)} \prod_{x:X} P(x,g(x)) \right)
\]

trivially determined.

******** TODO Proof

******* 2.15.11. Pullbacks
Given $f : A \to C$ and $g : B \to C$, we define the *pullback* as

\[
A \times_C B :\equiv \sum_{(a:A)}\sum_{(b:B)}(f(a) = g(b)).
\]

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (146 . 0.729471)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (149 . 0.525005)
:END:
******* TODO Exercise 2.1
#+begin_statement
Show that the three obvious proofs of Lemma 2.1.2 are pairwise equal.
#+end_statement

******* TODO Exercise 2.2
******* TODO Exercise 2.3
******* TODO Exercise 2.4
******* TODO Exercise 2.5
******* TODO Exercise 2.6
******* TODO Exercise 2.7
******* TODO Exercise 2.8
******* TODO Exercise 2.9
******* DONE Exercise 2.10
#+begin_statement
Prove that \Sigma-types are associative, in that for any $A : {\cal U}$ and
families $B : A \to {\cal U}$ and $C : \left(\sum_{x:A} B(x)\right) \to {\cal U}$, we have

\[
\left( \sum_{x:A}\sum_{y:B(x)} C(x,y) \right)
\simeq
\left( \sum_{p : \sum_{x:A}B(x)} C(p) \right)
\]
#+end_statement

We first define a function

\[
f : \left( \sum_{x:A}\sum_{y:B(x)} C(x,y) \right)
\to
\left( \sum_{p : \sum_{x:A}B(x)} C(p) \right)
\]

by induction on the argument, as

\[
f (x,y,c) :\equiv ((x,y),c).
\]

Now we have to prove that this is an equivalence with two homotopies,
with an inverse defined by induction

\[
g((x,y),c) :\equiv (x,y,c).
\]

In fact, given any $(x,y,c)$, or any $((x,y),c)$ it is trivial to check
that there exist two homotopies. Note how we use induction to get the
constructors of the pair.
******* TODO Exercise 2.11
#+begin_statement
A homotopy commutative square

\[\begin{tikzcd}
P\rar{h} \dar[swap]{k} & A \dar{f} \\
B\rar{g} & C
\end{tikzcd}\]

consists of functions $f,g,h$ and $k$ as shown, together with a path $f \circ h = g \circ k$.
Note that this is exactly an element of the pullback $(P \to A) \times_{(P \to C)} (P \to B)$
as defined in (2.15.11). A commutative square is called a (homotopy)
*pullback square* if for any $X$, the induced map

\[
(X \to P) \to (X \to A) \times_{(X \to C)} (X \to B)
\]

is an equivalence. Prove that the pullback $P :\equiv A \times_C B$ defined in (2.15.11)
is the corner of a pullback square.
#+end_statement

******* TODO Exercise 2.12
******* DONE Exercise 2.13
#+begin_statement
Show that $(2 \simeq 2) \simeq 2$.
#+end_statement

We have $(2 \simeq 2)$ with two possible elements determined by the function
given by $2 \to 2$. If we take $\id : 2 \to 2$, that is a trivial equivalence,
and if we take $\neg : 2 \to 2$ we have a different equivalence. 

Now, given any function $f : 2 \to 2$, we can apply induction to both
$f(1)$ and $f(0)$ and then, by function extensionality, assert that it
has to be a constant function or some of the previous equivalences.
We only have two possible equivalences then.

We declare a function taking $\id$ to $\mathsf{true}$ and $\neg$ to $\mathsf{false}$, the inverse
is trivially defined.

******* DONE Exercise 2.14
#+begin_statement
Suppose we add to type theory the equality reflection rule which says
that if there is an element $p : x = y$, then in fact $x \equiv y$. Prove that
for any $p : x = x$ we have $p \equiv \refl$.
#+end_statement

Given any $p : x = y$, we have $x \equiv y$, and we can prove the (well-typed!)
equality $p = \refl$ by path induction. Note that we have used the equality
reflection rule to prove that 

\[
\prod_{x,y : A}\prod_{p : x=y} p = \refl_x
\]

is actually well-typed.

******* TODO Exercise 2.15
******* TODO Exercise 2.16
#+begin_statement
Suppose that rather than function extensionality (Axiom 2.9.3),
we suppose only the existence of an element

\[
\mathsf{funext} : \prod_{A:{\cal U}} \prod_{B:A \to {\cal U}} \prod_{f,g : \prod_{x:A}B(x)} (f \sim g) \to (f = g).
\]
#+end_statement

******* TODO Exercise 2.18
#+begin_statement
State and prove a version of Lemma 2.4.3 for dependent functions.
#+end_statement

***** 3 Sets and logic
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (153 . 0.083235)
:END:

****** 3.1 Sets and n-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (153 . 0.661767)
:END:
******* 3.1.1. Sets
A type $A$ is a *set* if every two equalities $p,q : x =_A y$ are equal.

\[
\textsf{isSet}(A) \equiv \prod_{(x,y : A)} \prod_{(p,q : x = y)} p = q.
\]

******* 3.1.6. Dependent product of sets is a set
Given $A$ a set and $B : A \to {\cal U}$ such that each $B(x)$ is a set, $\prod_{x:A} B(x)$ 
is a set.

******** Proof
Suppose $f, g : \prod_{x:A} B(x)$ and $p, q : f = g$. Applying function
extensionality,

 * $p = \mathsf{funext}(\lambda x. \mathsf{happly}(p,x))$,
 * $q = \mathsf{funext}(\lambda x. \mathsf{happly}(q,x))$.

Since $B(x)$ is a set, 

 * $\mathsf{happly}(p,x) : f(x) = g(x)$
 * $\mathsf{happly}(q,x) : f(x) = g(x)$

must be equal. Thus, by function extensionality $(\lambda x. \mathsf{happly}(p,x)) = (\lambda x. \mathsf{happly}(q,x))$,
and applying $\mathsf{funext}$, $p = q$.

******* 3.1.7. 1-types
A type $A$ is a *1-type* if for all $x,y:A$ and $p,q : x = y$ and $r,s : p = q$,
we have $r = s$.

******* 3.1.8. Every set is a 1-type
Every set is a *1-type*.

******** Proof
If we have $x,y : A$, $p,q : x = y$ and $f : \isSet(A)$, then we
can define $g = f(x,y,p)$ by partial application, and

\[
g : \prod_{q : x = y}(p = q);
\]

we can now, given $r : q = q'$, use dependent application to get

\[
\apd_g(r) : r_{\ast}(g(q)) = g(q').
\]

By path transport, that means that $g(q) \cdot r = g(q')$. In particular,
given any two $r,s : p = q$;

\[
g(p) \cdot r = g(q) = g(p) \cdot s
\]

and $r = s$ by cancellation.

******* 3.1.9. Not all types are sets
The universe ${\cal U}$ is not a set.

******** TODO Proof
We take $2$ to be the type of the booleans. There exists a
function $\mathrm{not}\colon 2 \to 2$ which is an equivalence; by univalence,
there exists $\ua(\mathrm{not}) \colon 2 = 2$ which is not $\refl$. If it were
$\refl$, then, by univalence, $0_2 = 1_2$.

****** 3.2 Propositions as types?
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (156 . 0.601855)
:END:

******* 3.2.2. Negation of double negation
It is not true that $\neg(\neg A) \to A$ for each $A : {\cal U}$.

******** Proof
Given $f \colon \prod_{A:{\cal U}} \neg(\neg A) \to A$, we will arrive to a contradiction.

Let $p \colon 2 = 2$ be the non-trivial path of the booleans. We know
that $f(2) : \neg\neg 2 \to 2$ and

\[
\apd_f(p) : p_{\ast}(f(2)) = f(2),
\]

applying [[*2.9.4. Rules for dependent transport][rules for dependent transport]], we have

\[
p_{\ast}(f(2))(u) = (p_{\ast} \circ f(2) \circ p_{\ast}^{-1})(u).
\]

Every two $u,v : \neg\neg 2$ are equal by function extensionality; thus

\[
p^{-1}_{\ast}(u) = u
\]

and so

\[
p_{\ast}(f(2)(u)) = p_{\ast}(f(2))(u) = f(2)(u).
\]

We have now that $\fnot(f(2)(u)) = f(2)(u)$, and, at the same time,
it is obvious that $\prod_{x:2} \neg (\fnot(x) = x)$.

******* 3.2.7. Negation of LEM
It is not true that $A + (\neg A)$ for each $A \colon {\cal U}$.

******** Proof
An element of type $\prod_{A:{\cal U}} \neg\neg A \to A$ can be constructed from
an element of type $\prod_{A:{\cal U}} A + (\neg A)$.

****** 3.3 Mere propositions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (158 . 0.984877)
:END:

******* 3.3.1. Mere proposition
A type $P$ is a *mere proposition* when

\[
\isProp(P) : 
\prod_{x,y : P} x = y
\]

is inhabited.

******* 3.3.2. Truth is the only true mere proposition
If $P$ is a mere proposition and $x_0 : P$, then $P \simeq 1$.

******** Proof
A trivial equivalence can be constructed.

******* 3.3.3. Equivalence of connected mere propositions
If $P$ and $Q$ are mere propositions, $P \to Q$ and $Q \to P$
imply $P \simeq Q$.

******** Proof
If $f : P \to Q$ and $g : Q \to P$, then $f(g(x)) = x$ and
$g(f(x)) = x$ because both are mere propositions.

******* 3.3.4. Mere propositions are sets
Every mere proposition is a set.

******** Proof
Given $f : \isProp(A)$, we fix $x : A$ and define $g(y) :\equiv f(x,y)$
of type $\prod_{y : A} x = y$. Given two $y,z : A$ with $p : y = z$, we
have

\[
\apd_g(p) : p_{\ast}(g(y)) = g(z)
\]

hence $g(y) \cdot p = g(z)$, or $p = g(y)^{-1} \cdot g(z)$; thus given $p,q : x = y$
we have $p = g(x)^{-1} \cdot g(y) = q$.

******* 3.3.5. isProp and isSet are mere propositions
Given any type $A$, the types $\isSet(A)$ and $\isProp(A)$ are mere
propositions.

******** Proof
If we have $f,g : \isProp(A)$, we know that $f(x,y) = g(x,y)$ because
$A$ is a mere proposition. By function extensionality, $f = g$.

If we have $f,g : \isSet(A)$ we know that $f(x,y,p,q) = g(x,y,p,q)$
because $x = y$ is a mere proposition from the fact that $A$ is a set.
By function extensionality, $f = g$.

****** 3.4 Classical vs. intuitionistic logic
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (161 . 0.251814)
:END:

******* 3.4.1. Law of excluded middle
We define the *law of excluded middle* as

\[
\LEM :\equiv \prod_{A : {\cal U}} \Big( \isProp(A) \to (A + \neg A) \Big)
\]

whereas the usual general law of excluded middle is renamed as

\[
\LEM_{\infty} :\equiv \prod_{A : {\cal U}} (A + \neg A).
\]

The law of excluded middle can be assumed as an axiom.

******* 3.4.3. Decidable types
1. A type is *decidable* if $A + \neg A$.

2. A type family is *decidable* if

   \[
   \prod_{a : A} B(a) + \neg B(a)
   \]

3. A type has *decidable equality* if
   
   \[
   \prod_{a,b : A} (a = b) + \neg (a = b)
   \]

The Law of excluded middle says that all mere propositions are
decidable.

****** 3.5 Subsets and propositional resizing
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (163 . 0.371394)
:END:
******* 3.5.1. Uniqueness of dependent sum of mere propositions
Given $P \colon A \to {\cal U}$ such that $P(a)$ is always a mere proposition;
if $u,v \colon \sum_{x:A}P(x)$ are such that $\proj_1(u) = \proj_1(v)$, then
$u = v$.

******** Proof
Given $p : \proj_1(u) = \proj_1(v)$, we only have to show that

\[
p_{\ast}(\proj_2(u)) = \proj_2(v)
\]

and this is true because both are members of $P(\proj_1(v))$, a
mere proposition.

******* 3.5.1. Subtypes
If $P$ is a family of mere propositions, we write

\[
\sum_{x:A} P(x) \equiv \left\{ x : A\mid P(x) \right\}
\]

and call this a *subtype*. We can define membership and subsets
analogously.

******** Subuniverses of sets and mere propositions
We define

 * $\Set_{{\cal U}} :\equiv \left\{ A : {\cal U} \mid \isSet(A) \right\}$,
 * $\Prop_{{\cal U}} :\equiv \left\{ A : {\cal U} \mid \isProp(A) \right\}$.

There are natural maps $\Set_{{\cal U}_i} \to \Set_{{\cal U}_{i+1}}$.

******* 3.5.2. Propositional resizing
*Propositional resizing* is the fact that the natural map
$\Prop_{{\cal U}_i} \to \Prop_{{\cal U}_{i+1}}$ is an equivalence.

Propositional resizing can be taken as an axiom.

******** Omega-indexation of propositions
From propositional resizing follows the existence of $\Omega$, a
type that indexes mere propositions. If propositional resizing
is true, $\Omega :\equiv \Prop_{{\cal U}_0}$.

******** Powersets
If propositional resizing is true, we can define

\[
{\cal P}(A) :\equiv (A \to \Omega),
\]

which is independent of the universe.
****** 3.6 The logic of mere propositions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (165 . 0.61291)
:END:
******* TODO 3.6.1. Product of mere propositions is a mere proposition
******* TODO 3.6.2. Dependent functions to mere propositions are mere propositions
******* TODO 3.6.2. Sums of mere propositions are not mere propositions

****** 3.7 Propositional truncation
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (166 . 0.643534)
:END:

******* 3.7.0. Propositional truncation type
For any $A$ there is a *truncation type* $\trunc{A}$, with constructors

 * $|a| : \trunc{A}$ for any $a : A$;
 * $x=y$ for any $x,y : \trunc{A}$;

ensuring that it is a mere proposition.

******** Recursion principle
If $B$ is a mere proposition and $f : A \to B$, then there exists
$g : \trunc{A} \to B$ such that $g(|a|) \equiv f(a)$ for all $a:A$.

******* 3.7.1. Traditional logical notation
We define

 * $\top :\equiv 1$,

 * $\bot :\equiv 0$,

 * $P \land Q :\equiv P \times Q$,

 * $P \lor Q :\equiv \trunc{P + Q}$,

 * $P \Rightarrow Q :\equiv P \to Q$,

 * $P \Leftrightarrow Q :\equiv P = Q$,

 * $\neg P :\equiv P \to 0$,

 * $\forall (x:A). P(x) :\equiv \prod_{x:A} P(x)$,

 * $\exists (x:A).P(x) :\equiv \trunc{\sum_{x:A} P(x)}$.

******* 3.7.2. Traditional set notation
We define

 * $\left\{ x:A\mid P(x) \right\} \cap \left\{ x:A \mid Q(x) \right\} :\equiv \left\{ x:A \mid P(x) \wedge Q(x) \right\}$,
 * $\left\{ x:A \mid P(x) \right\} \cup \left\{ x:A \mid Q(x) \right\} :\equiv \left\{ x:A\mid P(x) \lor Q(x) \right\}$,
 * $A \setminus \left\{ x:A\mid P(x) \right\} :\equiv \left\{ x:A \mid \neg P(x) \right\}$.

Note how the latter are not complements in the absence of LEM.

****** 3.8 The axiom of choice
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (168 . 0.729181)
:END:
******* 3.8.1. The axiom of choice
Given a $X$ and type families $A : X \to {\cal U}$, $P : \prod_{x:X} (A(x) \to {\cal U})$
such that $X$ and $A(x)$ are always sets and $P(x,a)$ is always a
mere proposition; the *axiom of choice* asserts

\[
\left( \prod_{x:X} \trunc{\sum_{a:A(x)} P(x,a)} \right)
\to
\trunc{\sum_{(g: \prod_{x:X}A(x))} \prod_{(x:X)} P(x,g(x))}.
\]

In logical notation, this means,

\[
\bigg( \forall (x:X). \exists (a:A(x)). P(x,a) \bigg)
\Rightarrow
\left( \exists \bigg(g: \prod_{x:X}A(x)\bigg). \forall (x:X). P(x,g(x)) \right)
\]

******* 3.8.2. Simpler axiom of choice
The axiom of choice is equivalent to 

\[
\left( \prod_{x:X} \trunc{Y(x)} \right) \to
\trunc{ \prod_{x:X} Y(x) }
\]

for any $X$ and $Y(x)$ always sets.

******** TODO Proof

******* TODO 3.8.5. Counterexample to the simpler version

****** 3.9 The principle of unique choice
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (171 . 0.234132)
:END:
******* 3.9.1. Equivalence of mere propositions and truncations
If $P$ is a mere proposition, $P \simeq \trunc{P}$.

******** Proof
We apply the universal property to $\id$ to get $\trunc{P} \to P$;
and we have a $P \to \trunc{P}$ by definition. This [[*3.3.3. Equivalence of connected mere propositions][proves]] an
equivalence of mere propositions.

******* 3.9.2. The principle of unique choice
Given $P \colon A \to {\cal U}$ such that

 * $P(x)$ is always a mere proposition;
 * $\trunc{P(x)}$ is always true.

Then $\prod_{x:A}P(x)$.

******** TODO Proof

****** 3.10 When are propositions truncated?
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (172 . 0.437266)
:END:

****** 3.11 Contractibility
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (175 . 0.459436)
:END:
******* 3.11.1. Contractible type
A type $A$ is *contractible*, or *singleton* if there is a center of
contraction $a : A$ such that $a = x$ for all $x : A$.

\[
\isContr(A) :\equiv \sum_{a:A}\prod_{x:A}(a = x)
\]

******* 3.11.3. Characterization of contractibility
Given $A$, the following are equivalent

 1. $A$ is contractible,
 2. $A$ is a mere proposition, and there is a point $a:A$,
 3. $A$ is equivalent to $1$.

******** Proof
If $A$ is contractible, it has a point $a : A$ and every two other
points are equal to it.

If $A$ is an inhabited mere proposition, it is equivalent to $1$.

And $1$ is contractible.

******* TODO 3.11.4. Contr is a mere proposition
******* TODO 3.11.5. Contractibility of Contr
******* TODO 3.11.6. Dependent product of contractible types
******* TODO 3.11.7. Retracts and contractibility
******* TODO 3.11.8.
******* TODO 3.11.9.
******* 3.11.10. Mere propositions and contractibility
$A$ is a mere proposition iff for all $x,y : A$, the type $x = y$ is
contractible.

******** TODO Proof
If $A$ is a mere proposition, then $x = y$ must be true; it must be
also a set, so $x=y$ must be contractible.

If $x=y$ is contractible, it is inhabited, so $A$ is a mere
proposition.

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (178 . 0.596795)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (179 . 0.721841)
:END:
******* DONE Exercise 3.1
#+begin_statement
Prove that if $A \simeq B$ and $A$ is a set, then so is $B$.
#+end_statement

The equivalence gives us a pair of functions $f : A \to B$ and $g : B \to A$
with homotopies $\eta : g \circ f \sim \id$ and $\epsilon : f \circ g \sim \id$. By naturality of
$\eta$ we have, for any two paths $p,q : x =_B y$, that

\[\begin{tikzcd}
fg(x)\rar[equal]{fg(p)} \dar[swap,equal]{\eta_x} & 
fg(y)\dar[equal]{\eta_y} \\
x \rar[equal]{p} &
y
\end{tikzcd}\]

and

\[\begin{tikzcd}
fg(x)\rar[equal]{fg(q)} \dar[swap,equal]{\eta_x} & 
fg(y)\dar[equal]{\eta_y} \\
x \rar[equal]{q} &
y
\end{tikzcd}\]

but $g(p) = g(q)$ because $A$ is a set, so $fg(p) = fg(q)$ and therefore, $p=q$.

******* DONE Exercise 3.2
#+begin_statement
Prove that if $A$ and $B$ are sets, then so is $A + B$.
#+end_statement

Note that $A + B = \prod_{x:2} C(x)$ for some family $C$, and we know that the
[[*3.1.6. Dependent product of sets is a set][dependent product of sets is a set]].

******* TODO Exercise 3.3
******* TODO Exercise 3.4
#+begin_statement
Show that $A$ is a mere proposition if and only if $A \to A$ is
contractible.
#+end_statement

******* TODO Exercise 3.5
#+begin_statement
Show that $\isProp(A) \simeq (A \to \isContr(A))$.
#+end_statement

******* TODO Exercise 3.6
#+begin_statement
Show that if $A$ is a mere proposition, then so is $A + (\neg A)$. Thus,
there is no need to insert a propositional truncation in 3.4.1.
#+end_statement

******* TODO Exercise 3.9
#+begin_statement
Show that if $\LEM$ holds, then the type $\Prop :\equiv \sum_{A:{\cal U}} \isProp(A)$
is equivalent to $2$.
#+end_statement

******* TODO Exercise 3.21
#+begin_statement
Prove that $\isProp(P) \simeq (P \simeq \trunc{P})$.
#+end_statement

***** 4 Equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (183 . 0.083235)
:END:

****** 4.1 Quasi-inverses
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (184 . 0.216036)
:END:

******* 4.1.1. Characterization of the quasi-inverse type
If given $f : A \to B$, $\qinv{}(f)$ is inhabited,

\[
\qinv(f) \simeq \prod_{x:A}(x=x)
\]

******** TODO Proof
As $f$ is an equivalence, we apply univalence to get $p : A = B$.
Applying path induction, $p = \refl$ and $f = \id$. Then,

\[
\qinv(\id) \equiv \sum_{g : A \to A} (g \sim \id) \times (\id \sim g)
\]

which is equivalent by function extensionality to

\[
\sum_{g : A \to A} (g = \id) \times (g = \id)
\]

******* 4.1.2. Existence of center
Given $a : A$ and $q : a = a$ such that

 1. $a = a$ is a set,
 2. $\trunc{a = x}$ for all $x : A$,
 3. $p \cdot q = q \cdot p$ for all $p : a = a$,

there exists $f : \prod_{x:A}(x = x)$ such that $f(a) = q$.

******** TODO Proof
******* 4.1.3. qinv is not always a mere proposition
There exists a function such that $\qinv(f)$ is not a mere
proposition.

******** TODO Proof

****** 4.2 Half adjoint equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (186 . 0.984877)
:END:

******* 4.2.1. Half adjoint equivalence
A function $f : A \to B$ is a *half adjoint equivalence* if

\[
\ishae(f) :\equiv
\sum_{(g : B \to A)}
\sum_{(\eta : g \circ f \sim \id_{A})}
\sum_{(\epsilon : f \circ g \sim \id_{B})}
\prod_{(x:A)}
f(\eta(x)) = \epsilon(f(x))
\]

that is, there exist two homotopies and a coherence condition
between them.

******* 4.2.2. Logical equivalence of half adjoint equivalences
Given $f : A \to B$ and $g : B \to A$ with homotopies $\eta : g \circ f \sim \id$ and
$\epsilon : f \circ g \sim \id$, the following two types are logically equivalent

 * $\prod_{x:A}f(\eta(x)) = \epsilon(f(x))$,

 * $\prod_{x:A} g(\epsilon(x)) = \eta(g(x))$.

******** Proof
We will prove the second homotopy from $\tau : \prod_{x:A}f(\eta(x)) = \epsilon(f(x))$;
simmetry gives us the other direction.

By [[*2.4.4. Endonaturality of homotopies][endonaturality of homotopies]] in $\epsilon$ we have

\[\begin{tikzcd}
fgfg(x) \rar[equal]{fg \epsilon(x)} \dar[swap,equal]{\epsilon fg(x)} & 
fg(x) \dar[equal]{\epsilon(x)} \\
fg(x) \rar[equal]{\epsilon(x)} &
x
\end{tikzcd}\]

and applying $g$ to the complete diagram renders

\[\begin{tikzcd}
gfgfg(x) \rar[equal]{gfg\epsilon(x)} \dar[swap,equal]{g\epsilon fg(x)} & 
gfg(x) \dar[equal]{g\epsilon(x)} \\
gfg(x) \rar[equal]{g\epsilon(x)} &
gx
\end{tikzcd}\]

applying now the homotopy $\tau(g(x))$, we get $g \epsilon fg(x) = gf \eta g(x)$;
and again by [[*2.4.4. Endonaturality of homotopies][naturality]], we have $gf \eta g(x) = \eta gfg(x)$, and the
diagram is

\[\begin{tikzcd}
gfgfg(x) \rar[equal]{g fg\epsilon(x)} 
\dar[swap,equal]{\eta gfg(x)} & 
gfg(x) \dar[equal]{g\epsilon(x)} \\
gfg(x) \rar[equal]{g\epsilon(x)} &
gx
\end{tikzcd}\]

Meanwhile, by naturality of $\eta$ between $gfgfg$ and $gfg$, we have that

\[\begin{tikzcd}
gfgfg(x) \rar[equal]{g fg\epsilon(x)} 
\dar[swap,equal]{\eta gfg(x)} & 
gfg(x) \dar[equal]{\eta g(x)} \\
gfg(x) \rar[equal]{g\epsilon(x)} &
gx
\end{tikzcd}\]

and joining both diagrams we get $\eta g(x) = g \epsilon(x)$.

******* 4.2.3. qinv implies ishae
It is obvious that $\ishae$ implies $\qinv$.
For any $f : A \to B$ we have $\qinv(f) \to \ishae(f)$.

******** Proof
Given a quasiinverse $(f,g,\eta,\epsilon)$, we will define a new tuple
$(f,g,\eta,\epsilon',\tau')$; taking $\epsilon'$ to be

\[
\epsilon'(b) :\equiv \epsilon fg(b)^{-1} \cdot f\eta g(b) \cdot \epsilon(b)
\]

so we need to find an homotopy

\[
\tau(a) : f\eta(a) = \epsilon fgf(a)^{-1} \cdot f \eta gf(a) \cdot \epsilon f(a)
\]

but we know by [[*2.4.4. Endonaturality of homotopies][endonaturality]] that $\eta gf(a) = gf \eta(a)$ and by
homotopy that

\[
f \eta gf(a) \cdot \epsilon f(a) = fgf\eta(a) \cdot \epsilon f(a) = \epsilon fgf(a) \cdot f\eta(a).
\]

******* 4.2.4. Fiber of a map
The *fiber* of $f : A \to B$ over a point is

\[
\fib_f(y) :\equiv \sum_{x:A}(f(x) = y).
\]

******* 4.2.5. Equality of fibers
Given $f : A \to B$ and $(x,p), (x',p') : \fib_f(y)$,

\[
((x,p) = (x',p'))
\simeq
\left( \sum_{\gamma : x = x'} f(\gamma) \cdot p' = p \right)
\]

******** TODO Proof
# Path lemmas

******* 4.2.6. Fibers of half-adjoint equivalences are contractible
If $\ishae(f)$ for $f : A \to B$, then $\fib_f(y)$ is contractible for any $y : B$.

******** TODO Proof

******* 4.2.7. Left and right inverses
Given $f : A \to B$ we define

 * its *left inverses*, $\linv(f) :\equiv \sum_{g : B \to A} (g \circ f \sim \id)$,

 * its *right inverses*, $\rinv(f) :\equiv \sum_{g \colon B \to A}(f \circ g \sim \id)$.

****** 4.3 Bi-invertible maps
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (192 . 0.224058)
:END:
******* 4.3.1. Bi-invertible
A function $f : A \to B$ is *bi-invertible* if it
[[*4.2.7. Left and right inverses][has left and right inverses]]

\[
\biinv(f) :\equiv \linv(f) \times \rinv(f).
\]

******* 4.3.2. biinv is a mere proposition
The type $\biinv(f)$ is a mere proposition for any $f : A \to B$.

******** TODO Proof

******* 4.3.3. Equivalence biinv and ishae
Given $f : A \to B$, we have $\qinv(f) \simeq \ishae(f)$.

******** TODO Proof

****** 4.4 Contractible fibers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (192 . 0.979938)
:END:
******* 4.4.1. Contractible maps
A function $f : A \to B$ is *contractible* if $\fib_f(y)$ is contractible
for every $y : B$; that is, we define

\[
\isContr(f) :\equiv \prod_{y:B} \isContr(\fib_f(y)).
\]

******* 4.4.3. isContr implies ishae
For any $f : A \to B$, we have $\isContr(f) \to \ishae(f)$.

******** TODO Proof

******* 4.4.4. isContr is a mere proposition
For any $f$, the type $\isContr(f)$ is a mere proposition.

******** TODO Proof
******* 4.4.5. isContr is equivalent to ishae
For any $f : A \to B$, we have $\isContr(f) \simeq \ishae(f)$.

******** TODO Proof
****** 4.5 On the definition of equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (194 . 0.590656)
:END:

We have proved equivalent

\[
\isContr(f) \simeq \ishae(f) \simeq \biinv(f)
\]

so we choose $\isequiv{}(f) :\equiv \ishae(f)$.

****** 4.6 Surjections and embeddings
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (195 . 0.172333)
:END:

******* 4.6.0. Isomorphisms
When two sets are equivalent, we say that they is an *isomorphism*
or a *bijection*.

******* 4.6.1. Surjections and embeddings
A function $f : A \to B$ is

 * *surjective* if $\trunc{\fib_f(b)}$ for every $b : B$;
 * *embedding* if $\ap_f : (x=y) \to (f(x) = f(y))$ is an equivalence.

******** Split surjection
We say that a function $f : A \to B$ is a *split surjection* if

\[
\prod_{b:B}\sum_{a:A} f(a) = b.
\]

Note that it is a stronger assertion than being surjective, that
only asks for an inhabitant without constructive evidence.

******** Axiom of choice and split surjections
The [[*3.8.1. The axiom of choice][axiom of choice]] says exactly that every surjection between sets is
split.

******* 4.6.2. Characterization of embeddings
A function $f : A \to B$ between sets is an embedding if and only if

\[
\prod_{x,y:A} f(x) = f(y) \to x = y.
\]

And we say that it is an *injection*.

******** Proof
We apply that $f(x) = f(y)$ and $x = y$ are mere propositions to get
an equivalence from the logical implications.

******* 4.6.3. Equivalence is surjection and embedding
Any function $f : A \to B$ is an equivalence if and only if it is both
surjective and an embedding.

******** TODO Proof

******* 4.6.4. Equivalence is equivalent to surjection and embedding
For any $f : A \to B$,

\[
\isequiv(f) \simeq \isEmbedding(f) \times \isSurjective(f).
\]

****** 4.7 Closure properties of equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (196 . 0.792414)
:END:

******* 4.7.1. The 2-out-of-3 property
If any two $f,g,g\circ f$ are equivalences, so is the third.

******** Proof
Given $g \circ f$ and $g$ equivalences, we show that $(g \circ f)^{-1} \circ g$ is a
quasi-inverse to $f$ because

 * on the one hand,

   \[
   ((g \circ f)^{-1} \circ g) \circ f \sim \id_{A}
   \]

 * on the other hand,

   \[\begin{aligned}
   f \circ (g \circ f)^{-1} \circ g &\sim
   g^{-1} \circ g \circ f \circ (g \circ f)^{-1} \circ g \\
   &\sim g^{-1} \circ g \\
   &\sim \id.
   \end{aligned}\]

In a similar way, we can prove the other two pair of equivalences.

******* 4.7.2. Retracts
A function $g : A \to B$ is a *retract* of $f : X \to Y$ in

\[\begin{tikzcd}
A \rar{s}\dar{g} & X \rar{r}\dar{f} & A\dar{g} \\
B \rar{s'}& Y \rar{r'}& B
\end{tikzcd}\]

if

 * $R : s \circ r \sim \id$,
 * $R' : s' \circ r' \sim \id$,
 * $L : f \circ s \sim s' \circ g$,
 * $K : g \circ r \sim r' \circ f$.
 * a path $H(a)$ witnessing commutativity of

   \[\begin{tikzcd}
   grs(a) \dar[equal,swap]{g(Ra)}\rar[equal]{Ks(a)} & r'fs(a) \dar[equal]{r'(La)} \\
   g(a)  \rar[equal]{R'(ga)^{-1}} & r's'g(a) \\
   \end{tikzcd}\]

******* TODO 4.7.3. Retract of equivalence is equivalence
******* TODO 4.7.5. 

****** 4.8 The object classifier
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (199 . 0.979938)
:END:
******* TODO 4.8.1. Fiber of a type family
******* TODO 4.8.2. 
******* 4.8.3. Object classifier
Given any type $B$ there is an equivalence

\[
\chi :
\left( \sum_{A:{\cal U}}(A \to B) \right) \simeq (B \to {\cal U}).
\]

******** TODO Proof
We can define 

 * $\chi((A,f),b) :\equiv \fib_f(b)$

 * $\psi(P) :\equiv \left( \left(\sum_{b:B} P(b) \right), \pr_1 \right)$

and now verify that this constitutes an equivalence.

****** 4.9 Univalence implies function extensionality
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (202 . 0.553567)
:END:
We do not assume function extensionality on this section.

******* 4.9.1. Weak function extensionality principle
The *weak function extensionality principle* asserts that,
for any family $P : A \to {\cal U}$,

\[
\left( \prod_{x:A} \isContr(P(x)) \right)
\to
\isContr \left( \prod_{x:A}P(x) \right).
\]

******* 4.9.2. Equivalence on slice objects
If ${\cal U}$ is univalent, $A,B,X : {\cal U}$ and $e : A \simeq B$, there is 
an equivalence

\[
(X \to A) \simeq (X \to B).
\]

******** TODO Proof

******* TODO 4.9.3. 
******* TODO 4.9.5. Weak function extensionality implies function extensionality
The weak function extensionality principle implies the axiom
of [[*2.9.3. Function extensionality axiom][function extensionality]].

******** Proof

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (205 . 0.335925)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (205 . 0.984877)
:END:

***** 5 Induction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (209 . 0.083235)
:END:

****** 5.1 Introduction to inductive types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (209 . 0.463989)
:END:
******* 5.1.1. Uniqueness of functions over the natural numbers
Given $f,g : \prod_{n:\mathbb{N}} E(x)$ with

\[
e_z : E(0)
\quad\text{ and }\quad 
e_s : \prod_{n:\mathbb{N}}E(n) \to E(\succ(n)) 
\]

such that $f(0) = e_z = g(0)$ and

 * $\prod_{n:\mathbb{N}} f(\succ(n)) = e_s(n,f(n))$,
 * $\prod_{n:\mathbb{N}} g(\succ(n)) = e_s(n,g(n))$;

then $f$ and $g$ are equal.

******** Proof
We apply induction on $n$ over the type family $f(n) = g(n)$.
In the base case, $f(0) = g(0)$; and in the successor case,
knowing that $f(n) = g(n)$,

\[
f(\succ(n)) = e_s(n,f(n)) = e_s(n,g(n)) = g(\succ(n)).
\]

******* TODO 5.2. Uniqueness of inductive types
****** 5.2 Uniqueness of inductive types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (212 . 0.840835)
:END:

****** 5.3 W-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (216 . 0.171897)
:END:

****** 5.4 Inductive types are initial algebras
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (219 . 0.996777)
:END:
******* 5.4.1. N-algebra
A $\mathbb{N}\text{-algebra}$ is a type with two elements

\[
\mathbb{N}\text{alg} :\equiv \sum_{C:{\cal U}} C \times (C \to C).
\]

******* 5.4.2. N-homomorphism
A $\mathbb{N}\text{-homomorphism}$ between algebras is a function preserving
the zero and successor elements up to path equality

\[
\mathbb{N}\text{Hom}((C,c_0,c_s), (D,d_0,d_s)) :\equiv
\sum_{h \colon C \to D} (h(c_0) = d_0) \times \left( \prod_{c:C} h(c_s(c)) = d_s(h(c)) \right).
\]

******* 5.4.3. Homotopy initial N-algebra
An algebra is homotopy initial if the type of homomorphisms to any
other algebras is contractible; that is

\[
\isHinit_{\mathbb{N}}(I) :\equiv \prod_{C : \mathbb{N}\text{Alg}} \isContr(\mathbb{N}\text{Hom}(I,C)).
\]

******* TODO 5.4.4. Uniqueness of homotopy initial N-algebras

******* TODO 5.4.5. The naturals are an homotopy initial N-algebra
******* TODO 5.4.6. W-algebras
****** 5.5 Homotopy-inductive types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (223 . 0.673142)
:END:

****** 5.6 The general syntax of inductive definitions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (228 . 0.348776)
:END:

****** 5.7 Generalizations of inductive types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (233 . 0.839872)
:END:

****** 5.8 Identity types and identity systems
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (237 . 0.233446)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (242 . 0.701526)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (243 . 0.480867)
:END:

***** 6 Higher inductive types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (247 . 0.083235)
:END:

****** 6.1 Introduction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (247 . 0.377078)
:END:

****** 6.2 Induction principles and dependent paths
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (249 . 0.979938)
:END:

******* 6.2.1. Propositional equality by definition
In the case of higher inductive types, we give equalities by
definition that use non-fundamental parts of the type theory,
and so they are propositional instead of judgmental.

We write them as $f(\mathsf{loop}) := \ell$ to indicate this fact.

******* 6.2.2. Notation for dependent paths
We write dependent paths as

\[
(u =^P_p v) :\equiv \transport^P(p,u) = v.
\]

******* 6.2.5. Non-dependent computation rule of the circle
Given $a : A$ with $p : a = a$, there is a function $f : \mathbb{S}^1 \to A$ such
that

 * $f(\base) :\equiv a$,
 * $\ap_f(\mathsf{loop}) :\equiv p$.

******** TODO Proof

****** 6.3 The interval
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (255 . 0.575336)
:END:
******* 6.3.0. The interval
We denote $I$ to the type generated by

 * $0_I : I$, a start point,
 * $1_I : I$, an end point,
 * $\seg : 0_I = 1_I$, a segment between points.

******** TODO Induction principle of the interval
******** TODO Recursion principle of the interval
******* 6.3.1. The interval is contractible
The type $I$ is contractible.

******** Proof
We define a function of type $\prod_{i:I}(i = 1)$, by induction over the
interval

 * $f(0) :\equiv \seg$,
 * $f(1) :\equiv \refl_1$,

and $\apd_f(\seg) : \seg_{\ast}(\seg) = \refl$ can be defined knowing
that this type is equivalent to $\seg^{-1} \cdot \seg = \refl$, and
that path inverse is an inhabitant.

******* TODO 6.3.2. Extensionality from the interval type

****** 6.4 Circles and spheres
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (257 . 0.229024)
:END:

****** 6.5 Suspensions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (259 . 0.984877)
:END:
******* 6.5.0. Suspension of a type
The *suspension* of a type $A$ is a type $\Sigma A$ defined by the
generators

 * north, $\N : \Sigma A$;

 * south, $S : \Sigma A$;

 * and meridians, $\merid : A \to (\N = \S)$.

******** TODO Induction principle

******* 6.5.1. Circle as suspension
The circle can be seen as the suspension of the booleans,

\[
\Sigma 2 \simeq \mathbb{S}^{1}.
\]

******** TODO Proof

****** 6.6 Cell complexes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (263 . 0.984877)
:END:

****** 6.7 Hubs and spokes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (265 . 0.648892)
:END:

****** 6.8 Pushouts
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (267 . 0.457813)
:END:

****** 6.9 Truncations
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (271 . 0.620396)
:END:

****** 6.10 Quotients
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (275 . 0.1231)
:END:

****** 6.11 Algebra
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (280 . 0.984877)
:END:

****** 6.12 The flattening lemma
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (287 . 0.415721)
:END:

****** 6.13 The general syntax of higher inductive definitions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (293 . 0.998326)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (296 . 0.348989)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (297 . 0.789835)
:END:

***** 7 Homotopy n-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (299 . 0.083235)
:END:

****** 7.1 Definition of n-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (299 . 0.984877)
:END:
******* 7.1.1. is-n-type
We define $\istype{n} : {\cal U} \to {\cal U}$ as

\[
\istype{n}(X) :\equiv
\left\{\begin{array}{ll}
\isContr(X) & \mbox{if } n = -2, \\
\prod_{x,y:X} \istype{n'}(x = y) & \mbox{if } n = n' + 1.
\end{array}\right.
\]

******* TODO 7.1.4. Retraction of an n-type
******* TODO 7.1.5. Equivalence preserves n-types
****** 7.2 Uniqueness of identity proofs and Hedberg's theorem
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (304 . 0.230568)
:END:
******* TODO 7.2.0. Uniqueness of identity proofs (UIP)

******* 7.2.1. Axiom K
A type $A$ is a set if and only if it satisfies *Axiom K*, for
all $x:X$ and $p : x = x$, we have $p = \refl$.

******** TODO Proof

******* 7.2.2. Mere identity relations in sets
Given $R$ a reflexive mere relation on $X$ implying identity, $X$
is a set and $R(x,y) \simeq (x = y)$ for all $x,y :X$.

******** Proof
Given $\rho : \prod_{x:X}R(x,x)$ and $f : \prod_{x,y}R(x,y) \to (x = y)$, we have
that if $X$ is a set, $x = y$ is a mere proposition logically equivalent
to $R(x,y)$. On the other hand, if $x = y$ is equivalent to $R(x,y)$ and
it is a mere proposition, $X$ is a set.

We can give two proofs, either proving that $X$ is a set or that $R(x,y)$
is equivalent to $x = y$.

********* X is a set
Given $x:X$ and $p : x = x$, we consider

\[
\apd_{f(x)}(p) : p_{\ast}(f(x,x)) = f(x,x)
\]

which, by [[*2.9.6. Equivalence for the dependent function equality][path equalities for dependent functions]] gives us a path

\[
p_{\ast}(f(x,x,r)) = f(x,x,p_{\ast}(r)).
\]

Knowing that $R(x,x)$ is a mere proposition, $p_{\ast}(r) = r$; and transport
in the identity type is equal to concatenation, so

\[
f(x,x,r) \cdot p = f(x,x,r)
\]

and $p = \refl$, satisfying axiom K.

********* TODO R is equivalent to equality

******* 7.2.3. A type with double negation cancellation equality is a set
If $X$ has the property $\neg\neg (x=y) \to (x=y)$, it is a set.

******** Proof
We have $\neg\neg(x=y)$ as a reflexive mere relation implying identity,
so we can apply the previous [[*7.2.2. Mere identity relations in sets][lemma]].

******* 7.2.5. Hedberg's theorem
If a type has [[*3.4.3. Decidable types][decidable]] equality, it is a set.

******** TODO Proof.

******* TODO 7.2.6. Natural numbers form a set
The type of natural numbers has decidable equality, and hence is a set.

******** TODO Proof
Given $x,y : \mathbb{N}$, we proceed by induction in both arguments. In the first
case, $\refl_0$ proves the equality; in the case of a successor and a zero,
we can apply 

****** 7.3 Truncations
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (308 . 0.475366)
:END:

****** 7.4 Colimits of n-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (315 . 0.536449)
:END:

****** 7.5 Connectedness
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (320 . 0.65016)
:END:
******* 7.5.1. n-connected function
A function $f : A \to B$ is *n-connected* if $\trunc{\fib_f(b)}_n$ is
contractible for all $b : B$.

\[
\conn_n(f) :\equiv \prod_{b:B} \isContr(\trunc{\fib_f(b)}_n)
\]

A type $A$ is *n-connected* if the function $A \to 1$ is.

******* TODO 7.5.2. Surjectivity is (-1)-connectedness
A function is (-1)-connected iff it is [[*4.6.1. Surjections and embeddings][surjective]].
****** 7.6 Orthogonal factorization
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (326 . 0.984877)
:END:

****** 7.7 Modalities
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (332 . 0.979938)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (338 . 0.477456)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (339 . 0.682024)
:END:

**** II Mathematics
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (345 . 0.083235)
:END:

***** 8 Homotopy theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (347 . 0.083235)
:END:

We define the *homotopy groups* of a pointed type $(A,a)$ as

\[
\pi_n(A,a) :\equiv \trunc{\Omega^n(A,a)}_{0}.
\]

****** 8.1 Ïâ(SÂ¹)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (352 . 0.436728)
:END:

******* 8.1.1 Getting started
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (352 . 0.671895)
:END:
We define $\mathsf{code} : \mathbb{S}^1 \to {\cal U}$ by recursion as

 * $\mathsf{code}(\mathsf{base}) :\equiv \mathbb{Z}$,
 * $\mathsf{code}(\mathsf{loop}) :\equiv \ua(\succ)$.

******* 8.1.2 The classical proof
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (353 . 0.646195)
:END:
We have that

 * $\transport^{\mathsf{code}}(\mathsf{loop},x) = x + 1$,
 * $\transport^{\mathsf{code}}(\mathsf{loop},x) = x -1$.

******* 8.1.3 The universal cover in type theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (354 . 0.480867)
:END:

******* 8.1.4 The encode-decode proof
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (356 . 0.704682)
:END:

******* 8.1.5 The homotopy-theoretic proof
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (359 . 0.827644)
:END:
We define the function $\mathsf{encode} : \prod_{x:\mathbb{S}^1}(\base = x) \to \mathsf{code}(x)$ by

\[
\mathsf{encode}\ p :\equiv \transport^{\mathsf{code}}(p,0).
\]

******* 8.1.6 The universal cover as an identity system
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (361 . 0.409399)
:END:
We can define a function $\mathsf{decode} : \prod_{x:\mathbb{S}^1} \mathsf{code}(x) \to (\base = x)$.

******** TODO Definition
******* 8.1.7 Encode-decode of a path
For all $x : \mathbb{S}^1$ and $p : \base = x$,

\[
\mathsf{decode}(\mathsf{encode}(p)) = p.
\]

******** TODO Proof
****** 8.2 Connectedness of suspensions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (363 . 0.380733)
:END:

****** 8.3 Ï_(kâ¤n) of an n-connected space and Ï_(k<n)(Sâ¿)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (365 . 0.2169)
:END:

****** 8.4 Fiber sequences and the long exact sequence
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (365 . 0.842291)
:END:

****** 8.5 The Hopf fibration
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (371 . 0.459893)
:END:

******* 8.5.1 Fibrations over pushouts
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (372 . 0.66545)
:END:

******* 8.5.2 The Hopf construction
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (374 . 0.446319)
:END:

******* 8.5.3 The Hopf fibration
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (375 . 0.996941)
:END:

****** 8.6 The Freudenthal suspension theorem
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (378 . 0.839517)
:END:

****** 8.7 The van Kampen theorem
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (386 . 0.69401)
:END:

******* 8.7.1 Naive van Kampen
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (387 . 0.556292)
:END:

******* 8.7.2 The van Kampen theorem with a set of basepoints
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (393 . 0.233446)
:END:

****** 8.8 Whitehead's theorem and Whitehead's principle
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (397 . 0.828618)
:END:

****** 8.9 A general statement of the encode-decode method
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (402 . 0.451157)
:END:

****** 8.10 Additional Results
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (404 . 0.424413)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (405 . 0.501647)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (407 . 0.37052)
:END:

***** 9 Category theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (409 . 0.083235)
:END:

****** 9.1 Categories and precategories
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (410 . 0.745723)
:END:

****** 9.2 Functors and transformations
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (415 . 0.260174)
:END:

****** 9.3 Adjunctions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (419 . 0.682161)
:END:

****** 9.4 Equivalences
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (420 . 0.744738)
:END:

****** 9.5 The Yoneda lemma
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (427 . 0.984877)
:END:

****** 9.6 Strict categories
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (431 . 0.979938)
:END:

****** 9.7 â -categories
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (432 . 0.979938)
:END:

****** 9.8 The structure identity principle
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (434 . 0.399634)
:END:

****** 9.9 The Rezk completion
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (437 . 0.98337)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (447 . 0.083235)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (448 . 0.634425)
:END:

***** 10 Set theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (451 . 0.083235)
:END:

****** 10.1 The category of sets
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (452 . 0.193966)
:END:

******* 10.1.1 Limits and colimits
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (452 . 0.367507)
:END:

******* 10.1.2 Images
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (452 . 0.779886)
:END:

******* 10.1.3 Quotients
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (457 . 0.21413)
:END:

******* 10.1.4 Set is a Î W-pretopos
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (459 . 0.980218)
:END:

******* 10.1.5 The axiom of choice implies excluded middle
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (461 . 0.295056)
:END:

****** 10.2 Cardinal numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (463 . 0.255527)
:END:

****** 10.3 Ordinal numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (466 . 0.992398)
:END:

****** 10.4 Classical well-orderings
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (474 . 1.009469)
:END:

****** 10.5 The cumulative hierarchy
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (478 . 0.680281)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (485 . 0.979938)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (486 . 0.727911)
:END:

***** 11 Real numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (491 . 0.083235)
:END:

****** 11.1 The field of rational numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (492 . 0.622338)
:END:

****** 11.2 Dedekind reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (493 . 0.485995)
:END:

******* 11.2.1 The algebraic structure of Dedekind reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (495 . 0.639703)
:END:
******** Dedekind cuts
A *Dedekind cut* is a pair $(L,U)$ of mere predicates such that
it is

 1) /inhabited:/ $\exists (q : \mathbb{Q}). L(q)$ and $\exists (r : \mathbb{Q}) . U(r)$;

 2) /rounded:/ for all $q,r \in \mathbb{Q}$,

    * $L(q) \iff \exists (r : \mathbb{Q}). (q < r) \wedge L(r)$

    * $U(r) \iff \exists(q:\mathbb{Q}).(q < r) \wedge U(q)$

 3) /disjoint:/ $\neg (Lq \wedge Uq)$ for all $q:\mathbb{Q}$,

 4) /located:/ $(q < r) \implies Lq \vee Ur$ for all $q,r : \mathbb{Q}$.

We define $\isCut(L,U)$ as the mere proposition of the conjunction
of these conditions. The set of *Dedekind reals* is defined as

\[
\mathbb{R}_d :\equiv
\left\{ (L,U) : (\mathbb{Q} \to \Omega) \times (\mathbb{Q} \to \Omega)
\mid \isCut(L,U) \right\}. 
\]

******** Rational embedding
To each rational $q$, we associate $L_q(r) :\equiv (r < q)$ and
$U_q(r) :\equiv (q < r)$.

******** Algebraic structure
We define addition as

\[
L_{x+y}(r) :\equiv \exists (t,s :\mathbb{Q}),\quad L_x(t) \land L_y(s) \land (t + s = q)
\]

and multiplication

\[\begin{aligned}
L_{x\cdot y}(q) :\equiv \exists (a,b,c,d : \mathbb{Q}), &\quad
L_x(a) \land U_x(b) \land L_y(c) \land U_y(d) \land \\ 
& (q < \min(ac,ad,bc,bd))
\end{aligned}\]

This has structure of commutative ring.

******** TODO Order
******** Weak linearity
Linearity, $(x < y) \lor (y \leq x)$, is valid only if we assume LEM. We
have *weak linearity* $(x < y) \to (x < z) \lor (z < y)$.
******** Apartness
\[
(x \apart y) :\equiv (x < y) \lor (y < x)
\]

we have $(x \apart y) \to \neg (x = y)$, but the converse is not true.

********* TODO Apartness is cotransitive
******** Invertibility
A real is invertible if and only if it is apart from 0.

******** Archimedean principle for Rd
If $x,y : \mathbb{R}$ such that $x < y$, then there merely exists $q : \mathbb{Q}$
such that $x < q < y$.


******* 11.2.2 Dedekind reals are Cauchy complete
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (498 . 1.003216)
:END:
******** Cauchy sequence
A sequence $x : \mathbb{N} \to \mathbb{Q}$ is *Cauchy* if

\[
\prod_{(\epsilon : \mathbb{Q}_+)} \sum_{(n:\mathbb{N})}
\prod_{(m,k \geq n)} |x_m-x_k| < \epsilon
\]

note how we can get a modulus of convergence out of this explicit
existential by Theorem of Choice.

******** Cauchy approximation
A *Cauchy approximation* is a map $x \colon \mathbb{Q}_+ \to \mathbb{R}_d$ such that

\[
\forall (\delta,\epsilon : \mathbb{Q}_+), |x_{\delta} - x_{\epsilon}|
< \delta + \epsilon,
\]

and its *limit* is $l : \mathbb{R}_d$ such that

\[
\forall (\epsilon, \theta : \mathbb{Q}_+), |x_{\epsilon}- l| < \epsilon + \theta
\]

******** Completeness for Cauchy approximations
Every Cauchy approximation has a limit.

********* TODO Proof

******** TODO Completeness for Cauchy sequences

******* 11.2.3 Dedekind reals are Dedekind complete
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (500 . 0.669742)
:END:

****** 11.3 Cauchy reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (502 . 0.252284)
:END:

******* 11.3.1 Construction of Cauchy reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (503 . 0.586389)
:END:

******* 11.3.2 Induction and recursion on Cauchy reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (505 . 0.984877)
:END:

******* 11.3.3 The algebraic structure of Cauchy reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (520 . 0.657569)
:END:

******* 11.3.4 Cauchy reals are Cauchy complete
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (526 . 0.164536)
:END:

****** 11.4 Comparison of Cauchy and Dedekind reals
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (526 . 0.979938)
:END:

****** 11.5 Compactness of the interval
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (528 . 0.490225)
:END:
******* Notions of compactness

 * *Metrically compact:* Cauchy complete and totally bounded.
 * *Bolzano-Weierstrass:* every sequence has a convergent subsequence.
 * *Heine-Borel:* every open cover has a finite subcover.

These are equivalent in classical mathematics.
******* 11.5.1. Metric space
******* 11.5.2. Cauchy approximation
******* 11.5.2. Complete metric space
******* 11.5.3. e-nets
******* 11.5.3. Totally bounded space
******* 11.5.5. Uniform continuity
******* 11.5.6. Metrical compactness of the interval
****** 11.6 The surreal numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (537 . 0.300144)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (551 . 0.979938)
:END:

****** Exercises
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (553 . 0.282243)
:END:

**** Appendix
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (557 . 0.083235)
:END:

***** A Formal type theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (559 . 0.083235)
:END:

****** A.1 The first presentation
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (561 . 0.980218)
:END:

******* A.1.1 Type universes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (563 . 0.450923)
:END:

******* A.1.2 Dependent function types (Î -types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (563 . 0.964715)
:END:

******* A.1.3 Dependent pair types (Î£-types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (564 . 0.529346)
:END:

******* A.1.4 Coproduct types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (565 . 0.24388)
:END:

******* A.1.5 The finite types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (565 . 0.540825)
:END:

******* A.1.6 Natural numbers
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (565 . 0.783442)
:END:

******* A.1.7 W-types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (566 . 0.293082)
:END:

******* A.1.8 Identity types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (566 . 0.685745)
:END:

****** A.2 The second presentation
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (566 . 0.987412)
:END:

******* A.2.1 Contexts
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (567 . 0.777581)
:END:

******* A.2.2 Structural rules
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (568 . 0.425459)
:END:

******* A.2.3 Type universes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (569 . 0.650701)
:END:

******* A.2.4 Dependent function types (Î -types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (569 . 0.984877)
:END:

******* A.2.5 Dependent pair types (Î£-types)
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (571 . 0.260262)
:END:

******* A.2.6 Coproduct types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (572 . 0.171897)
:END:

******* A.2.7 The empty type 0
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (572 . 0.859729)
:END:

******* A.2.8 The unit type 1
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (573 . 0.127758)
:END:

******* A.2.9 The natural number type
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (573 . 0.543736)
:END:

******* A.2.10 Identity types
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (574 . 0.25004)
:END:

******* A.2.11 Definitions
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (574 . 0.737321)
:END:

****** A.3 Homotopy type theory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (575 . 0.461918)
:END:

******* A.3.1 Function extensionality and univalence
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (575 . 0.653945)
:END:

******* A.3.2 The circle
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (576 . 0.266769)
:END:

****** A.4 Basic metatheory
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (576 . 0.983344)
:END:

****** Notes
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (579 . 0.277915)
:END:

**** Bibliography
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (581 . 0.083235)
:END:

**** Index of symbols
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (593 . 0.083235)
:END:

**** Index
:PROPERTIES:
:INTERLEAVE_PAGE_NOTE: (601 . 0.305241)
:END:

*** [[https://abooij.github.io/wiwikwlhott/][What I wish I knew when learning HoTT]] - Auke Booij
*** HoTT-EST seminars
**** Carlo Angiuli - Computational semantics of cartesian cubical type theory
Link to the video: [[https://zoom.us/recording/play/XP5xo5yzDYuJ-tFYIvNTIEVG5jPVFl0_Rz5PsUZZPrjyZa83b_CIHnuLhdfKwkjP][video]]

Canonicity only holds for closed terms. 

Computational semantics are given by a model in which closed terms are
programs.

***** Operational semantics
Syntax of untyped preterms (modulo alpha-equivalence). Raw syntax of
the theory.

Each closed term reduces to a value. The meanings of non values are
determined by the values to which they reduce.

The meanings of open terms are determined by thei rbehavior as maps
from closed to closed terms.

*** Course on HoTT - Robert Harper
**** Lecture 1: Intuitionistic Type Theory
***** Intuitionistic Type Theory (TT),
The *Intuitionistic Type Theory* is based on the work of Per
Martin-LÃ¶f on the 1970s.  It is an analysis and expansion of Brouwer's
intuitionism.

***** Intensional Type Theory (ITT)
The *Intensional Type Theory* will be our base theory. Other forms of
type theory are extensions of this one.

***** Extensional Type Theory (ETT)
The *Extensional Type Theory* has the core of ITT plus the principles
of equality reflection (ER) and uniqueness of the identity proofs
(UIP).

This is the intuitionistic theory of sets in which NuPRL is based.
It is a form of constructive set theory, developed by Bishop; where
types are treated as sets.

***** Homotopy Type Theory (HoTT)
The *Homotopy Type Theory* is an elaboration of ITT with higher
inductive types (HIT) and the univalence axiom (UA).

It is an intuitionistic theory of weak $\infty\text{-groupoids}$. Here types
are spaces in an abstract sense.
***** Brower's program
The *Brower's program* is a philosophy of mathematics based on the
following ideas
 
 1. mathematics is a human social activity. The focus is on the /language/
    as a tool for communication of mathematical concepts.

 2. the fundamental human capability is the understanding an execution
    of /algorithms/ for performing /constructions/. Proofs are forms
    of construction.

In this setting, the only way to describe infinite things is by
communicate them with an algorithm. 

****** Proof relevance
From the second point, arises the principle of *proof relevance*.
Proofs are mathematical objects that we can see an manipulate. In
other foundations of mathematics, only a limiting enumerable set of
formal proofs can be viewed as proofs.

****** Proof relevance in HoTT
In HoTT, our proofs will be paths in a space. This conception
will provide a synthetic way of working with homotopy which is a
cleaner, shorter and mechanizable way writting proofs.

****** Synthetic perspective in mechanized reasoning
Synthetic geometry is what Euclides did; analytic geometry is what
Descartes did. The traditional formulation of Homotopy Theory, using
euclidean spaces and topology, is an analytic one. Synthetic
formulations of Homotopy Theory are based on Quillen model categories or
HoTT.

This distinction of synthetic and analytical is due to Lawvere.

/Twelf vs Coq is another example/
***** Type Theory
Type theory is an analysis and codification of Brower's intuitionism
drawning on Gentzen's proof theory. Types classify the admisible
constructions. A type is defined by

 * *introduction rules*, showing how to make a construction.
 * *elimination rules*, showing how to use a construction.

linked by the *inversion principle*, or principle of conservation of
proofs; stating that the introduction is inverse to the elimination.
This inversion principle is the basis for the computational content
of our language.

***** Axiomatic freedom of constructive mathematics
In the Hilbert/Brouwer debate, Hilbert believed that Brouwer was negating
everything that has been done so far; but, as fewer assumptions lead to
stronger results, the exclusion of certain principles leads only to axiomatic
freedom.

For example, the law of excluded middle is not negated on constructive
mathematics, they are simply independent of it; but it can still be
taken as an hypothesis on certain subfields.

We can now include certain assumptions locally, and so, the
constructivity is not a limitation.
***** Computational aspect
Type theory acts as an unified theory of computation. Programming
languages and computation are particular manifestations of this
unified theory.

***** Computational trinitarianism

\[\begin{tikzcd}[row sep=huge, col sep=tiny]
& \begin{matrix}\text{Type}\\ \text{Theory}\end{matrix} \drar[to-to]\dlar[to-to] & \\
\text{Logic}\arrow[rr,to-to] & & \begin{matrix}\text{Category}\\ \text{Theory}\end{matrix}
\end{tikzcd}\]

There is a complete correspondence between the three theories.
***** Intuitionistic logic
*Intuitionistic logic* is based on the principles of intuitionism.
It has the following judgements

 1. $A$ is a proposition.
 2. $A$ is a true proposition, it has a proof.

We do not expect that a proposition is either provable or refutable.
We assume also /open-endedness/, we cannot write all the proofs in a
systematic way.

***** Negative fragment of intuitionistic propositional logic
We will write a grammar of proofs.

 * The trivially true proposition, this is the *truth-formation*
   rule

   \begin{prooftree}
   \RightLabel{(T-form)}
   \AxiomC{}
   \UnaryInfC{T prop}
   \end{prooftree}

   this trivially true proposition is true

   \begin{prooftree}
   \RightLabel{(T-intro)}
   \AxiomC{}
   \UnaryInfC{T true}
   \end{prooftree}

   but there is no truth elimination rule, as we are not using any
   information when we write this proposition.

 * Conjunction formation

   \begin{prooftree}
   \RightLabel{($\wedge$-form)}
   \AxiomC{A prop}
   \AxiomC{B prop}
   \BinaryInfC{A $\wedge$ B prop}
   \end{prooftree}

   conjunction introduction

   \begin{prooftree}
   \RightLabel{($\wedge$-intro)}
   \AxiomC{A true}
   \AxiomC{B true}
   \BinaryInfC{A $\wedge$ B true}
   \end{prooftree}
   
   we will use two elimination rules to extract the two pieces 
   of information that went into that fact.

   \begin{prooftree}
   \RightLabel{($\wedge$-elim$_1$)}
   \AxiomC{A $\wedge$ B true}
   \UnaryInfC{A true}
   \RightLabel{($\wedge$-elim$_2$)}
   \AxiomC{A $\wedge$ B true}
   \UnaryInfC{B true}
   \noLine
   \BinaryInfC{}
   \end{prooftree}

 * Implication formation

   \begin{prooftree}
   \RightLabel{($\supset$-form)}
   \AxiomC{A prop}
   \AxiomC{B prop}
   \BinaryInfC{A $\supset$ B prop}
   \end{prooftree}

   and implication introduction, which uses only entailment

   \begin{prooftree}
   \RightLabel{($\supset$-intro)}
   \AxiomC{A true $\vdash$ B true}
   \UnaryInfC{A $\supset$ B true}
   \end{prooftree}

   in the Hilbert formulations of logic, we supress the difference
   between entailment and implication. The logical entailment is prior
   to the implication, it is a map of proofs; while the implication only
   captures that into the logic. The elimination rule is the modus ponens

   \begin{prooftree}
   \RightLabel{($\supset$-elim)}
   \AxiomC{A $\supset$ B true}
   \AxiomC{A true}
   \BinaryInfC{B true}
   \end{prooftree}

**** Lecture 2: Intuitionistic Propositional Logic
***** Negative fragment of intuitionistic propositional logic
We have talked about

 * the Gentzen principle of conservation of evidence.
 * the truth value.
 * the conjunction.
 * the implication.

Why are these "correct" rules? We are keeping a correspondence between
introduction and elimination rules; that is the beauty of the Gentzen
system and what gives rise to the computational interpretation.

These are not arbitrary rules, there is a coherence that is being kept.
***** Structural properties of entailment
The concept of *logical entailment* is a compound judgement. It express
the idea of a conclusion derived from a set of assumptions

\[\underbrace{
A_1 \text{ true},
A_2 \text{ true},
\dots,
A_n \text{ true}}_{\Gamma}
\vdash
A
\]

Logical entailment is a mapping between propositions.
The properties of logical entailment (aka hypothetical judgement) are
the following properties
 
 1. Reflexivity (R), $A \text{ true} \vdash A \text{ true}$.
 2. Transitivity (T),
    
    \begin{prooftree}
    \RightLabel{(T)}
    \AxiomC{$\Gamma_1 \vdash A$ true}
    \AxiomC{$\Gamma_2,A$ true $\vdash B$ true}
    \BinaryInfC{$\Gamma_1,\Gamma_{2} \vdash B$ true}
    \end{prooftree}

    in presence of the weakening, contraction, and exchange properties,
    this can be rewritten using only a $\Gamma$.

 3. Weakening (W),
    
    \begin{prooftree}
    \RightLabel{(W)}
    \AxiomC{$\Gamma$ $\vdash A$ true}
    \UnaryInfC{$\Gamma,B$ true $\vdash A$ true}
    \end{prooftree}

    where the two first properties are fundamental, and this third is
    not as fundamental. You can consider deniying this principle, and
    you will arrive at the notion of /relevant entitlement/, where every
    assumption has to be used in the entitlement.

 4. Contraction (C), 
    
    \begin{prooftree}
    \RightLabel{(C)}
    \AxiomC{$\Gamma,A$ true,$A$ true $\vdash B$ true}
    \UnaryInfC{$\Gamma, A$ true $\vdash B$ true}
    \end{prooftree}

    in certain logics, we may will want to keep an accounting of
    how many times have we used a lemma; we will have to deny this
    property.

 5. Exchange (E), the order of the assumptions does not matter

    \begin{prooftree}
    \RightLabel{(C)}
    \AxiomC{$\Gamma \vdash A$ true}
    \UnaryInfC{$\pi(\Gamma) \vdash A$ true}
    \end{prooftree}

    where $\pi$ is any permutation.

When any of these properties fail, we talk of substructural entailment.

***** Local form
We are writing the rules in local form. They can be used in the same
way on the presence of assumptions. A $\Gamma$ could be added to all
the rules to obtain the global form. It is implied in our rules.

There are certain scenarios in which we will want $\Gamma$ to be explicitely
empty.

***** Order-theoretic formulation
Let us define $A \leq B$, an order on propositions, meaning that
$A \text{ true} \vdash B \text{ true}$.

****** Preorder
This is a preorder,

  * it is reflexive,

    \begin{prooftree}
    \RightLabel{($\leq$-refl)}
    \AxiomC{}
    \UnaryInfC{$A \leq A$}
    \end{prooftree}


  * it is transitive,

    \begin{prooftree}
    \RightLabel{($\leq$-trans)}
    \AxiomC{$A \leq B$}
    \AxiomC{$B \leq C$}
    \BinaryInfC{$A \leq C$}
    \end{prooftree}

  * we have a greatest, final element

    \begin{prooftree}
    \RightLabel{($\leq_\top$)}
    \AxiomC{}
    \UnaryInfC{$A \leq \top$}
    \end{prooftree}
   
  * we have meets given by conjunction. That is, there is a lower
    bound

    \begin{prooftree}
    \RightLabel{($\leq,\wedge_1$)}
    \AxiomC{}
    \UnaryInfC{$A \wedge B \leq A$}
    \RightLabel{($\leq,\wedge_2$)}
    \AxiomC{}
    \UnaryInfC{$A \wedge B \leq B$}
    \noLine
    \BinaryInfC{}
    \end{prooftree}

    which is also universal

    \begin{prooftree}
    \RightLabel{($\leq,\wedge$-bound)}
    \AxiomC{$C \leq A$}
    \AxiomC{$C \leq B$}
    \BinaryInfC{$C \leq A \wedge B$}
    \end{prooftree}

Those follow from the properties of entailment. We can draw those
properties with Hasse diagrams, where we can see a similarity with
a product diagram on category theory

\[\begin{tikzcd}[column sep=tiny]
& C \dar[dashed] \ar[ddr,bend left]\ar[ddl, bend right] & \\
& A \wedge B \drar\dlar & \\
A & & B &.
\end{tikzcd}\]

****** Antisymmetry and equivalence
We have now a lower semilattice. Sometimes, lower semilattices are
defined to be partial orders, where we have antisymmetry

    \begin{prooftree}
    \RightLabel{}
    \AxiomC{$A \leq B$}
    \AxiomC{$B \leq A$}
    \BinaryInfC{$A = B$}
    \end{prooftree}

but we are going to work without antisymmetry. We haven't talked yet
about equality, but we are going to introduce the univalent principle.
We could define $A \simeq B$ when $A \leq B$ and $B \leq A$, they are not equal,
but equivalent. We could also work with equivalence classes $[A]_{\simeq}$ here.
Univalence will imply the equality of equivalent propositions.

***** Positive fragment of IPL
Now we write the grammar of the positive fragment

 * The false proposition, this is the *false-formation* rule

   \begin{prooftree}
   \RightLabel{($\bot$-form)}
   \AxiomC{}
   \UnaryInfC{$\bot$ prop}
   \end{prooftree}

   there is no introduction rule, only an elimination rule

   \begin{prooftree}
   \RightLabel{($\bot$-elim)}
   \AxiomC{$\bot$ true}
   \UnaryInfC{A true}
   \end{prooftree}

   since there is no introduction rule and this never happens,
   this preserves the coherence principle.

 * Disjunction formation

   \begin{prooftree}
   \RightLabel{($\vee$-form)}
   \AxiomC{A prop}
   \AxiomC{B prop}
   \BinaryInfC{A $\vee$ B prop}
   \end{prooftree}

   disjunction introduction

   \begin{prooftree}
   \RightLabel{($\vee$-intro$_{1}$)}
   \AxiomC{A true}
   \UnaryInfC{A $\vee$ B true}
   \RightLabel{($\vee$-intro$_{2}$)}
   \AxiomC{B true}
   \UnaryInfC{A $\vee$ B true}
   \noLine
   \BinaryInfC{}
   \end{prooftree}
   
   we will use an elimination rule to extract the piece of
   of information that went into that fact as in

   \begin{prooftree}
   \RightLabel{($\vee$-elim)}
   \AxiomC{A $\vee$ B true}
   \AxiomC{A true $\vdash$ C true}
   \AxiomC{B true $\vdash$ C true}
   \TrinaryInfC{C true}
   \end{prooftree}

***** Order-theoretical properties
We have now a least or initial element,
 
    \begin{prooftree}
    \RightLabel{($\leq$-$\bot$)}
    \AxiomC{}
    \UnaryInfC{$\bot \leq A$}
    \end{prooftree}

and joins or upper bounds

    \begin{prooftree}
    \RightLabel{($\leq,\vee_1$)}
    \AxiomC{}
    \UnaryInfC{$A \leq A \vee B$}
    \RightLabel{($\leq,\vee_2$)}
    \AxiomC{}
    \UnaryInfC{$A \leq A \vee B$}
    \noLine
    \BinaryInfC{}
    \end{prooftree}

where the bound is the least upper bound

    \begin{prooftree}
    \RightLabel{($\leq,\vee$-bound)}
    \AxiomC{$A \leq C$}
    \AxiomC{$B \leq C$}
    \BinaryInfC{$A \vee B \leq C$}
    \end{prooftree}

Note that those bounds are unique up to equivalence, as they follow
also a categorical universal diagram, in this case, the coproduct
diagram

\[\begin{tikzcd}[column sep=tiny]
A \drar\ar[ddr, bend right] & & B \dlar\ar[ddl, bend left] \\
& A \vee B \dar[dashed] & \\
& C  & &.
\end{tikzcd}\]

This is a lattice, that has all finite meets and joins.

***** Order-theoretic formulation of the implication
We have an exponential $B^A$ whenever $A \supset B$, this is defined
as the property

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$A\wedge (A \supset B) \leq B$}
\end{prooftree}

and the exponential is the universal element with this property

\begin{prooftree}
\AxiomC{$A \wedge C \leq B$}
\UnaryInfC{$C \leq A \supset B$}
\end{prooftree}

***** Heyting algebra
A *Heyting algebra* is a lattice with exponentials.

****** Yoneda Lemma
The Yoneda Lemma on lattices says that

$a \leq b \iff \left(\forall x: x \leq a \implies x \leq b\right)$.

It is trivial by transitivity and identity. This is
an instance of a more general fact.
***** Negation
We define $\neg A := A \supset \bot$. It is the largest proposition inconsistent
with $A$, the largest proposition such that $A \wedge \neg A \leq \bot$.

***** Complement
We define $\overline{A}$ as the universal element with the property that $\top \leq A \vee \overline{A}$
and $\overline{A} \wedge A \leq \bot$.

We have a complement distributive algebra (a boolean algebra!) and such thing
has exponentials.

\begin{prooftree}
\AxiomC{$\top \leq A \vee C$}
\UnaryInfC{$\overline{A} \leq C$}
\end{prooftree}

***** Boolean algebra
A *boolean algebra* is a complemented distributive lattice. Therefore, it has
exponentials, defined as $B^A := \overline{A} \vee B$.

***** Completeness theorem
If $A \leq B$ in every Heyting algebra, it must be deducible that $A \vdash B$.
If something is valid in all models, in all Heyting algebras, it must be
provable.

This logic is complete for Heyting algebras, but it is not going to be
complete for boolean algebras. $A \vee \neg A$ is not going to be provable in
our logic.

****** Proof
If something is provable in every Heyting algebra, you can construct the
propositional *Lindenbaum algebra*; and this is used to show completenaess.
If $A \leq B$ holds in every Heyting algebra, then $A \text{ true} \vdash B \text{ true}$. We
need to interpret the propositions as elements on a Heyting algebra.

****** Converse
If something is provable, it holds in every Heyting algebra. 

***** Issue: negation and complement
In a boolean algebra, $A \vee \neg A \simeq \top$.

**** Lecture 3: Propositions as Types
***** Last week
Last week we saw IPL from a provability perspective. $A$ is true if it
has a proof, and $A$ is false if it has a refutation. We got the
structure of Heyting algebra (a lattice (partial order with all finite
meets and joins) and exponentials). Every Heyting algebra is
distributive. We defined the negation.

****** Soundness incompleteness result
$\Gamma \vdash A$ true iff $\Gamma^{\ast} \leq A^{\ast}$ in every Heyting algebra.

****** Boolean and Heyting algebras
Not every boolean algebra is a Heyting algebra, but every Heyting
algebra is a boolean algebra.

****** DeMorgan Duality
$\overline{A \wedge B} = \overline{A} \vee \overline{B}$ and $\overline{A \vee B} = \overline{A} \wedge \overline{B}$.

***** Claim
In IPL, not all instances of LEM are provable. We cannot prove
in general that $A \vee \neg A \text{ true}$.

****** Idea
The disjunction property says that if $A \vee B \text{ true}$, then $A$ true
or $B$ true. This would imply that LEM gives us a proof or a
refutation of every element.

***** There exists a Heyting algebra which is not a boolean algebra
We need only a countermodel, a Heyting algebra where $\top \leq A \vee \neg A$
does not hold. It will show that this is not provable in general
in IPL.

***** Decidable proposition
A proposition is *decidable* iff $A \vee \neg A \text{ true}$. There are decidable
propositions even if LEM does not hold.

****** Example
Two decidable propositions are $\bot$ and $\top$.

Equality on natural numbers will be decidable, but equality on reals
will not.
***** Stable proposition
A proposition is *stable* iff $(\neg \neg A) \supset A \text{ true}$.

***** Negation of the negation of LEM
We can prove $\neg \neg (A \vee \neg A)$. This proves that not every proposition
is stable as a corollary.

****** Proof
We will assume $\neg (A \vee \neg A)$ and arrive at a contradiction. If we
assume $A$, we have $A \vee \neg A$, and then a contradiction, so it must
be the case that $\neg A$. We know now that $A \vee \neg A$, arriving at a
contradiction.

***** Prove the disjunction property for IPL
We interpret the rules of IPL as an inductive definition of
the entailment relation.

We have finitary derivation trees of every $\Gamma \vdash A$.

****** Disjunction property: formal statement
If $\varnothing \vdash A \vee B \text{ true}$, then $\varnothing \vdash A \text{ true}$ or $\varnothing \vdash B \text{ true}$.

******* Counterexample if the context were not empty
If we take $A \vee B$ as an assumption, this would trivially
not hold.
****** Disjunction property: draft of a proof
We will use an induction on derivations. We examine all possible
derivations $\varnothing \vdash A \vee B \text{ true}$ and show that there are derivations of
$A$ or $B$ also.

The last step of a derivation of $A \vee B$ should be an introduction
$\vee-I_{1}$ or $\vee-I_2$; so there should be a derivation of $A$ or $B$ in
the previous step. The assumption rule is also not applicable.
Conjunction introduction is not applicable, and the same hold for
true introduction and implication introduction. Elimination rules
are our real problem here. For example, implication elimination should
be proved.

To prove this for the implication elimination rule, we suppose that
we have the derivations for $\vdash C$ and $C \vdash (A \vee B)$, and then we could
inline the first derivation using transitivity of entailment to get
a derivation of $\vdash A \vee B$. Note that this is not a complete proof! the
derivation of $C \supset (A \vee B)$ could have be done by other elimination
rules and we should prove that for them too.
***** Structural properties are admissible
*Weakening is admissible*, if $\Gamma \vdash_{IPL} B\text{ true}$, then $\Gamma,A \text{ true} \vdash_{IPL} B \text{ true}$.
If you give a derivation of the first, you can get a derivation of the
second one.

****** Why is weakening admissible
Because the rules are polymorphic! They do not depend of Gamma. We
could inductively weaken every step $\Gamma \mapsto \Gamma,A\text{ true}$, and then, reapplying
the rules, would give us the same conclusion.

****** Similarly
You could do exchange or contraction. But reflexivity is a primitive rule!
Transitivity for $\text{IPL}^{-}$ is homework.
****** We have now defined a good logic
It is not simply a bunch of rules, they follow a criteria. The
structural properties should hold. There are substructural logics,
but those are not our topic of interest.
***** Gentzen's insight
Our previous idea to prove the disjunction property uses crucially an
inversion principle between implication introduction and implication
elimination.

*ELIM is post-inverse to INTRO.*

We are saying something like the following for introductions, eliminations
and derivations.

 * $(\wedge E_1 \circ \wedge I)({\cal D}_1,{\cal D}_2) = {\cal D}_1$

This gives rise to a dynamics of proof! We do not only look at provability,
we look at the proofs per se.

***** II. Proof relevant logic
We will write a grammar of proofs. $M : A$ means that $M$ is a proof
of $A$. In correspondence with the assumptions, there is the concept
of variables

\[ x_1:A_1, \dots , x_n:A_n \vdash M : A.\]

Transitivity now reads as a substitution rule

\begin{prooftree}
\AxiomC{$\Gamma, x : A\vdash M : B$}
\AxiomC{$\Gamma \vdash N : A$}
\BinaryInfC{$[N/x]M : B$}
\end{prooftree}

and reflexivity is only a use of a variable

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma, x:A \vdash x : A$}
\end{prooftree}

We will write this derivations as mappings on a bicartesian closed category

\[M : A_1 \times \dots \times A_n \to A.
\]

Proof-relevant logic will give rise to Type Theory and Category Theory.
**** Lecture 4: Proof Reduction and Equivalence
We first saw logic from the point of view of provability. We are going
to look at the idea of logic with proofs. From the first, we got Heyting
Algebras; from the second, we are going to get bicartesian closed categories.

We going to define equivalence of proofs $M \equiv N : A$.

***** Proof terms
We will need a grammar of proofs to construct proof terms.
The structural properties are now properties for this grammar.

 * Reflexivity is now the introduction of a variable.
 * Transitivity is now the substitution of a variable.
 * Weakening is now the ability to discard variables.
 * Contraction is now a replication of variables.
 * Exchange is a permutation of variables.

***** Logic
Now the negative fragment of our logic can be written as

\begin{prooftree}
\RightLabel{$(\top_{I})$}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash \left\langle  \right\rangle : \top$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$(\wedge-I)$}
\AxiomC{$\Gamma \vdash M : A$}
\AxiomC{$\Gamma \vdash N : B$}
\BinaryInfC{$\Gamma \vdash \left\langle M,N \right\rangle : A \wedge B$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$(\wedge_{E1})$}
\AxiomC{$\Gamma \vdash M : A \wedge B$}
\UnaryInfC{$\Gamma \vdash \text{fst}(M) : A$}
\RightLabel{$(\wedge_{E2})$}
\AxiomC{$\Gamma \vdash M : A \wedge B$}
\UnaryInfC{$\Gamma \vdash \text{snd}(M) : B$}
\noLine
\BinaryInfC{}
\end{prooftree}


\begin{prooftree}
\RightLabel{$(\supset_{I})$}
\AxiomC{$\Gamma, x:A \vdash M:B$}
\UnaryInfC{$\Gamma \vdash \lambda x . M : A \supset B$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$(\supset_{E})$}
\AxiomC{$\Gamma \vdash M . A \supset B$}
\AxiomC{$\Gamma \vdash N : A$}
\BinaryInfC{$\Gamma \vdash M(N) : B$}
\end{prooftree}

***** Gentzen's Inversion principle
***** Definitional equality
In first order logic, no one draws a distinction between propositional
equality and definitional equality.

*Definitional equality* is the least congruence closed under the following
rules
 
 * it is a equivalence relation.
 * it is compatible with the rules.

\begin{prooftree}
\AxiomC{$\Gamma \vdash M \equiv M' : A \wedge B$}
\UnaryInfC{$\Gamma \vdash \text{fst}(M) \equiv \text{fst}(M') : A$}
\end{prooftree}

Now the inversion principle can be written on proof terms.
Simplifications such as $\text{fst}\left\langle M,N \right\rangle \equiv M$ are now useful if we
interpret this as a running program with proof dynamics.

Those are called Beta rules.
The inversion principle on conjunction is now

\begin{prooftree}
\RightLabel{$(\beta\wedge_1)$}
\AxiomC{$\Gamma \vdash M : A$}
\AxiomC{$\Gamma \vdash N : B$}
\BinaryInfC{$\Gamma \vdash \text{fst}\left\langle M,N \right\rangle \equiv M : A$}
\RightLabel{$(\beta\wedge_2)$}
\AxiomC{$\Gamma \vdash M : A$}
\AxiomC{$\Gamma \vdash N : B$}
\BinaryInfC{$\Gamma \vdash \text{snd}\left\langle M,N \right\rangle \equiv N : B$}
\noLine
\BinaryInfC{}
\end{prooftree}

The inversion principle on implication is inlining

\begin{prooftree}
\RightLabel{$(\beta\supset_1)$}
\AxiomC{$\Gamma,x:A \vdash M : B$}
\AxiomC{$\Gamma \vdash N : A$}
\BinaryInfC{$\Gamma \vdash (\lambda x. M)(N) \equiv [N/x]M : B$}
\end{prooftree}

Now we can compute by calculation with closed terms written
as $M \equiv N$.

***** Gentzen's Unicity Principles
Those are $\eta$ rules.

\begin{prooftree}
\RightLabel{$(\eta\top)$}
\AxiomC{$\Gamma \vdash M : \top$}
\UnaryInfC{$\Gamma \vdash M \equiv \left\langle  \right\rangle : \top$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$(\eta\wedge)$}
\AxiomC{$\Gamma\vdash M: A \wedge B$}
\UnaryInfC{$M \equiv \left\langle \text{fst}(M),\text{snd}(M) \right\rangle$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$(\wedge\supset)$}
\AxiomC{$\Gamma\vdash M:A \supset B$}
\UnaryInfC{$\Gamma\vdash M \equiv \lambda x. Mx : A \supset B$}
\end{prooftree}

***** Propositions as types
The inversion and unicity principles will make a very strong
correspondence on categories.

\begin{tabular}{c|c|c|c}
Latticces & Propositions & Types & Categories \\
\hline
greatest & $\top$ & $1$ & final object \\
meets & $A \wedge B$ & $A \times B$ & finite products \\
exponential & $A \supset B$ & $A \to B$ & exponential \\
minimum & $\bot$ & $0$ & initial object \\
joins & $A \vee B$ & $A+B$ & coproducts
\end{tabular}

***** Category
A *category* is a generalized preoder with evidence.
The difference between preorder and partial order is
related to univalence

\begin{prooftree}
\AxiomC{$A \leq B$}
\AxiomC{$B \leq A$}
\BinaryInfC{$A \equiv B$}
\end{prooftree}

where this is an instance of univalence. 

In a category we have the structure of a preorder

 1) Reflexivity, $\mathrm{id} : A \to A$.
 2) Transitivity; if $f: A \to B$ and $g : B \to C$ then
    $g \circ f : A \to C$.

Those have to satisfy some coherence conditions, which are 
the following unit laws

 * $\mathrm{id_B}\circ f = f = f \circ \mathrm{id_A}$
 * $f \circ (g \circ h) = (f\circ g)\circ h$

The equality here is interesting. We could think of this structure
representing two paths and an homotopy between two paths on a 2-cell;
some kind of transformation. We are going to talk of a deformation
given by an associator

\[
\alpha : f \circ (g \circ h) \to (f \circ g) \circ h.
\]

And those notions of evidence (which act as natural transformations) need
also a notion of equivalence and a higher dimensional map between them. But 
this process could be repeated to infinity!

We are going to express the relation of types and terms in categorical terms.

***** Terminal object
Definition of final object

\begin{prooftree}
\AxiomC{$$}
\UnaryInfC{$\left\langle\right\rangle : A \to 1$}
\AxiomC{$M : A \to 1$}
\RightLabel{$(\eta{\top})$}
\UnaryInfC{$M = \left\langle  \right\rangle : 1$}
\noLine
\BinaryInfC{}
\end{prooftree}

this was, in our old notation, $A \vdash \left\langle  \right\rangle : 1 = \top$.

***** Product objects
There are maps

 1) $\mathrm{fst} : A \times B \to A$
 2) $\mathrm{snd} : A \times B \to A$

satisfying

\[\begin{tikzcd}[column sep=tiny]
& D \ar[bend left]{ddr}{M}\ar[swap,bend right]{ddl}{N}\dar[dashed]{\exists!} & \\
& A \times B \drar[swap]{\mathrm{fst}} \dlar{\mathrm{snd}} & \\
A && B
\end{tikzcd}\]

Pairing is the function taking two functions and returning
the function to the product

\begin{prooftree}
\AxiomC{$M : D \to A$}
\AxiomC{$N : D \to B$}
\BinaryInfC{$\left\langle M,N \right\rangle : D \to A \times B$}
\end{prooftree}

algebraically,

 * $\mathrm{fst}\circ \left\langle M,N \right\rangle = M : D \to A$
 * $\mathrm{snd}\circ \left\langle M,N \right\rangle = N : D \to B$

and there is a uniqueness condition; given

\begin{prooftree}
\RightLabel{$(\eta \times)$}
\AxiomC{$P : D \to A \times B$}
\AxiomC{$\mathrm{fst} \circ P = M$}
\AxiomC{$\mathrm{snd} \circ P = N$}
\TrinaryInfC{$P = \left\langle M,N \right\rangle : D \to A \times B$}
\end{prooftree}

the uniqueness can be seen as the existence of homotopy between
any two functions making the product diagram commute.

In particular, $\left\langle  \mathrm{fst}\circ P, \mathrm{snd} \circ P  \right\rangle = P$. Or we can say that $\left\langle \mathrm{fst}, \mathrm{snd} \right\rangle = \mathrm{id}$
or $\left\langle M,N \right\rangle \circ P = \left\langle M\circ P,N \circ P \right\rangle$.

Lawvere and Lambek first saw those connections on the 70s.

***** Exponentials
The exponential $B^A$, gives the application map with the universal
diagram

\[\begin{tikzcd}
C \dar[dashed,swap]{\exists! \lambda(h)} & 
C \times A \ar{dr}{h}\dar[dashed,swap]{\lambda(h) \times id_A} & \\
B^{A} & B^{A} \times A \rar[swap]{app} & B \\
\end{tikzcd}\]

If we write that on syntax, that is equal to

 * $app(\lambda(h) \times \mathrm{id}) = ap \circ \left\langle \lambda(h) \circ \mathrm{fst}, \mathrm{snd} \right\rangle = h$.
 * if there is any $g$ such that $ap \circ (g \times \mathrm{id}) = h$, then $g = \lambda(h)$.

We get the $\eta\text{-rule}$ of

\[
g = \lambda(\mathrm{ap} \circ (g \times \mathrm{id}))
  = \lambda(\mathrm{ap} \circ \left\langle g \circ \mathrm{fst}, \mathrm{snd} \right\rangle).
\]

The essence of all this is

\begin{prooftree}
\AxiomC{$\Gamma, x:A \vdash h : B$}
\UnaryInfC{$\Gamma \vdash \lambda x. h: B^A$}
\end{prooftree}

***** DeBruijn indices
If we write contexts as $A_{n-1}\times \dots \times A_{1}$, and we refer to the
variables using $\mathrm{snd}(\mathrm{fst}(\mathrm{fst}\dots))$.

**** Lecture 5: Universal properties
In the previous weeks we talked about

 * logic via provability and truth.
 * the entailment relation.
 * an order theoretic interpretation.
 * a logic for proofs with proof terms.
 * a notion of equality for proofs.

***** Gentzen's inversion principle
Represented on the $\beta$ principles, rules such as

 * $\mathtt{fst} \left\langle M,N \right\rangle \equiv M$
 * $\mathtt{snd} \left\langle M,N \right\rangle \equiv N$
 * $(\lambda x. M)(N) \equiv [N/x]M$
 * $\mathtt{case}(\mathtt{inl}(M), x.P, y.Q) \equiv [M/x]P$
 * $\mathtt{case}(\mathtt{inr}(M), x.P, y.Q) \equiv [M/y]Q$

they act as rules for proof simplification and can be interpreted as
a dynamics for proofs. Proofs are programs.

***** Gentzen's unicity principles
Represented on $\eta$ principles.

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \top$}
\UnaryInfC{$\Gamma \vdash M \equiv \langle\rangle : \top$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A \wedge B$}
\UnaryInfC{$\Gamma \vdash M \equiv \left\langle \mathtt{fst}(M),\mathtt{snd}(M) \right\rangle : A \wedge B$}
\end{prooftree}

there is another way of saying this

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A \wedge B$}
\AxiomC{$\Gamma \vdash \mathtt{fst}(M) \equiv P : A$}
\AxiomC{$\Gamma \vdash \mathtt{snd}(M) \equiv Q : B$}
\TrinaryInfC{$\Gamma \vdash M \equiv \left\langle P,Q \right\rangle : A \wedge B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A \supset B$}
\UnaryInfC{$\Gamma \vdash M \equiv \lambda x. M(x) : A \supset B$}
\end{prooftree}

***** Categorical interpretation
A derivation

\[
x_1:A_1,\dots,x_n:A_n \vdash M : A
\]

is interpreted as a morphism

\[
M : A_1 \times \dots \times A_n \to A\]
\[M \equiv N : A_1 \times \dots \times A_n \to A
\]

The product diagram relates

 * the existence with the introduction.
 * the uniqueness with the $\eta$ rules.
 * the commutativity with the $\beta$ rules.

***** Unicity principle for the disjunction [40:00]
It is more difficult to see how the disjunction property should be
written. An inspiration is the notion of Shannon expansion: the type
of booleans can be written as $\top \vee \top$; then $\mathtt{case}$ acts as a binary decision 
diagram. The Shannon expansion is a substitution using booleans where
 
 * $\mathtt{inl} \left\langle  \right\rangle \equiv true$
 * $\mathtt{inl} \left\langle  \right\rangle \equiv false$

then

\[
[M/x]P \equiv \text{ if } M \text{ then } [true/x]P \text{ else } [false/x]P.
\]

and, in particular,

\[
P \equiv \text{ if } x \text{ then } [true/x]P \text{ else } [false/x]P.
\]

The eta rule for disjunction is then

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A \vee B$}
\AxiomC{$\Gamma, z:A \vee B \vdash P : C$}
\BinaryInfC{$\Gamma \vdash [M/z]P \equiv \mathrm{case}(M; x : [\mathtt{inl}(x)/z]P; y : [\mathtt{inr}(y)/z]P) : C$}
\end{prooftree}

like a generalized Shannon expansion. As an special case, we
get that $M \equiv \mathtt{case}(M, x . \mathtt{inl}(x), y . \mathtt{inr}(y))$.

***** Remark
We could have defined the relationship on variables $x \equiv \left\langle \mathtt{fst}(x), \mathtt{snd}(x) \right\rangle$, but
to derive the general rule from there, we would have needed another property
to get a correct substitution rule.

***** Coproduct
We write the coproduct as $A + B$, and its diagram as

\[\begin{tikzcd}[column sep=tiny]
& C  & \\
& A+B  \uar[dashed]{\exists! \left\{ P,Q \right\}}  & \\
A\ar[bend left]{uur}{P}       \urar[swap]{\mathtt{inl}} &&
B\ar[swap,bend right]{uul}{Q} \ular{\mathtt{inr}}
\end{tikzcd}\]

where

\[
\left\{ P,Q \right\} \equiv \mathtt{case} ( - , x.P, y.Q) 
\]

And the unicity simply says that

\begin{prooftree}
\AxiomC{$\Gamma, x:A \vdash [ \mathtt{inl}(x)/z ]M \equiv P : C$}
\AxiomC{$\Gamma, y:B \vdash [ \mathtt{inr}(y)/z ]M \equiv Q : C$}
\BinaryInfC{$\Gamma, z : A+B \vdash M \equiv \mathtt{case}(z,x:P,y:Q) : C$}
\end{prooftree}

This is an induction principle. We can caracterize the behaviour of $M$ simply
by giving its behaviour on the $\mathtt{inl}$ and the $\mathtt{inr}$.
***** Beta/eta rules
The beta rules are analytic judgements. Self-evident.
The eta rules are synthetic judgements. They require proof.

The beta rules correspond to definitional equality and the 
eta rules correspond to propositional equality; it will be
expressed typically by a type. The definitional equality, on
the other hand, is simply a judgement and it is also called
a judgmental equality.
**** Lecture 6: Dependency, families of types
So far, we have seen a propositions/types correspondence.
We will add a type of natural numbers, and look for its correspondence
in intuitionistic logic.

***** GÃ¶del's T
We will call GÃ¶del's T to the system we have developed so far plus
a natural numbers type. This is not exactly GÃ¶del's T in the literature,
where it is defined only with function types.

\begin{prooftree}
\RightLabel{(Nat$_{I-0}$)}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash 0 : Nat$}
\RightLabel{(Nat$_{I-S}$)}
\AxiomC{$\Gamma \vdash M : Nat$}
\UnaryInfC{$\Gamma \vdash s(M) : Nat$}
\noLine
\BinaryInfC{}
\end{prooftree}

The elimination form is just definition by recursion

\begin{prooftree}
\RightLabel{(Nat$_{E}$)}
\AxiomC{$\Gamma \vdash M : Nat$}
\AxiomC{$\Gamma \vdash P : A$}
\AxiomC{$\Gamma, x:A \vdash Q:A$}
\TrinaryInfC{$\Gamma \vdash \mathtt{rec}(P,x.Q)(M) : A$}
\end{prooftree}

We need now two beta rules to comply with the inversion principle.

 * $\mathtt{rec}(P,Q)(0) \equiv P$
 * $\mathtt{rec}(P,Q)(s(M)) \equiv [ \mathrm{rec}(P,Q)(M)/x ]Q$

so, if $\overline{n} = s(\dots s(0)\dots)$, $\mathtt{rec}(P,Q)(\overline{n}) \equiv Q(Q(\dots (Q(P))\dots)$.

There is also a eta rule, that was not considered by GÃ¶del at the moment.
Suppose an $M$ acting the same way on the $0$ and the $s$, then it is the
recursor.

\begin{prooftree}
\AxiomC{$\Gamma,z : Nat \vdash M : A$}
\AxiomC{$\Gamma \vdash [0/z] M \equiv P:A$}
\AxiomC{$\Gamma, z:Nat \vdash [ S(z)/z ]M \equiv [M/x]Q$}
\TrinaryInfC{$\Gamma,z:Nat \vdash M \equiv \mathtt{rec}(P,Q)(z)$}
\end{prooftree}

****** Special case on the recursor
Plugging naturals on the recursor

$z : Nat \vdash \mathtt{rec}(0,y.s(y))(z) \equiv z : Nat$

****** Commuting conversion

$\Gamma, z:Nat \vdash [ \mathtt{rec}(0,y.s(y))(z)/z  ]M \equiv \mathtt{rec}([0/z]M, y.[s(y)/z]M)(z)$

***** Natural numbers object in a category
The natural numbers object is the universal object in the following
diagram

\[\begin{tikzcd}[column sep=huge]
1 \rar{0}\drar[swap]{P} &
\mathbb{N} \dar[dashed]{\exists! \mathtt{rec}(P,Q)} &
\mathbb{N} \rar{s}\dar[dashed]{\exists! \mathtt{rec}(P,Q)} &
\mathbb{N} \dar[dashed]{\exists! \mathtt{rec}(P,Q)} \\
&
A &
A \rar[swap]{Q}&
A
\end{tikzcd}\]

***** Reorging the NNO into an initial algebra
This is equivalent to this universality property

\[\begin{tikzcd}[column sep=60pt]
1+\mathbb{N} \dar[swap]{\left\{ 0,s \right\}} \rar[dashed]{ id + \mathtt{rec}(P,Q) } & 
1+A \dar{\left\{ P,Q \right\}} \\
\mathbb{N} \rar[dashed]{\mathtt{rec}(P,Q)}  & 
A
\end{tikzcd}\]

where $f+g : A+B \to A'+B'$ is defined componentwise on the coproduct.

***** Initial algebras
This is an instance of a more general phenomenon, where a functor $F$ satisfies
the diagram with the initial object $I$.

\[\begin{tikzcd}
F(I)\rar{F(!)} \dar[swap]{i} & F(A) \dar{f} \\
I\rar{(!)} & A
\end{tikzcd}\]

This is called an *initial algebra*.

***** Defining addition
We can define addition on the second argument

$\mathtt{plus} := \lambda x. \lambda y. \mathtt{rec}(x;z.s(z))(y)$

and we can check that $\mathtt{plus}\ \overline{m}\ \overline{n} \equiv \overline{m+n}$. But we also can
define addition on the first $\mathtt{q}:= \lambda x.\lambda y. \mathtt{p}\ y\ x$, and this also
implements addition: $\mathtt{q}\ \overline{m}\ \overline{n} \equiv \overline{m+n}$.

Be we cannot prove

\[
x:Nat, y:Nat \vdash \mathtt{p}\ x\ y \equiv \mathtt{q}\ x\ y  \equiv \mathtt{p}\ y\ x
\]

as these are NOT definitionally equal! It can be proved that it is
not provable using only beta rules. This would require a proof by
induction: to show something for all the numerals is the same thing
as to show it for any numeral variables.

***** Extensional and intensional equality
Those are *extensionally* equal, but they are not intensionally equal
(definitional equality). They represent a different algorithm. In the
*sense of Frege*, they have the same reference, but not the same sense.
They have the same IO but different algorithms.

 * Extensional equality is analytic, it does not require proof.
 * Intensional equality is synthetic, does requires proof.

Extensional equality on $(\mathbb{N}\to \mathbb{N})\to(\mathbb{N}\to \mathbb{N})$ has a high quantifier
complexity. A bunch of nested forall and exist.

***** Extensional equality
Intensional equality is an inductive defined judgment, whereas
Extensional equality is a proposition such as

\[ \mathtt{p}\ x\ y =_{Nat} \mathtt{q}\ x\ y
\]

that is an atomic proposition. By the propositions as types
principle, extensional equality is a family of types.

\[
x: Nat, y:Nat \vdash x = y \text{ type}
\]

sometimes $x=y$ is written as $Id_{Nat}(x,y)$. It is a propositional
function or a binary relation.

This family can be instantiated by substitution

\[
Id_{Nat}(M,N) \text{ type}
\]

whenever $M,N:Nat$.

***** Example of extensional equality
We can define the finite sequence of naturals of length $x:Nat$

\[
x : Nat \vdash Seq(x) \text{ type.}
\]

In this case, $Seq(p\ \overline{m}\ \overline{n}) \equiv Seq(q\ \overline{m}\ \overline{n})$ because of the fact that
$p\ \overline{m}\ \overline{n} \equiv q\ \overline{m}\ \overline{n}$. But

\[
x:Nat, y:Nat \vdash Seq(p\ x\ y) \not\equiv Seq(q\ x\ y)
\]

will not be definitionally equal. But they are isomorphic! In some
sense, they should be equivalent. $A \simeq B$ should mean that for some
$f,g$, we should get

\[\begin{aligned}
\alpha :&\quad g \circ f = \mathrm{id} \\
\beta :&\quad f \circ g = \mathrm{id}
\end{aligned}\]

but again, we are we meaning here by equality? In this case we are
talking about propositional equality. There should be transformations
$\alpha,\beta$ between the compositions and the identities.

***** Univalence axiom
In some sense, we expect them to be equal. Univalence says that $A=B \iff A \simeq B$.
There will be an equivalence between those two types.
***** Setup for dependent types
Context/closed types. We have judgements
 
 * $\Gamma \text{ ctx}$
 * $\Gamma \equiv \Gamma'$

Open types/families

 * $\Gamma \vdash A \text{ type}$
 * $\Gamma : A \equiv A'$

Elements of types

 * $\Gamma \vdash M : A$
 * $\Gamma \vdash M \equiv M' : A$

We will have a notion of empty context and the notion of adding anything
to a context

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\cdot \text{ ctx}$}
\AxiomC{$\Gamma \text{ ctx}$}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\BinaryInfC{$\Gamma, x:A \text{ ctx}$}
\noLine
\BinaryInfC{}
\end{prooftree}

and the equality

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\cdot \equiv \cdot$}
\AxiomC{$\Gamma \equiv \Gamma'$}
\AxiomC{$\Gamma \vdash A \equiv A'$}
\BinaryInfC{$\Gamma, x:A \equiv \Gamma, x:A'$}
\noLine
\BinaryInfC{}
\end{prooftree}

we can take variables

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma,x:A,\Delta \vdash x :A $}
\end{prooftree}

here it is necessary a weakening rule

\begin{prooftree}
\AxiomC{$\Gamma,\Delta \vdash J$}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\BinaryInfC{$\Gamma, x:A, \Delta \vdash J$}
\end{prooftree}

and a substitution

\begin{prooftree}
\RightLabel{(substitution/transitivity)}
\AxiomC{$\Gamma, x:A, \Delta \vdash J$}
\AxiomC{$\Gamma \vdash M:A$}
\BinaryInfC{$\Gamma [M/x]\Delta \vdash [M/x]J$}
\end{prooftree}

and the principle of functionality

\begin{prooftree}
\AxiomC{$\Gamma, x:A, \Delta \vdash N:B$}
\AxiomC{$\Gamma\vdash M\equiv M' :A$}
\BinaryInfC{$\Gamma [M/x] \Delta \vdash [M/x]N \equiv [M'/x]N : [M/x]B$}
\end{prooftree}

another simpler rule is

\begin{prooftree}
\AxiomC{$\Gamma \vdash M:A$}
\AxiomC{$\Gamma \vdash A \equiv A'$}
\BinaryInfC{$\Gamma \vdash M:A'$}
\end{prooftree}

and similarly

\begin{prooftree}
\AxiomC{$\Gamma \vdash M \equiv M':A$}
\AxiomC{$\Gamma \vdash A \equiv A'$}
\BinaryInfC{$\Gamma \vdash M\equiv M':A'$}
\end{prooftree}

All this is written on the section 2 of the appendix of HoTT.

****** Exercise
Consider exchange and contraction
***** Formation rules
The identity type is constructed as

\begin{prooftree}
\RightLabel{(Id-F)}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\AxiomC{$\Gamma \vdash M:A$}
\AxiomC{$\Gamma \vdash N:A$}
\TrinaryInfC{$\Gamma \vdash Id_A(M,N) \text{ type}$}
\end{prooftree}

iterated identity types can be defined $Id_{Id_A(M,N)}$ to any dimension.
The introduction rule should be

\begin{prooftree}
\RightLabel{(Id-I)}
\AxiomC{$\Gamma \vdash M:A$}
\UnaryInfC{$\Gamma \vdash \mathrm{refl}(M) : Id_A(M,M)$}
\end{prooftree}

being a witness of the fact that $M$ is equal to itself.
**** Lecture 7: Dependent Types
***** Last week
The basic judgements are

 1) $\Gamma \text{ ctx}$
 2) $\Gamma \equiv \Gamma'$
 3) $\Gamma \vdash A \text{ type}$
 4) $\Gamma \vdash A \equiv A'$
 5) $\Gamma \vdash M:A$
 6) $\Gamma \vdash M \equiv M' :A$

and they follow structural properties. For example, typing respect definitional
equivalence

\begin{prooftree}
\AxiomC{$\Gamma \vdash M:A$}
\AxiomC{$\Gamma \vdash A \equiv A'$}
\BinaryInfC{$\Gamma \vdash M : A'$}
\end{prooftree}

We left open the exact formulation.

****** Example
An example of dependent type is $x : Nat \vdash Seq(x) \text{ text}$.

\begin{prooftree}
\AxiomC{$M \equiv M' : Nat$}
\UnaryInfC{$Seq(M) \equiv Seq(M')$}
\end{prooftree}

But we do not get $x,y : Nat \not\vdash Seq(x+y) \equiv Seq(y+x)$. The
reason is that $x + y \not\equiv y + x$, they are only intensionally equivalent.
To apply $\eta$ you need to establish an invariant about a candidate $M$,
and the $\eta$ rule would not be enough to write a proof of induction of
that fact. We do not want an induction eta-rule.

***** Proof-relevance
An element $P : Id_A(M,N)$ can be seen as

 1) a proof that $M$ is $N$.
 2) an identification of $M$ with $N$.
 3) a path from $M$ to $N$.

This $x =_A y$ is called propositional equality.

***** Generalization to dependent types
We will review the initial structure of types to generalize the
propositional negative connectives to their dependent forms.
For example, $A \times B$ will generalize to a sigma type $\sum_{x:A}B_x$;
and $A \supset B$ generalizes to $\prod_{x:A}B_{x}$.

\[
\prod_{x:Nat} \sum_{y:Nat} Id_{Nat}(y, \mathtt{succ}(x))
\]

They will represent logical conectives as

\[
\forall x:Nat. \exists y:Nat.\quad y = \mathtt{succ}(x).
\]

***** Pi Types
Formation rules

\begin{prooftree}
\RightLabel{($\pi$-F)}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\AxiomC{$\Gamma, x : A \vdash B_x \text{ type}$}
\BinaryInfC{$\Gamma \vdash \prod_{x:A}B_{x} \text{ type}$}
\end{prooftree}

introductory rules

\begin{prooftree}
\RightLabel{($\pi$-I)}
\AxiomC{$\Gamma, x:A \vdash M_{x} : B_{x}$}
\UnaryInfC{$\Gamma \vdash \lambda x. M_x : \prod_{x:A}B_{x}$}
\end{prooftree}

elimination rules

\begin{prooftree}
\RightLabel{($\pi$-E)}
\AxiomC{$\Gamma \vdash M : \prod_{x:A}B_x$}
\AxiomC{$\Gamma \vdash N:A$}
\BinaryInfC{$\Gamma \vdash MN : [N/x]B$}
\end{prooftree}

There is a beta-rule

\[
(\lambda x.M)N \equiv [N/x]M
\]

and an eta-rule

\[
(\lambda x.M x)\equiv M.
\]
***** Particular case
$A \supset B$ is a particular case of a pi-type where $B$ does
not depends on $A$.

***** Sigma type
****** Formation

\begin{prooftree}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\AxiomC{$\Gamma, x:A \vdash B_{x} \text{ type}$}
\BinaryInfC{$\Gamma \vdash \sum_{x:A}B_x \text{ type}$}
\end{prooftree}

****** Introduction
This is constructive existence, you are required to show evidence
of a particular case where it does hold

\begin{prooftree}
\AxiomC{$\Gamma \vdash M :A $}
\AxiomC{$\Gamma \vdash N : [M/x]B$}
\BinaryInfC{$\Gamma \vdash \left\langle M,N \right\rangle : \sum_{x:A} B_x$}
\end{prooftree}

****** Particular case
The product of types is a particular case where $B_x$ is
independent from $x:A$.

****** Elimination
Will not be the same as in the HoTT book.

\begin{prooftree}
\RightLabel{($\Sigma_{E_1}$)}
\AxiomC{$\Gamma \vdash M : \sum_{x:A} B_x$}
\UnaryInfC{$\Gamma \vdash \mathtt{fst}(M) : A$}
\RightLabel{($\Sigma_{E_2}$)}
\AxiomC{$\Gamma \vdash M : \sum_{x:A} B_x$}
\UnaryInfC{$\Gamma \vdash \mathtt{snd}(M) : [ \mathtt{fst}(M)/x]B_x$}
\noLine
\BinaryInfC{}
\end{prooftree}

****** Beta/eta rules
Beta rules

 * $\mathtt{fst}\left\langle M,N \right\rangle \equiv N$,
 * $\mathtt{snd}\left\langle M,N \right\rangle \equiv N$

and an eta-rule

 * $\left\langle \mathtt{fst}(M), \mathtt{snd}(M) \right\rangle \equiv M$.
 
***** Constructive logic
Can be seen as a refinment of classical logic, not as anything opposite
to it.
***** Positive fragment
In the positive fragment, we have $(0,A+B,Nat,\dots)$. But are not
going to change the types. Issue: the positive elims reach into
arbitrary types.

For example, the elim of $+$ was

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A + B$}
\AxiomC{$\Gamma,x:A \vdash N:C$}
\AxiomC{$\Gamma, y:B \vdash P:C$}
\TrinaryInfC{$\Gamma \vdash \mathtt{case}(M,x.N,y.P) : C$}
\end{prooftree}

but here there is no dependency. $C$ captures the join point of two
branches; or proof by cases.

****** Example: induction
Let $2 := 1 + 1$, $tt := \mathtt{inl}\langle\rangle$ and $ff := \mathtt{inr}\langle\rangle$. We define

\[ \mathtt{if}(M,N,P) := \mathtt{case}(M,-.N,-.P)
\]

and we want to prove that every element of $2$ is one of those

\[
\prod_{x:2} \left( Id_2(x, \mathtt{tt}) + Id_2(x, \mathtt{ff}) \right).
\]

We have to prove both

 * $Id_2(\mathtt{tt}, \mathtt{tt}) + Id_2(\mathtt{tt}, \mathtt{ff})$
 * $Id_2(\mathtt{ff}, \mathtt{tt}) + Id_2(\mathtt{ff}, \mathtt{ff})$

So we use

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A + B$}
\AxiomC{$\Gamma, z:A+B \vdash C_{z}$ type}
\AxiomC{$\Gamma, x:A \vdash N: [\mathtt{inl}(x)/z] C$}
\noLine
\UnaryInfC{$\Gamma, y:B \vdash P: [\mathtt{inr}(y)/z] C$}
\TrinaryInfC{$\Gamma \vdash \mathtt{case} [z.C] (M;x.N;y.P) : [M/z]C$}
\end{prooftree}

where $[z.C]$ is called the *motive* (term by Connor McBride). In this
particular case

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : 2$}
\AxiomC{$\Gamma, z:2 \vdash C_{z}$ type}
\AxiomC{$\Gamma\vdash N: [\mathtt{tt}/z] C$}
\noLine
\UnaryInfC{$\Gamma\vdash P: [\mathtt{ff}/z] C$}
\TrinaryInfC{$\Gamma \vdash \mathtt{if}(M;N;P) : [M/z]C$}
\end{prooftree}

This is a rule of induction.

****** Example
We have the expression

$\mathtt{if}(M,17, \mathtt{tt}) : \mathtt{if} (M,Nat,2)$

but, it is a well-typed expression? not yet. We cannot type
those as types.

***** Induction on naturals
\begin{prooftree}
\AxiomC{$\Gamma \vdash M : Nat$}
\AxiomC{$\Gamma, z:Nat \vdash C \text{ type}$} 
\AxiomC{$\Gamma \vdash N : [0/z]C$}
\noLine
\UnaryInfC{$\Gamma,x:Nat, y:[x/z]C \vdash P:[s(x)/z]C$}
\TrinaryInfC{$\Gamma \vdash \mathtt{rec}[z.C](M,N;x,y.P) : [M/z]C$}
\end{prooftree}

with the two beta rules

 * $\mathtt{rec}[z.C](0,N; x,y.P) \equiv N$
 * $\mathtt{rec}[z.C](s(M), N; x,y.P) \equiv [M, \mathtt{rec}[z.C](M,N;x,y.P)/x,y]P$

It has an eta rule which is not useful.

****** Exercise

$\prod_{x:Nat} \left(Id(s(x),0) \to \bot\right)$

we will use

\[
\lambda x. \mathtt{rec}[-](x; -,-)
\]

****** Hard exercise
We cannot solve this yet

$\prod_{x,y : Nat} (Id_{Nat}(sx,sy) \to Id_{Nat}(x,y))$
***** The other form of product types, sigma variant
Idea: elimination as pattern-matching.

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \Sigma_{x:A}B_x$}
\AxiomC{$\Gamma, z : \Sigma_{x:A}B_x \vdash C \text{ type}$}
\AxiomC{$\Gamma, x:A, y:B \vdash P : [\langle x,y \rangle/z]C$}
\TrinaryInfC{$\Gamma \vdash \mathtt{split}[z.C](M;x,y.P) : [M/z]C$}
\end{prooftree}

the beta rule is

 * $\mathtt{split}[z.C](\left\langle M_1,M_2 \right\rangle; x,y.P) \equiv [M_1,M_2/x,y]P$

and the eta rule is similar to previous $\eta$ rules. Anything like split is split.

****** Exercise
Define =split= from fst,snd.
Define =fst=, =snd= from split. (Not yet)
**** Lecture 8: Identity types
***** Polarity
Negative and positive fragments. The difference here is
if the type is based on the elimination or the introduction
rule; the other part of the rule is determined by this first
rule. In category theory, it corresponds to the universal
property mapping /in/ or /out/ the definition of the type.

\begin{tabular}{c|cc}
            & negative    & positive     \\
\hline
type theory & elimination & introduction \\
category theory & UP mapping in & UP mapping out
\end{tabular}

For example, $A\times B$ is /negative/. We write the elimination rule:
given a product, we have =fst= and =snd=. The introduction rule is
a pair, needing an $A$ and a $B$.
***** Last week
Dependent formulations of 

 1) negatives $\Pi,\Sigma$.
 2) positives, the type does not change, but the elimination forms do;
    they become induction principles

Elim for the booleans is an example of branching. Today

 * identity types
 * universes
 * ITT. Limitations and peculiarities

***** Identity types
They have this rule of formation

\begin{prooftree}
\RightLabel{(Id-F)}
\AxiomC{$\Gamma \vdash A \text{ type}$}
\AxiomC{$\Gamma \vdash M : A$}
\AxiomC{$\Gamma \vdash N : A$}
\TrinaryInfC{$\Gamma \vdash Id_A(M,N) \text{ type}$}
\end{prooftree}

if we read this type propositionally, this is the type of proofs of
equality between $M$ and $N$. As a notation we use $M =_A N$.

\begin{prooftree}
\RightLabel{(Id-T)}
\AxiomC{$\Gamma \vdash M : A$}
\UnaryInfC{$\Gamma \vdash \mathtt{refl}_A(M) : Id_A(M,M)$}
\end{prooftree}

In ITT, this would be the only intro rule. We can think of $Id$ as an
inductively generated family of types. 

***** Elimination rule for identity types
The elimination rule would then work as

\begin{prooftree}
\RightLabel{(Id-E)}
\AxiomC{$\Gamma \vdash P: Id_{A}(M,N)$}
\AxiomC{$\Gamma, x:A, y:A, z:Id_A(x,y) \vdash C \text{ type}$}
\AxiomC{$\Gamma, x:A \vdash Q: [x,x,\mathtt{refl}(x)/x,y,z]C$}
\TrinaryInfC{$\Gamma \vdash J[x,y,z.C](P;x.Q) : [M,N,P/x,y,z]C$}
\end{prooftree}

This principle is called *path induction*, where a path is an element
of the identity type. The beta rule is then

 * $J[x,y,z.C]( \mathtt{refl}(M), x.Q) \equiv [M/x]Q : [M,M, \mathtt{refl}(M)/x,y,z]C$

This $J$ is the computational content of the proofs by path induction.
***** Equivalence relation of identity
The identity type should be an equivalence relation

 1) it is reflexive by definition, $Id_A(M,M) \text{ true}$.
 2) it is symmetric showing that there is a function

    \[ \mathtt{sym}_A : \prod_{x,y:A}Id_A(x,y) \to Id_A(y,x)
    \]

 3) it is transitive with

    \[ \mathtt{trans}_A:
    \prod_{x,y,z:A} Id_A(x,y) \to Id_A(y,z) \to Id_A(x,z)
    \]

To define symmetry, we will take

\[ \mathtt{sym}_A :=
\lambda x,y:A.\quad \lambda z:Id_A(x,y).\quad
J[x,y, - : Id_A(y,x)}](z;x. \mathtt{refl}_A(x))
\]

Note that $\mathtt{sym}(M)(M)(\mathtt{refl}(M)) \equiv \mathtt{refl}(M)$ due to the beta rule for $J$.

To define transitivity 

\[ \mathtt{trans}_A :=
\lambda m,n,p. \ \lambda u{:}Id_A(m,n).\ \lambda v{:} Id_A(n,p).\  
(J[x,y, -:Id_A(y,p) \to Id(x,p) ](u; x.\lambda w.w))(v)
\]

# It would be better to stop using lambdas for the parameters and
# write the arguments as arguments.

Note that, in particular, $\mathtt{trans}(M)(M)(P)(\mathtt{refl_A(M)})(Q) \equiv Q$.

****** Exercise
Find two other proofs, not definitionally equivalent, of transitivity.
Hint: double induction.
***** Simple functionality
Suposse $x:A\vdash F:B$ where $A,B$ are types. We have $F\colon A \to B$.
We would like to have a way to prove that maps preserve equality

\[
x,y{:}A, u{:}Id_A(x,y) \vdash Id_B(Fx,Fy)
\]

We will define $\mathtt{ap}\ F\ u$, also called $F(|u|)$; the functorial action

\[ \mathtt{ap}\ F\ u = 
J[x,y, -:Id_B(Fx,Fy)](u, x. \mathtt{refl}_B(F\ x))
\]

***** Transportation property
Suposse $x:A \vdash B \text{ type}$, two pictures are useful

 1) Assigning $a{:}A \mapsto B[a]$ should be functorial.
 2) $\int_{A} B$ should have a display map with fibers sending elements
    on $B[a]$ to $a$.

In some sense, $a = a'$ must imply $B[a] \simeq B[a']$. Transportation could
be thought as functionality for families.

We would want to have 

\[
m,m':A, u:Id_A(m,m'), v : [m/x]B \vdash \mathtt{tr}[x.B](u)(v) : [m'/x]B
\]

the notation for $\mathtt{tr}[x.B](u)(v)$ is $u_{\ast}(v)$.

# Diagram of the lifting property [1:19:50]

This should be defined using path induction

\[ \mathtt{tr}[x.B](u)(v) :=
J\Big[x,y, -:[x/z]B \to [y/z]B\Big](u; z. \lambda w.w)(v)
\]

***** Exercise
Find a map 

\[
x,y{:}Nat \vdash  -{:} Seq(x+y) \to Seq(y+x)
\]

To do this we need

 1) to find a path $x,y{:} Nat \vdash -{:}x+y =_{Nat} y+x$.
 2) transport along that path.
**** Lecture 9: Universes
***** Last week
Maps preserve proofs of identity (functionality)

 * if $F \colon A \to B$ and $p : Id_A(a,a')$, then $\mathtt{ap}\ f\ p : Id_B(f(a),f(a'))$.
   If $a = a'$ is true, $f(a) = f(a')$ is true.

The transportation creates isomorphisms between families of types.

 * If $x:A\vdash B$ is a type and $p : Id_A(a,a')$, then
   $\mathtt{tr}[x.B](p) : B[a] \to B[a']$. This is family functionality.
   If $a=a'$, then $B[a] \iff B[a']$.
***** Universes and large elims
Last notion on ITT.

****** Example
We defined $\mathtt{if}(M,N;P) : B[M]$, and we would want to write things like
$\mathtt{if}(M;17;\mathtt{tt}) : \mathtt{if}(M; Nat,Bool)$; a type depending on a boolean. But we cannot
write it because those are types instead of terms.

****** Large eliminations (ad hoc)
We simply introduce a new type

\begin{prooftree}
\AxiomC{$M:Bool$}
\AxiomC{$A$ type}
\AxiomC{$B$ type}
\TrinaryInfC{$IF(M,A,B)$ type}
\end{prooftree}

where

 * $IF(\mathtt{tt},A,B) \equiv A$
 * $IF(\mathtt{ff},A,B) \equiv B$

Those are called *large eliminations*.

****** Universes of types
A *universe* is a type of types.

\begin{prooftree}
\AxiomC{}
\UnaryInfC{${\cal U}$ type}
\end{prooftree}

\begin{prooftree}
\AxiomC{}
\UnaryInfC{${\cal U} \equiv {\cal U}$}
\end{prooftree}

where the introduction rules are the previous formation rules.
For example,

\begin{prooftree}
\RightLabel{(UI-Id)}
\AxiomC{$A: {\cal U}$}
\AxiomC{$M,N:A$}
\BinaryInfC{$Id_A(M,N) : {\cal U}$}
\end{prooftree}

The universe should be closed to type formation, for example, closed
to pi-types, sigma-types, 0, 1, sum of two types...

\begin{prooftree}
\AxiomC{$A : {\cal U}$}
\AxiomC{$x:A \vdash B:{\cal U}$}
\BinaryInfC{$\prod_{x:A} B:{\cal U}$}
\end{prooftree}

But we do NOT postulate that ${\cal U} : {\cal U}$. Its formation rule is the unique
formation rule that is not translated. In other way, the Burali-Forte
Paradox could be reproduced.

****** Solving the problem with universes
Now, we could form $\mathtt{if}(M,Nat,Bool) : {\cal U}$; except we should define some
elimination rules.

Universes allow us to prove

 * injectivity of succ.
 * define =fst=, =snd= from =split=.

If we were to have large elims, we could write things like $\mathtt{If}(M,U,U\to U)$; but
not with the universal type, where is not true that $U : U$.

***** Hierarchy of universes
To solve previous problems, we could write a cumulative hierarchy of
universes. We have a family of rules

\begin{prooftree}
\RightLabel{(U-I)}
\AxiomC{$$}
\UnaryInfC{${\cal U}_i : {\cal U}_{i+1}$}
\RightLabel{(U-$\equiv$)}
\AxiomC{$$}
\UnaryInfC{${\cal U}_i \equiv {\cal U}_{i}$}
\noLine\BinaryInfC{}
\end{prooftree}

defining ${\cal U}_1,{\cal U}_2,\dots$ with a trivial definitional equality and closure
properties for every universe, for example

\begin{prooftree}
\AxiomC{$A : {\cal U}_i$}
\AxiomC{$x:A \vdash B:{\cal U}_i$}
\BinaryInfC{$\prod_{x:A} B:{\cal U}_i$}
\end{prooftree}

and the *principle of cumulativity*

\begin{prooftree}
\RightLabel{(cumulativity)}
\AxiomC{$A:{\cal U}_i$}
\UnaryInfC{$A : {\cal U}_{i+1}$}
\RightLabel{(cumulativity-$\equiv$)}
\AxiomC{$A \equiv B : {\cal U}_{i}$}
\UnaryInfC{$A \equiv B : {\cal U}_{i+1}$}
\noLine\BinaryInfC{}
\end{prooftree}

This architecture is forced on us by the Burali-Forte paradox. This
corresponds to the idea of inaccesible cardinals; to a size hierarchy.
***** Dimension
There will be another different notion (on a different axis) than
size (universes). Dimensions are new in HoTT.

***** Formation rules in HoTT
In the HoTT book, formation rules are written as introductions to
the universal type

\begin{prooftree}
\AxiomC{$A : {\cal U}$}
\AxiomC{$x:A \vdash B:{\cal U}$}
\BinaryInfC{$\prod_{x:A} B : {\cal U}$}
\end{prooftree}

This uses *typical ambiguity*, ${\cal U}$ can be ${\cal U}_i$. The type inference algorithms
of proof assistants solve these constraints, specifying the level in which
we are working. Is a kind of Universe Polymorphism: it let you pretend
${\cal U} : {\cal U}$.

It is very difficult to write something where it is not possible to get an
error at the time of inferring universes. The Burali-Forte paradox cannot be
written in this setting.

***** ITT
At this point, we have introduced ITT with Nats, Sigma, Pi, Identity and
Universes. [Martin-LÃ¶f 73]

****** Theorem of Choice in ITT
When $C$ is a total relation, we can pick up a canonical representative of
the elements to which $x$ is related to.

\[
\left(\prod_{x:A} \sum_{y:B} C(x,y)\right) \to \sum_{f:A\to B}\prod_{x:A} C(x,f(x))
\]

where $f$ is called the choice function.

In set theory, this is indepent of the axioms of sets; but in type
theory, it is a theorem because of proof-relevance.

\[
\lambda F.\ 
\left\langle \lambda x. \mathtt{fst}(F(x)) , \lambda x. \mathtt{snd}(F(x)) \right\rangle
\]

where $\mathtt{snd}(F(x)) = C(x, \mathtt{fst}(F(x)))$. 

****** Axiom of choice on sets
How about translating this proof to set theory? We would need for the proof
to be a parametrized object! Proof relevance is key to prove the theorem of
choice. If we cannot look inside the first proof, we could not prove the
theorem.

***** Martin-Lof theorem
If $p:Id_A(M,N)$ for any closed $M,N,A$; without hypothesis (a theorem),
then $M \equiv N : A$.

****** Example
If we have that 

$A = Nat \to Nat \to Nat$
$M = \lambda x. \lambda y.\ x+y$
$N = \lambda x. \lambda y.\ y+x$
$M \not\equiv N$

There is no proof $p : Id(M,N)$, no term of that type. Yet, given $x,y,z: Nat$,
there exists a proof of $p : Id(x+y,y+x)$. The axiom of extensionality fails
here. It is NOT true that

\begin{prooftree}
\AxiomC{$x : A \vdash p:Id_{B}(fx,gx)$}
\UnaryInfC{\vdash -:Id$_{A\to B}(f,g)$}
\end{prooftree}

So the ordinary notion of function is not true here anymore. We cannot
even prove that $\lambda x.fx = \lambda x.gx$. This is a weakness of ITT.

****** Function extensionality as an axiom
If we introduce extensionality as an axiom, we would have introduced another
intro for identity types, and $J$ should have to be redefined.

In HoTT, we can construct this from the axioms, and this makes very difficult
the task of giving it a meaningful computational interpretation.

***** Extensional theory of types (ETT)
In ETT (Extensional here has another meaning), the only identifications are
=refl=. We have the principle of *identity reflection*

\begin{prooftree}
\AxiomC{$p : Id_{A}(M,N)$}
\UnaryInfC{$M \equiv N : A$}
\end{prooftree}

and the only possible proof of identity is =refl=

\begin{prooftree}
\AxiomC{$p : Id_{A}(M,N)$}
\UnaryInfC{$p \equiv \mathtt{refl}(M): Id_A(M,N)$}
\end{prooftree}

In this case, the problem is solved. $Id_B(fx,gx)$ gives us $fx \equiv gx$,
so $\lambda x. fx \equiv \lambda x. gx$, and by eta-reduction, $f \equiv g$. This implies function
extensionality and obviates the need for a rule of transport, we can in fact
show that $B[a] \equiv B[a']$. NuPRL was based on ETT, Coq is based on ITT.

***** The disadvantage of ETT
$M:A$ is decidable (not feasible) in ITT, while $M : A$ is undecidable in ETT.
We need arbitrary theorem proving to decide the equality on ETT. It is instead
decidable if a derivation is a valid one.

**** Lecture 10: Groupoid structure of types
***** Last week
We considered ITT vs ETT. ETT has advantages

 * in ETT, there is no necessity for transport.
 * ETT is similar to standard mathematics, in that there is "just
   equality".
 * function extensionality is implied from the rules.

And disadvantages

 * type checking is no longer decidable; judgmental equality
   is not decidible and it involves proof search and arbitrary theorem
   proving.

A *setoid* in ITT is a set with a given equivalence relation chosen to
have properties like function extensionality. So, the alternative to ETT
is to work with setoids on ITT.

Types are (limited to) sets (aka hsets). The reason why ETT looks like standard
mathematics is because we are working with Sets, where the only paths/identifications
are reflexivities. A sufficient condition for being discrete is decibility of equality.

***** Groupoids
Types in ITT, in contrast, are more general. They are $\infty\text{-groupoids}$, 
and they have richer path structure. Paths can be composed

 * $p : Id_A(a,b)$
 * $q : Id_A(b,c)$

When ITT was introduced, it had power to use gropoids. At the very
beginning, no one realized this feature. ETT works for set-level 
mathematics and it is easier.

****** Strict/weak refl
In ETT we can have two views of refl. NuPRL uses strict sets.

****** HoTT
ITT + axioms introducing new paths. Examples of new types are the
interval.

\[\begin{tikzcd}
I : & \underset{0}\cdot \rar[no head] & \underset{1}\cdot 
\end{tikzcd}\]

which is different from boolean types in that it has a nontrivial path

\[\begin{tikzcd}
Bool : & \underset{0}\cdot & \underset{1}\cdot 
\end{tikzcd}\]

There elements $0,1 : I$ and $seg : \mathrm{Id}_I(0,1)$. The booleans have a mapping
out property where a function $f : Bool \to A$ is defined by

 * $f(0) : A$
 * $f(1) :A$

on intervals, to create a function $f : I \to A$, you must also specify what the
function does on the segment

 * $f(\mathtt{seg}) : f(0) = f(1)$

just intuitively, we want some free structure on the data defining the type,
so that if we want to map out a type, we are required to define how to get
from a type to another on all constructors. $f : I \to A$, for example, picks
a path in $A$.
***** How does J deal with new paths?
Where do you get off adding axioms to type theory? In type theory, 
we have introductions, eliminations, and the Gentzen's inversion/unicity
principle gives us computation by beta rules. Everything has a beta-normal
form.

But when we add axioms, we get into trouble. What should we do on these cases

 * $J[\ ](\mathtt{seg}, x.Q) \equiv ?$
 * $J[\ ](\lambda x.p, y.Q) \equiv ?$

the principal problem with HoTT is how to recover constructivity/computation on
HoTT. We can here express non set-level math. If every equalities are decidable,
we have to be working with sets.

***** I. types are infinite-groupoids
Types are $\infty\text{-groupoids}$. 

Recall that $id_A(M) := \mathtt{refl}_A(M) : Id_A(M,M)$ and that
$p: Id_A(M,N) \vdash p^{-1} : Id_A(N,M)$ was defined by $J$. We also
had composition of paths (transitivity). =trans(p,q)= is defined
by $J$; so we have an equivalence relation.

We have to write code to prove that this is, in fact, an equivalence
relation. Those paths follow the groupoid laws.

****** Groupoid laws
In a groupoid, these laws has to be satisfied

  * $p \cdot p^{-1} \equiv \mathrm{id}$, could be an option, but we usually will accept
    equality on a weaker sense $p \cdot p^{-1} =_{\mathrm{Id}_A(M,N)} \mathrm{id}$.

So we would have elements giving us

  * $\mathtt{unitr} : Id_{Id_A(M,N)}(pp^{-1}, \mathrm{id}(M))$
  * $\mathtt{unitl} : p^{-1}p = \mathrm{id}(N)$
  * $\mathrm{idr} : p \cdot \mathrm{id}(N) = p$
  * $\mathrm{idl} : \mathrm{id}(M) \cdot p = p$

And associativity

  * $\mathtt{assoc} : (p \cdot q) \cdot r = p \cdot ( q \cdot r)$

All of those paths are definible from $J$, they are implicit on the
structure of types and they are called higher-coherences. If we consider only
paths from $M$ to $M$, we would get a higher-group.

****** Defining groupoid laws from J
The groupoid laws can be proven from J.

***** II. Maps are functors
If $f : A \to B$ and $p: M = M'$, we get $\mathrm{ap}_f(p) : fM = fM'$. 
That is the principle of equality functionality. This is written using
$J$ again. We know that $\mathrm{ap}(\mathtt{refl}(M)) \equiv \mathtt{refl}(f(M))$. $\mathtt{ap}$ preserves
identities in this sense. Does it preserve all the groupoid structure?

We can prove that $\mathtt{ap}$ preserves all the structure, it gives us a principle
of *equality functoriality*; it is functorial.

**** Lecture 11: Functoriality
***** Last week
It is a generalization of the structure given by the equivalence
relation on paths. Path satisfy the groupoid laws with the
concatenation of paths. The data associated to every path can be
thought as a higher dimensional path.

For $f : A \to B$, we have

\[ \mathtt{ap}_f : Id(M,N) \to Id(fM, fN)
\]

and it is a functorial application, respecting the groupoid structure
due to the eta-rules for $J$ as

 * $ap(id) = id$
 * $ap(p^{-1}) = ap(p)^{-1}$
 * $ap(p\cdot q) = ap(p) \cdot ap(q)$

and having also that $ap_{id} = id$ and $ap_{f \circ g} = ap_g \circ ap_f$. Suppose $f : \prod_{x{:}A}B_x$,
we would like to have the path

\[ Id_A(M,N) \to Id_B(fM,fN)
\]

but $fM : [M/x]B$ and $fN:[N/x]B$ are not of the same type! But we could use
the transport property to get $p_{\ast} : [M/x]B \to [N/x]B$. If we apply the transport
property to the inversed path, we get the inverted path $B[N] \to B[M]$. We would
get different proofs of the lemma if we use $p_{\ast}$ or $p_{\ast}^{-1}$.

***** Path-over-path
The lemma we want to do is

\[
M =_A N \longrightarrow f(M) =_p^{x:B_{x}} f(N)
\]

this is a notation that says that $f(M)$ and $f(N)$ are correlated by
$p$ on the fiber $x:B_x$. We will be able to send $p$ to a
path-over-path $q : f(M) =_p^{x:B_x} f(N)$.

#+begin_definition
Given $x : A \vdash B : {\cal U}_x$ and $p : M =_A N$,

\[
\Big(Q =_p^{x:B} R \Big)
:=
\Big(p_{\ast} Q =_{[N/x]B} R\Big)
\]
#+end_definition

It is possible to prove a lot of lemmas about this type. We can
state reflexivity, symmetry properties and so on.
***** Equivalence of types: motivation
Equivalence vs definitional and propositional equality of types as
elements of the universe.

 * In an informal treatment of classical logic, we mix $\iff$ and $=$.
   Nothing can distinghish between them. In classical logic there are
   only two propositions, so it is difficult to distingish.
 * Equality should be the relationship respected by everything inside
   the language.
 * But when we are working on a proof relevance setting, there could
   be many different proofs of two propositions, and two implications
   do not have to be inverses!
 * Isomorphisms of sets work on a similar way. It is not important to
   distinghish between two isomorphic sets given any isomorphism $f,f^{-1}$.

We could write bijections like $\omega = \omega^2$, but there are
contexts where we want to make a difference between the two. Set theory
allows us to ask nonsensical things like $0 \in 1$.

The condition of bijection on types can be translated as a function
$f : A \to B$ with a $g : B \to A$ such that

In ITT, let's suppose a set of functions $N \to N$ and a non-trivial 
bijection to itself.

 * $F(f) = f'$ and $G(f') = f$
 * $F(g) = g'$ and $G(g') = g$

but what do those equalities mean? 

 * $F(f)(x) = f'(x)$ and $G(f)(y) = f(y)$ would be an interpretation.
   Those would be extensionally equal, but different functions.

The notion of bijection is not very useful when working on higher-order
types.
***** Equivalence on types involving a universe
The elements of ${\cal U}$ have structure, each one of them is a groupoid.
The idea of bijection $FG(A) = A$ is not workable; we are interested
in nontrivial isomorphisms, in an equivalence rather than equality
$FG(A) \simeq A$.

This is similar to isomorphisms and equivalence of categories. We want
$FG(A) \cong A$, we do not need $FG(A) = A$. Equivalence is isomorphism up
to isomorphism.

And when we have a universe of universes, the same question repeats
itself. We need another level of comparison now. We would get the
higher-group structure of a type.

Isomorphism here is not a proposition but a structure.
***** Equivalence
Given $f : A \to B$, a quasiinverse of $f$ is given by
$(g,\alpha,\beta)$ such that

 * $g : B \to A$
 * $\alpha : \prod_{a:A} g(f(a)) =_A a$
 * $\beta : \prod_{b:B} f(g(b)) =_B b$

The type of *quasiinverses* is

\[
QI(f)_{A\to B} := \sum_{g : B \to A} 
\left(
\left(\prod_{a:A} g(f(a)) = a\right)
\times
\left(\prod_{b:B} f(g(b)) = b\right)
\right)
\]

****** Another version
In ITT, this is different from

 * $\alpha' : g \circ f = id_A$
 * $\beta' : f \circ g = id_B$

this uses two paths instead of two homotopies.
Function extensionality is now

\[
(f =_{A \to B} g) \simeq
\left( \prod_{a:A}f(a)=_B g(a) \right)
\]

saying that every homotopy defines an equation. Those
two types of proofs are equivalent.

***** Univalence axiom
There is an equivalence between equivalence and equality.

\[
(A \simeq B) \simeq (A = B)
\]

Equivalences are given by

\[
\sum_{f : A \to B} \sum_{g : B \to A} 
\left( f \circ g = id_B \times g \circ f = id_A \right)  \times \text{ some coherence condition }
\]

to avoid the function extensionality issue, we can write
homotopies instead

\[
\sum_{f : A \to B} \sum_{g : B \to A} 
\left( f \circ g \sim id_B \times g \circ f \sim id_A \right) \times \text{ some coherence condition }
\]

but we are going to have function extensionality.
**** Lecture 12: Equivalence of Types
***** Equivalence of types
We write the equivalence of $A \simeq B$. We say that $f : A \to B$ 
is an *equivalence* if there exists

\[
\left( \sum_{g:B\to A} f \circ g \sim id_B \right) \times
\left( \sum_{h:B\to A} h \circ f \sim id_A \right)
\]

the type of equivalences is

\[
A \simeq B := \sum_{f:A\to B} \mathrm{isequiv}(f)
\]

***** Elements of the equivalence
An equivalence is defined whenever those functions
exist

 1. $f : A \to B$
 2. $g:B\to A$
 3. $\alpha: \prod_{y:B} f(g(y)) =_B y$
 4. $h : B\to A$
 5. $\beta : \prod_{x:A} h(f(x)) =_A x$

***** Quasiinverse
Quasiinverses are defined as

\[ \mathrm{quasiinverse}(f) :=
\sum_{g:B\to A} f\circ g \sim id_B \times g \circ f \sim id_{A}
\]

There are three important properties

 1. $qinv(f) \to isequiv(f)$
 2. $isequiv(f) \to qinv(f)$
 3. $isequiv(f)$ expresses an HPROP, it has at most one proof up
    to higher homotopy.

we can transform the data from a quasiinverse to a equivalence.
from  $H : f \sim_{A \to B} g$ we get $\prod_{x:A} Id_B(f x, gx)$. $H$ is functorial in
$x:A$; in the sense that this diagram commutes for any $p : a = a'$

\[\begin{tikzcd}
f(a)\rar[no head]{H(a)} \dar[swap,no head]{ap_f(p)} & g(a) \dar[no head]{ap_g(p)} \\
f(a')\rar[no head]{H(a')} & g(a')
\end{tikzcd}\]

Homotopy is natural (or /polymorphic/) in $x$.

***** Funtion extensionality

 1) definable if $\mathtt{happly}: f =_{A \to B} g \to f \sim g$.
 2) the axiom of funtion extensionality says that the
    above map is an equivalence.

In the presence of function extensionality, we could write $\alpha$
and $\beta$ as $f \circ g \sim id$ and $h \circ f \sim id$. Once you
have function extensionality, you can write the homotopy as
an equality

 * $\prod_{y:B} f(g(y)) = y$
 * by definition of homotopy, $f\circ g \sim id$
 * by function extensionality, $f \circ g = id_{B}$

***** Exercises

 1) $id : A \to A$ is an equivalence, give the four parts of the
    equivalence.
 2) if $f$ is an equivalence, $f^{-1}$, given by the quasiinverse, is
    an equivalence.
 3) if $f$ and $g$ are equivalences, then so is $g \circ f: A \to C$.

***** Structure of paths in types
For example, if we take $Id_{A \times B}(-,-)$, the identity type seems invariant
to the way the type has been constructed; but a structure can be deduced
from the types.

There is a function $f$ with the type $Id_{A \times B}(x,y) \to Id_A(\pi_1x, \pi_2y) \times Id_B( \pi_2x, \pi_2y)$
that we can define as

\[
f := \lambda p. \left\langle \mathrm{ap}_{\pi_1}(p), \mathrm{ap}_{\pi_2}(p) \right\rangle
\]

And we can prove that $f$ is an equivalence

\[
Id_A(x,y) \simeq Id_A(\pi_1x,\pi_1y) \times Id_B(\pi_2x, \pi_2y)
\]

because it suffices to exhibit a quasiinverse for $f$.

 * $g : Id_A(\pi_1x,\pi_1y) \times Id_B(\pi_2x,\pi_2y) \to Id_{A \times B}(x,y)$
 * $\alpha : g(f(p)) =_{Id_{A\times B}(x,y)} p$
 * $\beta : f(g(q)) =_{Id_A(-,-) \times Id_B(-,-)} q$

we use pattern matching

\[
g := \lambda (p,q). ap^2_{pair}\ p\ q
\]

where $pair = \lambda x,y. (x,y)$, and $ap^2_f\ p\ q : Id(f x y, f x' y')$ where
$p: Id(x,x')$ and $q : Id(y,y')$.
***** Products
We need to show the following for the quasi-inverse

 * $\eta :\prod_p (ap^2_{pair} \left(ap_{\pi_1}(p), ap_{\pi_2}(p)) = p\right)$
 * $\beta_1 : \prod_p\prod_q ap_{\pi_1}(ap^2_{pair}\ p\ q) = p$
 * $\beta_2 : \prod_p\prod_q ap_{\pi_2}(ap^2_{pair}\ p\ q) = p$

We need by path induction $x : A \times B \vdash - : ap^2_{pair}\ (ap_{\pi_1}(refl(x))\ ap_{\pi_2}(refl(x)) = refl(x)$

 * $ap_{\pi_1}(refl(x)) \equiv refl(\pi_1(x))$
 * $ap_{\pi_2}(refl(x)) \equiv refl(\pi_2(x))$

Note that the type checking will depend on the computation rules, and
this is antimodular. If you change anything on the code, everything that
relies on it could break.

 * $ap^2_{pair}\ (refl(\pi_1 x))\ (refl(\pi_2 x)) \equiv refl(\pi_1(x),\pi_2(x)) \equiv refl(x)$
***** Nullary case

\[
Id_{1}(x,y) \simeq 1
\]

***** Coproducts
In coproducts, we would like to prove that

 * $Id_{A+B}(inl(a),inl(a')) \simeq Id_A(a,a')$
 * $Id_{A+B}(inr(b),inr(b')) \simeq Id_B(b,b')$
 * $Id_{A+B}(inl(a),inr(b)) \simeq 0$
 * $Id_{A+B}(inr(b),inl(a)) \simeq 0$

If we were to find a map from

 * $x : Id_{A+B}(inl(a),inl(a')) \vdash -:Id_A(a,a')$

using path induction on $p$, we would have to find a motive $C = ?$
and the conclusion should be $C(inl(a),inl(a'), p)$, where $J[C](p,\dots)$
would be the induction on paths. But how do we get rid of the $inl$?
We should define a motive as

\[
D(u,v) = Id_{A}(outl(u), outl(u))
\]

but there are not $outl$ functions! We want the motive to be

\[
F : (A+B) \times (A+B) \to {\cal U}
\]

such that
 
 * $F(inl(a),inl(a')) \equiv Id_A(a,a')$
 * $F(inr(b),inl(b')) \equiv Id_A(b,b')$
 * $F(inl(-),inr(-)) \equiv 0$
 * $F(inr(-),inr(-)) \equiv 0$

Exercise: define such an $F$ by double induction.

The critical lemma is $x : A+B \vdash -:F(x,x)$, which we need to
use path induction.

\[ \mathtt{case}[z.F(z,z)](x; m:A. refl(m) ; n:B. refl(n) ) : F(x,x)
\]

where $[inl(m)/z]F(z,z) \equiv F(inl(n),inl(n))$.
**** Lecture 13: Path structure of Types
***** Last week
We characterized paths in coproducts.

****** Lemma
$x : A +B \vdash -:F(x,x)$

we use induction on $x$

 * $a:A \vdash refl(a) : F(inl(a),inl(a))$
 * $b:B \vdash refl(b) : F(inr(b),inr(b))$

what we want to define a quasiinverse 

\[
f : \prod_{x,x' : A+B} Id_A(x,x') \to F(x,x')
\]

\begin{aligned}
f := \lambda x. \lambda x'. \mathtt{case}(x;\ a:A.\ \mathtt{case}( & \\
& x'; a':A. \lambda p:Id_A(inl(a),inl(a')) . J[F](p; z.F(z,z), \\
& \dots )
\end{aligned}

****** Quasiinverse
Now we have to define

\[
g : \prod_{x,x':A+B} F(x,x') \to Id_{A+B}(x,x')
\]

as

\[
g := \lambda x,x',z:F(x,x')\ \text{cases on }\ x\ x'
\]

and now we want to show

 * $z:F(x,x') \vdash \alpha(z) : f(g(z)) = z$
 * $z:Id_{A+B}(x,x') \vdash \beta(z) : g(f(z)) = z$

***** Positive types
We work with coproducts as examples of positive types. We will
characterize $Id_0(-,-)$ and the path structure of $\mathbb{S}^1$.

\[
Id_{s'}(b,b) = \Omega_b(S') \simeq \mathbb{Z}
\]

We are doing synthetic homotopy theory.

***** Characterizing paths on identity types
Given a type $A$, we consider $Id_{Id_{\dots_A}}(-,-)$. We cannot say much, because
it includes as special cases the spheres $\Omega(S^n)$.

If $f : A \to B$ is an equivalence, then so is $ap_f : a =_A a' \to f(a) = f(a')$.
What we have is $f : A \to B$, so $\alpha : \prod_{a:A}f^{-1}(f(a)) =_A a$ and $\beta : \prod_{b:B}f^{-1}(f(b)) =_B b$.
because it has quasiinverses. To prove this, we define

\[ ap^{-1}_f := \alpha(a)^{-1} \cdot ap_{f^{-1}} \cdot \alpha(a')
\]

now we need

 * $\alpha' : \prod_{p:a =_A a'} ap^{-1}_f(ap_f(p)) =_{a = a'} p$
 * $\beta' : \prod_{q:f(a) =_B f(a')} ap_f(ap_{f}^{-1}(q)) =_{f(a) = f(a')} q$

and both can be proved by path induction.
***** Identity types are homs in an (infinite,1)-category
The type $Id_A(x,y)$ is similar to $Hom_A(x,y)$. $Id_A(-,-)$ is a family
of types, and hence a fibration.

We look at the transport/fibration properties taking
the notation $E(x,y) := Id(x,y)$

  1) fix $x_{0}:A$, consider $\lambda y:A.E(x_0,y)$,

     \[
     tr[y.E(x_0,y)](q) : E(x_0,y) \to E[x_0,y']
     \]

     it maps $p:E(x_0,y) \mapsto p \cdot q$

  2) fix $y_0 : A$, consider $x . E(x,y_0)$,

     \[
     tr[x.E(x,y_0)](p) : E(x,y_0) \to E(x',y_0)
     \]

     mapping $q \mapsto p^{-1}\cdot q$.

  3) for $p : x = x'$, $q : E(x,x)$; $tr[x.E(x,x)](p) : q \mapsto p^{-1} \cdot q \cdot p$.

It can be checked by path induction. This show that they behave like
Hom's and that they exhibit the infinity-groupoid structure.
***** Recall: identity elimination rule
The idea is that, in ITT, this can be thought of as an induction
principle arising from taking the Id to be the least reflexive
relation, because the only introduction rule says so; and the 
elimination works as that.

\begin{prooftree}
\AxiomC{$\Gamma \vdash p:Id_A(M,N)$}
\noLine
\UnaryInfC{$\Gamma,x:A,y:A,z:Id(x,y) \vdash F : {\cal U}$}
\AxiomC{$\Gamma, x:A \vdash q : F(x,x,refl) $}
\BinaryInfC{$\Gamma \vdash J[F](p,x.q) : F(M,N,p)$}
\end{prooftree}

For doing set-level mathematics, this works. In HoTT, we interpret the
identities as paths in $A$, not as inductive types. We conclude things
about non-trivial paths only reasoning about reflexivity!
**** Lecture 14: Identity elimination
***** Last week exercise
If $ - : qinv(f)$, then $-: ap_f$.

 1) $ap_f^{-1} = ap_{f^{-1}}$
 2) $\alpha : \prod_{p : a=_A a'} \dots$
 3) $\beta : \prod_{q : f(a) = (a')} ap_f(\alpha(a)^{-1} \cdot ap_{f^{-1}}(Q) \cdot \alpha(a')) = q$

the (2) is proved by path induction, but (3) is not so easy. If we use
path induction there, we will get $F[f(a),f(a'),q]$ as motive, which should
be then $ap_f(\alpha(a)^{-1} \cdot ap_{f^{-1}}(Q) \cdot \alpha(a')) = q$. Recall that, for coproducts,

 * $F[inl(a),inl(a')] \equiv (a = a')$

but here, we cannot do that, we would need to define it such that
$F[f(a),f(a'),q] =_{{\cal U}} \left(ap_f(\alpha(a)^{-1} \cdot ap_{f^{-1}}(Q) \cdot \alpha(a')) = q\right)$. If $M:A$ and
$p : A =_{{\cal U}} A'$, then $p_{\ast}(M) : A'$. So you use the fact that

 * $f^{-1}(f(a)) =_A a$, and
 * $(f^{-1},\alpha,\beta) : qinv(f)$.
***** Justifying the J operator
The J operator has different meanings in ITT and HoTT

 1. In ITT, J expresses an induction principle on proofs of identity,
    of which there is exactly one.

 2. In HoTT (or ITT + FUNEXT), the situation is less clear,

    \[J[\ ](refl(M), x.Q) \equiv [M/x]Q\]
    
    but then, we have the problem

    \[J[\ ](funext(H); x.Q) \equiv ?
    \]

    where the Gentzen's principle does not hold. How should apply
    $J$ with the Univalence Axiom or to other defined equalities?

    * $J[\ ](UA(E), x.id) \equiv ?$
    * $J[\ ](seg, x.id) \equiv ?$
    * $J[\ ](loop, x.id) \equiv ?$

 3. In ETT, it does not work like this. Equality reflection allows us
    to replace to equalities; we do not need J at all. We get FUNEXT
    without special arrangement. It has a computational interpretation.
    You end using a theory of realizability.

 4. In OTT, we can have FUNEXT without special arrangement. It has a
    computational interpretation.

In HoTT, we give up on computation; but maybe we can recover one. We
justify the theory by interpretation into the classical ZF using
simplicial sets.
***** Solving (partially) the problem
Idea: have $x:A \vdash Q : C[x,x,refl(x)]$ where the motif
$x:A,y:A,z: x=y \vdash C : {\cal U}$. We want to get $- : C[M,N,P]$ 
where $M,M':A,P: M=M'$. This is what the J-rule is saying. 

We know that $[M/x]Q : C[M,M,refl(M)$, and $Q$ depends functorially
on $x$. The logic in HoTT is integrated with the whole structure of
maps. There is a continuous dependency from $Q$ to $A$. So we also
know that $[P/x]Q : [M/x]Q =_p^{C(-,-,refl(-))} [N/x]Q$ (in an abuse of language)
and $ - : p_\ast [M/x]Q =_{C(M',M',refl(M'))} [M'/x]Q$.

Since $refl(M) : M = M and $p : M = M'$, now it suffices to find
$\alpha : refl(M) =^{Id(-,-)}_{(refl(M),p)} p$, wich is to say that

\[
\alpha : \left\langle refl(M),p \right\rangle(refl(M)) = p
\]

the triple $(refl,p,\alpha)$ would take $C(M,M,refl(M))$ into $C(M,M',p)$.

\[ tr[x.Id(x,x)](p)(q) = p^{-1}\cdot q \cdot p\]

it works choosing $refl_{Id_{A}(M,M')}(p)$. $C$ does the work! A priori, $C$ respects
whatever it is that $Id$ internalizes!

***** The identity type
The equality is respected by all the theory, that is why the identity
type has those special properties. In HoTT, $Id$ internalizes homotopy
equivalence, and, by univalence, everything respects homotopy
equivalence. In contrast, in ITT, $Id$ internalizes definitional
equality.

The identity type does not define homotopy equivalence, it only 
internalizes the notion.
***** Homotopy types
The slogan is that Homotopy (Type Theory) is (Homotopy Type) Theory.
# AsÃ­ que debe traducirse por teorÃ­a de tipos homotÃ³picos o
# por teorÃ­a de tipos homotÃ³pica.

#+begin_definition
A type $A$ is a *set*, aka 0-type, iff for all $p,q : x =_{A} y$, we have
that $p=q$.
#+end_definition

\[ \mathrm{isSet}(A) :=
\prod_{x,y:A}\prod_{p,q: x=y} p = q
\]

it is a discrete groupoid up to higher homotopy. The only paths are
the reflexivities, but up to higher-homotopy! There can be other loops,
but they are homotopic to the reflexivity.

You can form a type theory there every type is a set.
**** Lecture 15: Sets and propositions
***** Last week
We saw a justification for the J-rule and the interaction with
the functioriality of $C$ and the inductive analysis of J.

***** The Interval Type
Formation rule

\begin{prooftree}
\RightLabel{$(I-F)$}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash I : {\cal U}$}
\end{prooftree}

Introduction rules

\begin{prooftree}
\RightLabel{(iI0)}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash  0 : I$}
\RightLabel{(iI1)}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash  1 : I$}
\noLine
\BinaryInfC{}
\end{prooftree}

And another introduction rule

\begin{prooftree}
\RightLabel{(iIseg)}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash seg : Id_I(0,1)$}
\end{prooftree}

This is an inductive definition of the type. The type should be
freely generated by these constructors.

\begin{prooftree}
\AxiomC{$\Gamma, x:I \vdash C[x] : {\cal U}$}
\AxiomC{$\Gamma \vdash M_0 : C[0]$}
\noLine
\UnaryInfC{$\Gamma \vdash M_1 : C[1]$}
\AxiomC{$\Gamma \vdash p : M_0 =_{seg}^{x:C} M_1$}
\TrinaryInfC{$\Gamma, x:I \vdash \mathrm{rec}_I[x.C](x;M_0,M_1 )  : C[x]$}
\end{prooftree}

where it is defined as

 * $rec[x.C](0;M_0,M_1, -) \equiv M_0 : C[0]$
 * $rec[x.C](1;M_0,M_1, -) \equiv M_1 : C[1]$
 * $dap(\lambda x . rec[x.C](0;M_0,M_1,p)) \equiv p : M_0 =^{x:C}_{seg} M_1$

the $dap$ map should be functorial

\[ dap : \prod_f \prod_{x:A}B \longrightarrow \prod_{p:Id(x,y)} Id(fx,fy)
\]

***** The total path space of a type
The total path space are the morphisms from the interval

\[
\sum_{x:A}\sum_{y:A} Id(x,y) \simeq (I \to A)
\]

****** Proof
The rec function goes from left to right

$\lambda x,y,p. \lambda t. rec[-:A](t;x,y,p)$

And the description of the path goes the other way

\[ \lambda h. (h(0),h(1), ap(seg)).
\]

***** Conclusion on the total path space
A path between functions is an homotopy, a path between
every pair of points

\begin{align*}
\int Id_{A \to B} &
\simeq I \to (A \to B) \\&
\simeq (I \times A) \to B \\&
\simeq (A \times I) \to B \\&
\simeq A \to (I \to B) \\&
\simeq A \to \int Id_{B}
\end{align*}

Function extensionality says that

\[
Id(f,g) \simeq \prod_{x:A}Id_{A \to B}(fx,gx)
\]

which can be proved by definition and taking the right-to-left
direction as an axiom.
***** Sets
A type is a set if it is homotopically discrete.

\[
\mathrm{isSet}(A) \equiv \prod_{x,y:A}\prod_{p,q : x=y} p=q
\]

up to higher homotopy, the only equality is reflexivity.
Pure ITT is a theory of sets. All of the constructs of ITT
preserve the property of being a set

 1) $1$ is a set.
 2) if $A,B$ are sets, $A \times B$ is a set.
 3) if $A,B[x:A]$ are sets, $\sum_{x:A} B$ is a set.
 4) if $A,B$ are sets, $A+B$ is a set.
 5) $Nat$ is a set.
 6) if $A,B$ are sets, $A \to B$ is a set.
 7) if $A,B[x:A]$ are sets, $\prod_{x:A} B$ is a set.

In the NPS book, they use the terminology =Sets=.
***** Problems interpreting ITT as a theory of sets
Why are the identities sets? Why is the universe a set?

ETT is also a set theory, and it is easier to work with it. In
ITT, we have to pay the price of higher dimensionality without
using it.

***** Is universe a set?
Are the elements of ${\cal U}$ codes (names of types) or types? We can
introduce the elements of ${\cal U}$ inductively. The codes will form a
set.

NuPRL and HoTT take the elements to be types; but NPS take the
elements to be codes and to form a set, using ITT as a set theory.
**** Lecture 16: ITT
All of the basic constructs of ITT preserve the relation of being a Set.
Up to higher homotopy, there is at most one proof of equality of any two
elements.

${\cal U}$ is rigged to be a set and it is a set of codes; an inductively defined
set.

***** The identity type is a set
$Id_A(x,y)$ is a set if $A$ is a set

****** Proof
Assume that $A$ is a set

\[
H : \prod_{x,y:A}\prod_{p,q:Id_A(x,y)}Id_{Id_A(x,y)}(p,q)
\]

and we want to show that $Id(x,y)$ is a set. Assume $u,v : A$,
$r,s:Id(u,v)$ and $\alpha,\beta : Id(r,s)$. We have to show that

\[
Id_{Id_{Id_A(u,v)}(r,s)}(\alpha,\beta)
\]

We specialize $H' := H(u)(v)(r) : \prod_{q:Id_A(u,v)} Id(r,q)$; and we are going
to exploit the functoriality of $H'$. So

\[ apd_{H'} : \prod_{q,q': Id_A(u,v)} \prod_{\gamma : Id(q,q')} Id(\gamma_{\ast}(H'(q)),H'(q')
\]

being a path-over scenario. Here,

\[
apd_{H'}(r,s,\alpha) : Id(\alpha_{\ast}(H'(r)), H'(s))
\]

and, similarly

\[
apd_{H'}(r,s,\beta) : Id(\beta_{\ast}(H'(r)), H'(s)).
\]

So, we can conclude that I can get an element of the identity
$Id(\alpha_{\ast}(H'(r)), \beta_{\ast}(H'(r)))$. What this is telling us is that, by
post-composition given by transport in the identity, $Id(H'(r)\alpha, H'(r)\beta)$.

We have that $H'(r)\cdot \alpha = H'(r)\cdot \beta$, so I can multiply by the inverses
to get

\[
H'(r)^{-1}H'(r)\alpha = H'(r)^{-1}H'(r)\beta
\]

and then $\alpha = \beta$. We have used here the groupoid structure.
***** ETT
ETT is also a set theory because ITT is a set theory. ETT is easier to
use when working with sets than ITT. HoTT adds the univalence axiom
and Higher Inductive Types. Here, homotopy can be thought as a branch
of logic. 

***** ITT+UA
But ITT+UA is *not* a set theory; not all types are sets! In particular,
${\cal U}$ is a proper groupoid; there are non-trivial paths between elements.

For example, we will show two non-related paths between $1+1=2$ and
$2 = 1+1$. We know that

\[ UA: (A = B) \simeq (A \simeq B)
\]

and we are going to use it to create two different paths.

 * $ud(id)$
 * $ud(not)$

and $id \neq not : 2 \to 2$ by function extensionality; they are two different
equivalences. We also have $refl(tt) : tt =_2 tt$; and by transport if the
two paths were the same up to higher homotopy, $tr('') : ff = tt$, which is 
falsable.

***** H-props
Start with $n \geq -2$. 

#+begin_definition
A type $A$ is an *h-prop* or prop iff

\[ \mathrm{IsProp(A)} :\equiv
\prod_{x,y:A} Id(x,y)
\]
#+end_definition

It is a subsingleton and it has at most one element up to higher
homotopy. The problem with this naming is that this collides with
the idea that propositions are types! It is better to call them
*h-props* instead of *props*.

The truth of these propositions is proof-irrelevant for types that
are called h-props.

****** Example: NuPRL and Markov's principle
In NuPRL, the types were specifications, and proofs where programs.
If we want to look for a zero on a sequence

\[s : \left( \sum_{t:Nat\to Nat}\sum_{i:Nat} t(i) = 0 \right)
\to 
\left(\sum_{i : [0..|s|-1]} s(i) = 0\right)
\]

but here there is a solution in constant time! The $i$ is part of the
specification, we provided too much information on the input. The
problem here is proof-relevance. How could we suppress this information
in a type?

The first idea is an observation by *Brower*: we can change the specification
to use double negation

\[s : \left(\neg\neg \sum_{t:Nat\to Nat}\sum_{i:Nat} t(i) = 0 \right)
\to 
\left(\sum_{i : [0..|s|-1]} s(i) = 0\right)
\]

but now, should a while terminate? *Markov's principle* says that, if you
can prove that a machine can't fail to halt, then it must halt. This is a
very contentfull statement in a constructivist setting. This is a very
strong axiom, the characteristic of the Russian school of constructivism.
(Markov, Kolmogorov).

****** Double negation and computational content
In NuPRL, we do not have Markov's principle. We would change $Nat \to Nat$ to
$FinSeq(k)$ in order to have a bound. Double negation kill computational,
proof relevant content.

#+begin_proposition
For any $A$, $\mathrm{IsProp}(\neg \neg A)$.
#+end_proposition

It has a simple proof.

****** GÃ¶del's double negation translation
The idea of GÃ¶del was to embed classical into constructive logic.
Here, classical logic is just a particular case of constructive
logic. This is called /squashing/

 * $\|1\| = 1$
 * $\|A \wedge B\| = \|A\| \wedge \|B\|$
 * $\|0\| = 0$
 * $\|A \vee B\| = \neg\neg(\|A\| \vee \|B\|)$

For implication, we have to choices

 * if we only want to just squash, $\|A \supset B\| = \|A\|\supset \|B\|$, suffices.
 * but if we want to recover classical logic, $\|A\| \supset (\neg\neg \|B\|)$.

Classical logic is constructive + double negation elimination; the
notion of $\neg\neg A \supset A$.

 * with the first option, $\|\neg\neg A \supset A\| = \neg\neg A \supset A$.
 * with the second one, $\|\neg\neg A \supset A\| = \neg\neg A \supset \neg\neg A$, which is true!

This is called the CTS transform for compilers. Where $\neg A$ is interpreted
as a countinuation for compilers. This is the type of a continuation

\[
(\|A\| \times (\|B\| \to 0)) \to 0
\]

With this technique, classical logic can be recovered from the constructivistic
logic.

***** Propositional truncation
We will abstract the idea of squasing into truncation; the idea is
to quotient by the full relation. You take a type and a relation where
you quotient by all the relationships. The notion of subsingleton is
also useful to do proof-irrelevance.

The idea of a subquotient does not work well with set theories. In
HoTT we will use the idea of a quotient.

***** Hedberg's theorem
A type with decidable equality is a set.

If $\prod_{x,y:A} Id(x,y) \vee \neg Id(x,y)$, then $isSet(A)$.

****** Corollary
Classical logic destroy the higher-homotopy structure! If you
postulate excluded-middle, everything is now gone.

****** Proof
 1) Decidable equality implies stable equality.

    \[
   \neg \neg Id(x,y) \to Id(x,y)
   \]

 2) Stability implies sethood.
**** Lecture 17: Hedbergs theorem, truncation
***** Last week
A type is a set if any two proofs of equality are equal. In other
words, if the equality is a proposition. A proposition is a type such
that any two elements of it are equal.

***** The negations are propositions
The negation of any type is a proposition.

****** Proof
If $x,y : A \to \bot$, then $x = y$, as we have function extensionality;
and given any $a : A$, we could use ex falso quodlibet, $\mathtt{abort}(xa) : xa = ya$.
# Check this line on agda with hott

***** Hedberg's theorem
A type with decidable equality is a set. Decidable equality
can be written as

\[
\prod_{x,y : A} Id(x,y) \vee \neg Id(x,y)
\]

and a type is a set if

\[
\prod_{x,y: A} \prod_{p,q : x=y} p = q.
\]

****** Proof 1: Decidable equality implies Stable equality
Stable equality, by definition, is

\[
\prod_{x,y : A} \neg \neg Id(x,y) \to Id(x,y).
\]

In general, what we know is that if $A \vee \neg A$, then $\neg\neg A \to A$. This
is only an instance of that.

****** Proof 2: Stable equality implies Sethood
Suppose that the equality on $A$ is stable, $h :\prod_{x,y: A} \neg \neg (x = y) \to (x = y)$.
It suffices to show that every $p : x = x$ is $p = \mathtt{refl}$. We can apply the
path to get, by transport

\[
p_{\ast}(h(x)(x)) =_{\neg\neg x = x \to x = x} h(x)(x)
\]

so we know that, for any $r: \neg\neg (x = x)$, we know that

\[
p_{\ast}(h(x)(x)) (r) =_{x = x} h(x)(x)(r).
\]

And Lemma 2.9.6 from HoTT book is a technical result, saying that

\[
h(x)(x)(r)(p) = h(x)(x)(p_\ast r)= p_{\ast}(h(x)(x)) (r) =_{x = x} h(x)(x)(r) = h(x)(x)(r),
\]

where we use that negated types are propositions.

****** Example: N is a set
By double induction, we can show the decidability of equality on this
type.
***** Every proposition is a set
In general, we will get that any n-type is a n+1-type. If
$\prod_{x,y : A} x = y$, then $\prod_{x,y:A}\prod_{p,q:x=y} p = q$. Suppose a function given
with $f : \prod_{x,y:A} x = y$; then we can fix $x_0:A$ and let $g(y) \equiv f(x_0)(y)$.

By functioriality, if we have $p : y = y'$, then $apd(p) : p_{\ast}(g(y)) = g(y')$.
And if $q : y = y'$ and $q = g(y^{-1})g(y')$, so $p = q$.

***** The statement of anything being a proposition or a set is a proposition

 * $isProp(isProp(A))$
 * $isProp(isSet(A))$

In the book, there is a chapter on when is a proposition an equivalence of
two types. $isProp(isEquiv(A)(B))$? In the case of the definition by quasiinverses,
it is not a proposition. A function can has many quasiinverses.

****** First proof
Given $f,g : isProp(A)$, we will show that they are equal. It suffices to show that
$x,y:A \vdash - : f(x)(y) =_{x=y} g(x)(y)$. Since $isProp(A)$ implies $isSet(A)$, the desired
equation holds.

****** Second proof
We can use a similar argument.
***** Propositional truncation, aka "squashing"
When we worked in the Godel double negation translation,

\[
\|A \to B\| = \|A\| \to \neg\neg \|B\|
\]

A more abstract notion of truncation is do the squashing and not to
worry about recovering classical logic. What you do is to introduce
the type $\|A\|_{-1}$ of a truncation of $A$. It has the introduction form

\begin{prooftree}
\RightLabel{($\|\cdot\|$-I)}
\AxiomC{$M:A$}
\UnaryInfC{$|M| : \|A\|$}
\end{prooftree}

and the rule that any two elements are going to be the same up to higher
homotopies

\begin{prooftree}
\AxiomC{$M:A$}
\AxiomC{$N:A$}
\BinaryInfC{$- : Id_{\|A\|}(|M|,|N|)$}
\end{prooftree}

This is the quotient of $A$ by the full relation. The elimination form has
to be, then

\begin{prooftree}
\RightLabel{$\|\cdot\|$ - E}
\AxiomC{$M : \|A\|$}
\AxiomC{$x : A \vdash N : B$}
\AxiomC{$p : isProp(B)$}
\TrinaryInfC{$\mathtt{trunc}(M, x.N, p):B$}
\end{prooftree}

the requirement of $B$ to be a proposition, ensures that N's behaviour is
independent of the choice of representative of the suplied equivalence
class. We could relax the condition to a weaker one requiring only this.
***** Contractibility
A type is contractible if it has an element and every other element is
equal to it

\[
isContr(A) = \sum_{x:A}\prod_{y:A} x=y
\]

****** Lemma

\[
isProp(A) \iff
\prod_{x,y:A} isContr(x=y)
\]

***** n-types
Something is a -2type iff it is contractible. And something is an
n+1-type iff for all $x,y$, $x=y$ is a n-type.

 * A proposition is a -1 type
 * A set is a 0 type
 * A groupoid is a type
 * A 2-groupoid is a 2-type
 * and so on

Any n-type is also an n+1-type. It is a *cumulative hierarchy*.

***** Not any type is an n-type
Not any type is an n-type for some n! There are types with a higher structure
up to infinity.

**** Lecture 18: Homotopy n-types, contractability
***** Last week
We defined contractability using centers of contraction.
It expresses the idea of unique existence $\exists!$. It is sometimes
written as $\Sigma!$.

Something is contractible if it is a proposition and it has
one element.

***** Fact of contractability
If you fix any point $a : A$; you can consider the neighborhood of $A$
and we can consider the star of $A$ and that is contractible.
We can prove that

\[
isContr\left(\sum_{x:A} a = x\right)
\]

given $a : A$.
***** The special case of the propositional truncation
We are going to call propositional truncation as -1-truncation.
We can write it as $\|A\|_{-1}$. It will be useful to define other
truncations. The idea is to have

\[
isProp(\|A\|)
\]

for any $A : {\cal U}$. That is to say that the equality type of this type
is contractible as

\[
\prod_{x,y:\|A\|} isContr(x = y).
\]

The intuition is that

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : A$}
\UnaryInfC{$\Gamma \vdash |M| : \|A\| $}
\end{prooftree}

and in the elimination rule, we should prevent proofs for depending on
the witness of the inhabitation.

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma, x : \|A\|, y: \|A\| \vdash \mathtt{squash}(x,y) : x = y$}
\end{prooftree}

The eliminator was defined as

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : \|A\|$}
\AxiomC{$\Gamma, x : A\vdash N : B$}
\AxiomC{$\Gamma \vdash p : isProp(B)$}
\TrinaryInfC{$\Gamma \vdash elim[B](M,x.N,p) : B$}
\end{prooftree}

by using that $B$ was a proposition, we were sure that the result did not
depend on the representative of $A$.

***** Gentzen and squash
Squash is another case of a new primitive equality. By Gentzen's principle,
we would need a beta and an eta rule

 * $elim[B](|M|; x.N, p) \equiv [M/x]N : B$

we would like to have a rule such as

 * $ap(\lambda z. elim[B](z;x.N,p))(squash(|M|,|N|)) \equiv (|M| =|N|)$

this is problematic. (?) If you are using a $J$ and the argument is a
squash, what should that be definitionally equal to?
***** Revisit the axiom of choice
The Axiom of Choice $AC_{\infty}$ has a formulation as

\[
\prod_{A : {\cal U}}
\prod_{B : A \to {\cal U}}
\prod_{C : \prod_{x:A} B \to {\cal U}}
\left(
\prod_{x:A}\sum_{y:B_x} C(x,y)
\overset{\simeq}\longrightarrow
\sum_{f : \prod_{x:A} B} \prod_{x:A} C(x,f(x))
\right)
\]

and we can check in fact that this is a definable *equivalence*. It is
not an axiom! it is a theorem. The theorem of choice. We use crucially
the proof relevance to prove it.

****** Proof
From left to right

\[
\lambda F. \left(
 \lambda x. \mathtt{fst}(F x),
 \lambda x. \mathtt{snd}(F x)
\right)
\]

and from right to left

\[
\lambda \left\langle f,g\right\rangle . \lambda x .(f x, gx)
\]

and those are mutually inverses. We will need eta rules for products and
eta rules for sums.
****** It is a theorem
It is not saying exactly what the axixom of choice says usually.
***** Versions of the axiom of choice
If we use propositional truncations we get the actual axiom of choice,
that we will call $AC_{-1}$.

\begin{aligned}
AC_{-1} : 
  \prod_{A :{\cal U}} isSet(A) \to 
  \prod_{B : A \to {\cal U}} \prod_{x : A} isSet(B(x)) \to
  \prod_{C : \prod_{x:A} B \to {\cal U}} \prod_{x:A} \prod_{y:B} isProp(C(x,y)) \to \\
  \left(
    \left( \prod_{x:A} \| \sum_{y:B} C(x,y) \| \right) \to
    \left\| \sum_{f : \prod_{x:A} B} \prod_{x:A} C(x,f(x)) \right\|
  \right)
\end{aligned}

And this is *not* a theorem. We are using truncation and this is expressing the idea
that there is no functional dependency but we can, nevertheless, build a function.

****** NuPRL
Using squashing, we can express this also on the NuPRL system.
****** Expressivity
The constructive setting is more expressive than the classical one. We can choose,
by introducing identifications, to work on the classical setting.
****** Restatement
If we use the equivalence from before, we can write that

\[
\left\| \sum_{f : \prod_{x:A} B} \prod_{x:A} C(x,f(x)) \right\| \simeq
\left\| \prod_{x:A}\sum_{y:B} C(x,y)  \right\|
\]

and that means that we can restate the axiom of choice using

\[
    \left( \prod_{x:A} \left\| \sum_{y:B} C(x,y) \right\| \right) \to
    \left\| \prod_{x : A} \sum_{y:B} C(x,y) \right\|
\]

instead. And we can reduce this to a simpler form as

\[
\prod_{x:X}\|Y(x)\| \to
\left\| \prod_{x:A} Y(x)  \right\|
\]

which can be read as "the product of a family of inhabited sets
is an inhabited set". This is also an equivalence.

This version of the axiom of choice is false if $X$ is not constrained
to be a set.
***** Quasiinverses
Recall that a quasiinverse was

\[ \mathtt{qinv}( f : A \to B) :\equiv
  \sum_{g : B\to A} f \circ g \sim id \times g \circ f \sim id
\]

in the presence of function extensionality, we can replace $\sim$ with
$=$. We will show that $qinv$ is not necessarily a -1-type (a proposition).

****** Characterization of quasiinverses
If $f : A \to B$ and $e : qinv(f)$ then 

\[
qinv(f) \simeq \prod_{x:A} x = x
\]

sometimes we write $x =_{A} x \equiv \Omega(A,x)$. This uses univalence.

****** Existence of a not-proposition
There is a type $X$ such that

\[
\prod_{x : X} x=_Xx
\]

is not a proposition. An example of this is $X = \pi(\mathbb{S}) \simeq \mathbb{Z}$, which
will be a set. Another one is $X = K(G,1)$, a space with its fundamental
group being $G$.

As a corollary, $qinv(f)$ need not be a proposition.

***** A good definition of equivalence
The criterion is that it should be a proposition, so the quasiinverses
definition does not qualify. We create new definitions

 * $isContr(f)$
 * $isBiequiv(f)$, we have a left inverse and a right inverse.
 * $isHalfAdjoint(f)$, defined by quasiinverses plus a coherence condition.

all these definitions are equivalent. And these are all propositions.

***** Contractability definition of equivalence
For $f : A \to B$,

\[
isContr(f) :\equiv
\prod_{y:B} isContr(fibers_f(y))
\]

where

\[
fibers_f(y) \equiv \sum_{x:A} f(x) = y
\]

the things that are sent by $f$ to $y$. The total "sum of fibers" is
the total $A$.

****** Voevodsky's definition of equivalence
The definition can be stated as that exists a unique

\[
\prod_{y:B}\sum_{z: fib_f(y)} \prod_{z' : fib_f(y)} z = z'
\]

so the function is a bijection up to homotopy.
**** Lecture 19: Inductive types I
***** Last week
We suppose that we had a quasiinverse for $f$ given by an inverse,
and two proofs of the inverse, $\left\langle g_0,\alpha_0,\beta_0 \right\rangle$. The claim was that

\[
qinv(f) \simeq \prod_{x:A} x = x
\]

in the presence of FUNEXT, this is equivalent to the fact that
$id = id$. The type of quasiinverses is

\[
\sum_{g : B \to A}\sum_{\alpha : g \circ f = id} \sum_{\beta : f \circ g = id} 1
\]

and a similar type is contractible

\[
\sum_{g : B \to A} \sum_{\beta : f \circ g = id} 1
\]

with center $(g_0,\beta_0)$. This says that $f$ has exactly one right inverse.
If we consider $(g_1,b_1) : \sum_{g : B \to A}\sum_{\beta : f \circ g = id} 1$, we have to show that
$p : g_1 = g_0$ and then, by transport $- : \beta_0 = \beta_1$.
***** Biinverse
If we use the definition of biinverses

\[
binv(f) :\equiv
\left(\sum_{r : B \to A} f \circ r = id\right) \times
\left( \sum_{l:B \to A} l \circ f = id \right)
\]

as both factors are contractible, the type is contractible. This
definition is related to the half-adjoint definition. $f$ is
bijective up to homotopy if this holds.

It is reasonable to speak of $biinv(f)$ true as it were a proposition.
***** Inductive types (the nat case)
Reconsider Nat in simple types. We had introductory rules

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash 0 : Nat$}
\AxiomC{$\Gamma \vdash M : Nat$}
\UnaryInfC{$\Gamma \vdash succ(M) : Nat$}
\noLine
\BinaryInfC{}
\end{prooftree}

elimination rules

\begin{prooftree}
\AxiomC{$\Gamma \vdash M : Nat$}
\AxiomC{$\Gamma \vdash M_{0} : A$}
\AxiomC{$\Gamma, x:A \vdash M_1 : A$}
\TrinaryInfC{$\Gamma \vdash rec[A](M,M_0,x.M_1) : A$}
\end{prooftree}

with beta-like rules (inversion principle), which can be
expressed as a commutative diagram

 * $rec[A](0,M_0; x.M_1) \equiv M_0$
 * $rec[A](succ(M);M_0,x.M_1) \equiv [rec[A](M,M_0,x.M_1)/x]M_0 : A$
 
and an eta-like rule (unicity principle), which is the
unicity of the diagram

\begin{prooftree}
\AxiomC{$[0/x]N \equiv M_{0}$}
\AxiomC{$z : Nat \vdash [succ(z)/x]N \equiv [[z/x]N/x]M_{1}:A$}
\BinaryInfC{$\Gamma, x:Nat \vdash N \equiv rec[A](x,M_0,x.M_1)$}
\end{prooftree}
***** Local definition
We could write the introductions as

 * $\vdash 0 : Nat$
 * $x : Nat \vdash succ(x) : Nat$

and the global version works on a suitable theory. We could write
them even as

 * $0 : 1 \to Nat$
 * $succ : Nat \to Nat$

and in a suitable theory, we can derive the rules from the constants.
Note that those two notations are NOT the same thing!

We could write also the induction as an element of a function type

\begin{prooftree}
\AxiomC{$\Gamma \vdash M_{0} : A$}
\AxiomC{$\Gamma, x:A \vdash M_1 : A$}
\BinaryInfC{$\Gamma, z:Nat \vdash rec[A](M_0, x.M_1)(z) : A$}
\end{prooftree}
***** Nat-algebras
We can write this as a single function

\[
z : 1 + Nat \vdash case\{-.0 ; x.succ(x)\}(z) : Nat
\]

we can write this as

\[
z : 1 + Nat \vdash \{0,succ\}(z) : Nat
\]

This notation uses eta/beta properties of coproducts and
products to get its etea/beta properties. Any mapping
$\alpha : 1 + Nat \to Nat$ is called a *Nat-algebra*. More generally,
a Nat-algebra is $\alpha : 1 + A \to A$. 

A *Nat-algebra category* is defined by the idea that if we have
two Nat-algebras, we can define a mapping between them as

\[\begin{tikzcd}
1+A\dar{\alpha}\rar{1+h} & 1+B\dar{\beta} \\
A\rar{h} & B
\end{tikzcd}\]

and we call this a Nat-homomorphism. The previous definition of the
naturals is in fact the initial object in the category on
nat-homomorphisms. This is an initial algebra, there is a unique
morphism from this algebra to all the others; and this morphism is
precisely the recursor. Any other morphism is unique up to higher
homotopy to the other morphism (eta-rule).

There are implicit uses of the Yoneda Lemma here.
***** F-algebra
Given a functor $F$ on some category, we are going to define a
F-algebra. The particular case of Nat is $F(X) = 1 + X$.

\[\begin{tikzcd}
F(A)\dar{\alpha}\rar{F(h)} & F(B)\dar{\beta} \\
A\rar{h} & B
\end{tikzcd}\]

***** F-coalgebra
We can define coalgebras and study the final objects on coalgebra
categories. The unique function to a final coalgebra is a corecursor.

****** Exercise
What is the final coalgebra for Nat? $F(X) = 1+X$.

***** Lambek's lemma
If $i : F(I) \to I$ is an initial F-algebra, then $i$ is an
isomorphism; and then, $F(I) \cong I$.

This is called a *fixed point*.

****** Proof
If we have an algebra, we also have $FI$ as an algebra

\[\begin{tikzcd}
FI  \rar[dashed]{F!}\dar{i} & FFI \dar{Fi} \\
I  \rar[dashed]{!} & FI
\end{tikzcd}\]

we use that the first one is initial. And we also have

\[\begin{tikzcd}
FI \rar{i}\dar[dashed]{F!}\ar[dd,bend right,swap,"Id"] & 
I \dar[dashed]{!}\ar[dd,bend left,"Id"] \\
FFI  \rar{Fi}\dar{Fi} & FI \dar{i} \\
FI  \rar{i} & FI
\end{tikzcd}\]

so we know that $i \circ ! = id$ and $!\circ i = F(i \circ !) = id$.
****** CoLambek
If we have a final coalgebra it is also an isomorphism.
**** Lecture 20: Inductive types II
***** Last week
We reexamined Nat as an inductive type. We claim that $1+Nat \to Nat$
is initial in the category of Nat-algebras. Any other Nat-algebra can
be written as $\alpha = \left\langle \alpha_0,\alpha_{1} \right\rangle$; and the function from the initial algebra
is simply the recursor $rec[A](\alpha_0,x.\alpha_1)$.

***** Nats Inside type theory
A Nat-algebra would be

\[
NatAlg :\equiv
  \sum_{A : {\cal U}} 1+A \to A \simeq
  \sum_{A : {\cal U}} A \times (A \to A)
\]

and a Nat-homomorphism is

\[
NatHom(\alpha,\beta) :\equiv
  \sum_{h : A \to B} \beta \circ (1+h) = h \circ \alpha
\]

then $\nu :\equiv \left\langle Nat, \{0,succ\} \right\rangle$ is a Nat-algebra an we can prove that this is
in fact initial, which is to say that

\[
NatHom(\nu,\alpha) \text{ is contractible}
\]

Two terminologically different traditions collide here, so we are using
them both.
***** Derivation of mathematical induction
The recursor for the naturals is

\begin{prooftree}
\AxiomC{$M_{0}:A$}
\AxiomC{$x.A \vdash M_1:A$}
\BinaryInfC{$x:Nat \vdash rec[A](M_0;x.M_1) : A$}
\end{prooftree}

with its beta and eta rules. In HoTT, we take the eta rules
to be not definitional equalities $\equiv$ but propositional equalities $=$.

***** The principle of induction
The induction principle says that, given,

 * $x : Nat \vdash P(x) : {\cal U}$
 * $M_0 : P(0)$
 * $x:Nat, y:P(x) \vdash M_1 : P(succ(x))$

we have

 * $z:Nat \vdash ind[x.P](M_0,x.y.M_1)(z) : P(z)$

and the beta and eta rules are similar to those of the recursor.
But, with respect to what equality?
***** Idea
Consider $\int P :\equiv \sum_{x:Nat} P(x)$, we define an auxiliary function

 * $i :\equiv \lambda z:Nat.\  rec[\int P](\left\langle 0,M_0 \right\rangle; \left\langle x,y \right\rangle.\left\langle succ(x), M_1 \right\rangle)(z)$
 * $i0 \equiv \left\langle 0,M_0 \right\rangle$
 * $i(succ(M)) \equiv \left\langle succ(fst(M)), [snd(iM)/y]M_1 \right\rangle$

****** Kleene discovered how to define the predecessor

***** Lambek inside type theory
We can define functors up to higher homotopy. When we apply Lambek, we
get a fixed point $FI \cong I$ up to isomorphism.

****** Not functors
There are cases where, if we define things like $F X =X \to X$, we get a 
*non-algebraic* datatype. There is a way of solving this equations using
fixpoint induction.

****** Nat+
We could define a final coalgebra as

\[
Nat^+ \to 1 + Nat^{+}
\]

and we could think of this loosely as $Nat \cup \{\infty\}$.
 
***** Positive and negative types
Positive types correspond to inductive types and negative types
correspond to coinductive types. Negative types are limits, and
positive types are colimits.
***** Brower ordinals (aka W types)
Well-founded trees or preorders. The ordinals codify what transfinite
induction is.

****** Brower ordinals
Nodes labeled as

 - $0$                   z

 - $1 = sup(0)$          s -> z

 - $2 = sup(1)$          s -> s -> z

 - ...

 - $\omega = sup(0,1\dots)$     w -> z
                         \-> s -> z
                          \-> s -> s -> z
                           \-> ...

****** Well-founded
There is no infinite descendent branches, there may be as many width
branches as we want.

The right definition is that "The principle of transfinite induction is
valid". Is something holds for all predecessors.

***** Formation rule for W-types
Given a type of node sorts, $A$, 

\begin{prooftree}
\AxiomC{$A:{\cal U}$}
\AxiomC{$x:A \vdash B:{\cal U}$}
\BinaryInfC{$W_{x:A}B : {\cal U}$}
\end{prooftree}

given $x:A \vdash B(x)$ is the branching factor; the index type for
the predecessors. 

****** Example
As an example, $A :\equiv 1+1 = [Z,S]$ is an enumeration type, and
you define $B$ by case analysis.

 * $B(Z) :\equiv \bot : {\cal U}$
 * $B(S) :\equiv 1 : {\cal U}$

and $Nat$ will be $W_{x:A}B$.

***** Introduction rule for W-types

\begin{prooftree}
\AxiomC{$a:A$}
\AxiomC{$x:B(a) \vdash w(x) : W_{x:A}$}
\BinaryInfC{$sup[a](x.w) : W_{x:A}B$}
\end{prooftree}

****** Example

 * $0 :\equiv sup[z](x.abort(x))$
 * $1 :\equiv sup[s](-.0)$

***** Elimination rule, non-dependent form

\begin{prooftree}
\AxiomC{$\Gamma,\ a:A,\ p:B(a) \to W_{x:A}B,\ r:B(a) \to C \vdash M:C$}
\UnaryInfC{$z: W_{x:A}B \vdash  wrec[c](  a,p,r.M     )(z)      :C$}
\end{prooftree}

and we are going to have a beta rule

 * $wrec[c]( a,p,r.M )(sup[a](w)) \equiv [a,w,\lambda z. wrec[c](a,p,r.M) /a,p,r] M$

and the eta rule says that it is the only such thing.

****** Polynomial functors
The W types determine polynomial functors over certain classes.

\[
F(x) :\equiv \sum_{a:A}B(a) \to X
\]

in the case of naturals, it is $(1\to X)\times (X\to X)$.

***** Transfinite induction - dependent elimination rule

\begin{prooftree}
\AxiomC{$\Gamma,z: W_{x:A} B \vdash P:{\cal U}$}
\noLine\UnaryInfC{$\Gamma,a:A, p:B(a) \to W_{x:A}B, h:\prod_{b:B(a)}P(p(b)) \vdash M : P(sup[a](p))$}
\UnaryInfC{$\Gamma,z:W_{x:A}B \vdash wind[x.P](  ): P(z)$}
\end{prooftree}

***** Exercise: Invent the M type, dual to the W type
**** Lecture 21: Higher inductive types I
***** Last week: lower inductive types
Well-founded trees $W_{x:A} B$ with an elimination rule based on
transfinite induction. It can be characterized as the homotopy
initial algebras for polynomial functors.

The universal property is here propositional.

***** Higher inductive types
The idea is to take some type of free structure using inductive
definitions with equational laws. It is a similar idea to the
presentation of algebraic structures using generatos and relations.
This will be a 0-type concept; but higher inductive types must be
more general than that.

They are relevant because of

 1. full higher-dimensional structure or path structure.
 2. proof-relevance means generators and relations are the same thing.

Informally, we are building the free infinity groupoid on the structure
we are building.

***** Current status of HoTT
HoTT is ITT + UA + HIT. The Univalence Axiom is a matter of mathematical
efficiency; and the HIT is a matter of expressiveness on higher types.

HITs are not yet fully worked out.
***** Example: interval
We can write the interval using inductive definitions

 * $0 : I$, a 0-cell.
 * $1 : I$, a 0 cell.
 * $seg : Id_I(0,1)$, a 1-cell.

This definition implies the existence of other paths. For example,
$seg^{-1} : Id(1,0)$ or $refl(0) : Id(0,0)$. Moreover, they are significant
and they induce even higher paths.

It is an open problem what is implied for a given inductive
definition.

***** Recursor
An inductive definition induces a recursor; a function from an
initial object on a given category.

In the example, given any "interval algebra", we have a function
from the interval to it

\begin{prooftree}
\AxiomC{$a:A$}
\AxiomC{$b:B$}
\AxiomC{$\beta : a=_Ab$}
\TrinaryInfC{$z:I \vdash rec[A](a,b,\beta)(z):A$}
\end{prooftree}

the Gentzen's inversion principle holds as

 * $rec[A](a,b,\beta)(0) \equiv a : A$
 * $rec[A](a,b,\beta)(1) \equiv b : A$
 * $ap_{rec[A](a,b,\beta)}(seg) = \beta : a =_{A} b$

we could think of the first two cases as an $ap$ on 0-types. Note that
the third case uses a propositional equality; $ap$ is a defined
function! we cannot define what its behaviour should be.
***** Induction
The induction can be written as

\begin{prooftree}
\AxiomC{$z: I \vdash A(z) : {\cal U}$}
\AxiomC{$a_0 : A(0)$}
\noLine\UnaryInfC{$a_1 : A(1)$}
\noLine\UnaryInfC{$p : a_0 =_{seg}^{z.A} a_1$}
\BinaryInfC{$z:I \vdash ind[z.A](a_0,a_1,p) : A(z)$}
\end{prooftree}

where we are using a transportation to have that

\[
trans[z.A](seg)(a_0) =_{A(1)} a_1
\]

And now we expect the following equations to hold

 * $ind[z.A](a_0,a_1,p)(0) \equiv a_0 : A(0)$
 * $ind[z.A](a_0,a_1,p)(1) \equiv a_1 : A(1)$
 * $ap_{ind[z.A](a_0,a_1,p)}(seg) = p$ true.

and there is also a unicity condition
***** Example: circle
The circle $\mathbb{S}^1$ is a type given by the higher inductive definition

 * $base : \mathbb{S}^{1}$, a 0-cell
 * $loop : Id(base,base)$, a 1-cell

This definition induces loops such as $loop \cdot loop : Id(base,base)$ or
$loop^{-1} : Id(base,base)$. Those will form the integers; a set-level
group.

****** Recursor
It is defined as

\begin{prooftree}
\AxiomC{$a_0 : A$}
\AxiomC{$l : a_0 =_A a_0$}
\BinaryInfC{$z : \mathbb{S}^1 \vdash rec[A](a_0,l)(z) : A$}
\end{prooftree}

where

 * $rec[A](a_0,l)(base) \equiv a_0 : A$
 * $ap_{rec[A](a_0,l)}(loop) = l$ true

and we can have the unicity

 * $z : \mathbb{S}^1\vdash P : {\cal U}$
 * $b : P(base)$
 * $l : b =^{z.P}_{loop} b$, so this is preserved going around the loop
***** Recall: the interval
The interval characterizes the total path space of $A$

\[
I \to A \simeq
\int Id_A :\equiv \sum_{x,y:A} Id(x,y)
\]

the identifications are paths. The usual way to do this in Topology is
similar. Here,

\[ \mathbb{S}^{1} \to A \simeq
\int \Omega_A := \sum_{x:A}Id(x,x)
\]

is the loop space.

 1) define $f : (\mathbb{S}^1 \to A) \to \int \Omega_A$ by $\lambda g. \left\langle g(base), ap_g(loop) \right\rangle$.
 2) we show that $\prod_{l : \int \Omega_A} fib(l)$ is contractible

***** Suspension: circle
Another picture of $\mathbb{S}^1$ could use two poles and two
meridians. This is called $Susp(2)$.

 * $N : \mathbb{S}^1$
 * $S :\ \mathbb{S}^1$
 * $mer : 2 \to (N = S)$

and we can check that this is equivalent to the previous
definition.

***** Suspensions
We define $Susp(A)$ as two zero cells

 * $N : Susp(A)$
 * $S : Susp(A)$

and the meridians

 * $mer : A \to (N = S)$

****** Example: susp^2 is the sphere
$Susp^2(2)$ has two meridians associated with N and S and also two
higher-order paths W E associated with the two meridians on
$Susp(2)$.

We will prove that this is the sphere.

***** Suspensions in type theory
We can define the introduction

\begin{prooftree}
\AxiomC{$x:A\vdash m(x) : n =_{B} s$}
\AxiomC{$n:B$}
\noLine\UnaryInfC{$s:B$}
\BinaryInfC{$z:Susp(A) \vdash rec[B](n,s,x.m):B$}
\end{prooftree}

with the beta and eta usual rules.
**** Lecture 22: Higher inductive types II
***** Last week
We defined suspensions

 * $N : Susp(A)$
 * $S : Susp(A)$
 * $merid : \prod_{x:A} N =_{Susp(A)} S$

We defined functorial mappings from the suspension of a type
to another type.

***** Iterated suspension
It can be showed that

$\mathbb{S}^1 \simeq Susp(2)$

by defining an invertible function between them.

***** Pointed types
A point is preserved up to homotopy by mappings

\[
X \multimap Y := \sum_{f:X\to Y} f(x_0) = y_0
\]

***** Characterization of suspensions
Suspensions follow a kind of adjunction with the loop space

\[
(Susp(A) \multimap B) \simeq (A \to \Omega(B))
\]

***** Pushouts
Generally used for amalgamation properties and quotients. In classical
set theory. They are the dual of pullbacks, the constrained subset of
a product.

We glue two sets in a way that a deisgnated subset of the two sets is
regarded as the same; as $A \sqcup^{C} B$, where $C$ is the diagram for the 
considered subset. The coproduct is the special case where $C = \varnothing$.

****** Denotational semantics
In denotational semantics, we want to form the disjoint union of two
types with common elements.

****** In sets
In sets, pushouts do always exist.

***** Pushouts as HIT
We define the inclusions

 * $inl : A \to A \sqcup^C B$
 * $inr : B \to A \sqcup^C B$

and a glue term

 * $glue : \prod_{c:C} inl(f(c)) = inr(g(c))$

****** Universal constructions

\begin{prooftree}
\AxiomC{$x:A \vdash l:D$}
\noLine\UnaryInfC{$y:B \vdash r:D$}
\AxiomC{$u:C \vdash q: [f(u)/x]l = [g(u)/x]r$}
\BinaryInfC{$z: A \sqcup^C B \vdash rec[D](x.l,y.r;u.q) : D$}
\end{prooftree}

with the usual beta/eta rules.

****** Suspension
In particular,

$Susp(A) := 1 \sqcup^{A} 1$
**** Lecture 23: Pushouts
***** Last week
Pushouts as HITs.

***** Quotients as HITs
We can define the expected quotients

 * $a:A \vdash q(a) : A/R$
 * $a,b:A, r:R(a,b) \vdash wd(a,b,r) : q(a) = q(b)$

and a truncation rule

 * $x,y:A/R, p,q:x=y \vdash tr(x,y,p,q):p=q$

so that $A/R$ is a set.

****** Example: Integers as formal differences of naturals

***** Truncations as HITs
The propositional truncation $\|A\|_{-1}$ can be defined as a type
with an induction principle.

****** Induction on integers

***** Fundamental group of S1

\[
\pi_1(\mathbb{S}^1) \simeq \mathbb{Z}
\]

we can show that $\Omega(\mathbb{S}^1,base) \simeq \mathbb{Z}$.

****** Proof
In the proof, we define the winding function from the loop
space to the integers. We then use induction on $\mathbb{Z}$.
**** Exercises
***** Homework 1: Heyting algebra and IPL [5/6]
****** DONE Task 1
#+begin_statement
Show that $A \wedge (B \vee C) \leq (A \wedge B) \vee (A \wedge C)$ in any Heyting algebra.
Hint: use the Yoneda Lemma.
#+end_statement

The Yoneda Lemma in this setting says that the statement is equivalent
to say that, for all $D$, if $(A \wedge B) \vee (A \wedge C) \leq D$, then $A \wedge (B \vee C) \leq D$.
In this case we have

 * $A \wedge B \leq D$
 * $A \wedge C \leq D$

and crucially using the definition of exponential

 * $B \wedge C \leq B,C \leq A \supset D$.

****** DONE Task 2
#+begin_statement
Show that in any Heyting algebra, $A \supset \bot$ is one of the largest elements
inconsistent with $A$, and is equivalent to any largest inconsistent one.
#+end_statement

By definition, $A \wedge (A \supset \bot) \leq \bot$, and for any other element $C$ such that
$A \wedge C \leq \bot$, $C \leq (A \supset \bot)$. Any other largest inconsistent element should
satisfy $(A \supset \bot) \leq C$.

****** DONE Task 3
#+begin_statement
Show that, in any Boolean algebra (complemented distributive lattice),
$\overline{A} \vee B$ is a valid implementation of $A \supset B$. That is, it satisfies all
properties of $A \supset B$.
#+end_statement

We know that

\[
A \wedge (\overline{A}\vee B) \leq 
(A \wedge \overline{A}) \vee (A \wedge B) \leq
(A \wedge B) \leq B
\]

and if $A \wedge C \leq B$, then

\[
C \leq
C \wedge (A \vee \overline{A}) \leq
B \vee (C \wedge \overline{A}) \leq \overline{A} \vee B.
\]
****** TODO Task 4
#+begin_statement
Show that IPL is transitive, which is to say ...
#+end_statement

****** DONE Task 5
#+begin_statement
Show that for any Heyting algebra and any evaluation function on
atoms, if $\Gamma \vdash P$ true then $\Gamma^+\leq P^{\ast}$. You only have to consider the
cases in which the last rule applied is $(\supset I)$ or $(\supset E)$.
#+end_statement

  * In the first case, $(\supset I)$, we know that $\Gamma, A \vdash B$. By induction,
    we know that $\Gamma^{+} \wedge A^{\ast} \leq B^{\ast}$, and then $\Gamma^{ +} \leq (A^{\ast} \supset B^{\ast})$.
  * In the second case, we know by induction that $\Gamma^{ +} \leq A^{\ast} \supset B^{\ast}$ and
    $\Gamma^{+} \leq A^{\ast}$, so $\Gamma^{ +} \leq A^{\ast} \wedge (A^{\ast} \supset B^{\ast}) \leq B^{\ast}$.

****** DONE Task 6
#+begin_statement
Consider the Lindembaum algebra of IPL where the elements are all
propositions in IPL (with the translation $(-)^{\ast}$ being the identity function) 
and the relationship $\leq$ is defined by provability in IPL. That is, $A\leq B$ 
iff $A \text{ true} \vdash B\text{ true}$. Show that this is a Heyting algebra. You only have to
prove the transitivity. You may assume weakening and exchange of IPL,
or cite previous tasks as lemmas.
#+end_statement

If $A \leq B$ and $B \leq C$, we know that, by weakening, $A \text{ true},B \text{ true} \vdash C \text{ true}$.
We now can apply transitivity to $A \text{ true} \vdash B \text{ true}$ and the previous formula
to obtain $A \text{ true} \vdash C \text{ true}$.
***** Homework 2: Kindom of Kittens [0/7]
****** TODO Task 1
#+begin_statement
Weite down a suitable morphism in terms of the primitive constructs and
the morphisms immediately available in each subtask. The primitive
constructs include $\mathrm{id}$, $f \circ g$, $\left\langle f,g \right\rangle$, $\mathtt{fst}$, $\mathtt{snd}$, $\mathtt{inl}$, $\mathtt{inr}$, $\left\{ f,g \right\}$, $\lambda(f)$ and $\mathtt{map}$.

 * *Reflexivity*, write down a morphism from $\Gamma^+ \times P^{\ast}$ to $P^{\ast}$.
 * *Contraction*, write down a morphism from $\Gamma^{ +} \times P^{\ast}$ to $Q^{\ast}$ in terms of
   a morphism $f \colon (\Gamma^{ +}\times P^{\ast})\times P^{\ast} \to Q^{\ast}$.
 * *Weakening*, write down a morphism from $\Gamma^{ +} \times P^{\ast}$ to $Q^{\ast}$ in terms of a
   morphism $f \colon \Gamma^{ +} \to Q^{\ast}$.
 * *Exchange*, write down a morphism from $(\Gamma^{ +}\times Q^{\ast}) \times P^{\ast}$ to $R^{\ast}$ in terms
   of a morphism $f \colon (\Gamma^{ +}\times P^{\ast}) \times Q^{\ast} \to R^{\ast}$.
 * *Substitution*, write down a morphism from $\Gamma^{ +}$ to $Q^{\ast}$ in terms of two 
   morphisms $f \colon \Gamma^{ +}\to P^{\ast}$ and $g\colon \Gamma^{ +}\times P^{\ast} \to Q^{\ast}$.
#+end_statement

****** TODO Task 2
****** TODO Task 3
****** TODO Bonus Task 1
****** TODO Task 4
****** TODO Task 5
****** TODO Task 
**** Bibliography
 * Awodey, Category theory.
 * Programming in Martin-LÃ¶f Type theory.
 * Homotopy Type Theory book.
*** Working group on Univalent Foundations - Michael Shulman

Categories can have an internal language or internal logic.

| Type theory    | Categories                                                            |
|----------------+-----------------------------------------------------------------------|
| Extensional TT | 1-categories (toposes, pretoposes, sheaves, realizability, gluing...) |
| Homotopy TT    | (â,1)-categories, model categories, â-toposes                         |

The first example is the simplicial model of type theory.

**** Type-theoretic fibration category
Two classes of examples

 1. Simplicial sets
 2. Sintactic category of a type theory

It should have

 * a terminal object,
 * fibrations (Kan fibrations, display maps) closed under composition and pullback.

Given a fibration $g \colon A \twoheadrightarrow B$, the pullback functor ${\cal C}/B \to {\cal C}/A$ has a right adjoint
taking fibrations to fibrations. (Pullback preserves acyclic cofibrations)

***** Weak factorization system
Two classes of maps $({\cal L}, {\cal R})$ and every morphism factors
as something in ${\cal L}$ and something in ${\cal R}$. Every square

\[\begin{tikzcd}
\cdot \rar{} \dar[swap]{{\cal L}} & \cdot \dar{{\cal R}} \\
\cdot\rar{} \urar[dashed] & \cdot 
\end{tikzcd}\]

factors as shown.
*** Fields lectures
http://www.fields.utoronto.ca/video-archive//event/2012/2016

**** [May 16] Michael Shulman: synthetic homotopy theory
It shows a proof of the equality between integers and the foundamental
group of the circle.

***** Introduction: type theory
Homotopy type theory is a variation on ML-TT with Higher inductive
types and univalence. In type theory we have

 - types,
 - terms,
 - type constructors, like Pi or Sigma,
 - several universes

From the homotopic point of view, we think of types as spaces or
groupoids. A type family is a homotopy-theoretic fibration: if
we have $B : A \to {\cal U}$, $B(x)$ is the fiber over $x : A$.

***** Identity types
Identity types provide the groupoid structure to a type. Equalities
are paths between two points of a type.

 - Leibniz/Lawvere/Martin-Lof notion. The family of equality types for
   any given type is freely generated by the reflexivity path.

****** Why does this work? How can exist non-trivial paths?
The space of paths with a fixed endpoint is contractible. There are no
paths but reflexivity. Every path can be retracted back to
reflexivity.  You can use the Yoneda lemma here; the hom functor is
determined by the identity map.

****** Homotopy structure
Any type has an internal homotopy structure

***** Observational/definitional/cubicaal approach
The meaning of the identity type is defined recursively on the
structure of the type A. For example, if we have a product type,
two pairs are equal if their components are equal.

We have operations of transport defined in terms of the structure of
the type.

***** n-types & propositions
We regard types as representing propositions, things that we can prove;
and elements of a type are their proofs.

 - A type is a proposition if every two elements of that type are
   propositionally equal.

Note that propositional equalities have nothing to do with these mere
propositions.

****** Propositional truncation
A type such that every map from a type factors uniquely throught its
propositional truncation.
***** Synthetic homotopy theory
Synthetic homotopy theory is the study in HoTT of properties of types
that traditionally belong to homotopy spaces. We view types as spaces
and we try to apply ideas from homotopy theory.

For example, we classically define the homotopy group of a space as a
map from the sphere to a space.

We can define the loop space of a type. If we iterate over this, we get
higher homotopy groups. The zero truncation of these types gives us the
homotopy paths.

****** Traditional vs synthetic
Traditional

 - spaces are sets of points,

Synthetic

 - spaces are fundamental notions,
 - paths are fundamental notions.

Synthetic homotopy theory models other homotopy theories, things like
higher toposes are expected to be models of this theory.

***** Our tools: HITs and univalence
HITs give us a way to construct spaces in type theory.

In classical homotopy theory we have cell-complexes and we use them to
build complicated spaces. A higher inductive is a cell-complex what an
inductive type is a 0-dimensional cell-complex.

The circle can be constructed directly as a HIT.

We can prove that
\[
\Omega(\mathbb{S}^1, \mathrm{base}) = \mathbb{Z},
\]
by defining a map from the integers to the paths, $n \mapsto \mathrm{loop}^n$.

We need the UA in order to create the converse function.
**** [May 19] Thorsten Altenkirch: why does homotopy type theory matter?

 * *Realist* foundation of mathematics. Mathematical objects are real
   things instead of constructed notions. We use sets.

 * *Constructivist* foundation of mathematics. We are not basing this
   in real objects, we are only communicating ideas. Type theory can be
   thought of as an implementation of constructivism.

   * We can reason about propositions.
   * We use types instead of sets.

We can translate propositions as types, and this is not a sintactic
trick but a new way of understanding a proposition. If we use types
instead of sets, we have to introduce types and terms from that type
at the same time. There is no way to reference untyped objects.

\[
\mathtt{Bool} \cong \mathtt{Prop},
\]

if and only if the LEM holds.

***** What has homotopy theory ever done for us?
In Martin-LÃ¶f first version, membership was impredicative. Then he
introduced extensional type theory; and then later he introduced
intensional type theory.

 - Programming in Martin-Lof's type theory, NordstrÃ¶m.

In intensional type theory, two objects are equal only if they are
defined the same way; that is a problem. These two functions

 - $\lambda x.x + 0$,
 - $\lambda x.0+x$, 

for example, are not equal.

***** Setoids
Setoids are a way to solve the problem with intensional type theory.
A setoid is a type with some equivalence relation. From any two
setoids $A,B$, we can define a new setoid $A \to B$ whose functions
must preserve the equivalence relation. That is, we can translate the
usual type constructors into setoid constructors.

***** TODO Setoid interpretation
** Topology                                                                                                 :topology:
*** Basic topology
**** Covering space                                                                                          :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       915fad6c-c2b3-479f-b16d-b24f97abbd13
:DRILL_LAST_INTERVAL: 4.4901
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:08]
:END:
Definition of *covering space*.

***** Definition
A *continuous surjective* $p : E \to B$ such that every $b \in B$ has a
neighborhood $V$ such that $p^{-1}(V)$ is an open partition $\{V_n\}$ of
*slices*, every one of them isomorphic to $V_b$.

**** Topological retraction                                                                                  :drill:
SCHEDULED: <2019-03-17 Sun>
:PROPERTIES:
:ID:       36cc4b72-f8d6-45ab-8377-7aa2dfb4420b
:DRILL_LAST_INTERVAL: 267.9428
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:00]
:END:
Definition of topological *retraction*.

***** Definition
An inclusion $i : A \subseteq X$ with a continuous map $r : X \to A$ 
such that $r \circ i = \mathsf{id}$.

**** Compact space (by open covers)                                                                          :drill:
SCHEDULED: <2018-07-13 Fri>
:PROPERTIES:
:ID:       9944a022-526f-42bc-92b9-16b8b5519958
:DRILL_LAST_INTERVAL: 65.0035
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-09 Wed 15:15]
:END:
Give the classical definition of compact space by open covers.

***** Definition
$X$ is compact if any open cover ${\cal O}$,

\[
\bigcup_{O \in {\cal O}} O = X
\]

has a finite subcover ${\cal O}' \subset {\cal O}$.

**** Are the open and closed intervals homeomorphic?                                                         :drill:
SCHEDULED: <2019-01-19 Sat>
:PROPERTIES:
:ID:       72ebf2a8-8fe0-4179-a952-5ccddf9b7315
:DRILL_LAST_INTERVAL: 251.9582
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:47]
:END:
Prove or disprove: are the open and closed intervals homeomorphic?

***** Answer
No, [0,1] is compact and (0,1) is not. Compactness is preserved
by homeomorphisms.

***** Answer 2
We can take a point on both intervals and see that only one of them
has to be connected.

**** The fundamental group of a topological group
Why do we know that the fundamental group of a topological group is abelian?

***** Answer
By a Eckmann-Hilton argument.

In categorical terms, we can use that the foundamental group functor from
path-connected topological groups to groups, $\pi_1 \colon \mathsf{pcTop} \to \mathsf{Grp}$, respects
products. A group object in $\mathsf{pcTop}$ is sent to a group object in $\mathsf{Grp}$, that
is, an abelian group. ([[https://math.stackexchange.com/a/686923/85067][Math.SE]])

*** Classification of Surfaces - Chen Hui George Tao
**** 1. IntroducciÃ³n
Vamos a demostrar que todas las superficies compactas son homeomorfas
a la esfera, la suma conexa de toros o la suma conexa de planos proyectivos.

**** 2. Superficies
***** Superficies
Una *superficie* es una 2-variedad. Un espacio Hausdorff contable
localmente homeomorfo a $\mathbb{R}^2$.

***** Idea del artÃ­culo
Dado un polÃ­gono, si identificamos las aristas en pares, tendremos una
superficie. Veremos que toda superficie se construye a partir de un
polÃ­gono con las aristas identificadas.

**** 3.1. Triangulaciones. Complejos simpliciales
***** Simplex
Dados $v_0,\dots,v_k$ en posiciÃ³n general, el *simplex* que generan es el
conjunto de combinaciones convexas bajo la topologÃ­a inducida.

***** Complejo simplicial euclÃ­deo
Un *complejo simplicial* es una colecciÃ³n $K$ de sÃ­mplices cumpliendo:

  1. Si $\sigma \in K$, cada cara suya estÃ¡ en $K$.
  2. Si $\sigma,\tau \in K$, $\sigma \cap \tau$ es vacÃ­a o una cara de ambas.
  3. Cada punto tiene un entorno que interseca a sÃ³lo finitos sÃ­mplices.

***** Poliedro
La uniÃ³n de todos los sÃ­mplices de $K$ es un espacio simplicial llamado
su *poliedro*, $|K|$.

***** Homomorfismo simplicial
FunciÃ³n continua entre dos poliedros cuya restricciÃ³n a cada simplex
es afÃ­n. Es *isomorfismo simplicial* cuando es homeomorfismo.

**** 3.2. Triangulaciones
***** TriangulaciÃ³n
Una triangulaciÃ³n es un homeomorfismo entre un espacio topolÃ³gico
y un espacio simplicial euclÃ­deo.

***** Teorema de RadÃ³
Toda superficie es un poliedro de un complejo simplicial 2-dimensional.
Donde ademÃ¡s, cada 1-sÃ­mplex es cara de dos 2-sÃ­mplex.

****** DemostraciÃ³n
La demostraciÃ³n es larga. La idea es recubrir toda la superficie con
discos regulares y usar el Teorema de Schonflies.

**** 4.1. PresentaciÃ³n poligonal. PolÃ­gonos
***** RegiÃ³n poligonal
Compacto $P$ del plano cuya frontera es un 1-sÃ­mplex cumpliendo:

  1. Cada $q$ que no es vÃ©rtice tiene un entorno $U$ tal que $P \cap U$ es
     intersecciÃ³n de $U$ con un plano.
  2. Cada $q$ que es vÃ©rtice tiene un entorno $U$ tal que $P \cap U$ es
     intersecciÃ³n de $U$ con dos planos con fronteras intersecando en $q$.

***** Una regiÃ³n poligonal relacionada a pares es una superficie compacta
Sea $P$ regiÃ³n poligonal. Dada una relaciÃ³n que identifique cada 
arista con exactamente otra por isomorfismo simplicial, el
espacio cociente resultante es una superficie compacta.

****** DemostraciÃ³n
Sea $M = P/\sim$, con proyecciÃ³n $\pi:P \longrightarrow M$. Por compacidad, $\pi(P) = M$
es compacto. Podemos dividir los puntos de $M$ en:

******* Puntos en una cara
Como la proyecciÃ³n es homeomorfismo local en el interior del polÃ­gono,
tenemos que son localmente euclÃ­deos.

******* Puntos en una arista
Claramente, existe un entorno sin vÃ©rtices. Por definiciÃ³n de la
relaciÃ³n, el punto estÃ¡ identificado con exactamente otro y podemos
usar los entornos $V_1,V_2$ que son discos de intersecciones con planos.

Ahora creamos aplicaciones afines $\alpha_1,\alpha_2$ que peguen las dos partes del 
disco en $\mathbb{R}^2$ y las usamos para construir una proyecciÃ³n de $V_1\cup V_2$ a
$\mathbb{R}^2$. Por tener la misma relaciÃ³n de equivalencia que $\pi$, los espacios
cocientes son homeomorfos, y podemos ver que el punto tiene un
entorno euclÃ­deo en este espacio.

******* VÃ©rtices
Repetimos exactamente lo mismo que hemos hecho con la arista pero
sabiendo que cada identificaciÃ³n del vÃ©rtice nos da un Ã¡ngulo que
debemos pegar despuÃ©s en $\mathbb{R}^2$.

**** 4.2. PresentaciÃ³n poligonal. Suma conexa de superficies
***** Suma conexa
Dadas superficies $M_1,M_2$, bolas regulares $B_1,B_2$, y un homeomorfismo
$f : dM_2' \longrightarrow dM_1'$. El espacio que identifica cada punto con su imagen
es la *suma conexa*.

***** Suma conexa de superficies conexas
La suma conexa de superficies conexas es una superficie conexa.

****** DemostraciÃ³n
Debemos ver que es localmente euclÃ­dea y Hausdorff. Tomamos como
proyecciÃ³n:

\[
\pi : M_1' \sqcup M_2' \longrightarrow M_1\# M_2
\]

Y tenemos dos tipos de puntos.

******* Puntos en el interior
Los puntos que no tocan al disco de uniÃ³n tienen a la proyecciÃ³n
localmente homeomorfa en ellos y por eso son localmente euclÃ­deos.

******* Puntos en el borde
Tomamos un entorno de ambos puntos tal que contengan los mismos
puntos identificados del borde. Los proyectamos a $\mathbb{R}^2$ pegando
ambos bordes y nos damos cuenta de que es la misma relaciÃ³n de
equivalencia que darÃ­a $\pi$, luego son espacios homeomorfos y el
punto en ellos, llevado al $0$, es localmente euclÃ­deo.

**** 4.3. PresentaciÃ³n poligonal
***** PresentaciÃ³n poligonal
Una *presentaciÃ³n poligonal* es un conjunto finito con finitas palabras
$W_1,\dots,W_k$, cada una de longitud 3 o mayor.

***** RealizaciÃ³n geomÃ©trica de una presentaciÃ³n poligonal
La *realizaciÃ³n geomÃ©trica* de una presentaciÃ³n poligonal se construye:

  1. Cada palabra $W_i$ da $P_i$, regiÃ³n poligonal de $k$ lados construÃ­da del
     polÃ­gono regular modelo.
  2. Damos una biyecciÃ³n de cada sÃ­mbolo con los lados de $P_i$ en orden.
  3. Unimos disjuntamente los $P_i$ e identificamos aristas con el mismo
     nombre y homeomorfismos afines.

***** PresentaciÃ³n de superficie
PresentaciÃ³n poligonal donde cada sÃ­mbolo ocurre exactamente dos veces.

****** La realizaciÃ³n de una presentaciÃ³n de superficie es superficie compacta
Hemos probado antes que en este caso, obtenÃ­amos una [[*Una regiÃ³n poligonal relacionada a pares es una superficie compacta][superficie compacta]]
en la realizaciÃ³n.

***** Presentaciones topolÃ³gicamente equivalentes
Dos presentaciones son equivalentes si tienen la misma realizaciÃ³n 
geomÃ©trica.

***** Toda superficie compacta tiene una presentaciÃ³n de superficie
Toda superficie compacta tiene una presentaciÃ³n de superficie.

****** DemostraciÃ³n
Dada una superficie $M$, por triangulaciÃ³n es homeomorfa a un complejo
simplicial donde cada arista es cara de dos sÃ­mplices. Dado un complejo
simplicial podemos construir una presentaciÃ³n donde:

  - Cada 2-sÃ­mplex es una palabra de longitud 3.
  - Dos aristas se llaman igual si vienen del mismo sÃ­mplex.

La presentaciÃ³n entonces nos da dos proyecciones desde los polÃ­gonos
hasta la realizaciÃ³n de la presentaciÃ³n y al sÃ­mplex.

  - $\pi_K : P_1\sqcup\dots\sqcup P_n \longrightarrow |K|$
  - $\pi_{\cal P} : P_1\sqcup\dots\sqcup P_n \longrightarrow |{\cal P}|$

******* Ambas proyecciones identifican los mismos puntos
Es claro que identifican las mismas aristas por construcciÃ³n.
Debemos comprobar que identifican los mismos vÃ©rtices. Sea $v$
un vÃ©rtice, que debe estar en una arista que debe estar en dos 
2-sÃ­mplex $\sigma,\sigma'$. Definimos una relaciÃ³n entre 2-sÃ­mplices si
comparten una arista. Para comprobar que los vÃ©rtices se mantienen
por una proyecciÃ³n entre aristas, comprobaremos que hay una sola
clase de equivalencia.

Si hubiera dos clases de equivalencia $\{\sigma_i\},\{\tau_i\}$, podemos tomar una
bola suficientemente pequeÃ±a (por la condiciÃ³n de finitud de los
complejos simpliciales) para que interseque sÃ³lo a sÃ­mplices 
conteniendo a $v$. Esto nos da una bola homeomorfa a $\mathbb{R}^2$, luego
$U \setminus \{v\}$ es conexo. PodrÃ­amos quitar el $v$ en los complejos simpliciales
de ambas clases de equivalencia y serÃ­an disconexas.

***** ExtensiÃ³n de isomorfismo de bordes
Sean $P_1,P_2$ polÃ­gonos convexos con $f : bP_1 \longrightarrow bP_2$ isomorfismo simplicial,
entonces se extiende a un homeomorfismo $F : P_1 \longrightarrow P_2$.

****** DemostraciÃ³n
Cualquier punto en el interior forma uniÃ©ndose con los vÃ©rtices un
complejo simplicial. Los poliedros de ambos son homeomorfos porque
los complejos simpliciales lo son.

***** Las transformaciones elementales dan realizaciones equivalentes
Las transformaciones elementales de las presentaciones dan lugar a
superficies topolÃ³gicamente equivalentes

****** ReflexiÃ³n
Claramente una aplicaciÃ³n afÃ­n de reflexiÃ³n nos da lo buscado.

****** RotaciÃ³n
La rotaciÃ³n es una aplicaciÃ³n afÃ­n que nos da lo buscado.

****** Cortar
Tomamos las dos proyecciones de presentaciÃ³n antes y despuÃ©s de
cortar y comprobamos que identifican los mismos puntos.

****** Doblar
Tomamos las dos proyecciones y aÃ±adimos las aristas que faltan para
comprobar que identifican los mismos puntos.

***** PresentaciÃ³n de la suma conexa
La presentaciÃ³n de la suma conexa es la uniÃ³n de las palabras.

****** DemostraciÃ³n
Dadas $W_1,W_2$, cortamos un disco como $W_1c^{-1}b^{-1}a^{-1}$ y $abcW_2$ e 
identificamos las aristas dadas.

**** 5. Teorema de clasificaciÃ³n
***** Lema: Botella de Klein
***** Lema: Suma de toro y plano proyectivo
***** Teorema de clasificaciÃ³n
Toda superficie compacta conexa es homeomorfa a una de las siguientes:

  - $\mathbb{S}^2$
  - $\mathbb{T}^{\#n}$
  - $\mathbb{RP}^{2\#n}$

****** DemostraciÃ³n
Tomamos transformaciÃ³n desde la presentaciÃ³n hasta llegar a la
presentaciÃ³n de un modelo.

******* Paso 1: Una sola cara
******* Paso 2: Sin pares complementarios adyacentes
******* Paso 3: Todos los pares retorcidos adyacentes
******* Paso 4: Identificamos todos los vÃ©rtices en un punto
******* Paso 5: Comprobamos que los complementarios estÃ¡n entrelazados
******* Paso 6: Llevamos los complementarios juntos
******* Paso 7: Comprobamos que es una presentaciÃ³n modelo
** Locales                                                                                          :topology:locales:
Some [[https://math.stackexchange.com/a/2825138/85067][reference texts on locales]] from SO, quick reviews:

 * Picado, Pultr, Tozzi: Locales
 * Johnstone: The point of pointless topology

and standard textbooks:

 * Johnstone: Stone spaces (1986) -- for a duality-theory oriented introduction to frames/locales.
 * Vickers: Topology via logic (1996) -- for the relations with computer science and logic.
 * Pultr, Picado: Frames and Locales - topology without points (2011) -- for the more classical, topology oriented approach.

*** The point of pointless topology - Johnstone                                                       :paper:locales:
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/johnstone_the_point_of_pointless_topology.pdf
:END:

**** Frames and locales
:PROPERTIES:
:interleave_page_note: 3
:END:

Frames and locales.

***** Frame                                                                                         :drill:locales:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       869b43f9-6d28-4f26-b139-0eb67e9649f8
:DRILL_LAST_INTERVAL: 3.6719
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:20]
:END:
*Frame*

****** Answer
A poset with 

 * arbitrary joins (small coproducts) $\bigvee$,

 * finite meets (finite products) $\wedge$,

 * satisfying the /infinite distributive law/

   \[
   x \wedge \left( \bigvee_i y_i \right)  = \bigvee_i \left( x \wedge y_i \right)
   \]

***** Locales                                                                                       :drill:locales:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       563403ea-8ed2-439b-907f-04f9679da424
:DRILL_LAST_INTERVAL: 3.9336
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:56]
:END:
How the category of locales is constructed.

****** Answer
The category of locales is the opposite category to the category of frames.

\[
\mathsf{Locale} = \mathsf{Frame}^{op}.
\]

** Statistics                                                                                             :statistics:
*** Normal distribution                                                                                       :drill:
SCHEDULED: <2018-07-02 Mon>
:PROPERTIES:
:ID:       9d9e5534-15b0-4c90-a482-97100e0507bb
:DRILL_LAST_INTERVAL: 5.2815
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.572
:DRILL_EASE: 2.52
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:07]
:END:
Probability density function of the normal distribution.

**** Function

\[
f(x \mid \mu,\sigma^2) = \frac{1}{\sqrt{2\sigma^2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]

*** Bayes' theorem                                                                                            :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       b716bff8-b809-4e11-94e3-1ddd6ad697e4
:DRILL_LAST_INTERVAL: 12.6717
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:14]
:END:
Probability of $P(A \mid B)$.

**** Statement

\[
P(A\mid B) = \frac{P(B \mid A)P(A)}{P(B)}
\]

** Algebra                                                                                                   :algebra:
*** Basic algebra
**** Cayley-Hamilton theorem
The characteristic polynomial of an $n \times n$ matrix

\[p(\lambda) = \mathrm{det}(\lambda I_n - A),\]

is zero when evaluated (interpreted as a n-th order polynomial)
in the matrix, $p(A) = 0$.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       69035ffd-8039-4e89-ac2d-f0ae7712d8a1
:DRILL_LAST_INTERVAL: 4.0564
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:52]
:END:
Given a characteristic polynomial, $p(\lambda) = \mathrm{det}(\lambda I_n - A)$,
evaluate $p(A)$ symbolically.

****** Answer
$p(A) = 0$ is the Cayley-Hamilton theorem.

**** Rank-nullity theorem                                                                                    :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       b4ed18f9-2f91-4028-8379-6d79acbe6cc3
:DRILL_LAST_INTERVAL: 4.0
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:09]
:END:
Given $V$ finite dimensional and $T \colon V\to W$ linear, state the
rank-nullity theorem.

***** Statement

\[
\mathrm{dim}(\mathrm{im}(T))+ \mathrm{dim}(\mathrm{ker}(T)) = \mathrm{dim}(V)
\]

**** Exponentiation of versors                                                                               :drill:
SCHEDULED: <2019-02-11 Mon>
:PROPERTIES:
:ID:       8404f823-c268-4627-8229-8432570411eb
:DRILL_LAST_INTERVAL: 272.3165
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-15 Tue 23:14]
:END:
Given a quaternion $q$, how we define and compute $\mathrm{exp}(q)$?

***** Definition
We define the exponentiation using Taylor Series

\[\exp(q) =  1 + q + \frac{q^2}{2!} + \frac{q^3}{3!} + \dots + \frac{q^n}{n!} + \cdots\ .\]

***** Computation
If we write any quaternion as a versor, $q = \cos\theta + v\sin\theta$, we can
use that $v$ is unitary, $v^2 = -1$, to compute the Euler's formula for
quaternions

\[q = \exp(\theta v) = \cos \theta + v \sin \theta,\]

and then

\[q^t = \exp(t \theta v) = \cos (t \theta) + v \sin (t \theta).\]
**** Snake lemma                                                                                             :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       159f12f8-6ca4-4723-8eb7-378a418d38b7
:DRILL_LAST_INTERVAL: 11.9157
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.285
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:14]
:END:
Given a morphism of short exact sequences as 

\[\begin{tikzcd}
& A \rar{f}\dar{a} & B \rar{g}\dar{b} & C \rar\dar{c} & 0  \\
0 \rar & A' \rar{f'} & B' \rar{g'} & C' \\
\end{tikzcd}\]

what does the snake lemma provide?

***** Answer
There exists a morphism $\delta \colon \operatorname{ker} c \to \operatorname{coker} a$ such that the 
following sequence is exact

\[\begin{tikzcd}
0 \rar &
\mathrm{ker}(a) \rar{f} &
\mathrm{ker}(b) \rar{g} &
\mathrm{ker}(c) \arrow[out = 0,in =180,swap]{dll}{\delta} \\&
\mathrm{coker}(a) \rar{f'} &
\mathrm{coker}(b) \rar{g'} &
\mathrm{coker}(c) \rar &
0
\end{tikzcd}\]

Diagramatically,

\[ \begin{tikzcd}
& 0 \dar              & 0 \dar            & 0 \dar           &   \\
0 \rar & ker(a) \rar \dar  & ker(b) \rar \dar    & ker(c) \dar \ar[out=355, in=175,looseness=1, overlay, swap]{dddll}{\delta}       &   \\
& A \rar{f} \dar{a}  & B \rar{g} \dar{b} & C \rar \dar{c}        & 0 \\
0 \rar & A' \rar{f'} \dar & B' \rar{g'} \dar & C' \dar        &  \\
& coker(a) \rar \dar & coker(b) \rar \dar  & coker(c) \rar \dar & 0 \\
& 0                   & 0                 & 0                &
\end{tikzcd} \]

**** TODO Difference between direct sum and direct product in abelian groups                                 :drill:
:PROPERTIES:
:ID:       b3903d4f-4550-414a-8b1f-9a6fdc37048c
:END:
**** Norm inequality                                                                                         :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       5a7545ad-d087-43f1-affa-07101400c452
:DRILL_LAST_INTERVAL: 4.1209
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:59]
:END:
Prove that, for any $N$ norm in a vector space.

\[
\abs{N(x)-N(y)} \leq N(x-y)
\]

***** Answer
Norm is [[https://en.wikipedia.org/wiki/Norm_(mathematics)][subadditive]] (triangle inequality). It is trivial from the properties.

*** Algebra: chapter 0 - Aluffi
**** III. Anillos y mÃ³dulos
***** 7. Complejos y homologÃ­a
****** 7.1. Complejos y secuencias exactas.
 #+begin_definition
 *Complejo*. Un complejo es una serie de morfismos $d_i$ entre R-MÃ³dulos:

 \[\dots \longrightarrow M_{i+1} \longrightarrow M_i \longrightarrow M_{i-1} \longrightarrow \dots\]

 tales que $d_i \circ d_{i+1} = 0$.
 #+end_definition

 AdemÃ¡s lo llamamos *exacto* cuando $im (d_{i+1}) = ker (d_i)$.

 #+begin_proposition
 *Exactitud de monomorfismos y epimorfismos*. Dos complejos de la forma:

 \[ \dots \longrightarrow 0 \longrightarrow L \overset{\alpha}\longrightarrow M \longrightarrow \dots \]
 \[ \dots \longrightarrow M \overset{\beta} \longrightarrow N \longrightarrow 0 \longrightarrow \dots \]

 Son exactos en $L$ y $N$ ssi $\alpha$ y $\beta$ son monomorfismo y epimorfismo, 
 respectivamente.
 #+end_proposition

 #+begin_definition
 *Secuencia exacta corta*. Una secuencia exacta corta es un complejo de la forma:

 \[ 0 \longrightarrow L \overset{\alpha}\longrightarrow M \overset{\beta}\longrightarrow N \longrightarrow 0 \]
 #+end_definition

 El primer teorema de isomorfÃ­a nos dice que $N \cong \frac{M}{ker(\beta)} = \frac{M}{im(\alpha)}$ lo que nos 
 lleva a identificar   $N \cong \frac{M}{L}$. De hecho, cada monomorfismo da lugar a una 
 secuencia exacta corta:

 \[ 0 \longrightarrow \ker(\phi) \longrightarrow M \longrightarrow im(\phi) \longrightarrow 0 \]

****** 7.2. Secuencias exactas escindidas
 #+begin_definition
 *Secuencia escindida*. Una secuencia exacta corta:

 \[ 0 \longrightarrow M_1 \longrightarrow N \longrightarrow M_2 \longrightarrow 0 \]

 es escindida si es isomorfa a una secuencia de la forma siguiente:

 \[ \begin{tikzcd}
 0   \arrow{r}{} & 
 M_1 \arrow{d}{\sim}\arrow{r}{} & 
 N   \arrow{d}{\sim}\arrow{r}{} & 
 M_2 \arrow{d}{\sim}\arrow{r}{} & 
 0 \\
 0   \arrow{r}{} & 
 M_1 \arrow{r}{} & 
 M_1 \oplus M_2   \arrow{r}{} & 
 M_2 \arrow{r}{} & 
 0
 \end{tikzcd} \]

 Es decir, hay un isomorfismo entre secuencias.
 #+end_definition

 #+begin_theorem
 *RelaciÃ³n entre secuencias escindidas e inversas*. Sea $\phi$ un homomorfismo;
 entonces tiene inversa izquierda ssi la secuencia siguiente escinde:

 \[ 0 \longrightarrow M \overset{\phi}\longrightarrow N \longrightarrow coker(\phi) \longrightarrow 0 \]

 Y tiene inversa derecha si la secuencia siguiente escinde:

 \[ 0 \longrightarrow ker(\phi) \longrightarrow M \overset{\phi}\longrightarrow N \longrightarrow 0 \]
 #+end_theorem

****** 7.3. HomologÃ­a, y el lema de la serpiente
 #+begin_definition
 *HomologÃ­a*. La i-Ã©sima homologÃ­a de un complejo,

 \[ \dots \longrightarrow M_{i+1} \overset{d_{i+1}}\longrightarrow M_i \overset{d_i}\longrightarrow M_{i-1} \longrightarrow \dots \]

 es el R-mÃ³dulo:

 \[H_i(M) = \frac{ker(d_i)}{im(d_{i+1})}\]
 #+end_definition

 La homologÃ­a mide lo que se aleja de ser exacto en un punto determinado, y
 es $0$ cuando el complejo es exacto. Puede verse como una generalizaciÃ³n de
 kernel y cokernel; que los realiza en este caso extremo:

 \[ 0 \longrightarrow M_1 \overset{\phi}\longrightarrow M_0 \longrightarrow 0 \]

 En el que $H_1(M) \cong ker(\phi)$ y $H_0(M) \cong coker(\phi)$.

 #+begin_theorem
 *Lema de la serpiente*. Teniendo dos secuencias exactas en el diagrama 
 conmutativo siguiente:

 \[ \begin{tikzcd}
 0 \rar & L_1 \rar{\alpha_1}\arrow{d}{\lambda} & M_1 \rar{\beta_1}\arrow{d}{\mu} & N_1 \rar\arrow{d}{\eta} & 0 \\
 0 \rar & L_0 \rar{\alpha_0}                   & M_0 \rar{\beta_0}               & N_0 \rar                & 0
 \end{tikzcd} \]

 Existe una secuencia exacta de la forma:

 \[ 0 \overset{}\longrightarrow 
 ker(\lambda) \overset{}\longrightarrow 
 ker(\mu) \overset{}\longrightarrow 
 ker(\eta) \overset{\delta}\longrightarrow 
 coker(\lambda) \overset{}\longrightarrow 
 coker(\mu) \overset{}\longrightarrow 
 coker(\eta) \overset{}\longrightarrow 
 0\]
 #+end_theorem

 El diagrama desde el que se deduce todo esto, con columnas exactas, es
 el siguiente:

 \[ \begin{tikzcd}
	& 0 \dar              & 0 \dar            & 0 \dar           &   \\
 0 \rar & ker(\lambda) \rar \dar  & ker(\mu) \rar \dar    & ker(\eta) \dar \ar[out=355, in=175,looseness=1, overlay, swap]{dddll}{\delta}       &   \\
 0 \rar & L_1 \rar{\alpha_1} \dar{\lambda}  & M_1 \rar{\beta_1} \dar{\mu} & N_1 \rar \dar{\eta}        & 0 \\
 0 \rar & L_0 \rar{\alpha_0} \dar & M_0 \rar{\beta_0} \dar & N_0 \rar \dar        & 0 \\
	& coker(\lambda) \rar \dar & coker(\mu) \rar \dar  & coker(\eta) \rar \dar & 0 \\
	& 0                   & 0                 & 0                &
 \end{tikzcd} \]

**** IV. Ãlgebra lineal
***** 4. Presentaciones y resoluciones
****** 4.1. TorsiÃ³n
 #+begin_definition
 *TorsiÃ³n*. Un elemento $m \in M$ mÃ³dulo de $R$ es de *torsiÃ³n* si $\{m\}$ es linealmente
 dependiente. Es decir,

   \[ \exists r \in R,\ r \neq 0\ :\ rm = 0 \]

 El conjunto de elementos de torsiÃ³n se llama $Tor(M)$. Un mÃ³dulo es *libre de torsiÃ³n*
 si $Tor(M) = 0$ y *de torsiÃ³n* si $Tor(M)=M$.
 #+end_definition

 Un anillo conmutativo es libre de torsiÃ³n sobre sÃ­ mismo si y sÃ³lo si es dominio de
 integridad. Cuando esto ocurre, $Tor(M)$ es siempre submÃ³dulo de $M$. SubmÃ³dulos o
 sumas de mÃ³dulos libres de tensiÃ³n serÃ¡n libres de torsiÃ³n, y por todo esto, los mÃ³dulos
 libres sobre dominios de integridad serÃ¡n libres de torsiÃ³n.

 #+begin_definition
 *CÃ­clico*. Un mÃ³dulo es *cÃ­clico* cuando es generado por un elemento. Es decir,
 cuando $M \cong R/I$ para algÃºn ideal.
 #+end_definition

 Cuando en un dominio de integridad todos sus
 mÃ³dulos cÃ­clicos son libres de torsiÃ³n, es un cuerpo. Otra forma de pensar sobre un mÃ³dulo
 cÃ­clico es como aquel que admite un epimorfismo:

 \[ R \longrightarrow M \longrightarrow 0 \]

****** 4.2. MÃ³dulos finitamente presentados y resoluciones libres
 #+begin_definition
 *Anulador.* El anulador de un mÃ³dulo $M$ es:

 \[Ann_R(M) = \{ r \in R\ |\ \forall m \in M, rm = 0 \}\]
 #+end_definition

 Es un ideal de $R$. Cuando $M$ es finitamente generado y $R$ es dominio de integridad,
 $M$ es de torsiÃ³n si y sÃ³lo si $Ann(M) \neq 0$.

 #+begin_definition
 *MÃ³dulos finitamente generados y presentados*. Sabemos que todos los mÃ³dulos admiten un
 epimorfismo de la forma:

 \[ R^{\oplus A} \longrightarrow M \longrightarrow 0\]

 Cuando lo admiten con $A$ finito, se tiene $M$ *finitamente generado*. Un mÃ³dulo se dice
 *finitamente presentado* si hay una secuencia exacta de la forma:

 \[R^n \overset{\phi}\longrightarrow R^m \longrightarrow M \longrightarrow 0\]

 .
 #+end_definition

 Si $R$ es Noetheriano, todo mÃ³dulo finitamente generado es finitamente presentado.

 #+begin_definition
 *ResoluciÃ³n*. Una resoluciÃ³n de $M$ mediante mÃ³dulos libres finitamente generados es
 un complejo exacto:

 \[ \dots \rightarrow R^{m_3} \rightarrow R^{m_2} \rightarrow R^{m_1} \rightarrow R^{m_0} \rightarrow M \rightarrow 0 \]
 #+end_definition

 AquÃ­ podemos entender que $R^{m_0}$ contiene los generadores, $R^{m_1}$ las relaciones
 entre los generadores, $R^{m_2}$ las relaciones entre relaciones, y asÃ­ sucesivamente.

 Un dominio de integridad es *cuerpo si y sÃ³lo si todos sus mÃ³dulos son finitamente generados*,
 esto es equivalente a tener:

 \[ 0 \longrightarrow R^m \longrightarrow M \longrightarrow 0 \]

 para cualquier mÃ³dulo.

 Un dominio de integridad es *PID si todas las resoluciones como finitamente generado 
 extienden a finitamente presentado*, de la forma:

 \[0 \longrightarrow R^{m_1} \longrightarrow R^{m_0} \overset{\pi}\longrightarrow M \longrightarrow 0\]

 esto equivale a pedir que $\ker(\pi)$ sea libre.

****** 4.3. Leyendo una presentaciÃ³n
 Hemos visto que podemos estudiar un mÃ³dulo finitamente presentado por un
 morfismo $\phi: R^n \longrightarrow R^m$, donde $M = coker(\phi)$. Esto quiere decir que 
 podemos asignarle una matriz explÃ­cita.

 #+begin_theorem
 *Producto de mÃ³dulos en matrices*. Sean $M,N$ mÃ³dulos con matrices $A,B$.
 Tenemos $M \oplus N$ con matriz:

 \[\left(\begin{array}{c|c}
 A & 0 \\ \hline 0 & B 
 \end{array}\right)\]
 #+end_theorem

 AdemÃ¡s nÃ³tese que las *matrices equivalentes* representan el mismo 
 homeomorfismo, y por tanto el mismo mÃ³dulo.

 #+begin_theorem
 *Transformaciones de matrices de mÃ³dulos*. Una matriz representa el mismo mÃ³dulo
 tras las transformaciones de:
  - Permutar filas o columnas
  - AÃ±adir filas o columnas linealmente dependientes
  - Multiplicar filas o columnas por una unidad
  - Quitar una fila y columna en la que sÃ³lo queda una unidad
 #+end_theorem

 Las primeras son consecuencia de la equivalencia. La Ãºltima puede colocarse como
 una parte de identidad en una matriz de la forma:

 \[A = \left(\begin{array}{c|c}
 u & 0 \\ \hline 0 & A' 
 \end{array}\right)\]

 Que no afecta al cokernel.

**** VII. Cuerpos
***** 1. Extensiones de cuerpos I
****** 1.1. Definiciones bÃ¡sicas
******* CategorÃ­a de los cuerpos
Los cuerpos forman la *categorÃ­a $\mathtt{Fld}$* con los homomorfismos de 
anillos entre ellos. Todo homomorfismo de anillos entre cuerpos
es inyectivo y todo morfismo en esta categorÃ­a es monomorfismo.

AsÃ­, todo morfismo entre cuerpos en $Hom(k,K)$ es una extensiÃ³n $K/k$.

******* CaracterÃ­stica de un cuerpo
      La *caracterÃ­stica* de $K$ es el generador de $ker(i)$ para 
      $i : \mathbb{Z} \longrightarrow K$. Las extensiones preservan la 
      caracterÃ­stica, asÃ­ que podemos particionar la categorÃ­a en categorÃ­as 
      $\mathtt{Fld}_p$.

******* Cuerpos primos
      El inicial de $\mathtt{Fld}_0$ es $\mathbb{Q}$, y el de $\mathtt{Fld}_p$ es $\mathbb{F}_p = \mathbb{Z}/p\mathbb{Z}$. Todos los
      cuerpos son extensiones de uno de estos llamados *cuerpos primos*.

******* Grado de una extensiÃ³n
El *grado*, $[F : K]$, de una extensiÃ³n es su dimensiÃ³n como espacio
vectorial sobre la base. Es *finita* o *infinita* si lo es su grado.

****** 1.2. Extensiones simples
******* ExtensiÃ³n simple
Una extensiÃ³n es *simple* si es de la forma $K(\alpha)$ donde 
$K(\alpha)$ es la intersecciÃ³n de todos los subcuerpos de algÃºn
$F$ conteniendo al cuerpo $K$ y el elemento $\alpha$.

******* Polinomio irreducible mÃ­nimo
Dada una extensiÃ³n simple $K(\alpha)$, consideramos la evaluaciÃ³n
$\epsilon : K[X] \longrightarrow K(\alpha)$ por casos:

 - Es *inyectiva* ssi es una *extensiÃ³n infinita*. En este
   caso $K(\alpha) \cong K(X)$ es el cuerpo de funciones racionales.
 - No es *inyectiva*. Existe un Ãºnico polinomio mÃ³nico
   irreducible $p$ que genera el nÃºcleo,

   \[ K(\alpha) \cong \frac{K[t]}{(p(t))}\]

   Se le llama *polinomio mÃ­nimo*.

******* TODO ExtensiÃ³n de isomorfismos a extensiones simples
Proposition 1.5
******* Automorfismos de una extensiÃ³n
El *grupo de automorfismos* de una extensiÃ³n $Aut_K(F)$, es el
grupo de los automorfismos de cuerpos que dejan fijo $K$.
******* Automorfismos y raÃ­ces
Sea $K(\alpha)$ con $p$ polinomio mÃ­nimo. Entonces $p$ tiene $|Aut_K(K(\alpha))|$ raÃ­ces
distintas en $K(\alpha)$. En particular,

\[ |Aut_K(K(\alpha))| \leq [K(\alpha):K] \]

y el caso de igualdad se tiene con $p$ factorizando en factores 
lineales sobre $F$.
****** 1.3. Extensiones finitas y algebraicas
******* Elementos algebraicos y trascendentes
Sea $F/K$ una extensiÃ³n con $\alpha \in F$, entonces $\alpha$ es *algebraico*
cuando $K(\alpha)/K$ es finita, y *trascendente* si no. Una extensiÃ³n
es *algebraica* si todos sus elementos lo son.

***** 6. Un poco de teorÃ­a de Galois
****** 6.1. Correspondencia de Galois y extensiones de Galois
******* Cuerpo fijo
Sea $F/k$ extensiÃ³n y $G \subseteq Aut_k(F)$. Llamamos *cuerpo fijo* de $G$ a:

\[ F^G = \{ \alpha\in F \mid \forall g \in G, g\alpha=\alpha\}\]

******* Correspondencia de Galois
Hay correspondencia entre los cuerpos intermedios de la extensiÃ³n
y los subgrupos del grupo de automorfismos.

Dado $E$ cuerpo intermedio, lo enviamos a $Aut_E(F)$. Dado $G$ lo enviamos
a $F^G$.

******* InclusiÃ³n y correspondencia
Para cualesquiera subgrupo $G$ y cuerpo intermedio $E$:

 - $E \subseteq F^{Aut_E(F)}$
 - $G \subseteq Aut_{F^G}(F)$

Si llamamos $E_1E_2$ al menor subcuerpo de $F$ conteniendo $E_1,E_2$ y llamamos
$<G_1,G_2>$ al menor subgrupo de los automorfismos conteniendo $G_1,G_2$:

 - $Aut_{E_1E_2}(F) = Aut_{E_1}(F) \cap Aut_{E_2}(F)$
 - $F^{<G_1,G_2>} = F^{G_1} \cap F^{G_2}$

******* Extensiones de Galois
Sea $F/k$ extensiÃ³n, equivalen:

 - $F$ es cuerpo de descomposiciÃ³n de algÃºn $f \in k[t]$.
 - $F/k$ es normal y separable.
 - $|Aut_k(F)| = [F : k]$.
 - La correspondencia de Galois es biyecciÃ³n.
 - $F/k$ separable y, si $E/F$ es algebraica con $\sigma \in Aut_k(E)$, $\sigma(F)=F$.

Llamamos a esto una *extensiÃ³n de Galois*.
**** VIII. Vuelta al Ã¡lgebra lineal
***** 1. Preliminares
****** 1.1. Funtores
 #+begin_definition
 *Funtor*. Un funtor covariante:

 \[{\cal F} : C \longrightarrow D\]

 Asigna a cada $A \in C$ un ${\cal F}(A) \in D$ y mapea los morfismos entre cada par de objetos:

 \[Hom_C(A,B) \rightarrow Hom_D({\cal F}(A),{\cal F}(B))\]

 Respetando la identidad y la composiciÃ³n de morfismos. 

 Un *funtor contravariante* es un funtor desde la categorÃ­a opuesta:

 \[{\cal F} : C^{op} \longrightarrow D\]
 #+end_definition

 Los funtores preservan los diagramas conmutativos. Llamamos *prehaz* a un funtor
 contravariante $C \longrightarrow \mathtt{Set}$.

 #+begin_definition
 *Funtor aditivo*. Llamamos a un funtor 
 ${\cal F}: R-\mathtt{Mod} \longrightarrow S-\mathtt{Mod}$ *aditivo* cuando
 la funciÃ³n $Hom_{R}(A,B) \rightarrow Hom_{S}({\cal F}(A),{\cal F}(B))$ es homomorfismo de grupos.
 #+end_definition

****** 1.3. Equivalencia de categorÃ­as
 #+begin_definition
 *Funtores plenamente fieles*. Dada la funciÃ³n inducida:
 \[Hom_C(A,B) \rightarrow Hom_D({\cal F}(A),{\cal F}(B))\]
 Un funtor es *fiel* si es inyectiva, *pleno* si es sobreyectiva y *plenamente fiel*
 si es biyectiva.
 #+end_definition

 #+begin_definition
 *Equivalencia de categorÃ­as*. Un funtor es una equivalencia de categorÃ­as si 
 es plenamente fiel y esencialmente sobreyectivo, es decir, para cada $Y \in D$,
 existe un $X \in C$ tal que $F(X) \cong Y$.
 #+end_definition

****** 1.4. LÃ­mites y colÃ­mites

 #+begin_definition
 *LÃ­mite*. Para un funtor ${\cal F}: {\cal I} \longrightarrow C$, su lÃ­mite es
 un objeto $L \in C$ con morfismos $\lambda_I: L \longrightarrow {\cal F}(I)$ tales que

 - Conmuta el siguiente diagrama para cualquier $\alpha : I \longrightarrow J$:

 \[ \begin{tikzcd}[column sep=1.5em]
  & L \arrow{dr}{\lambda_J} \arrow{dl}[swap]{\lambda_I} \\
 {\cal F}(I) \arrow{rr}{{\cal F}(\alpha)} && {\cal F}(J)
 \end{tikzcd} \]

 - $L$ es final en este diagrama.
 #+end_definition

 SerÃ¡ esencialmente Ãºnico y puede notarse por $\varprojlim {\cal F}$.

 #+begin_theorem
 *LÃ­mites sobre cadenas en R-Mod*. En R-Mod siempre existe un lÃ­mite llamado \(\varprojlim {\cal A}_i\) sobre una
 cadena de la forma:

 \[ \begin{tikzcd}
 & & A 
 \arrow{lld}[swap]{\phi_5}
 \arrow{ld}{\phi_4}
 \arrow{d}{\phi_3}
 \arrow{rd}[swap]{\phi_2}
 \arrow{rrd}{\phi_1} 
 & & \\
 \dots \arrow{r}[swap]{\phi_{45}}  &
 A_4 \arrow{r}[swap]{\phi_{34}} &
 A_3 \arrow{r}[swap]{\phi_{23}} &
 A_2 \arrow{r}[swap]{\phi_{12}} &
 A_1
 \end{tikzcd} \]
 #+end_theorem

 Este lÃ­mite es el submÃ³dulo de las /secuencias coherentes/ en $\prod_i A_i$, es decir, de
 aquellas tales que $a_i = \phi_{i,i+1}(a_{i+1})$; teniendo como morfismos $\phi_i$ las proyecciones
 canÃ³nicas


 #+begin_definition
 *ColÃ­mite*. La nociÃ³n dual de lÃ­mite es el *colÃ­mite*, es decir, para
 un funtor ${\cal F} : I \longrightarrow C$, su colÃ­mite es un objeto $L \in C$ con morfismos $\gamma_i : {\cal F}(I) \longrightarrow L$
 tales que

 - Conmuta el siguiente diagrama para cualquier $\alpha : I \longrightarrow J$:

 \[ \begin{tikzcd}[column sep=1.5em]
  & L  \\
 {\cal F}(I) \arrow{ur}{\gamma_I} \arrow{rr}{{\cal F}(\alpha)} && {\cal F}(J) \arrow{ul}[swap]{\gamma_J}
 \end{tikzcd} \]

 - $L$ es inicial en este diagrama.
 #+end_definition

****** 1.5. Comparando funtores
 #+begin_definition
 *TransformaciÃ³n natural*. Una transformaciÃ³n natural entre dos funtores ${\cal F} \Longrightarrow {\cal G}$ 
 consiste en morfismos $\upsilon_X : {\cal F}(X) \longrightarrow {\cal G}(X)$ tales que conmuta el diagrama:

 \[ \begin{tikzcd}
 {\cal F}(X) \arrow{r}{{\cal F}(\alpha)} \arrow{d}{\upsilon_X} & {\cal F}(Y) \arrow{d}{\upsilon_Y} \\
 {\cal G}(X) \arrow{r}{{\cal G}(\alpha)} & {\cal G}(Y)
 \end{tikzcd}
 \]

 para cualquier morfismo $\alpha$.

 Llamamos *isomorfismo natural* a una transformaciÃ³n natural donde cada $\upsilon$
 es un isomorfismo.
 #+end_definition

 #+begin_definition
 *Funtor adjunto*. Llamamos ${F}$ y ${G}$ adjuntos si tenemos:

 \[ Hom_C(X,GY) \cong Hom_D(FX,Y) \]

 Isomorfismos naturales.
 #+end_definition

 Lo que nos da realmente un isormorfismo natural de $Hom_C(F-,-)$ con $Hom_D(-,G-)$,
 entendidos como funtores. Llamamos aquÃ­ adjunto izquierdo a $F$ y adjunto derecho a $G$.
 Tenemos mÃ¡s sobre funtores adjuntos en la lista de reproducciÃ³n de [[https://www.youtube.com/playlist?list=PL54B49729E5102248][The Catsters]].

 #+begin_theorem
 *Continuidad de adjuntos*. Los funtores adjuntos derechos son continuos, los adjuntos
 izquierdos son cocontinuos. Es decir, para $I : {\cal I}\longrightarrow D$, $J : {\cal J}\longrightarrow C$

 \[G(\varprojlim I) = \varprojlim (G \circ I)\]
 \[F(\varinjlim J) = \varinjlim (F \circ J)\]
 #+end_theorem

 Siempre que existan los lÃ­mites. La demostraciÃ³n de esto se puede hacer aplicando los
 funtores en los diagramas conmutativos y usando las propiedades universales de los lÃ­mites.

 #+begin_definition
 *Funtor exacto*. Un funtor exacto respeta la exactitud de las secuencias. Es decir,
 siendo la siguiente secuencia exacta:

 \[ 0 \longrightarrow A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C \longrightarrow 0\]

 La siguiente secuencia serÃ¡ exacta:

 \[ 0 \longrightarrow FA \overset{F\phi}\longrightarrow FB \overset{F\psi}\longrightarrow FC \longrightarrow 0\]
 #+end_definition

 En particular, lo llamamos /exacto a la izquierda/ si preserva la exactitud de:

 \[ 0 \longrightarrow A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C\]

 Y /exacto a la derecha/ si preserva la exactitud de:

 \[ A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C \longrightarrow 0\]

***** 2. Producto tensor y el funtor Tor
****** 2.1. Aplicaciones bilineales
 #+begin_definition
 *AplicaciÃ³n bilineal*. Una aplicaciÃ³n $\phi:M\times N \longrightarrow P$ es bilineal si
 son lineales $\phi(\_,n)$ y $\phi(m,\_)$ para cualesquiera $m,n$.
 #+end_definition

 #+begin_definition
 *Producto tensor*. $M \otimes_R N$ es el producto tensor de $M$ y $N$ como mÃ³dulos de $R$
 si cualquier aplicaciÃ³n bilineal factoriza de forma Ãºnica a travÃ©s de Ã©l:

 \[ \begin{tikzcd}
 M \times N \arrow{r}{\phi} \arrow{d}{\otimes} & P \\
 M \otimes N \arrow{ru}[swap]{\exists! \overline\phi} &
 \end{tikzcd} \]
 #+end_definition

 Usando universalidad podemos ver que $R \otimes N \cong N$ y que $M\otimes N \cong N\otimes M$. La construcciÃ³n
 explÃ­cita del producto tensor se hace sobre el mÃ³dulo libre sobre $M \times N$ provocando un
 cociente sobre los submÃ³dulos generados por:

 \[(m,r_1n_1+r_2n_2) - r_1(m,n_1) - r_2(m,n_2)\]
 \[(r_1m_1+r_2m_2,n) - r_1(m_1,n) - r_2(m_2,n)\]

 Lo que nos permite actuar con ellos de forma bilineal. La demostraciÃ³n se basa en usar
 la propiedad universal de la proyecciÃ³n sobre ese cociente.

****** 2.2. AdjunciÃ³n con Hom
 Dado un mÃ³dulo $N$ de $R$, tenemos un funtor covariante $\otimes_R N$, que serÃ¡ *adjunto izquierdo*
 a $Hom_{R-mod}(N,-)$. Podemos observar simplemente que una aplicaciÃ³n bilineal, al currificarse,
 determina una funciÃ³n que va de $M$ a $Hom(N,P)$, y que es lineal. Sabiendo esto, es trivial
 que:

 \[ Hom_R(M, Hom_R(N,P)) \cong Hom_R(M \otimes N, P)\]

 La naturalidad y el hecho de que es un isomorfismo se comprueban fÃ¡cilmente. El hecho de
 que exista una adjunciÃ³n nos dice ademÃ¡s que $\otimes_R N$, o $N\otimes_R$ por la isomorfÃ­a anterior,
 son cocontinuos.

 #+begin_fact
 Para cualesquiera \(R\)-mÃ³dulos, se tiene:

 \[(M_1 \oplus M_2) \otimes N \cong (M_1 \otimes N) \oplus (M_2 \otimes N)\]

 \[N \otimes (M_1 \oplus M_2) \cong (N \otimes M_1) \oplus (N \otimes M_2)\]

 \[(\oplus_\alpha M_\alpha) \otimes N \cong \oplus_\alpha (M_\alpha \otimes N)\]
 #+end_fact

 Por cocontinuidad.

 #+begin_fact
 Para cualesquiera dos conjuntos $A,B$, se tiene:

 \[R^{\oplus A} \otimes R^{\oplus B} \cong R^{\oplus A \times B}\]
 #+end_fact

 Teniendo \(R^{\oplus n} \otimes R^{\oplus m} \cong R^{\oplus nm}\). De hecho, la base del espacio producto
 tensor la forman los vectores puros que emparejan elementos de las 
 bases de cada uno de los espacios.

 #+begin_theorem
 *Producto tensor de cocientes*. Dado un $N$ mÃ³dulo de $R$, e $I$ ideal,
 tenemos:

 \[\frac{R}{I}\otimes N \cong \frac{N}{IN}\]

 Y desde ahÃ­, aplicando ademÃ¡s el tercer teorema de isomorfÃ­a, tenemos:

 \[\frac{R}{I} \otimes \frac{R}{J} \cong \frac{R}{I+J}\]
 #+end_theorem

 Esto se deduce de aplicar el funtor $\_ \otimes N$ a la secuencia exacta del 
 ideal:

 \[I \longrightarrow R \longrightarrow \frac{R}{I} \longrightarrow 0\]
 
 \[I \otimes N \longrightarrow N \longrightarrow \frac{R}{I} \otimes N \longrightarrow 0\]

 Desde donde se obtiene $IN$ como inclusiÃ³n de $I\otimes N$ en $N$.

****** 2.3. Exactitud y planitud
 #+begin_definition
 *MÃ³dulo plano*. El mÃ³dulo $N$ es *plano* si el funtor $\_ \otimes N$ es un
 funtor exacto.
 #+end_definition

 Un *mÃ³dulo libre* serÃ¡ siempre plano.

****** 2.4. Los funtores Tor
 #+begin_definition
 *El funtor Tor*. Lo que se aleja de la exactitud el funtor $\_ \otimes N$
 es medido por el funtor $Tor_1(\_,N)$. De hecho, si tenemos una secuencia
 exacta:

 \[0\longrightarrow A \longrightarrow B \longrightarrow C \longrightarrow 0\]

 Obtenemos aplicando el funtor $\otimes N$ esta otra secuencia:

 \[Tor_1(C,N) \longrightarrow A \otimes N \longrightarrow B \otimes N \longrightarrow C \otimes N \longrightarrow 0\]

 Y de hecho, esta secuencia podrÃ¡ extenderse aÃºn mÃ¡s con /funtores derivados/,
 que se definen como:

 \[Tor_i^R(M,N) = H_i(M_{\bullet} \otimes N)\]
 #+end_definition

 AquÃ­ entendemos $M_\bullet \otimes N$ como el complejo que se obtiene tomando una resoluciÃ³n
 libre de $M$:

 \[\dots \longrightarrow R^{\otimes S_2} \longrightarrow R^{\otimes S_1} 
 \longrightarrow R^{\otimes S_0} \longrightarrow M \longrightarrow 0}\]

 Y retirando $M$ y tensando sobre $N$, para tener:

 \[\dots \longrightarrow N^{\otimes S_2} \longrightarrow N^{\otimes S_1} 
 \longrightarrow N^{\otimes S_0} \longrightarrow 0}\]

 Todo esto se obtendrÃ¡ de manera natural aplicando el lema de la serpiente a una secuencia
 de resoluciones compatibles, algo que, si los mÃ³dulos fueran PID y tuvieran una resoluciÃ³n
 de grado 2, serÃ­a de la forma:

 \[ \begin{tikzcd}
    & 0 \dar & 0 \dar & 0 \dar &   \\
 0 \rar & R^{\oplus a_1}\rar\dar & R^{\oplus b_1} \rar\dar & R^{\oplus c_1} \rar\dar & 0 \\
 0 \rar & R^{\oplus a_0}\rar\dar & R^{\oplus b_0} \rar\dar & R^{\oplus c_0} \rar\dar & 0 \\
 0 \rar & A\rar\dar & B \rar\dar & C \rar\dar & 0 \\
  & 0 & 0 & 0 & 
 \end{tikzcd} \]

 Tensando las dos filas superiores, que son libres, nos quedarÃ­an dos filas sobre las que aplicar
 el lema de la serpiente y obtener los funtores derivados tal y como los hemos definido.

***** 5. Funtor Hom y dualidad 
****** 5.1. Adjunciones, de nuevo
 Ya sabemos que el funtor $Hom(N,\_)$ es adjunto derecho a $\_\otimes N$, ahora
 estudiamos el funtor $Hom(\_,N)$.

 #+begin_theorem
 *AdjunciÃ³n de Hom contravariante*. El funtor $Hom(\_,N)$ es adjunto derecho
 de su funtor opuesto, $Hom^{op}(\_,N)$.
 #+end_theorem

 Aplicando currificaciÃ³n tenemos trivialmente:

 \[Hom(L,Hom(M,N)) \cong Hom(M,Hom(L,N))\]

 Que, teniendo en cuenta que estamos usando la categorÃ­a opuesta, prueba la
 adjunciÃ³n.

 #+begin_proposition
 *Exactitud de Hom*. Ambos funtores $Hom$ son adjuntos derechos y por tanto,
 exactos por la izquierda. Teniendo en cuenta que uno es contravariante, quiere
 decir que:

 \[ A \overset{}\longrightarrow B \overset{}\longrightarrow C \overset{}\longrightarrow 0\]

 Lleva a:

 \[ 0 \overset{}\longrightarrow Hom(C,N) \overset{}\longrightarrow 
 Hom(B,N) \overset{}\longrightarrow Hom(A,N)\]
 #+end_proposition

****** 5.2. MÃ³dulos duales.
 #+begin_definition
 *MÃ³dulo dual*. El dual de un R-mÃ³dulo $M$ es el mÃ³dulo $M^{\vee} = Hom_R(M,R)$.
 #+end_definition

 Tenemos que $Hom(M,R^n) \cong M^{\vee} \otimes R^n$.

***** 6. MÃ³dulos proyectivos e inyectivos, y el funtor Ext
****** 6.1. Proyectividad e inyectividad
 #+begin_definition
 *MÃ³dulos proyectivos e inyectivos*. Un R-mÃ³dulo es /proyectivo/ si $Hom(P,\_)$
 es exacto; e /inyectivo/ si $Hom(\_,P)$ es exacto.
 #+end_definition

 Esto es equivalente a decir que cada epimorfismo $M \longrightarrow N$ lleva un
 morfismo $P \longrightarrow N$ a $P \longrightarrow M$, en el caso de /proyectividad/:

 \[ \begin{tikzcd}
  & P \dlar[swap,dashed]{\exists p'} \dar[swap]{p} \drar{0} & \\
 M \rar & N \rar & 0
 \end{tikzcd} \]

 O que cada monomorfismo $L \longrightarrow M$ lleva un morfismo $L \longrightarrow Q$ a
 un monomorfismo $M \longrightarrow Q$, en el de la /inyectividad/:

 \[ \begin{tikzcd}
  & Q & \\
 0 \urar{0} \rar & N \rar \uar[swap]{q} & M \ular[dashed,swap]{\exists q'}
 \end{tikzcd} \]

 AdemÃ¡s, esto es equivalente a decir que un mÃ³dulo $P$ es /proyectivo/ si toda secuencia

 \[ 0 \overset{}\longrightarrow L \overset{}\longrightarrow M \overset{}\longrightarrow P \overset{}\longrightarrow 0 \]

 es escindida, y $Q$ es /inyectivo/ si toda secuencia:

 \[ 0 \overset{}\longrightarrow Q \overset{}\longrightarrow M \overset{}\longrightarrow N \overset{}\longrightarrow 0 \]

 es escindida.

****** 6.2. MÃ³dulos proyectivos
 #+begin_theorem
 *CaracterizaciÃ³n de proyectividad*. Un mÃ³dulo es proyectivo ssi es el sumando
 directo de un mÃ³dulo libre.
 #+end_theorem

 AsÃ­, la suma directa de dos mÃ³dulos proyectivos es proyectiva; el producto tensor
 de dos mÃ³dulos proyectivos es proyectivo, y todo mÃ³dulo proyectivo es plano.

****** 6.3. MÃ³dulos inyectivos
 #+begin_theorem
 *CaracterizaciÃ³n de inyectividad*. Un mÃ³dulo es *inyectivo* ssi toda aplicaciÃ³n
 $f : I \longrightarrow Q$ extiende a una aplicaciÃ³n $\hat f : R \longrightarrow Q$, donde I es ideal de R.
 #+end_theorem

****** 6.4. El funtor Ext
 ExistirÃ­an dos formas naturales de definir *Ext*, que coinciden no trivialmente:

 #+begin_definition
 *Funtor Ext*. Dado $M$ con una resoluciÃ³n proyectiva:

 \[ \dots \overset{}\longrightarrow P_1 \overset{}\longrightarrow P_0 \overset{}\longrightarrow M \overset{}\longrightarrow 0 \]

 aplicamos el funtor contravariante $Hom(\_,N)$ eliminando $M$ para obtener:

 \[ 0 \overset{}\longrightarrow Hom(P_0,N) \overset{}\longrightarrow Hom(P_1,N) \overset{}\longrightarrow Hom(P_2,N) \overset{}\longrightarrow \dots \]

 Y tomamos la cohomologÃ­a de este complejo $Hom(M_\bullet,N)$, dejando como definiciÃ³n:

 \[Ext^i_R(M,N) = H^i(Hom(M_\bullet,N))\]
 #+end_definition

 #+begin_definition
 *Funtor Ext*. Dado $N$ con una resoluciÃ³n inyectiva:

 \[ 0 \overset{}\longrightarrow N \overset{}\longrightarrow Q_0 \overset{}\longrightarrow Q_1 \overset{}\longrightarrow \dots \]

 aplicamos el funtor covariante $Hom(M,\_)$ eliminando $N$ para obtener:

 \[ 0 \overset{}\longrightarrow 
 Hom(M,Q_0) \overset{}\longrightarrow 
 Hom(M,Q_1) \overset{}\longrightarrow 
 Hom(M,Q_2) \overset{}\longrightarrow \dots \]

 Y tomamos la cohomologÃ­a de este complejo $Hom(M,N_\bullet)$, dejando como definiciÃ³n:

 \[Ext^i_R(M,N) = H^i(Hom(M,N_\bullet))\]
 #+end_definition

**** IX. Ãlgebra homolÃ³gica
**** Complejos y homologÃ­a, de nuevo
***** 3.1. Recordatorio de definiciones bÃ¡sicas
 #+begin_definition
 *ResoluciÃ³n*. La /resoluciÃ³n/ de un objeto $A$ es un complejo
 exacto excepto en un punto, donde es isomorfa a $A$.
 #+end_definition

 Esto es equivalente a tener un complejo exacto de la forma:

 \[ \dots \overset{}\longrightarrow 
 M_2 \overset{}\longrightarrow 
 M_1 \overset{}\longrightarrow 
 M_0 \overset{}\longrightarrow 
 A \longrightarrow
 0\]

***** 3.2. La categorÃ­a de los complejos
 #+begin_definition
 *CategorÃ­a de complejos de cocadenas*. La categorÃ­a $C(A)$ tiene como objetos
 los complejos de cocadenas en una categorÃ­a $A$; y como morfismos entre dos 
 cocadenas,   $Hom(M^\bullet,N^\bullet)$, los diagramas conmutativos entre ellas. Por ejemplo:

 \[ \begin{tikzcd}
 \dots \rar & M^{i-1} \rar\dar{\alpha^{i-1}} & M^{i} \rar\dar{\alpha^{i}} &  M^{i+1} \rar\dar{\alpha^{i+1}} & \dots \\
 \dots \rar & N^{i-1} \rar & N^{i} \rar & N^{i+1} \rar & \dots
 \end{tikzcd} \]

 representa el morfismo $\alpha_\bullet$.
 #+end_definition

 Esta es una categorÃ­a abeliana. De ella definiremos ademÃ¡s dos variantes:

 - $C^+(A)$, subcategorÃ­a plena de los complejos acotados por debajo.
 - $C^-(A)$, subcategorÃ­a plena de los complejos acotados por arriba.
**** Exercises [20/62]
***** I. Preliminaries: Set theory and categories [1/1]
****** I.5. Universal properties
******* DONE Exercise I.5.12
#+begin_statement
Define notions of /fibered products/ and /coproducts/, as terminal objects of
the categories $C_{\alpha,\beta}$ and $C^{\alpha,\beta}$ considered in Example 3.10, by stating carefully
the corresponding universal properties.

As it happens, $\mathtt{Set}$ has both fibered products and coproducts. Define these
objects 'concretely', in terms of naive set theory.
#+end_statement

******** Fibered products
Given $\alpha\colon A \to C$ and $\beta \colon B \to C$, we take the objects of the category
to be commutative diagrams

\[\begin{tikzcd}[row sep=tiny]
 & A \drar{\alpha} & \\
Z\drar{g}\urar{f} && C\\
& B \urar{\beta} & &.
\end{tikzcd}\]

And morphisms to be of the form

\[\begin{tikzcd}[row sep=tiny]
& & A \drar{\alpha} & \\
Z' \rar{h}\ar[bend right]{drr}{g'} \ar[bend left]{urr}{f'} &
Z\drar{g}\urar{f} && C\\
& & B \urar{\beta} & &.
\end{tikzcd}\]

With the trivial identity $\mathrm{id}_Z \colon (Z,f,g) \to (Z,f,g)$. The terminal object
should be the one such that every other object's morphisms descompose
through it.

\[\begin{tikzcd}[row sep=tiny]
& & A \drar{\alpha} & \\
Z' \rar[dashed]{\exists! h}\ar[bend right]{drr}{g'} \ar[bend left]{urr}{f'} &
F\drar{\pi_{\beta}}\urar{\pi_{\alpha}} && C\\
& & B \urar{\beta} & &.
\end{tikzcd}\]

******** Fibered coproducts
The same idea can be applied to the dual construction

\[\begin{tikzcd}[row sep=tiny]
& & A \dlar{\iota_A}\ar[bend right]{lld}{f'} & \\
Z &
F \lar[dashed]{\exists! h} && 
C \ular{f}\dlar{g}\\
& & B \ular{\iota_B}\ar[bend left]{llu}{g'} & &.
\end{tikzcd}\]

******** In the category of sets
In $\mathtt{Set}$, the fibered product will be

\[
\left\{ (a,b) \in A \times B \mid \alpha(a) = \beta(b) \right\},
\]

as we can show using the product universal property. The fibered coproduct
is the coproduct divided by the equivalence relation generated by the
pairs $a \sim b$ such that $\exists c\colon a = f(c), b = g(c)$.

***** II. Groups, first encounter [16/28]
****** II.1. Definition of group
******* DONE Exercise II.1.1
#+begin_statement
Write a careful proof that every group is the group of isomorphisms of a
grupoid. In particular, every group is the group of automorphisms of some
object in some category.
#+end_statement

Given a group $(G,\bullet)$, we can define an object $G$ with morphisms the elements
of the group. Composition will be the binary operation of the group, and we
can check, using the group properties, that category axioms hold

 1. *Identity*, there is an identity element on $G$ which acts as the identity
    morphism: $e \circ a = a = a \circ e$.
 2. *Associativity* holds directly: $a \circ (b\circ c) = (a \circ b)\circ c$.

This only object defines a category which is also a grupoid, as every arrow
has an *inverse* by the last property of groups: every element of the group has
an inverse.

******* DONE Exercise II.1.2
#+begin_statement
Consider the 'sets of numbers' listed in 1.1, and decide which are made into 
groups by conventional operations such as $+$ and $\cdot$. Even if the answer is negative,
see if variations on the definition of these sets lead to groups.
#+end_statement

******** The empty set
It is not a group, as it has no identity.

******** Naturals
They form no group with addition, as not every element has an inverse.

******** Integers
They are a commutative group with the addition.

******** Rational numbers
They are a group with addition, and also a group with multiplication if we
consider $0$ out of the group.

******** Real numbers
They follow the same logic as the rationals.

******** Complex numbers
Same logic as the rationals.
******* TODO Exercise II.1.3
******* TODO Exercise II.1.4
******* TODO Exercise II.1.5
******* TODO Exercise II.1.6
******* TODO Exercise II.1.7
******* TODO Exercise II.1.8
******* TODO Exercise II.1.9
******* TODO Exercise II.1.10
******* TODO Exercise II.1.11
******* TODO Exercise II.1.12
******* DONE Exercise II.1.13
#+begin_statement
Give an example showing that $|gh|$ is not necessarily equal to $\mathrm{lcm}(|g|,|h|)$, even
if $g$ and $h$ commute.
#+end_statement

In $(\mathbb{Z}/\mathbb{Z}_4,+)$, we have $\mathrm{lcm}(|1|,|1|) = |1| = 4$, but $|1+1| = 2$.

******* DONE Exercise II.1.14
#+begin_statement
As a counterpoint to [[*Exercise II.1.13][Exercise 13]], prove that if $g$ and $h$ commute, 
and $\mathrm{gcd}(|g|,|h|) = 1$, then $|gh| = |g||h|$.
#+end_statement

We know that $|gh| = N \mid |g||h|$. If we divide to obtain $g^{N} = (h^{-1})^{N}$, we have

 * $1 = \left( g^{-1} \right)^{N|g|} = h^{N|g|}$
 * $1 = \left( h^{-1} \right)^{N|h|} = g^{N|h|}$

and then, $|h| \mid N|g|$ and they are coprimes, so $|h| \mid N$. Likewise, $|g| \mid N$.
Finally, $|gh| = \mathrm{lcm}(|h|,|g|) \mid N$.

******* DONE Exercise II.1.15
#+begin_statement
Let $G$ a commutative group, and let $g \in G$ be an element of maximal /finite/
order: that is, such that if $h \in G$ has finite order then $|h| \leq |g|$. Prove that
in fact if $h$ has finite order in $G$ then $|h|$ divides $|g|$.
#+end_statement

If $|h|$ does not divide $|g|$, then there is a prime $p$ such that

 * $|h| = p^{a+b}m$
 * $|g| = p^bn$, with $\mathrm{gcd}(n,p) = 1$.

then we know that $|h^m| = p^{a+b}$ and $|g^{p^b}| = n$. Using [[*Exercise II.1.14][exercise 14]] (we are in a
commutative group), we know that $|g^{p^b}h^m| = p^{a+b}n$, contradicting maximality.

****** II.2. Examples of groups
******* DONE Exercise II.2.2
#+begin_statement
Prove that if $d \leq n$, then $S_n$ contains elements of order $d$.
#+end_statement

The element $(1\;2\;\dots\;d)$ has order $d$.

******* TODO Exercise II.2.5
****** II.3. The category Grp
******* DONE Exercise II.3.1
#+begin_statement
Let $\varphi\colon G \to H$ be a morphism in a category $C$ with products. Explain why there
is a unique morphism

\[(\varphi\times\varphi)\colon G\times G \to H \times H.\]
#+end_statement

The real morphism is $(\varphi\circ \pi_1 \times \varphi \circ \pi_2)$, using the projections from $C$ as presented
in the following diagram

\[\begin{tikzcd}[column sep=small,row sep=tiny]
& G \times G\drar\dlar\ar[dashed]{dd}{\varphi\times\varphi} & \\
G\ar{dd} & & G\ar{dd} \\
& H \times H\drar\dlar & \\
H & & H \\
\end{tikzcd}\]

******* DONE Exercise II.3.8
#+begin_statement
Define a group $G$ with two generators $x,y$, subject (only) to the relations
$x^2=e$, $y^3=e$. Prove that $G$ is a coproduct of $C_2$ and $C_3$ in $\mathtt{Grp}$.
#+end_statement

Given any two morphisms $f\colon C_2 \to H$ and $g \colon C_3 \to H$, we define $h(x) = f(1)$ 
and $h(y) = g(1)$ as it should be to make the diagram commutative. There is
only a possible way to extend this morphism to $h \colon G \to H$.

****** II.4. Group homomorphisms
******* DONE Exercise II.4.3
#+begin_statement
Prove that a group of order $n$ is isomorphic to $\mathbb{Z}/n\mathbb{Z}$ if and only if it
contains an element of order $n$.
#+end_statement

If it contains $a$ of order $n$, then $e,a,a^2,\dots,a^{n-1}$ are $n$ different elements.
As the group is of order $n$, they constitute the whole group.

******* DONE Exercise II.4.8
#+begin_statement
Let $G$ be a group, and $g \in G$. Prove that the function $\gamma_g \colon G \to G$ defined
by $\gamma_g(a) = gag^{-1}$ is an automorphism of $G$. Prove that the function $G \to \mathrm{Aut}(G)$
defined by $g \mapsto \gamma_g$ is a homomorphism. Prove that this homomorphism is trivial
if and only if $G$ is abelian.
#+end_statement

The function $\gamma_g$ is trivially a homomorphism, and it has the inverse $\gamma_{g^{-1}}$.
We can check that $g\mapsto \gamma_g$ is a homomorphism, as

\[
\gamma_h\gamma_g(a) = hga(hg)^{-1} = \gamma_{hg}(a).
\]

If the homomorphism is trivial, $a = gag^{-1}$ for any $a,g \in G$; this is 
equivalent to abelianity.

******* DONE Exercise II.4.11
#+begin_statement
In due time, we will prove the easy fact that if $p$ is a prime integer then
the equation $x^d=1$ can have at most $d$ solutions in $\mathbb{Z}/p\mathbb{Z}$. Assume this fact,
and prove that the multiplicative group $G = (\mathbb{Z}/p\mathbb{Z})^{\ast}$ is cyclic.
#+end_statement

If the group is not cyclic there is no element of order $p-1$. So the element
of maximal order has order $d < p-1$, and every other element has a [[*Exercise II.1.15][divisor
of this order]] as its order. Then the equation $x^d = 1$ has more than $p-1$ roots,
contradicting the assumption.

******* TODO Exercise II.4.16
#+begin_statement
Prove /Wilson's theorem/: a positive integer $p$ is prime if and only if

\[(p-1)! \equiv -1  \mod p.
\]
#+end_statement

We are multiplying all elements of $(\mathbb{Z}/p\mathbb{Z})^{\ast} \cong (\mathbb{Z}/(p-1)\mathbb{Z})$

****** II.5. Free groups
******* DONE Exercise II.5.3
#+begin_statement
Use the universal property of free groups to prove that the map $j \colon A \to F(A)$ is
injective, for all sets $A$.
#+end_statement

If it were not injective, with $j(a) = j(b)$, every $f \colon A \to G$ should follow 
$f(a)=f(b)$, but given any two $a,b$ we can define $f \colon A \to \mathbb{Z}\times\mathbb{Z}$ by sending
$f(a) = (1,0)$, $f(b) = (0,1)$, and every other element to $0$.

******* DONE Exercise II.5.6
#+begin_statement
Prove that the group $F(\left\{ x,y \right\})$ is a coproduct $\mathbb{Z}\ast\mathbb{Z}$ of $\mathbb{Z}$ by itself in the
category $\mathtt{Grp}$.
#+end_statement

Given $d \colon \left\{ a,b \right\} \to G$, we use the coproduct inclusions on $\mathtt{Set}$ to define 
individual arrows

\[\begin{tikzcd}[column sep=tiny]
& G & \\
& \left\{ a,b \right\}\uar{d} & \\
\left\{ a \right\} \arrow[bend left]{uur} \urar{i} & & 
\left\{ b \right\} \arrow[bend right]{uul} \ular[swap]{i}
\end{tikzcd}\]

and then simply use the universal property of the free modules of one
element as follows

\[\begin{tikzcd}[column sep=small]
& G & \\
& \mathbb{Z}\ast\mathbb{Z} \uar[dashed] & \\
\mathbb{Z}\ar{ur}\ar[dashed,bend left]{uur} & & 
\mathbb{Z}\ar{ul}\ar[dashed,bend right]{uul} \\
& \left\{ a,b \right\} \ar[dashed]{uu} &\\
\left\{ a \right\}\urar \arrow[bend left=90]{uuuur}  \ar{uu} &&
\left\{ b \right\}\ular \arrow[bend right=90]{uuuul} \ar{uu}
\end{tikzcd}\]

******* DONE Exercise II.5.7
#+begin_statement
Extend the result of [[*Exercise II.5.6][Exercise 5.6]] to free groups $F(\left\{ x_1,\dots,x_n \right\})$ and to free
/abelian/ groups $F^{ab}(\left\{ x_1,\dots,x_n \right\})$.
#+end_statement

The same argument can be repeated $n$ times to obtain $\mathbb{Z}\ast \overset{n}\dots \ast\mathbb{Z}$ as free group.
As $\mathbb{Z}\oplus\mathbb{Z}$ is the abelian coproduct, $\mathbb{Z}\oplus\dots\oplus\mathbb{Z}$ is the free group.

****** II.6. Subgroups
******* DONE Exercise II.6.1
#+begin_statement
The group of invertible $n \times n$ matrices with entries in $\mathbb{R}$ is denoted $GL_n(\mathbb{R})$.
Similarly, $GL_n(\mathbb{C})$ denotes the group of $n \times n$ invertible matrices with complex
entries. Consider the following sets of matrices:

 * $SL_n(\mathbb{R}) = \left\{ M \in GL_n(\mathbb{R}) \mid \mathrm{det}(M)=1 \right\}$;
 * $SL_n(\mathbb{C}) = \left\{ M \in GL_n(\mathbb{C}) \mid \mathrm{det}(M)=1 \right\}$;
 * $O_n(\mathbb{R}) = \left\{ M \in GL_n(\mathbb{R}) \mid MM^t = M^tM = I_n \right\}$;
 * $SO_n(\mathbb{R}) = \left\{ M \in O_n(\mathbb{R}) \mid \mathrm{det}(M)=1 \right\}$;
 * $U(n) = \left\{ M \in GL_n(\mathbb{C}) \mid MM^{\dag} = M^{\dag}M = I_n \right\}$;
 * $SU(n) = \left\{ M \in U_n(\mathbb{C}) \mid \mathrm{det}(M) = 1 \right\}$;

Here $I_n$ stands for the $n \times n$ identity matrix, $M^t$ is the /transpose/ of $M$,
$M^{\dag}$ is the /conjugate transpose/ of $M$, and $\mathrm{det}(M)$ denotes the /determinant/
of $M$. Find all possible inclusions among these sets, and prove that in every
case the smaller set is a subgroup of the larger one.
#+end_statement

We are dealing with three different properties:

  1. The matrix has entries in $\mathbb{C}$.
  2. The matrix has its conjugate transpose as its inverse, $MM^{\dag}=I_n$.
  3. The matrix has determinant $1$, $\mathrm{det}(M) = 1$.

None of them implies the others. The three properties give rise to this 
three-dimensional cube

\[\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash}]
&& GL(\mathbb{C}) & \\
& U(n) \urar & SL_n(\mathbb{C})\uar & GL(\mathbb{R})\ar{ul} \\
& SU(n) \uar\ar{ur} & O_n(\mathbb{R}) \ular\urar & SL_n(\mathbb{R}) \ular\uar\\
&& SO_n(\mathbb{R})\uar \urar\ular & & .
\end{tikzcd}\]

******* DONE Exercise II.6.3
#+begin_statement
Prove that every matrix in $SU(2)$ may be written in the form

\[\begin{pmatrix}
a+bi & c+di \\
-c+di & a-bi
\end{pmatrix}\]

where $a,b,c,d \in \mathbb{R}$ and $a^2+b^2+c^2+d^2 = 1$. (Thus, $SU(2)$ may be
realized as a three-dimensional sphere embedded in $\mathbb{R}^4$; in particular,
it is /simply connected/.)
#+end_statement

If we take $\alpha,\beta,\gamma,\delta \in \mathbb{C}$, and create a special unitary matrix, we have 
the relationships

 * $|\alpha|+|\beta| = 1$
 * $\alpha\delta-\beta\gamma = 1$
 * $\alpha\overline{\gamma} + \beta\overline{\delta} = 0$

that are solved by $\alpha = \overline{\delta}$ and $\beta = -\overline{\gamma}$, while the first one gives us the
sphere condition.

***** III. Rings and modules [3/15]
****** III.1. Definition of a ring
******* DONE Exercise III.1.1
#+begin_statement
Prove that if $0=1$ in a ring $R$, then $R$ is a zero-ring.
#+end_statement

By definition,

\[
r = 1r = 0r = 0r+0r = 0.
\]

****** III.5. Modules over a ring
******* DONE Exercise III.5.4
#+begin_statement
Let $R$ be a ring. A nonzero $R\text{-module}$ is /simple/ (or /irreducible/) if its
only submodules are $\left\{ 0 \right\}$ and $M$. Let $M,N$ be simple modules, and let
$\varphi \colon M \to N$ be a homomorphism of $R\text{-modules}$. Prove that either $\varphi = 0$, or
$\varphi$ is an isomorphism. (This rather innocent statement is known as *Schur's 
Lemma*.)
#+end_statement

The kernel and image of $\varphi$ will be submodules, they only can be the total
submodule or the zero submodule, as $M,N$ are simple modules. There are two 
cases

  - If $\operatorname{ker}\phi = \{0\}$, it is a monomorphism. Its image must be different
    from $0$, thus it must be $N$.

  - If $\operatorname{ker} \phi = M$, we have $\phi = 0$.

****** III.6. Products and coproducts in R-Mod
******* CHECK Exercise III.6.16
#+begin_statement
Let $R$ be a ring. A (left-)$R\text{-module}$ $M$ is /cyclic/ if $M = \left\langle m \right\rangle$ for
some $m \in M$. Prove that simple modules (cf. Exercise 5.4) are cyclic.
Prove that an $R\text{-module}$ $M$ is cyclic if and only if $M \cong R/I$ for some
(left-)ideal $I$. Prove that every quotient of a cyclic module is cyclic.
#+end_statement

******** Un mÃ³dulo simple es cÃ­clico
Tomemos un elemento suyo cualquiera y
creamos $<m>$. Ocurre que debe ser un submÃ³dulo y por ser simple, todo
el mÃ³dulo.

******** Un cociente por ideal es cÃ­clico.
 Sea $M = R/I$, un mÃ³dulo sobre $R$ podemos generarlo simplemente 
 por $<1>$, luego es cÃ­clico.
 Sea $M=<m>$ un mÃ³dulo cÃ­clico. Podemos tomar un isomorfismo que lleve
 $r \mapsto rm$ y definir $I = \{r\;|\;rm=0\}$. Por 1er Teorema de isomorfÃ­a:

 \[M \cong R/ker(\phi) \cong R/I\]

******** Todo cociente de cÃ­clico es cÃ­clico.
Usando el tercer teorema de isomorfÃ­a:

\[\frac{\frac{R}{I}}{J} \cong \frac{\frac{R}{I}}{\frac{I+J}{I}} \cong \frac{R}{I+J}\] (?)

****** III.7. Complexes and homology
******* TODO Exercise III.7.1. Exactitud entre ceros.

 \[ \dots \overset{}\longrightarrow 0 \overset{}\longrightarrow M \overset{}\longrightarrow 0 \overset{}\longrightarrow \dots \]

 Tenemos que el nÃºcleo de la segunda debe ser igual a la imagen de la primera y
 por tanto, cero. Eso sÃ³lo es posible si $M=0$.

******* TODO Exercise III.7.2. Exactitud entre isomorfÃ­as
     \[ \dots \overset{}\longrightarrow 0 \overset{}\longrightarrow M \overset{}\longrightarrow M' \overset{}\longrightarrow 0 \longrightarrow \dots\]
 Tenemos por el primer 0 la funciÃ³n inyectiva y por el segundo la funciÃ³n 
 sobreyectiva. Debe ser por tanto isomorfismo.

******* TODO Exercise III.7.3. Kernel y cokernel en secuencia exacta
     \[ \dots \overset{}\longrightarrow 0 \overset{}\longrightarrow L \overset{}\longrightarrow M 
     \overset{\phi}\longrightarrow M' \longrightarrow N \longrightarrow 0 \longrightarrow \dots \]
 Por el primer 0 tengo una inyecciÃ³n $i$ de $L$ en $M$, que lo identifica con
 $im(i) = ker(\phi)$. Del segundo 0 tengo que la imagen de la proyecciÃ³n $\pi$ de
 $M'$ en $N$ es todo $N$. Entonces, por teorema de isomorfÃ­a y por exactitud:

 \[N = im(\pi) \cong \frac{M'}{ker(\pi)} = \frac{M'}{im(\phi)} = coker(\phi)\]

******* TODO Exercise III.7.4. Hotel de Hilbert
 Dada una secuencia de enteros, podemos moverla un paso a la derecha:

 \[(a_1,a_2,a_3,\dots) \longrightarrow (0,a_1,a_2,\dots)\] 

 Para tener el morfismo $\alpha$ que nos da la secuencia exacta:

 \[ 0 \overset{}\longrightarrow \mathbb{Z}^{\oplus \mathbb{N}} \overset{\alpha}\longrightarrow \mathbb{Z}^{\oplus \mathbb{N}} \overset{\pi_1}\longrightarrow \mathbb{Z} \overset{}\longrightarrow 0 \]

 Dada una secuencia de enteros, podemos moverla a los sitios pares y hacer
 proyecciÃ³n de los impares luego:

 \[(a_1,a_2,a_3,\dots) \longrightarrow (0,a_1,0,a_2,0,a_3,\dots)\]

 Para tener los morfismos $\beta$ y $\pi$ que nos dan la secuencia exacta:

 \[ 0 \overset{}\longrightarrow \mathbb{Z}^{\oplus \mathbb{N}} \overset{\beta}\longrightarrow \mathbb{Z}^{\oplus \mathbb{N}} \overset{\pi}\longrightarrow \mathbb{Z}^{\oplus \mathbb{N}} \overset{}\longrightarrow 0 \] 

******* TODO Exercise III.7.5. Exactitud entre noetherianos
 Tenemos la secuencia exacta:

 \[ \dots \overset{}\longrightarrow L \overset{\alpha}\longrightarrow M \overset{\beta}\longrightarrow N \overset{}\longrightarrow \dots \]

 Y supongamos $L,N$ noetherianos. Sea entonces una sucesiÃ³n de ideales \(\{M_i\}\),
 tenemos que las sucesiones de ideales \(\{\alpha^{-1}(M_i)\}\) y \(\{\beta(M_i)\}\) se estabilizarÃ¡n
 a partir de un cierto $j$. Tomando un $i > j$ tendremos que $M_i = M_{i+1}$ y por tanto,
 se estabilizarÃ¡ la secuencia inicial.

 Supongamos que existiera $x \in M_{i+1}$ pero $x \notin M_i$. Dividimos en dos casos:

 *Caso 1*. $x \in im(\alpha)$, tendrÃ­amos que existirÃ­a algÃºn elemento 
 $a \in \alpha^{-1}(x) \subset \alpha^{-1}(M_{i+1}) = \alpha^{-1}(M_{i})$, pero por definiciÃ³n entonces $x = \alpha(a) \in M_i$.

 *Caso 2*. $x \notin im(\alpha)$, tendrÃ­amos $\beta(x) \in \beta(M_{i+1})$. Existe un $y \in M_i$ tal que 
 $\beta(y) = \beta(x)$, es decir, $x-y \in ker(\beta)$. Pero entonces $x-y \in im(\alpha)$ y por tanto,
 $x-y \in M_i$, llevando a $x\in M_i$.

******* TODO Exercise III.7.6. Epimorfismo escindido
 Sea una sucesiÃ³n:

 \[ 0 \overset{}\longrightarrow ker(\phi) \overset{}\longrightarrow M \overset{\phi}\longrightarrow N \overset{}\longrightarrow 0 \]

 Supongamos que *escinde*, entonces $\phi$ es la proyecciÃ³n hacia $N$ y tiene
 como inversa derecha a la inclusiÃ³n.

 Supongamos que *tiene inversa* derecha $\psi$, entonces buscamos un isomorfismo
 entre $M \cong ker(\phi) \oplus N$, que tenemos con estos dos morfismos:

 \[(k,n) \mapsto \psi n + k\]
 \[m \mapsto (m-\psi \phi m, \phi m)\]

******* TODO Exercise III.7.10. Lema corto de los cinco 
 Si en el lema de la serpiente son $\lambda$ y $\nu$ isomorfismos, tenemos la sucesiÃ³n:

 \[0 \longrightarrow 0 \longrightarrow ker(\mu) \longrightarrow 0 \overset{\delta}\longrightarrow
   0 \longrightarrow coker(\mu) \longrightarrow 0 \longrightarrow 0\]

 Por tanto, el kernel y cokernel de $\mu$ son nulos y es isomorfismo.

******* TODO Exercise III.7.11. Todo morfismo de escisiÃ³n es isomorfismo
 Directamente aplicando el ejercicio anterior, tenemos que $N \cong M_1 \oplus M_2$.

******* TODO Exercise III.7.12. Lema de los cuatro (1)
 Lo probamos por caza del diagrama. Primero tomamos un elemento en el nÃºcleo
 de C y aplicamos:

 - Inyectividad de $\delta$. 
 - Exactitud de $BCD$.
 - Exactitud de $ABC$.
 - Sobreyectividad de $\alpha$.
 - Exactitud de $ABC$.

 Teniendo que el elemento es nulo.

******* TODO Exercise III.7.13. Lema de los cuatro (2)
 Volvemos a cazar diagramas. Tomamos un $c'$ en $coker(\gamma)$ y hacemos:

 - Exactitud de CDE.
 - Inyectividad en E.
 - Sobreyectividad de D.
 - Exactitud de CDE.

 Y asÃ­ llegamos a un $z \in C$ que tiene como imagen un $z' \in C'$. Tomamos $c'-z'$,
 que tiene imagen nula en $D$ y aplicamos:

 - Exactitud de BCD.
 - Sobreyectividad en $B$.

 Y obtenemos un $x \in C$ que tiene como imagen a $c'-z'$. Finalmente: $\gamma(x+z) = c'$.

******* TODO Exercise III.7.14. Lema de los cinco
 Trivial uniendo ambos lemas de los cuatro.

***** VI. Linear Algebra [0/13]
****** VI.1. Free modules revisited
******* TODO Exercise VI.1.1. R y C son isomorfos como espacios vectoriales de Q
Sabemos que $C \cong R \oplus R$. Dada una base $B$ de $\mathbb{R}$, podemos ver que serÃ¡
infinita y por axioma de elecciÃ³n isomorfa a $B+B$, que serÃ¡ a su
vez una base de $\mathbb{R}^2$. Luego $\mathbb{R} \cong \mathbb{R} \oplus \mathbb{R}$.

******* TODO Exercise VI.1.4. Ãlgebras de Lie
 Demostramos que $[u,v] = -[v,u]$. Ya que tenemos:

 $$[u,v] + [v,u] = [u,v] + [u,u] + [v,v] + [v,u] = [u+v,v] + [u+v,u] = [u+v,u+v] = 0$$

 Para todas las K-Ã¡lgebras, tomar $[v,w] = vw-wv$ nos da un Ã¡lgebra de Lie.
 Podemos verlo porque cumple las tres primeras propiedades que se le piden a un
 Ã¡lgebra de Lie y ademÃ¡s:

 \begin{align*}
 [[u,v],w] + [[v,w],u] + [[w,u],v] & = \\
 (uvw-vuw-wuv+wvu) &+\\
 (vwu-wvu-uvw+uwv) &+\\
 (wuv-uwv-vwu+vuw) &=\\
 0
 \end{align*}

******* TODO Exercise VI.1.5. Sistemas generadores e independientes en dominios de integridad
 Un sistema independiente puede no crecer a base y un sistema generador
 puede no reducirse a base en un dominio de integridad. Como ejemplos
 tenemos $\mathbb{Z}$ con: {2} como sistema independiente y $\{2,3\}$ como sistema generador.
 Ninguno puede crear base porque las Ãºnicas bases posibles serÃ­an $\{1\}$ y $\{-1\}$.

******* TODO Exercise VI.1.13. Un grupo abeliano con endomorfismos de caracterÃ­stica 0.
 Si tiene endomorfismos que forman un cuerpo de caracterÃ­stica 0, podemos
 identificar $\mathbb{Z}$ con los endomorfismos por propiedad universal y
 luego podemos extenderlo por contener $Q$ las inversas. De otro modo, 
 $Q$ es inicial en la categorÃ­a de cuerpos de caracterÃ­stica 0, asÃ­, hay
 forma de identificarlo con endomorfismos del cuerpo.

 AsÃ­, nuestro grupo $A$ es espacio vectorial sobre $Q$. Y es de dimensiÃ³n 1,
 porque si tuviera dimensiÃ³n mayor y una base de mÃ¡s de un elemento, colapsar
 dos elementos de la base en uno serÃ­a un endomorfismo sin inversa.

******* TODO Exercise VI.1.14. La potencia de un isomorfismo estabiliza kernel e imagen.
 Tenemos que $ker(\phi^n) \subset ker(\phi^{n+1})$ y que dos subespacios contenidos de la misma
 dimensiÃ³n deben ser iguales. Por tanto, la dimensiÃ³n debe crecer o estabilizarse
 a cada paso. Si la dimensiÃ³n es finita debe estabilizarse en algÃºn punto.

 Por otro lado, tenemos que las imÃ¡genes deben estabilizarse en dimensiÃ³n
 para tener $ker(\phi^n) \oplus im(\phi^{n+1}) = V$. Y entonces, para que el kernel no crezca,
 ninguno de los vectores que forman la base de $im(\phi^n)$ pueden tener como
 imagen algo que estÃ© en $ker(\phi)$, asÃ­ que vuelven a tener como imagen algo en
 $im(\phi^{n+1})$, que debe estar contenido en $im(\phi^n)$ y ser de la misma
 dimensiÃ³n.

****** VI.2. Homomorphisms of free modules I
******* TODO Exercise VI.2.1. Grupo isomorfo a la suma
 Tenemos que:

 \[
 \left( \begin{matrix} 1 & 0 \\ r & 1 \end{matrix} \right)
 \left( \begin{matrix} 1 & 0 \\ p & 1 \end{matrix} \right) =
 \left( \begin{matrix} 1 & 0 \\ r+p & 1 \end{matrix} \right)
 \]

 Luego la proyecciÃ³n del tercer elemento es un isomorfismo
 de grupos.

******* TODO Exercise VI.2.6. Row echelon form
 Cuando trabajamos en un cuerpo podemos pasar a /row echelon form/ usando
 los siguientes pasos:

  - Pasamos el primer elemento no nulo a la fila mÃ¡s alta.
  - Lo hacemos uno con su inversa y reducimos toda la columna restante.
  - Hacemos lo mismo con la submatriz a la derecha y debajo de ese 1.

 Esto debe dejarnos sÃ³lo ceros debajo y encima de los 1 pivotes.

****** VI.4. Presentations and resolutions
******* TODO Exercise VI.4.1. Tor(M) es submÃ³dulo de M cuando R es dominio de integridad.
 Tenemos $Tor(M) = \{ m | \exists r \in R : r \neq 0, rm = 0\}$, y siendo dos elementos $m,n$ en $Tor(M)$, 
 que cumplen que $rm = 0$ y $qn = 0$, podemos
 ver que su suma serÃ¡ cerrada y que el producto por $r\in R$ serÃ¡ cerrado cuando
 $R$ es conmutativo:

  - $rq(m+n) = rqm+qrn = 0+0 = 0$
  - $r(pm) = p(rm) = 0$

 Usando aquÃ­ que es dominio de integridad y por tanto $rq \neq 0$.

******* TODO Exercise VI.4.2. Hom(M,N) es libre de torsiÃ³n cuando lo es N.
 Supongamos que no lo fuera, existirÃ­a un $f \in Hom_R(M,N)$ tal que 
 $rf = 0$ para algÃºn $r$ no divisor de $0$. Pero entonces, esto harÃ­a
 que en el anillo $N$ existiese $rf(m) = 0$ para cualquier $m$, y por 
 ser libre de torsiÃ³n, se tendrÃ­a $f(m) = 0$ para todo $m$.
 Luego $f=0$.

 En particular $Hom_R(M,R)$ es libre de torsiÃ³n.

******* TODO Exercise VI.4.4. Propiedades del anulador
 Suponiendo $p,q \in Ann(R)$, tenemos que para todo $m \in M$ se tendrÃ¡
 $pm=0$ y $qm=0$. Por lo tanto $(p+q)m=0$ y $rpm = 0$, haciÃ©ndolo ideal.

******** M de torsiÃ³n si y sÃ³lo si el anulador es no nulo.
 Si $Ann(M) \neq 0$, existe un elemento de $R$ que anula todo $M$, como
 ademÃ¡s $R$ es dominio de integridad, este elemento no serÃ¡ divisor de 0, y $M$
 serÃ¡ torsiÃ³n. Si $M$ es torsiÃ³n y finitamente generado, tendrÃ¡ un elemento
 $r_i$ que anularÃ¡ cada uno de sus generadores $m_i$. Siendo $R$ conmutativo,
 el elemento producto estarÃ¡ en el anulador

                      \[
 \prod_{i} r_i
 \] 

 NÃ³tese que si quitamos la condiciÃ³n de que $M$ sea finitamente generado, existen
 mÃ³dulos como \(\mathbb{Z}_2 \oplus \mathbb{Z}_4 \oplus \mathbb{Z}_8 \dots\) que son torsiÃ³n porque todo elemento se anula pero
 tienen anulador vacÃ­o porque no existen elementos que anulen todo el mÃ³dulo.

******* TODO Exercise VI.4.13. Complejo de Koszul

******** Es un complejo.
 Comprobamos que es un complejo viendo que las siguientes composiciones son $0$:

  - \(d_1 \circ d_2 (t) = bta - atb = 0\)
  - \(\pi \circ d_1 (r,s) = (ra+sb)\ mod(a,b)) = 0 \)

******** Es un complejo exacto cuando la secuencia es regular.
 Y comprobamos que es exacto en el caso en el que la secuencia es regular viendo
 que:

 - \(ker(d_2) = 0\), ya que $a$ no es divisor de cero.
 - \(ker(d_1) = <(b,-a)>\). Tenemos que $b$ no es divisor de cero mÃ³dulo $a$, asÃ­, para que
   sea linealmente dependiente con $a$ necesitamos algo que sea cero mÃ³dulo $a$. Este
   caso requiere $s$ mÃºltiplo de $a$. Esto requiere estar dentro del ideal generado por
   $(b,-a)$.
 - Que la imagen de $d_1$ es el nÃºcleo de $\pi$ y que la proyecciÃ³n es sobreyectiva
   es trivial.

******* TODO Exercise VI.4.14. Complejo de Koszul en el caso de 3 elementos
******** Es un complejo
 Volvemos a comprobar que las composiciones son nulas. Tenemos de hecho que:

 \[d_2 \circ d_1 = d_3 \circ d_2 = 0\]

 Y que la proyecciÃ³n coincide con el generado por $d_1$.

******** Es un complejo exacto cuando la secuencia es regular
 Otra vez, como $c$ no es divisor de cero mÃ³dulo $(a,b)$, tenemos que el kernel
 de $d_3$ es nulo. De la misma forma, se tiene que el $ker(d_2)=im(d_3)$, aplicando
 en cada caso el no ser divisor de cero. Vuelve a tenerse una ecuaciÃ³n similar
 que demuestra $ker(d_1) = im(d_2)$. El caso de la proyecciÃ³n es trivial.
******* TODO Exercise VI.4.15. ResoluciÃ³n de Z sobre Z[x,y]
 Podemos encontrar una resoluciÃ³n como:

 \[0 \longrightarrow 
 \mathbb{Z}[x,y] \overset{\phi} \longrightarrow 
 \mathbb{Z}[x,y]^2 \overset{\delta} \longrightarrow 
 \mathbb{Z}[x,y] \overset{\pi} \longrightarrow 
 \mathbb{Z} \longrightarrow 0 \]

 Donde $\pi$ es un morfismo que cancela $x,y$. $\delta$ es un morfismo que lleva
 cada una de las copias del $1$ a $x$ e $y$. Finalmente, $\phi$ es monomorfismo
 que lleva $1$ a $(y,-x)$ que es generador de $ker(\delta)$.

***** VIII. Linear algebra, reprise [0/5]
****** VIII.1. Preliminaries, reprise
******* TODO Exercise VIII.1.2. Funtor plenamente fiel respeta isomorfÃ­as 
 Sea ${\cal F}(A) \cong {\cal F}(B)$, gracias a dos morfismos inversos $\alpha,\beta$. Como
 el funtor es pleno, existen dos morfismos preimagen de ambos
 llamados $\alpha',\beta'$ y tenemos que:

 \[{\cal F}(\alpha' \circ \beta') = \alpha \circ \beta = 1\]

 Por ser fiel, debemos tener $\alpha' \circ \beta' = 1$.
******* TODO Exercise VIII.1.3. AcciÃ³n de grupo como funtor
 Sea $G$ un grupo. Su acciÃ³n sobre un objeto $C$ serÃ¡ un morfismo que
 envÃ­e cada elemento del grupo a un isomorfismo de $C$. Es decir, un
 homomorfismo de grupos:

 \[(G,\ast) \longrightarrow (Aut(C),\circ)\]

 Pero como podemos ver $G$ como un objeto tal que cada uno de sus elementos
 sea un isomorfismo, tenemos claramente un isomorfismo:

 \[(Aut(G),\circ) \cong (G,\ast) \longrightarrow (Aut(C),\circ)\]

 Y podemos definir el funtor que lleva $G$ a $C$ y que lleva cada endomorfismo
 de $G$ a uno de $C$.

******* TODO Exercise VIII.1.17. CompleciÃ³n de un Ã¡lgebra
 Tenemos que los $R/I^n$ son mÃ³dulos en R-Mod, por tanto, la cadena siguiente
 tendrÃ¡ lÃ­mite. Donde los morfismos serÃ¡n las inclusiones naturales:

 \[\dots \longrightarrow R/I^3 \longrightarrow R/I^2 \longrightarrow R/I \]

 Ese lÃ­mite lo llamamos $R_I$, y es el submÃ³dulo de secuencias coherentes de $\prod_i R/I^i$.
 Es decir, un elemento suyo es una secuencia tal que cada elemento es la proyecciÃ³n
 del siguiente. Este submÃ³dulo es conmutativo porque lo es el producto de todos los mÃ³dulos.

 Podemos incluir $R$ en $R_I$ llevando el $1$ a $(1,1,1,\dots)$. Y esto conmutarÃ¡ con las
 proyecciones naturales que nos daba la propiedad universal.
 Para que $x$ se anule al incluirlo en $R_I$ desde $R$, necesitamos que todas las proyecciones
 de su imagen sean $0$, asÃ­ que necesitamos que pertenezca a $I_n$ para cada $n$.

******* TODO Exercise VIII.1.19. Enteros p-Ã¡dicos
 Llamamos enteros p-Ã¡dicos al lÃ­mite $\mathbb{Z}_p = \varprojlim \mathbb{Z}/p^i\mathbb{Z}$, y nÃºmeros p-Ã¡dicos a su cuerpo de fracciones
 $\mathbb{Q}_p$. Por definiciÃ³n, un entero p-Ã¡dico es una secuencia de enteros $\{a_i\}$ tales que:

 \[ a_s \equiv a_r  \mod (p^s)\]

 Para cualesquiera $s \leq r$. De otra forma, cada entero tiene una expansiÃ³n Ãºnica:

 \[ A = b_0 + b_1 p + b_2 p^2 + b_3 p^3 + \dots\]

 Donde $b_i < p$. Esto es asÃ­ porque dada una secuencia $(a_i)$, tenemos la igualdad:

 \[b_0 = a_0\]
 \[b_i p^i + a_{i+1} = a_i\]

 Y se puede construir una desde la otra usando que $a_i - a_{i+1} \equiv_{p^i} 0$. 

 A partir de aquÃ­ podemos hacer aritmÃ©tica como usualmente desde estos desarrollos de los
 nÃºmeros p-Ã¡dicos.
****** VIII.2. Tensor products, and the Tor functors
******* TODO Exercise VIII.2.14. Tor en 0 es el producto tensor
 La definiciÃ³n inicial de Tor es como:

 \[Tor^R_i(M,N) = H_i(M_\bullet \otimes N)\]

 Y como tenemos que el complejo $M_\bullet \otimes N$ es el siguiente,
 siendo $S_0$ una base de $M$, y $S_1$ base de las relaciones de $M$:

 \[ \dots \overset{}\longrightarrow N^{\oplus S_2} 
 \overset{\phi_2}\longrightarrow N^{\oplus S_1} 
 \overset{\phi_1}\longrightarrow N^{\oplus S_0} 
 \overset{}\longrightarrow 0 \]

 Que ha salido de tensar el siguiente complejo exacto:

 \[ \dots \overset{}\longrightarrow R^{\oplus S_1} \overset{\psi_2}\longrightarrow R^{\oplus S_0} \overset{\psi_1}\longrightarrow M \overset{}\longrightarrow 0 \]

 Tenemos que:

 \[H_i(M_\bullet \otimes N) \cong \frac{N^{\otimes S_0}}{im(\phi_1)} \cong 
 \frac{R^{\otimes S_0}}{im(\psi_2)} \otimes N \cong M \otimes N \]

 Donde usamos la exactitud de la segunda secuencia con el primer teorema de isomorfÃ­a
 y el hecho de que el functor $\otimes N$ respeta los colÃ­mites y por tanto el cociente, que puede
 verse como coecualizador.
*** An introduction to homological algebra - Rotman
**** 1. Introduction
***** 1.1. Simplicial Homology
****** Motivation: Green's Theorem
******* Original statement
Let $C$ be a positively oriented, smooth and simple closed curve in
a plane; being $D$ the region bounded by $C$. If $L,M$ have continuous
partial derivatives in $D$, then:

\[ \oint_C (L dx + M dy) = 
\iint_D \left(
  \frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}
\right) dx dy\]

******* A rewrite
If we have some "bad points" that we want to delete from $C$.
We can define multiple $\gamma_i$ around them and have our integral to be:

\[ \oint_C (L dx + M dy) +
\sum^n_{i=1} \left( \int_{\gamma_i} L dx + Q dy \right) 
= \iint_D \left(
  \frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}
\right) dx dy\]

As the diagram is:

[[./images/greentheorem.png]]

In this setting, the notion of $\mathbb{Z}$ linear combinations of paths
makes sense. We can take the free abelian group $G[Y]$ with $Y$ being
the set of paths $\gamma : [0,1] \longrightarrow X$.

******* An equivalence relation
For functions satisfying $\frac{\partial Q}{\partial x} = \frac{\partial P}{\partial y}$, the double integral dissapears,
and we have:

\[ \int_{m\gamma + \sum_i m_i\gamma_i} P dx + Q dy = 0\]

Here we can define an equivalence relation between pairs of paths,
where $\beta \sim \beta'$ if:

\[ \int_\beta P dx + Q dy = \int_{\beta'} P dx + Q dy \]

The equivalence class of $\beta$ is called its *homology class*.

****** Boundaries
If we take the simplices to form abelian groups, the boundaries
are homomorphisms.

[[./images/rectangle.png]]

For instance, if we can take this rectangle and compute its boundary.
We use free abelian groups of $n\text{-simplexes}$, called $C_n(X)$.

******* Boundary of a triangle
We use the minus sign to denote the inverse path, and we have:

\[ \delta([a,b,c]) = [a,b] + [b,c] - [a,c]\]

******* Boundary of the boundary of a triangle
As the double boundary is the boundary of a sphere, it is 
automatically null:

\[
\delta(\delta([a,b,c])) = (a - b) + (b - c) - (a - c) = 0
\]

******* Boundary of the rectangle
Now, we can compute the boundary of the rectangle; assuming that
the boundary function is a homomorphism preserving the union:

\[\begin{aligned}
\delta(\square) &=  \delta[a,b,c] + \delta[a,c,d] \\ 
&= [a,b]+[b,c]-[a,c]+[a,c]+[c,d]-[a,d] \\
&= [a,b]+[b,c]+[c,d]-[a,d]
\end{aligned}\]

****** Simplicial boundary maps
Let $X$ be a finite simplicial complex. We define:

\[ \delta_n [v_0,\dots,v_n] 
= \sum^n_{i=0} (-1)^i [v_0,\dots,\hat{v_i},\dots,v_n]\]

being a map from $C_n(X)$ to $C_{n-1}(X)$. We define $\delta_0 = 0$ as a convention.

****** Boundary maps are exact
For all $n > 0$, 

\[\delta_{n-1}\delta_n = 0\]

******* Proof
We can see that, for every pair of indexes, we have the same term 
twice, depending on whether we take the two indexes ordered or using
an inverse order:

\[ 
[x_0,\dots,\hat{x_i},\dots,\hat{x_j},\dots,x_n](-1)^{i+(j-1)} +
[x_0,\dots,\hat{x_i},\dots,\hat{x_j},\dots,x_n](-1)^{j+i} = 0
\]

****** Simplicial cycles and boundaries
The elements in $Z_n(X) = \ker \delta_n \subset C_n(X)$ are called *simplicial cycles*.
The elements in $B_n(X) = \im \delta_{n+1} \subset C_n(X)$ are called *simplicial 
boundaries*.

****** Exactness for cycles and boundaries
For all $n$,

\[ B_n(X) \subseteq Z_n(X)\]

******* Proof
It is trivial knowing that boundary maps are [[*Boundary maps are exact][exact]].

****** Simplicial homology group
The nth simplicial homology group of a finite simplicial complex is:

\[ H_n(X) = Z_n(X) / B_n(X) \]

What survives in this group are the cycles that are not boundaries;
that is, the boundaries of punctured sections.

****** Two modifications
We can consider *homology* with coefficients in $G$ by tensoring the
sequence of chain groups by $G$ and taking homology groups. We can
consider the *cohomology* with coefficients in $G$ applying $Hom(-,G)$
to the chain of groups and then taking homology groups.

***** 1.2. Categories and Functors
****** 1.2.1. Russell's paradox
The Russell paradox is solved in with the Zermelo-Fraenkel axioms,
specifically, the *axiom of comprehension*. It says that any definable
subclass of a set is a set; restricting the comprehension to only
already defined sets.

****** 1.2.2. Classes and sets
A class in ZFC is called *small* if it has a cardinal number. A *set*
is only a small class. In this book, we only worry about classes and
sets that are not a member of themselves.

# A cardinal number?
# More details in Mac Lane, Categories for the working mathematician.

****** 1.2.3. Categories
A category ${\cal C}$ consists of:

 - $obj({\cal C})$, a class of objects.
 - $Hom(A,B)$, a set of morphisms for every ordered pair $(A,B)$.
 - $\circ : Hom(A,B) \times Hom(B,C) \longrightarrow Hom(A,C)$, composition of functions.

****** 1.2.4. Axioms of categories
A category has disjoint $Hom$ sets, and there must be an identity element
$1_A \in Hom(A,A)$ for every morphism, following these rules:

 - The identity is a neutral element: $f \circ 1_A = f$ and $1_B \circ f = f$.
 - Composition is associative: $f \circ (g \circ h) = (f \circ g) \circ h$

****** 1.2.5. Examples of categories
******* Sets
******* Groups
******* Partially ordered sets
******* Inclusion of open sets
******* Topological spaces
******* Abstract simplicial complexes
******** Abstract simplicial complexes
We denote =Abs= the category of abstract simplicial complexes.
An abstract simplicial complex $K$ is a set of *vertices* $Vert(K)$ and
a family of nonempty finite subsets called *simplexes* 
$\sigma \subseteq Vert(K)$ such that:

 1. $\{v\}$ is a simplex for every $v \in Vert(K)$.
 2. Every subset of a simplex is a simplex.

******** Simplicial maps
A *simplicial map* is a function $\phi : Vert(K) \longrightarrow Vert(L)$ 
such that, if $\sigma$ is a simplex in $K$, then $\phi(\sigma)$ is a simplex
in $L$.

******** Dimension
A simplex with $|\sigma| = n+1$ is called a *n-simplex*. Simplicial
maps don't have to preserve dimension.

******* Nerves
If ${\cal U} = \{U\}_{i\in I}$ is the cover of a topological space, we define an 
abstract simplicial complex ${\cal N}({\cal U})$ having vertices $Vert({\cal N}({\cal U})) = {\cal U}$
and simplexes $\{U_0,\dots,U_n\} \subseteq {\cal U}$ such that:

\[ \bigcap_{k=0}^n U_k = \varnothing\]

******* Monoids
******* Homotopy category
****** 1.2.6. Algebraic examples of categories
******* Abelian groups
******* Rings (unital)
******* Commutative rings
****** 1.2.7. Modules
A left R-module, where $R$ is a ring, is an additive abelian group $M$
with a scalar multiplication $R \times M \longrightarrow M$, such that:

 1. $r(m+m') = rm+rm'$
 2. $(r+r')m = rm + r'm$
 3. $(rr')m = r(r'm)$
 4. $1m = m$

A right module is defined anagously.

****** 1.2.8. Examples of modules
******* Vector spaces over a field
******* Abelian groups over Z
******* Every ring over itself
******* Every ring over its center

****** 1.2.9. Homomorphisms of R-modules
A function $f : M \longrightarrow N$ such that:

 1. $f(m+m') = f(m)+f(m')$
 2. $f(rm) = rf(m)$

In the case of right modules, we can define them anagously.

******* The composite and inverse of homomorphisms is an homomorphism
Trivial.

****** 1.2.10. Examples of homomorphisms
******* Linear transformations in vector spaces
******* Homomorphisms of abelian groups for Z-modules
******* Homothety
Let $M$ be an R-module, and $r \in Z(R)$; multiplication by $r$, $\mu_r$, is
an homomorphism because:

\[ \mu_r(am) = r(am) = a(rm) = a\mu_r(m)\]

****** 1.2.11. Opposite rings
If $R$ is a ring, its opposite ring $R^{op}$ is the same ring with the
opposite multiplication, defined by:

\[ \mu^o(r,t) = \mu(t,r)\]

****** 1.2.12. Categories of modules
We call $_RMod$ the category of *left* R-modules, and $Mod_R$ to the 
category of *right* R-modules.

****** 1.2.13. Subcategories
A category ${\cal S}$ is a subcategory of ${\cal C}$ when:

 1. $obj({\cal S}) \subseteq obj({\cal C})$.
 2. $Hom_S(A,B) \subseteq Hom_C(A,B)$.
 3. Identities and compositions are the same.

****** 1.2.14. Full subcategories
A full subcategory has $Hom_S(A,B) = Hom_C(A,B)$ for every $A,B \in obj({\cal S})$.

****** 1.2.15. Functors
A functor $T : {\cal C} \longrightarrow {\cal D}$ is a function such that:

 1. $T : obj({\cal C}) \longrightarrow obj({\cal D})$.
 2. $T : Hom(A,B) \longrightarrow Hom(TA,TB)$.
 3. Preserves composition: $T(f \circ g) = Tf \circ Tg$.
 4. Preserves identities: $T(1_A) = 1_{T(A)}$.

****** 1.2.16. Examples of functors
******* Subcategories as inclusion functors
******* Identity functor
******* Hom(A,-) functor
******* Chains as functors from the partially ordered integers
******* Forgetful functors

****** 1.2.27. Diagrams
A diagram is a functor whose domain is a *small category*; that
is $T : {\cal D} \longrightarrow {\cal C}$, where $obj({\cal D})$ is a set.

****** 1.2.28. Paths
A path is a functor $P : n+1 \longrightarrow {\cal C}$, where the domain is the partial 
ordering of integers $0,\dots,n+1$. A path is *simple* if the functor is
injective.

****** 1.2.29. Commutativity of diagrams
A diagram commutes if the composites of the labels on any two simple path
are equal.

****** TODO 1.2.30. Contravariant functors
****** TODO 1.2.31. Examples of contravariant functors
******* Hom(-,B) functor
******* The dual space functor
\[( )^\ast = Hom_k(-,k) : \sideset{_k}{}{Mod} \longrightarrow \sideset{_k}{}{Mod}\]
******* Order-reversing functions on partially ordered sets

******* Presheaves
If ${\cal U}$ is a topology with the inclusion, a contravariant functor 
${\cal P} : {\cal U} \longrightarrow {\cal C}$ is a presheaf.

****** 1.2.32. Faithful functors
A functor is faithful if all the functions 
$Hom(A,B) \longrightarrow Hom(TA,TB)$ are injective.
****** 1.2.33. Concrete categories
A category is concrete if there is a faithful functor ${\cal C} \longrightarrow \mathtt{Set}$.

****** 1.2.33. Opposite category
We define ${\cal C}^{op}$ to be the category with:

 - $obj({\cal C}^{op}) = obj({\cal C})$
 - $Hom_{{\cal C}^{op}}(A,B) = Hom_{\cal C}(B,A)$
 - $g \circ_{op} f = f \circ g$

****** 1.2.34. Isomorphisms
A morphism $f : A \longrightarrow B$ such that exists $g : B \longrightarrow A$ with
$f \circ g = 1$ and $g \circ f = 1$.

****** 1.2.35. Functors preserve isomorphisms
Let $T$ be a functor, if $f$ is an isomorphism, then $T(f)$ is an isomorphism.

******* Proof
If $g$ is its inverse, then:

\[ T(f)T(g) = T(fg) = 1\]
\[ T(g)T(f) = T(gf) = 1\]

If $T$ is a contravariant functor, the proof remains the same.

****** 1.2.36. Natural transformations
Let $F,G : {\cal A} \longrightarrow {\cal B}$ be covariant functors. A natural transformation
$\tau : F \Longrightarrow G$ is a family of morphisms $\tau_A : S A \longrightarrow T A$, making the following
diagram commute for every $f \in Hom(A,B)$:

\[ \begin{tikzcd}
FA \rar{\tau_A} \dar{Ff} & GA \dar{Gf} \\
FB \rar{\tau_B} & GB
\end{tikzcd} \]

We write the natural transformations as $Nat(F,G)$.

****** 1.2.37. Natural isomorphisms
A natural transformation $\tau$ for which each $\tau_A$ is an isomorphism.

****** 1.2.38. Composition of natural transformations
If $\tau : F \Longrightarrow G$ and $\sigma : G \Longrightarrow H$ are natural transformations, then the
composition is a natural transformation.

******* Proof
Composing the two commutative diagrams gives us the proof.

****** 1.2.39. Identity natural transformation
For any functor $F : {\cal A} \longrightarrow {\cal B}$, we can describe an identity natural 
transformation using the identity morphisms.

****** TODO 1.2.40. Examples of natural transformations
****** TODO 1.2.41. Natural transformations are proper classes
****** 1.2.42. Yoneda Lemma
Let $A \in obj({\cal C})$ and $G : {\cal C} \longrightarrow \mathtt{Set}$ be a covariant functor. 
There is a bijection:

\[ y : Nat(Hom_C(A,-), G) \longrightarrow G(A)\]

given by $y : \tau \longrightarrow \tau_A(1_A)$.

******* Proof
******** Every choice of p determines a natural transformation
Given $p \in GA$, we can create an unique natural transformation having
$\eta_A(1_A) = p$. A natural transformation has to obey the following 
commutative diagram:

\[\begin{tikzcd}
Hom(A,A) \rar{f \circ \_}\dar{\eta}& Hom(A,B)\dar{\eta} \\
GA \rar{Gf}& GB
\end{tikzcd}\]

Then, the image of $\eta_B(f)$ is determined.

\[\begin{tikzcd}
id \rar{f \circ \_}\dar{\eta}& f\dar{\eta} \\
p \rar{Gf}& (Gf)(p)
\end{tikzcd}\]

******** Every choice gives us a natural transformation
This gives us, in fact, a natural transformation which makes every
natural square to commute:

\[\begin{tikzcd}
Hom(A,B) \rar{g \circ \_}\dar{\eta}& Hom(A,C)\dar{\eta} \\
GB \rar{Gg}& GC
\end{tikzcd}\]

Given any element $f \in Hom(A,B)$, we can check the commutativity:

\[\begin{tikzcd}
f \rar{g \circ \_}\dar{\eta}& g \circ f\dar{\eta} \\
(Gf)(p) \rar{Gg}&  G(g \circ f)(p)
\end{tikzcd}\]

Knowing that $G(g \circ f)(p) = (Gg \circ Gf) (p)$.

****** 1.2.43. Representable functors
A covariant functor $F: {\cal C} \longrightarrow \mathtt{Set}$ is representable if $F \cong Hom(A,-)$
for some $A$.

****** 1.2.44. Yoneda Corollary
For $A,B \in obj({\cal C})$:

  1. If $\eta \in Nat(Hom(A,-),Hom(B,-))$, then $\eta = (\_ \circ \psi)$ for some unique $\psi$.
  2. If $\eta = (\_ \circ \psi)$ and $\tau = (\_\circ\phi)$, then $\tau\circ\eta = (\_ \circ \psi\circ\phi)$.
  3. $\eta = (\_\circ\psi)$ is a natural isomorphism iff $\psi$ is an isomorphism.

******* Proof
******** Corollary 1
If we apply Yoneda Lemma, every transformation is defined by
$\eta(id_A) = \psi$. The transformation has to be $(\_ \circ\psi)$ because of commutativity:

\[\begin{tikzcd}
Hom(A,A) \rar{f \circ \_}\dar{\eta}& Hom(A,C)\dar{\eta} \\
Hom(B,A) \rar{f \circ\_}& Hom(B,C)
\end{tikzcd}\]

So, given any element $f \in Hom(A,C)$, we have $\eta(f) = f \circ \psi$:

\[\begin{tikzcd}
id \rar{f \circ \_}\dar{\eta}& f\dar{\eta} \\
\psi \rar{f \circ\_}& f \circ \psi
\end{tikzcd}\]

******** Corollary 2
Trivial consequence of the first corollary.

******** Corollary 3
It is trivial given the previous corollaries and:

\[(\_\circ\psi^{-1})\circ \psi = (\_\circ id)\]

****** TODO Examples
****** TODO Yoneda Imbedding
***** 1.3. Singular Homology
****** 1.3.1. Hilbert spaces and euclidean spaces
A *Hilbert space* is the set ${\cal H}$ of all sequences $(x_i) \in \mathbb{R}$ such that
$\sum x_i^2 < \infty$. A *Euclidean space*, $\mathbb{R}^n$ is a subset of ${\cal H}$ consisting of
all sequences of the form $(x_0,x_1,\dots,x_{n-1},0,\dots)$.

****** 1.3.2. Standard n-simplex
The standard n-simplex is the set of all convex combinations:

\[\Delta^n = [e_0,e_1,\dots,e_n]\]

Where $e_i$ form an orthogonal basis.

****** 1.3.3. Singular n-simplex
Given a topological space $X$, a singular n-simplex is a continuous map
$\sigma : \Delta^n \longrightarrow X$.

****** 1.3.4. Singular n-chains
We define $S_n(X)$ as the free group with singular n-simplexes as basis.
By convention, $S_{-1}(X) = \{0\}$. The elements on this group are called
singular n-chains.

****** TODO 1.3.5. Face maps
The ith face map $\epsilon^n_i : \Delta^{n-1} \longrightarrow \Delta^n$ is defined by:

***** Exercises
****** Exercise 1.1
#+begin_statement
1. Prove, in every category ${\cal C}$, that each object $A \in {\cal C}$ has a unique identity
   morphism.
2. If $f$ is an isomorphism in a category, prove that its inverse is unique.
#+end_statement

We have $id = id \circ id' = id'$ and $\varphi^{-1} = \varphi' \circ \varphi \circ \varphi^{-1} = \varphi'$.

**** 2. Hom and Tensor
***** 2.1. Modules
****** Representation of a ring
A *representation* of $R$ is an homomorphism $\varphi : R \longrightarrow End_\mathbb{Z}(M)$.

******* Equivalence of representations and modules
The product of a R-module is a representation, and every
representation gives an R-module.

******** Equivalence of types
Type of a representation:

\[R \longrightarrow End(M)\]

Type of an R-module product:

\[R \times M \longrightarrow M\]

Both types are equivalent.

****** Example: Group ring
Given $G$, a group, and $R$, a ring; we define the group ring, $RG$ to be
the set of functions $G \longrightarrow R$ of finite support, with the operations:

  - Sum of functions: $(f+g)(a) = f(a)+g(a)$
  - Convolution (product): $(f\cdot g)(a) = \sum_{uv = a} f(u)g(v)$
  - Product by a scalar on $R$: $(kf)(a) = kf(a)$

It defines a ring and an R-module.

******* Inclusion of the group
The group can be included on $RG$ with the indicator function $y \mapsto 1_{(=y)}$,
defined as:

\[
y(a) = 1_{(=y)}(a) =
\left\{\begin{array}{ll} 
1 & \mbox{if } a = y  \\
0 & \mbox{if } a \neq y
\end{array} 
\right.
\]

A function that only outputs $1$ when its input is $y$.

******** Preservation of the product

\[
1_{(=y)}\cdot 1_{(=z)} (a)
=
\sum_{u\cdot v = a} 1_{u=y}1_{v=z}
=
1_{a=yz}
\]

****** Additive functors
A functor $T : \mathtt{RMod} \longrightarrow \mathtt{Ab}$ is called *additive* if, for every pair of R-maps,
$f,g$, we have:

\[
T(f+g) = Tf + Tg
\]

******* Properties of additive functors
Let $T : \mathtt{RMod} \longrightarrow \mathtt{Ab}$ be an additive functor:

  1. $T(0) = 0$, the zero map.
  2. $T(\{0\}) = \{0\}$, the zero group.

******** Proof
********* First property
$T0 = T(0+0) = T0+T0$, and then $T0 = 0$.

********* Second property
$Hom(A,\{0\})$ only has one element.

****** Homomorphisms of r-modules are abelian groups
Given $A,B$ R-modules, $Hom_{R-mod}(A,B)$ is an abelian group with the
componentwise sum.

****** Hom as an additive functor
$Hom_{R-mod}(A,-)$ is an additive functor.

******* TODO Central case

****** TODO Hom as a contravariant functor
****** Submodules
Given $M$, an R-module, a *submodule* $N \subseteq M$ is an additive subgroup
closed under scalar multiplication.

******* Examples of submodules
******** Subgroups
A submodule of a Z-module (abelian group) is a subgroup.

****** Quotient of modules
****** Kernels, images and cokernels
****** First Isomorphism Theorem
****** Second Isomorphism Theorem
****** Third Isomorphism Theorem
****** Correspondence Theorem
****** Simple module
A proper R-module $M$ is simple if it has no proper submodules.

******* Characterization
$M$ is simple iff $M \cong R/I$, where $I$ is a maximal left ideal.

******** TODO Proof

****** Exact sequences
****** Zero-ended exact sequences
****** Short exact sequences
****** External direct sum
******* Properties of the external direct sum
****** Internal direct sum
****** Direct summands and complements
******* Retractions
****** Direct product
******* Direct sum
******* Projections
******* Injections
****** Free modules
******* Basis
******* Free abelian groups
****** Basis
******* Invariant basis number
******* Rank
****** Left exactness
***** 2.2. Tensor products
****** Bilinearity
******* Biadditivity
****** Tensor product
******* Uniqueness
******* Existence
****** Tensor functors
****** Universal property of the tensor product
****** Commutativity
****** Enveloping algebra
****** Right exactness
****** Tensor and direct sum
****** Four Lemma I
****** Four Lemma II
****** Five Lemma
****** Divisible abelian group
***** 2.2.1. Adjoint isomorphisms
****** Adjoint isomorphisms
****** Right exactness
**** 3. Special Modules
***** 3.1. Projective modules
****** Exact functor
****** Lifting on free modules
****** Lifting
****** Projective modules
****** Characterization of projective modules
****** Projective modules and direct summands
****** Kaplansky theorem
****** Projective basis
****** Schnauel's Lemma
****** Ascending chaing condition
****** Noetherian rings
****** Characterization of noetherian rings
****** Hilbert basis theorem
***** 3.2. Injective modules
****** Injective module
****** Characterization of injective modules
****** Product of injective modules
****** Baer criterion
****** Divisible modules
****** Bass-Papp theorem
****** Characterization by short exact sequences
****** Essential extension
****** Characterizacion by essential extensions
****** Injective envelope
****** Eckmann-SchÃ¶pf
***** 3.3. Flat modules
****** Flat module
****** Direct sum of flat modules
****** Finitely generated submodules and flat modules
****** Torsion module
****** Torsion-free module
******* PID module
******* Flat modules
****** Character module
****** Lambek theorem
****** Villamayor theorem
****** Left coherent ring
****** Chase theorem
***** 3.3.1. Purity
****** Pure exact sequence
******* Pure submodule
****** Characterization of flatness
****** Cohn theorem
**** 4. Specific Rings
***** 4.2. Von Neumann Regular Rings
****** Von Neumann Regular ring
A ring $R$ is *Von Neumann regular* if:

\[
\forall r \in R: \exists r' \in R: rr'r = r
\]

******* Boolean rings
A ring is boolean if every element is idempotent. Every boolean ring
is a commutative Von Neumann regular ring.

**** 5. Setting the stage
***** 5.4. Sheaves
****** Protosheaves
 #+begin_definition
 *Local homeomorphism*. Continuous map $p : E \longrightarrow X$ such that for each $e \in E$ there is
 an open neighboorhood $S$ of $e$ such that $p|_S$ is an isomorphism.
 #+end_definition
 #+begin_definition
 *Protosheaf*. Surjective local homeomorphism.
 #+end_definition

****** Etale-sheaves
 #+begin_definition
 *Etale-sheaf of abelian groups*. A *protosheaf* such that:

 - The stalk $E_x$ is an abelian group.
 - Inversion and adition are continuous.
 #+end_definition

 #+begin_definition
 *Etale-map*. Given two etale-sheaves $E$ and $E'$, a map $\phi : E \longrightarrow E'$ such
 that $p'\phi = p$, and each $\phi|_{E_x}$ is a homomorphism.
 #+end_definition

 Here, etale-sheaves of abelian groups over a topological space X form an
 abelian category $\mathtt{Sh}_{et}(X,\mathtt{Ab})$.

***** 5.5. Abelian categories
****** Additive category
 #+begin_definition
 *Additive category*. ${\cal C}$ is additive if:

 - $Hom(A,B)$ is an *abelian group*.
 - *Distributivity* holds: $b \circ (f+g) = b\circ f + b \circ g$ and $(f+g)\circ a = f\circ a + g\circ a$.
 - Has a *zero object*.
 - Has finite *products* and *coproducts*.

 A functor $T$ between two additive categories is additive if $T(f+g) = Tf+Tg$.
 #+end_definition

 #+begin_theorem
 *Sums and products are the same*. Products and coproducts are isomorphic:

 \[A \mathbin{\Pi} B \cong A \amalg B\]

 So we call them *direct sums*, $A \oplus B$. And there are canonical morphisms:

 \[ \begin{tikzcd}
 & A \oplus B \dlar[bend right,swap]{\pi_A} \drar[bend left]{\pi_B} $ \\
 A \urar[bend right,swap]{i_A} & & B \ular[bend left]{i_B}
 \end{tikzcd} \]

 Such that: \(i_A \circ \pi_A + i_b \circ \pi_B = id\) and \(\pi_B \circ i_A = \pi_A \circ i_B = 0\).
 #+end_theorem

****** Monomorphisms and epimorphisms

#+begin_definition
*Monomorphism*. A morphism $u$ such that:
\[u \circ f = u \circ g \quad \Rightarrow \quad f = g\]
#+end_definition

#+begin_definition
*Epimorphism*. A morphism $u$ such that:
\[f \circ u = g \circ u \quad \Rightarrow \quad f = g\]
#+end_definition

We have that $u : B \longrightarrow C$ is *monomorphism* iff the induced 
$u^\ast : Hom(A,B) \longrightarrow Hom(A,C)$ is injective. And $v : B \longrightarrow C$ is *epimorphism* 
iff the induced $v^* : Hom(B,D) \longrightarrow Hom(C,D)$ is surjective.

****** Kernels and cokernels
 #+begin_definition
 *Kernel*. The kernel of $u$ is the equalizer of $u$ and $0$. In a diagram:

 \[ \begin{tikzcd}
 & C \dar[dashed] \arrow[ddr, bend left] \arrow[ddl,bend right] &\\
 & \ker(u) \dlar[swap]{i} \drar{0} & \\
 A \arrow[shift left]{rr}{u} \arrow[shift right]{rr}[swap]{0} & & B
 \end{tikzcd} \]
 #+end_definition
 #+begin_definition
 *Cokernel*. The cokernel of $u$ is the coequalizer of $u$ ans $0$. In a diagram

 \[ \begin{tikzcd}
 & C &\\
 & \ker(u) \uar[dashed]   & \\
 A \urar{0} \arrow[uur, bend left]
 \arrow[shift left]{rr}{u} \arrow[shift right]{rr}[swap]{0} & & 
 B \ular[swap]{\pi} \arrow[uul,bend right]
 \end{tikzcd} \]
 #+end_definition

 #+begin_theorem
 *Monomorphisms and kernels*.
 - If $\ker(u)$ exists, $u$ is monomorphism iff $ker(u) = 0$.
 - If $coker(v)$ exists, $v$ is epimorphism iff $coker(v) = 0$.
 #+end_theorem
****** Abelian category
 #+begin_definition
 *Abelian category*. ${\cal C}$ is abelian if

 - Every morphism has *kernel* and *cokernel*.
 - Every monomorphism is a *kernel*.
 - Every epimorphism is a *cokernel*.
 #+end_definition

 Abelian categories are /self-dual/, if ${\cal A}$ is an abelian category, then
 ${\cal A}^{op}$ is an abelian category.

 #+begin_definition
 *Image*. Given $f : A \longrightarrow B$ in an abelian category, its image is:

 \[img(f) = ker(coker(f))\]
 #+end_definition
*** Koszul Pairs and applications - Pascual Jara, DragoÅ Åtefan                                      :algebra:koszul:
**** Introduction
***** Koszul ring
*Koszul ring*. A graded ring $A$ is *Koszul* if $A^0$ is a semisimple ring 
and it has a resolution $P_\ast$ by projective graded left A-modules such 
that each $P_n$ is generated by homogeneous elements of degree $n$.

***** Graded ring
*Graded ring*. A ring that is a direct sum of abelian groups:

\[ A = \bigoplus_{n \in \mathbb{N}} A_n\]

such that $A_iA_j \subset A_{i+j}$.

****** Homogeneous Elements
A *homogeneous element* is an element of any factor $A_i$ of the 
decomposition.

*Example:* A polynomial ring $A = \mathbb{K}[x_1,x_2, \dots]$ is graded with $A_i$ 
being the abelian group of polynomials with only monomials of 
degree $i$.
# QUESTION: Do they admit a different gradation?
# We can take $A_i$ to be the group of polynomials of degree 
# *equal or less* than i!

***** Semisimple group
*Semisimple group*. A group is semisimple if it has no non-trivial 
normal abelian subgroups.

Different uses of this term can be found [[http://planetmath.org/semisimplegroup][here]].
# QUESTION: Which are we interested in?

***** Semisimple module
*Semisimple module*. It is a direct sum of simple modules, that is, 
they have no non-zero proper submodules.

***** Semisimple algebra
An associative finite dimensional algebra $A$ is *semisimple* if
$A$ is a direct product of simple algebras or equivalently, if $A$ has
trivial Jacobson radical.

**** 1. Almost-koszul pairs
***** 1.1. R-rings
****** R-Ring
*R-ring*. Associative and unital algebra. It is an associative and 
unital ring $A$ together with a morphism $u : R \longrightarrow A$.

****** Graded and connected R-rings
*Graded and connected R-rings*. A R-ring is graded if it is equipped 
with a decomposition:

\[A = \bigoplus_{n \in \mathbb{N}} A^n \]

such that multiplicaton $m^{p,q}$ maps $A^p \otimes A^q$ into $A^{p+q}$. It is *connected* 
when $A_0 = R$. It is *strongly graded* when $m^{1,p}$ is surjective. We 
call $\pi^n_A$ to the projection of $A$ onto $A^n$.

***** 1.2. R-corings
****** Definition of coalgebra
A [[https://en.wikipedia.org/wiki/Coalgebra#Formal_definition][coalgebra]] over a field $K$ is a *vector space* $V$ together with linear
maps $\Delta : V \longrightarrow V \otimes V$ and $\varepsilon : V \longrightarrow K$ such that:

 1. $(id \otimes \Delta) \circ \Delta = (\Delta \otimes id) \circ \Delta$
 2. $(id \otimes \varepsilon) \circ \Delta = id 
    = (\varepsilon \otimes id) \circ \Delta$

Sometimes, the coalgebras use [[https://en.wikipedia.org/wiki/Coalgebra#Sweedler_notation][Sweedler notation]].

****** Examples of coalgebras
******* The divided power coalgebra
Consider $K[X]$, the polynomial ring, where we define by linearity:

\[\Delta(X^n) = \sum^n_{k=0} {n \choose k} X^k \otimes X^{n-k}\]

\[\epsilon(X^n) = \twopartdef{1}{n=0}{0}{n>0}\]

When the structures of algebra and coalgebra are compatible, they
are called [[https://en.wikipedia.org/wiki/Bialgebra][bialgebras]].

****** R-coring
*R-coring*. Coassociative and counital coalgebra. It is an R-bimodule 
with a /comultiplication/ $\Delta : C \longrightarrow C \otimes C$ and 
a /counit/ $\epsilon : C \longrightarrow R$.

****** Graded corings
*Graded corings*. Decomposition $C = \bigoplus_{n \in \mathbb{N}} C_n$, 
such that:

\[\Delta(C_n) \subset \bigoplus_{p=0}^n C_p \otimes C_{n-p}\]

***** 1.3. Almost-Koszul pair
*Almost-Koszul pair*. Connected R-ring and R-coring $(A,C)$ with an 
isomorphism $\theta_{C,A} : C_1 \longrightarrow A^1$, that satisfies the relation:

\[ m^{1,1} \circ (\theta_{C,A} \otimes \theta_{C,A}) \circ \Delta_{1,1}
= 0\]

Or, using Sweedler notation, for any $c \in C_2$:

\[ \sum \theta_{C,A}(c_{(1,1)}) \theta_{C,A}(c_{(2,1)}) = 0\]

***** 1.4. Opposite Koszul pair
If $(A,C)$ is a Koszul pair, then $(A^{op},C^{op})$ are Koszul pairs with
respect to:

\[\theta_{C^{op},A^{op}} = \theta_{C,A}\]

***** 1.5. The normalized bar resolution of R
For every strongly graded R-ring A, there is a graded coring C such that
$(A,C)$ is an almost-Koszul pair.

****** The normalized right bar resolution
The exact sequence $\beta_\ast^r(A)$:

\[ 0 \longleftarrow 
R \overset{\delta_0}\longleftarrow 
A \overset{\delta_1}\longleftarrow
\overline{A} \otimes A \overset{\delta_2}\longleftarrow
\overline{A} \otimes \overline{A} \otimes A \overset{\delta_3}\longleftarrow
\overline{A} \otimes \overline{A} \otimes \overline{A} \otimes A \longleftarrow
\dots
\]

is called the *normalized right bar resolution*. Where
the $\delta$ are defined as:

 - $\delta_0 = \pi^0_A$
 - \[ \delta_n(a_1 \otimes \dots \otimes a_n \otimes a_{n+1}) 
      = \sum_{i=1}^n (-1)^i  a_1 \otimes \dots \otimes a_ia_{i+1} \otimes \dots \otimes a_{n+1}\]

****** TODO Normalized bar complex

**** 2. Koszul Pairs

**** 3. Hochschild (co)homology of Koszul rings
***** 3.1. The cyclic tensor product
****** Enveloping algebra of R
The tensor product algebra $R^e = R \otimes_\mathbb{K} R^{op}$ is called the 
*enveloping algebra* of $R$.

**** 4. Almost-Koszul pairs associated to twisted tensor products

**** 5. The Hochschlid cohomology of a twisted tensor product

*** Affine group schemes seminar
**** I. Ãlgebras de Hopf
***** 1. Definiciones
****** Ãlgebra de Hopf
Un *Ã¡lgebra de Hopf* es una biÃ¡lgebra (Ã¡lgebra y coÃ¡lgebra) con un 
antiautomorfismo llamado /antÃ­poda/.

******* ExplÃ­citamente
Tenemos $(H, m, \eta, \Delta, \varepsilon, S)$ como componentes del Ã¡lgebra de Hopf sobre
un cuerpo $k$, donde:

 - $H$ es el Ã¡lgebra.
 - $m : H \otimes H \to H$ es el producto.
 - $\eta : k \to H$ es la unidad.
 - $\Delta : H \to H \otimes H$ es la comultiplicaciÃ³n.
 - $\varepsilon : H \to k$ es la counidad.
 - $S : H \to H$ la antÃ­poda.

Bajo ciertas condiciones de compatibilidad.

****** Group-like elements
Elementos no nulos cumpliendo $\Delta(x) = x \otimes x$. Forman un grupo con la inversa
dada por la antÃ­poda.

**** II. Introduction to affine group schemes
***** 1. Definition and examples
****** Affine group scheme
An *affine group scheme* over $k$ is a representable functor $\mathtt{Alg}_k \to \mathtt{Grp}$.
More precisely, the composition of the functor with $\mathtt{Grp}\to\mathtt{Set}$ is
representable.

****** Connection with affine algebraic varieties
If $V$ is an affine algebraic variety, the we can define the corresponding
affine scheme as $Alg_k(K[V], -)$, where $K[V]$ is the coordinate algebra.

****** Algebraic affine scheme
An affine scheme is said to be *algebraic* if its representing object is
finitely generated as a k-algebra.

**** III. Esquemas diagonalizables y constantes
***** 1. IntroducciÃ³n
****** Ãlgebra grupo
Dado un grupo $G$ y un cuerpo $k$, el Ã¡lgebra grupo $k[G]$ estÃ¡ formada como el
espacio vectorial libre sobre $G$ con el producto que induce el grupo.

******* Estructura de Ã¡lgebra de Hopf
Este Ã¡lgebra tiene estructura de Ã¡lgebra de Hopf si extendemos linealmente
las siguientes aplicaciones:

  - $\Delta(x) = x \otimes x$
  - $\varepsilon(x) = 1$
  - $S(x) = x^{-1}$
*** Harpreet Bedi's channel
**** Sheaves and coho
***** Preseaves and sheaves
****** Preseaf definition
#+begin_definition
*Preseaf*. A preseaf ${\cal F}$ of abelian groups on a topological space $X$ consists of:

- For each open set $U$, an abelian group ${\cal F}(U)$, whose elements are called 
  *sections*.
- For each inclusion $V \subseteq U$, a *restriction map*, homomorphism of the form:
  
 
\[p_{U,V} : {\cal F}(U) \longrightarrow {\cal F}(V)\]

such that $p_{U,W} = p_{V,W} \circ p_{U,V}$.
#+end_definition

We can write the restriction of an element $u \in U$ to a set $V \subseteq U$ as
$u|_V = p_{U,V}(u)$.

****** Sheaf definition
 #+begin_definition
 *Gluability axiom*. Given $U = \bigcup U_i$ with sections $s_i \in {\cal F}(U_i)$, if we have:

 \[ s_\alpha|_{U_\alpha \cap U_\beta} = s_\beta|_{U_\alpha \cap U_\beta} \]

 then there exists $s \in {\cal F}(U)$ such that $s|_U_\alpha = s_\alpha$.
 #+end_definition
 #+begin_definition
 *Uniqueness axiom*. Given $U = \bigcup U_i$ with sections $s,t \in {\cal F}(U)$ such that:

 \[\forall U_\alpha:\ s|_U_\alpha = t|_U_\alpha\]

 then $s=t$.
 #+end_definition
 #+begin_definition
 *Sheaves*. A presheaf satisfiying gluability and uniqueness.
 #+end_definition
**** Homological Algebra
***** 2. Chain Complex and Homology
***** 4. Homology Theorem
****** Setting
Given a SES of chain complexes $0 \longrightarrow {\cal A}
\longrightarrow{\cal B}
\longrightarrow{\cal C}
\longrightarrow 0$, we have a long exact
sequence like:

\[ \begin{tikzcd}
 & \dots\rar & H_{n+1}({\cal C}) \arrow[out=355,in=175,swap]{dll}{\delta_{n+1}} \\
H_{n}({\cal A})\rar & H_{n}({\cal B}) \rar & H_{n}({\cal C}) \arrow[out=355,in=175,swap]{dll}{\delta_n}\\
H_{n-1}({\cal A})\rar & \dots & 
\end{tikzcd} \]

****** Naturality
When we have two SES of chain complexes:

\[ \begin{tikzcd}
0 \rar & {\cal A}\rar\dar & {\cal B}\rar\dar & {\cal C}\rar\dar & 0 \\
0 \rar & {\cal A}'\rar & {\cal B}'\rar & {\cal C}'\rar & 0 \\
\end{tikzcd} \]

where it hols for every $n$ that:

\[ \begin{tikzcd}
H_n({\cal C}) \rar\dar & H_{n-1}({\cal A})\dar \\
H_n({\cal C}') \rar & H_{n-1}({\cal A}')
\end{tikzcd} \]

***** 8. Proj, inj and flat modules
****** Definitions
An R-module $D$ is:

 1. *Projective* if $Hom(D, -)$ is exact.
 2. *Injective* if $Hom(-,D)$ is exact.
 3. *Flat* if $D \otimes -$ is exact.

****** Considerations
We know that $Hom(D,-)$ and $Hom(-,D)$ are left-exact and that
$D\otimes -$ is right-exact; so for them to be exact, we only need:

 - A module $D$ is *projective* when $B \longrightarrow C$ surjective induces
   $Hom(D,B) \longrightarrow Hom(D,C)$ surjective.

   \[ \begin{tikzcd}
               & B \dar[two heads] \\
   D \rar\urar[dashed]{\exists} & C
   \end{tikzcd} \]

 - A module $D$ is *injective* when $A \longrightarrow B$ surjective induces
   $Hom(B,D) \longrightarrow Hom(A,D)$ surjective.

   \[ \begin{tikzcd}
     & A \dar[two heads]\dlar \\
   D & B \lar[dashed]{\exists}
   \end{tikzcd} \]

 - A module $D$ is *flat* when $A \longrightarrow B$ injective induces 
   $D\otimes A \longrightarrow D \otimes B$ injective.

***** 9. Resolutions: projective, injective and flat
****** Definitions
******* Resolutions
Resolutions are *exact sequences*.

******* Projective resolution
A resolution, with $d_i$ maps:

\[\dots\longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

where $P_i$ is projective.

******* Injective resolution
A resolution:

\[0 \longrightarrow M \longrightarrow E_0\longrightarrow E_1
\longrightarrow E_2 \longrightarrow \dots\]

where $E_i$ is injective.

******* Flat resolution
A resolution:

\[\dots\longrightarrow F_2\longrightarrow F_1\longrightarrow F_0
\longrightarrow M \longrightarrow 0\]

where $F_i$ is flat.

****** How to form a resolution
It is important to notice that, given a module $M$, we can always find
a surjection from a proyective module (if we have /enough
projectives/). So we can construct a projective resolution as follows:

\[ \begin{tikzcd}[column sep=tiny]
&\ker f_2 \drar&&&&\ker \pi\drar &&& \\
\dots&&P_2 \drar[two heads]{f_2}&&P_1 \urar[two heads]{f_1} && P_0 \ar[two heads,rr]{\pi} && M \rar & 0\\
&&&\ker f_1 \urar&&&&
\end{tikzcd} \]

We can also reverse the arrows to obtain an injective resolution.

***** TODO 10. Homotopic projective resolutions
****** Extending a morphism
Given two projective resolutions of two $R$ modules, $A$ and $A'$, and a morphism
between them, $f$. We can extend it to $f_n \in Hom(P_n,P_n')$.

\[ \begin{tikzcd}
\dots\rar & P_{n+1}\rar & P_n\rar& \dots
 \rar & P_1\rar{d_1} & P_0\rar{d_0}& A \dar{f} \rar& 0 \\
\dots\rar & P_{n+1}'\rar & P_n'\rar&\dots
 \rar & P_1'\rar{d_1Â¡} & P_0'\rar{d_0'}& A' \rar& 0 \\
\end{tikzcd} \]

******* Extending the morphism, base case
We use that $P_0$ is projective to construct:

\[ \begin{tikzcd}
     & P_0 \arrow[ddl,"f_0",dashed,swap] \dar\\
     & A \dar{f} \\
P_0' \rar[two heads] & A'
\end{tikzcd} \]

******* Extending the morphism, inductive case
We are going to show that $f_n(\im d_{n+1}) \subset \im d_{n+1}' = \ker d_n'$. That is, 
$d_n' \circ f_n \circ d_{n+1} = 0$. And that follows from diagram chasing. We use
again the projectivity of $P_{n+1}$.

\[ \begin{tikzcd}
     & P_{n_+1} \arrow[ddl,"f_{n+1}",dashed,swap] \dar\\
     & \im d_{n+1} \dar{f_n} \\
P_{n+1}' \rar[two heads] & \im d_{n+1}'
\end{tikzcd} \]

****** TODO Homotopic resolutions
***** 11. Derived functors Ext and Tor
****** Right derived functors
Let $F$ be additive, covariant and left-exact. Let 
$0 \longrightarrow M \longrightarrow E^\bullet$ be an injective resolution with $M$ deleted; then $F(E^\bullet)$ 
is a complex, and we define:

\[R^i F(M) = H^i(F(E^\bullet)) = 
\frac{\ker \{F(E_i) \longrightarrow F(E_{i+1})\}}
{\im\{ F(E_{i-1}) \longrightarrow F(E_i)\}}\]

That is, if we take the injective resolution:

\[ 0 \longrightarrow M \longrightarrow E_0 \longrightarrow E_1 
\longrightarrow \dots\]

Delete $M$ and apply $F$ to get a (non neccesarily exact) complex where 
we can compute the homology:

\[ 0 \longrightarrow F(E_0) \longrightarrow F(E_1)
\longrightarrow F(E_2) \longrightarrow \dots\]

****** Left derived functors
Let $F$ be additive, contravariant and left-exact. Let 
$P^\bullet \longrightarrow M \longrightarrow 0$ be a projective resolution with $M$ deleted; 
then $F(P^\bullet)$ is a complex, and we define:

\[R^i F(M) = H^i(F(P^\bullet)) = 
\frac{\ker \{F(P_i) \longrightarrow F(P_{i+1})\}}
{\im\{ F(P_{i-1}) \longrightarrow F(P_i)\}}\]

That is, if we take the injective resolution:

\[\dots \longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

Delete $M$ and apply $F$ to get a (non neccesarily exact) complex 
where we can compute the homology:

\[ 0 \longrightarrow F(P_0) \longrightarrow F(P_1)
\longrightarrow F(P_2) \longrightarrow \dots\]

***** 12. Computations of some standard Ext and Tor examples
***** 13. Long Exact Sequence for Tor
**** Algebraic Geometry
***** 1. Intro to Algebraic Geometry

** Analysis                                                                                                 :analysis:
*** Basic
**** Integrals
***** Variable change                                                                                       :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       d952ab85-d01d-42b8-8d72-735e6d8b9147
:DRILL_LAST_INTERVAL: 4.5028
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-30 Sat 11:53]
:END:
Given $\varphi$ differentiable and $f$ integrable, state the
integration by substitution rule.

****** Definition

\[
\int_a^b f(\varphi(t))\varphi'(t) \; dt = \int_{\varphi(a)}^{\varphi(b)} f(x)\; dx
\]

**** Definitions
***** Lipschitz continuous                                                                                  :drill:
SCHEDULED: <2018-09-11 Tue>
:PROPERTIES:
:ID:       921ecf4c-ac54-4c0f-a35e-6644cbe4cb02
:DRILL_LAST_INTERVAL: 75.9758
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:16]
:END:
A function $f$ between metric spaces is Lipschitz continuous when...

****** Definition
\[
d(f(x),f(y)) \leq k\, d(x,y)
\]

***** Locally Lipschitz continuous                                                                          :drill:
SCHEDULED: <2018-09-10 Mon>
:PROPERTIES:
:ID:       4e9b39cc-4e82-442d-883d-97f8105ae90b
:DRILL_LAST_INTERVAL: 79.6653
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.75
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:19]
:END:
A function $f \colon D \to \mathbb{R}^k$ is locally Lipschitz continuous when...

****** Definition
for each $x \in D$ there exists a open $x \in U \subset D$ such that

\[
\|f(x)-f(y)\| \leq k\|x-y\|
\]

***** Hyperbolic sine                                                                                       :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       ad50cfd5-ba16-4bb6-beb4-88764926513f
:DRILL_LAST_INTERVAL: 5.4797
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:23]
:END:
Definition of hyperbolic sine

****** Definition
\[
\sinh(x) = \frac{e^x-e^{-x}}{2}
\]
***** Hyperbolic cosine                                                                                     :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       8abc2ece-687c-434c-a63d-52e8ada791bd
:DRILL_LAST_INTERVAL: 3.9682
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:31]
:END:
Definition of hyperbolic cosine

****** Definition
\[
\cosh(x) = \frac{e^x+e^{-x}}{2}
\]

**** Measure theory
***** Measurable space                                                                                      :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       87ac46cb-8da5-4d58-ab9c-75c766932e94
:DRILL_LAST_INTERVAL: 4.3657
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.428
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:24]
:END:
Definition of *measurable space*.

****** Definition
A measurable space $(\Omega,\Sigma)$ is given by a set $\Omega$ and a /Ï-algebra/
on the set, that is, a set of subsets with

 * the empty set,
 * all complements,
 * all countable unions,
 * all countable intersections.

***** Measure                                                                                               :drill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:ID:       385a9a40-8890-4928-b5c5-2c938dec604e
:DRILL_LAST_INTERVAL: 9.9448
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.625
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:10]
:END:
Definition of *measure* $\mu$.

****** Definition
A *measure* over a Ï-algebra $\Sigma$ is a function $\mu \colon \Sigma \to [0,+\infty]$,

 * null, $\mu(\varnothing) = 0$,
 * Ï-additive $\mu\left( \bigcup^{\infty}_{k=1} E_k \right) = \sum_{k=1}^{\infty}\mu(E_k)$, if we have $E_i \cap E_j = \varnothing$.

***** Fatou's lemma                                                                                         :drill:
SCHEDULED: <2018-09-04 Tue>
:PROPERTIES:
:ID:       6af0cfe7-d340-4be8-b036-c14f636fd988
:DRILL_LAST_INTERVAL: 74.1784
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.285
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:16]
:END:
Given a measure space $\Sigma$ and functions $f_n \colon X \to [0,+\infty]$; that are
$\Sigma,{\cal B}_{\mathbb{R}}\text{-measurable}$, what does the Fatou's lemma states?

****** Statement
Taking $f(x) = \liminf_{n\to \infty} f_n(x)$; we have that $f$ is $\Sigma,{\cal B}_{\mathbb{R}}\text{-measurable}$ and

\[
\int_X \liminf_{n \to \infty }f_n\ d\mu \leq \liminf_{n\to\infty} \int_X f_n\ d\mu.
\]

**** Differential equations

*** The formation of swarms as a consesus problem - Ulrich Krause
Estructuras complejas globales emergiendo de interacciones locales.
We will use topology instead of differential equations.

**** Model
Ensemble of birds in $\mathbb{R}^{d}$. Others $d$ different than 3 are allowed.
Position $x_i$ and velocity $v_i$ of each bird. The align by averaging.

\[
v_i = \sum_{j \in N} a_{ij}(t) v_i(t)
\]

The coefficients $a_{ij}$ model intensity of interactions; they depend
on the time. The set of seen birds is

S\[
S(i,t) = \left\{ j \in N \mid a_{ij}(t) > 0 \right\}
\]

***** Swarm formation
A swarm can be formed if

\[
\lim_t v_{i}(t) = v
\]

**** Swarm formation - theorem 1
***** Two assumptions
 * Structure not too loose.
 * Interaction does not decay too fast.
**** Swarm formation 2
Interaction only at certain points of time.

***** Core of a stochastic matrix
If $A$ has positive diagonal, $\mathrm{cor}(A) \neq \varnothing \iff A^k$ is scrambling;
we call these matrices *coherent*.

New conditions

 * structure of matrix not too loose;
 * intensity of interaction decays not too fast;
 * intensity of interaction decays slowly.

That can be interpreted as

 * every bird sees itself,
 * there is a sight chain to a leader.

**** Flight formations
***** V-formation and echelon
A leader is the only one in the core
***** Other possible cores: sterling clouds
Loops in the sight chain; connected loops.

****** Systematic account of flight formations?
Graphs changing in time.

****** Computer simulations?

**** Sight cones / cones of vision
Given by direction of flight. Non-convex cones would be also an
option.

***** Farkas lemma
***** Helly's theorem
**** Models of intensity of interaction
Cucker-Smale model of bird flocking.
** Logic                                                                                                       :logic:
*** Basic logic
**** Definition of validity                                                                                  :drill:
SCHEDULED: <2018-11-28 Wed>
:PROPERTIES:
:ID:       edf5dad9-0999-4098-861e-61a16ee7fd2e
:DRILL_LAST_INTERVAL: 199.9033
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.2
:DRILL_EASE: 2.52
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:48]
:END:
When is a sentence $\varphi$ *valid*?

***** Answer
$\varphi$ is *valid*, written $\models \varphi$ iff $\model \models \varphi$ for every structure $\model$.
Every structure satisfies the sentence.

**** Definition of soundness                                                                                 :drill:
SCHEDULED: <2019-03-22 Fri>
:PROPERTIES:
:ID:       56f2cb1c-ccdb-4416-b52d-be6fae51723f
:DRILL_LAST_INTERVAL: 272.6464
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:08]
:END:
When is a proof system *sound*?

***** Answer
When $\Gamma \vdash \Phi$ implies $\Gamma \models \Phi$.

/You cannot prove anything that is wrong./

Where
 
 * $\Gamma \vdash \Phi$ is logical entailment with the inference rules of the system;
 * $\Gamma \models \Phi$, is implication in the desired semantics; every structure satisfies it.

**** Definition of completeness                                                                              :drill:
SCHEDULED: <2019-03-12 Tue>
:PROPERTIES:
:ID:       a9041c29-30d2-406e-a1d8-335b58aa4b5a
:DRILL_LAST_INTERVAL: 263.0187
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:09]
:END:
When is a proof system *complete*?

***** Answer
When $\Gamma \models \Phi$ implies $\Gamma \vdash \Phi$.

/You can prove everything that is true./

Where
 
 * $\Gamma \vdash \Phi$ is logical entailment with the inference rules of the system;
 * $\Gamma \models \Phi$, is implication in the desired semantics; every structure satisfies it.

*** Intermediate logic - Open Logic, Zach
**** II. First-order logic
***** 5. Syntax and Semantics
****** 5.1. Introduction
 - Syntax :: how well-formed terms and formulas can be defined.
 - Semantics :: how meaning can be given to expressions.

****** 5.2. First-order languages
Any first-order language ${\cal L}$ is determined by logical, non-logical
symbols and some punctuation marks.

******* Logical symbols
 1. Logical connectives: $\neg,\land,\lor,\forall,\exists$,
 2. Propositional constant for falsity: $\bot$,
 3. Binary identity predicate: $=$,
 4. Numerable set of variables: e$v_0,v_1,\dots$

We assume $\top$ and $\leftrightarrow$ as defined as abbreviatures. We could use 
"truth functionally complete" subsets of boolean operators such
as $\{\neg,\lor\}$.

******* Non-logical symbols
 1. A numerable set of n-ary predicates for each $n>0$, as $\{A_0^n,A_1^n,\dots\}$,
 2. A numerable set of constants $c_0,\dots$,
 3. A numerable set of n-ary functions, as $\{f_0^n,f_1^n,\dots\}$.

******* Examples
 - Arithmetic with $S,O,<,+,\times$.
 - Set theory with $\in$.
 - Orders with $\leq$.

****** 5.3. Terms and formulas
******* Terms
The set of *terms* of a language ${\cal L}$ is defined inductively

 1. variables are terms,
 2. constants are terms,
 3. given an n-ary function and $n$ terms, $f(t_1,\dots,t_n)$
    is a term.

Constants are regarded as 0-ary functions.

******* Formulas
The set of *formulas* of a language ${\cal L}$ is defined inductively

 1. $\bot$ is a formula;
 2. given any n-ary predicate and $n$ terms, $R(t_1,\dots,t_n)$ is
    a formula;
 3. given any two terms, $t_1 = t_{2}$ is a formula;
 4. $\neg \varphi$;
 5. $\varphi \lor \psi$;
 6. $\varphi \land \psi$;
 7. $\varphi \to \psi$;
 8. $\forall x. \varphi$;
 9. $\exists x.\varphi$.

******* Syntatic identity
Two strings of symbols are syntatically identical, $\varphi \equiv \psi$, if
they contain the same symbols in the same place.

****** 5.4. Unique readability
Every formula has a unique reading. The correct definitions, using
parentheses constraint the set of possible formulas.  The number of
left and right parentheses in a formula are equal, by induction.

******* Proper prefixes
A string $\varphi$ is a *proper prefix* of $\psi$ if it can be obtained by 
appending symbols to $\varphi$.

#+ATTR_LATEX: :options []
#+BEGIN_lemma
Every proper prefix of a formula is not a formula.
#+END_lemma
#+BEGIN_proof
Using the fact that there is an equal number of left and right
parentheses in every formula.
#+END_proof

******* Unique readability
Every atomic formula satisfies one and only one of the following
conditions

 1. $\varphi \equiv \bot$
 2. $\varphi \equiv R(t_1,\dots,t_n)$
 3. $\varphi \equiv t_1 = t_2$

And every formula is of the form

 1. atomic
 2. $\neg \psi$
 3. $\varphi \lor \psi$
 4. $\varphi \land \psi$
 5. $\psi \to \varphi$
 6. $\forall x.\psi$
 7. $\exists x.\psi$

The proof crucially uses the fact that no formula is a proper prefix
of any other formula.

****** 5.5. Main operator of a formula
The outermost operator of a formula exists if the formula is not
atomic. It is always unique, as we have proved earlier.

****** 5.6. Subformulas
******* Immediate subformulas
*Immediate subformulas* are defined inductively as

 1. no subformulas for atomic formulas;
 2. $\varphi$ and $\psi$ are immediate subformulas of $\varphi \ast \psi$;
 3. $\psi$ is an immediate subformula of $\forall x.\psi$;
 4. $\psi$ is an immediate subformula of $\exists x.\psi$.

******* Proper subformulas
The *proper subformulas* of a formula are its immediate subformulas and
their proper subformulas.

We also consider the formula to be a non-proper subformula of itself.

****** 5.7. Free variables and sentences
A variable appears *free* when it is not bounded by a quantifier. The
precise definition can be trivially written by induction. Every
bounded variable has a *scope*, a subformula over which the quantifier
acts.

******* Sentences
A formula is a *sentence* if it contains no free ocurrences of variables.

****** 5.8. Substitution
*Substitution* of a variable by a term, $s[t/x]$, can be recursively
defined as

 * $c[t/x]$ is $c$, provided $c$ is a constant;
 * $y[t/x]$ is $y$, provided $y$ is a variable;
 * $x[t/x]$ is $t$;
 * $f(t_1,\dots,t_n)[t/x]$ is $f(t_1[t/x],\dots,t_n[t/x])$.

Substitution can be extended trivially to formulas; but we have to
check that every term appears free for the variable in order to avoid
undesired bounds for a variable.

****** 5.9. Structures for first-order languages
*Structures* are the basis for /semantic notions/. A structure $\model$ for
a language ${\cal L}$ consists of

 1. a *domain*, a non empty set $|\model|$;
 2. an interpretation for each *constant*, $c^{\model} \in |\model|$;
 3. an interpretation for each *predicate*, $R^{\model} \subseteq |\model|^n$;
 4. an interpretation for each *function*, $f^{\model} \colon |\model|^n\to |\model|$.

Non emptiness ensures that the existential generalization is sound.

******* Examples
 - Standard model of arithmetic.
 - Structure of hereditarily finite sets.

****** 5.10. Covered structures for first-order languages
******* Values
The value of a term is defined recursively as

 * $\mathrm{Val}^{\model}(c) = c^{\model}$;
 * $\mathrm{Val}^{\model}(f(t_1,\dots,t_n)) = f^{\model}(\mathrm{Val}^{\model}(t_1),\dots \mathrm{Val}^{\model}(t_n))$.

******* Covered structures
A structure is covered if every element is the value of some
closed term.

****** 5.11. Satisfaction of a formula in a structure
******* Satisfaction
A formula is *satisfied* in a structure if the interpretation makes
the formula true.

******* Variable assignment
A problem with quantifiers arise when we try to interpret free variables.
We need to define *variable assignments*, functions $s : \mathrm{Var} \to |\model|$.

The value of a variable $x$ under an assignment $s$ is given by $s(x)$.

******* x-Variant
Any variable assignment $s'$ which differs from $s$ at most in one variable $x$ is
called an *x-variant*, and written as $s \sim_x s'$.

******* Satisfaction
*Satisfaction* of a formula $\varphi$ in a structure $\model$ relative to a variable
assignment $s$; written as $\model,s \models \varphi$ is defined recursively as

 1. $\model,s \not\models \bot$;
 2. $\model, s \models R(t_1,\dots,t_n)$ iff $\langle \mathrm{Val}^{\model}_s(t_1),\dots,\mathrm{Val}^{\model}_s(t_n) \rangle \in R^{\model}$;
 3. $\model,s \models t_1 = t_2$ iff $\mathrm{Val}^{\model}_s(t_1) = \mathrm{Val}^{\model}_s(t_2)$;
 4. $\model,s \models \neg\varphi$ iff $\model,s \not\models \varphi$;
 5. $\model,s \models \varphi \land \psi$ iff $\model,s \models \varphi$ and $\model,s \models \psi$;
 6. $\model,s \models \varphi \lor \psi$ iff $\model,s \models \varphi$ or $\model,s \models \psi$;
 7. $\model,s \models \varphi \to \psi$ iff  $\model,s \not\models \varphi$ or $\model,s \models \psi$;
 8. $\model,s \models \forall x. \varphi$ iff $\model,s' \models \varphi$ for every x-variant $s'$;
 9. $\model,s \models \exists x. \varphi$ iff $\model,s' \models \varphi$ for some x-variant $s'$;

Variable assignments are crucial here because we have to define a
formula for every $a \in |\model|$, but $a$ is not a formula.

****** 5.12. Variable assignments
Two assignments assigning the same value to the same free variables
produce the same values and entail the same formulas. In particular,
in the case of *sentences* without free variables, the truth value
is independent of the variable assignment.

******* Independence of variables in values
#+BEGIN_proposition
If $t$ has variables among $x_1,\dots,x_n$ and $s_1(x_i) = s_2(x_i)$; then
$\mathrm{Val}^{\model}_{s_1}(t) = \mathrm{Val}^{\model}_{s_2}(t)$.
#+END_proposition

Trivially by induction.

******* Independence of variables in formulas
#+BEGIN_proposition
If $\varphi$ has variables among $x_1,\dots,x_n$ and $s_1(x_i) = s_2(x_i)$; then
$\model,s_1 \models \varphi$ iff $\model,s_2 \models \varphi$.
#+END_proposition

Again by induction.

******* Satisfaction in a structure
A structure $\model$ *satisfies* $\varphi$, and it is written as $\model \models \varphi$, if
$\model, s \models \varphi$ for all variable assignments $s$.

****** 5.13. Extensionality
Where two structures agree on all elements, they entail the same truth
values. If $\model_1$ and $\model_2$ agree on constants, relations and functions;
$\model_1,s \models \varphi$ iff $\model_2,s \models \varphi$.

In particular, this happens for any sentence.

******* Dependence on subterms for values
Given a structure $\model$ and $s$ with $s \sim_x s'$ given by $s'(x) = \mathrm{Val}^{\model}_s(t')$.
Then $\mathrm{Val}^{\model}_s(t[t'/x]) = \mathrm{Val}^{\model}_{s'}(t)$.

******** Proof by induction
******* Dependence on subterms for formulas
Given a structure $\model$ and $s$ with $s \sim_x s'$ given by $s'(x) = \mathrm{Val}^{\model}_s(t)$.
Then $\model,s \models \varphi[t/x]$ iff $\model,s' \models \varphi$.

****** 5.14. Semantics notions
Semantic properties.

******* Validity
$\varphi$ is *valid*, written $\models \varphi$ iff $\model \models \varphi$ for every structure $\model$.

******* Entailment
A set of sentences $\Gamma$ *entails* $\varphi$, written $\Gamma \models \varphi$ iff $\model \models \varphi$ for
every structure such that $\model \models \Gamma$.

******* Satisfiability
A set of sentences $\Gamma$ is *satisfiable* if $\model \models \Gamma$ for some structure
$\model$.

******* Validity and entailment
A sentence $\varphi$ is valid iff $\Gamma \models \varphi$ for any set of sentences $\Gamma$.

******* Satisfiability and entailment
$\Gamma \models \varphi$ iff $\Gamma \cup \{\neg \varphi\}$ is unsatisfiable.

******* Strengthening
If $\Gamma \subseteq \Gamma'$ and $\Gamma \models \varphi$, then $\Gamma' \models \varphi$.

******* Semantic deduction theorem
$\Gamma \cup \{\varphi\} \models \psi$ iff $\Gamma \models \varphi \to \psi$.

******* Quantifiers and entailment
 1. $\varphi(t) \entail\exists x.\varphi(x)$,
 2. $\forall x. \varphi(x) \entail \varphi(t)$.
***** 6. Theories and their models
****** 6.1. Introduction
******* Closure
A set of sentences $\Gamma$ is *closed* if it is equal to its closure,
$\{ \varphi : \Gamma \models \varphi\}$. $\Gamma$ is *axiomatized* by $\Delta$ if it is its closure.

****** 6.2. Expressing properties of structures
******* Model
The structure $\model$ is a *model* of $\Gamma$ if $\model \models \varphi$ for all $\varphi \in \Gamma$.

****** 6.3. Examples of first-order theories
******* Strict linear orders
******* Theory of groups
******* Peano arithmetic with induction schemas
******* Pure sets with naive comprehension schemes
****** 6.4. Expressing relations in a structure
A formula $\varphi(v_1,\dots,v_n)$ expresses the relation $R \subseteq |\model|^n$ if
\[
Ra_1\dots a_n
\quad\mbox{ iff }\quad
\model,s \models \varphi(v_1,\dots,v_n)
\]
for any variable assignment such that $s(v_i) = a_i$.

****** 6.5. The theory of sets
ZFC is the most widely studied axiomatic system for set theory.
Inclusion can be defined by defining membership, and sets have
to be implicitely defined.

For example, the empty set $\varnothing$ is defined with
\[
\exists x. (\neg \exists y. y \in x) \land (\forall z. x \subseteq z)
\]
and operations on set could be defined in the same way.

The comprehension principle is inconsistent (Russell's paradox),
therefore, ZFC only allows the separation principle,
\[
\forall z. \exists y. \forall x. (x \in y \leftrightarrow (x \in z \land \varphi(x))).
\]

****** 6.6. Expressing the size of structures
There are sentences which are true in a structure iff the domain
has a specific size. The property of being non-enumerable or being
finite cannot be expressed even with an infinite set of sentences
(LÃ¶wenheim-Skolem theorems).

**** III. Proofs and completeness
***** 7. The Sequent Calculus
****** 7.1. Rules and derivations
******* 7.1. Sequent
A *sequent* is an expression $\Gamma \seq \Delta$ between sequences of
sentences. Semantically, it means that, if $\Gamma = \left\langle \varphi_1,\dots,\varphi_n \right\rangle$
and $\Delta = \left\langle \psi_1,\dots,\psi_m \right\rangle$,

\[
(\varphi_1 \land \dots \land \varphi_n) \to
(\psi_1 \lor \dots \lor \psi_m).
\]

******* 7.2. Initial sequent
An *initial sequent* is of the form

 1. $\varphi \seq \varphi$
 2. $\bot \seq$

where $\varphi$ is a sentence.

****** 7.2. Propositional rules
******* Rules for negation
Formation (L)

\[\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta,\varphi$
\UI$\neg \varphi, \Gamma \fCenter\seq \Delta$
\end{prooftree}\]

Formation (R)

\begin{prooftree}
\AX$\varphi, \Gamma \fCenter\seq \Delta$
\UI$\Gamma \fCenter\seq \Delta,\neg \varphi$
\end{prooftree}

******* Rules for conjunction
Formation (L)

\begin{prooftree}
\AX$\varphi, \Gamma \fCenter\seq \Delta$
\UI$\varphi \land \psi, \Gamma \fCenter\seq \Delta$
\end{prooftree}

Formation (R)

\begin{prooftree}
\AXC{$\Gamma \seq \Delta,\varphi$}
\AXC{$\Gamma \seq \Delta,\psi$}
\BIC{$\Gamma \seq \Delta,\varphi \land \psi$}
\end{prooftree}

******* Rules for disjunction
Formation (L)

\begin{prooftree}
\AXC{$\varphi, \Gamma \seq \Delta$}
\AXC{$\psi, \Gamma \seq \Delta$}
\BIC{$\varphi \lor \psi, \Gamma \seq \Delta$}
\end{prooftree}

Formation (R)

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta,\varphi$
\UI$\Gamma \fCenter\seq \Delta,\varphi \lor \psi$
\end{prooftree}

******* Rules for implication
Formation (L)

\begin{prooftree}
\AXC{$\Gamma \seq \Delta,\varphi$}
\AXC{$\psi, \Pi \seq \Lambda$}
\BIC{$\varphi \to \psi, \Gamma, \Pi \seq \Delta,\Lambda$}
\end{prooftree}

Formation (R)

\begin{prooftree}
\AX$\varphi, \Gamma \fCenter\seq \Delta, \psi$
\UI$\Gamma \fCenter\seq \Delta, \varphi \to \psi$
\end{prooftree}

****** 7.3. Quantifier rules
******* Rules for universal quantifiers
Formation (L), where $t$ is a closed term

\begin{prooftree}
\AX$\varphi(t), \Gamma \fCenter\seq \Delta$
\UI$\forall x.\varphi(x), \Gamma \fCenter\seq \Delta$
\end{prooftree}

Formation (R), where $a$ is an *eigenvalue*; a constant which must not
occur anywhere in the lower sequent

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta, \varphi(a)$
\UI$\Gamma \fCenter\seq \Delta, \forall x.\varphi(x)$
\end{prooftree}

******* Rules for existential quantifiers
Formation (L), where $a$ is an *eigenvalue*

\begin{prooftree}
\AX$\varphi(a), \Gamma \fCenter\seq \Delta$
\UI$\exists x.\varphi(x), \Gamma \fCenter\seq \Delta$
\end{prooftree}

Formation (R), where $t$ is a closed term

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta, \varphi(t)$
\UI$\Gamma \fCenter\seq \Delta, \exists x.\varphi(x)$
\end{prooftree}

****** 7.4. Structural rules
******* Weakening
Left weakening

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta$
\UI$\varphi, \Gamma \fCenter\seq \Delta$
\end{prooftree}

Right weakening

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta$
\UI$\Gamma \fCenter\seq \Delta, \varphi$
\end{prooftree}

******* Contraction
Left contraction

\begin{prooftree}
\AX$\varphi, \varphi, \Gamma \fCenter\seq \Delta$
\UI$\varphi, \Gamma \fCenter\seq \Delta$
\end{prooftree}

Right contraction

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta, \varphi, \varphi$
\UI$\Gamma \fCenter\seq \Delta, \varphi$
\end{prooftree}

******* Exchange
Left exchange

\begin{prooftree}
\AX$\Gamma, \varphi, \psi, \Pi \fCenter\seq \Delta$
\UI$\Gamma, \psi, \varphi, \Pi \fCenter\seq \Delta$
\end{prooftree}

Right exchange

\begin{prooftree}
\AX$\Gamma \fCenter\seq \Delta, \varphi, \psi, \Lambda$
\UI$\Gamma \fCenter\seq \Delta, \psi, \varphi, \Lambda$
\end{prooftree}

******* Cut
Cut is not necessary, but makes it easier to reuse derivations

\begin{prooftree}
\AXC{$\Gamma \seq \Delta,\varphi$}
\AXC{$\varphi, \Pi \seq \Lambda$}
\BIC{$\Gamma,\Pi \seq \Delta, \Lambda$}
\end{prooftree}

It follows from the implication rule.

****** 7.5. Derivations
******* LK-derivation
An *LK-derivation* of a sequent is a tree of sequents starting from
initial sequents and applying inference rules.

****** 7.6. Examples of derivations
****** 7.7. Derivations with quantifiers
****** 7.8. Proof-theoretic notions
******* Theorems
A *theorem* is a sentence $\varphi$ such that there is a derivation of $\seq \varphi$.
We write $\vdash \varphi$ if it is a theorem and $\not\vdash \varphi$ if it is not.

******* Derivability
A sentence $\varphi$ is *derivable* from $\Gamma$ if there is a finite subset $\Gamma' \subseteq \Gamma$
such that the system derives $\Gamma \seq \varphi$. We write $\Gamma \vdash \varphi$ if $\varphi$ is derivable,
we write $\Gamma \not\vdash \varphi$ if it is not.

******* Consistency
A set of sentences $\Gamma$ is *inconsistent* if a finite subset $\Gamma' \subseteq \Gamma$ derives
$\Gamma' \seq$ . If a system is not inconsistent, it is *consistent*.

******* Reflexivity
If $\varphi \in \Gamma$, then $\Gamma \vdash \varphi$.

******** Proof
$\varphi \seq \varphi$ is an initial sequent.

******* Monotony
If $\Gamma \subseteq \Delta$ and $\Gamma \vdash \varphi$, then $\Delta \vdash \varphi$.

******** Proof
Given $\Gamma' \subseteq \Gamma \subseteq \Delta$, we know that $\Gamma' \subseteq \Delta$.

******* Transitivity
If $\Gamma \vdash \varphi$ for every $\varphi \in \Delta$ and $\Delta \vdash \psi$, then $\Gamma \vdash \varphi$.

******** Proof
If $\Delta \vdash \psi$, then there exists a finite $\Delta_0 \seq \psi$. We proceed by
induction on the size of $\Delta_0$,

 * if $\Delta_0$ is empty, $\seq \psi$ and, in particular $\Gamma \vdash \psi$;
 * if $\varphi \in \Delta_0$, we define $\Delta_1 = \Delta_0 \setminus \{\varphi\}$; and we know that $\varphi, \Delta_1 \seq \psi$,
   so $\Delta_1 \seq \varphi \to \psi$. By induction hypothesis, there exist $\Gamma_0 \seq \varphi \to \psi$
   and $\Gamma_1 \seq \varphi$; thus $\varphi \to \psi, \Gamma_1 \seq \psi$ and, by cut elimination rule,
   $\Gamma_0, \Gamma_1 \seq \psi$.

******* Principle of explosion
$\Gamma$ is inconsistent iff $\Gamma \vdash \varphi$ for every $\varphi$.

******** Proof
If $\Gamma \seq \bot$, by cut elimination, $\Gamma \seq$ . If $\Gamma \seq$ , then by
weakening, $\Gamma \seq \varphi$.

******* Compactness
 1. If $\Gamma \vdash \varphi$, there exists a subset $\Gamma_0 \subseteq \Gamma$ such that $\Gamma_0 \vdash \varphi$.
 2. If every subset of $\Gamma$ is consistent, $\Gamma$ is consistent.

******** Proof
By definition of derivability.

****** 7.9. Derivability and consistency
******* Transitivity of inconsistency
If $\Gamma \vdash \varphi$ and $\Gamma \cup \{\varphi\}$ is inconsistent, $\Gamma$ is inconsistent.

******** Proof
We have $\Gamma_0,\Gamma_1 \subseteq \Gamma$ such that $\Gamma_0 \seq \varphi$ and $\varphi, \Gamma_1 \seq$ ; thus,
by cut elimination, $\Gamma_0, \Gamma_1 \seq$.

****** 7.10. Derivability and the propositional connectives
******* Conjunction
We know that

 * $\varphi \land \psi \vdash \varphi$
 * 4$\varphi \land \psi \vdash \psi$
 * $\varphi, \psi \vdash \varphi \land \psi$

******** Proof
Applying the propositional rules for conjunction, we know that
$\varphi \land \psi \seq \varphi$ and $\varphi \land \psi \seq \psi$; while applying the right hand
side rule, $\varphi, \psi \seq \varphi \land \psi$.

******* TODO Disjunction

****** 7.11. Derivability and the quantifiers
******* Derivability of the universal quantifier
If $\Gamma \vdash \varphi(c)$ and $c$ does not appear in $\Gamma$; $\Gamma \vdash \forall x.\varphi(x)$.

******** Proof
Trivial by definition of derivability.

******* Initial derivations for quantifiers
 1. $\varphi(t) \vdash \exists x.\varphi(x)$
 2. $\forall x.\varphi(x) \vdash \varphi(t)$

******** Proof
Both are derivable from the quantifier rules.

****** 7.12. Soundness
******* Satisfaction of a sequent
A structure $\model$ *satisfies* a sequent $\Gamma \seq \Delta$ if and only if $\model \not\models \varphi$ for
some $\varphi \in \Gamma$ or $\model \models \varphi$ for some $\varphi \in \Delta$.

******* Valid sequents
A sequent is *valid* if every structure $\model$ satisfies it.

******* Soundness
If LK derives $\Theta \seq \Xi$, then it is a valid sequent.

******** Proof
By structural induction on the derivation. If it has no inferences,
it has to be an initial sequent, and $\varphi \seq \varphi$ and $\bot \seq$  are valid
sequents. In other case, we apply structural induction to get

 1. left and right weakening, trivially;
 2. left and right negation, trivially;
 3. left conjunction, trivially;
 4. right disjunction, trivially;
 5. right implication, trivially;
 6. universal quantifiers, trivially using previous lemmas;

with one premise, and

 1. cut elimination,
 2. right conjunction,
 3. left disjunction,

with two premises. All are valid by the definition of [[*Satisfaction of a sequent][satisfaction]] and
the notion of [[*Satisfaction][satisfaction]] of a formula in a structure.

***** 8. The Completeness Theorem
****** 8.3. Complete consistent sets of sequences
******* Complete set
A set $\Gamma$ is *complete* iff for any sentence either $\varphi \in \Gamma$
or $\neg \varphi \in \Gamma$.

******** Membership
In particular, $\varphi \not\in \Gamma$ implies $\neg\varphi\in\Gamma$.

******* Complete consistent sets
If $\Gamma$ is complete and consistent,

 1. if $\Gamma \vdash \varphi$, then $\varphi \in \Gamma$;
 2. $\varphi \land \psi \in \Gamma$ iff $\varphi \in \Gamma$ and $\psi \in \Gamma$;
 3. $\varphi \lor \psi \in \Gamma$ iff $\varphi \in \Gamma$ or $\psi \in \Gamma$;
 4. $\varphi \to \psi \in \Gamma$ iff $\varphi \not\in \Gamma$ or $\psi \in \Gamma$.

****** 8.4. Henkin expansion
Henkin expansion adds infinitely many constant symbols to allow
existential quantifiers to be satisfied by one of these symbols.

******* Extension of consistency
If $\Gamma$ is consistent in ${\cal L}$ and we obtain a new language by adding
a numerable set of constants, ${\cal L}'$, then $\Gamma$ is consistent in ${\cal L}'$.

******** Proof
Trivial by definition of [[*Consistency][consistency]].

******* Saturated set
A set $\Gamma$ is *saturated* iff for each formula $\varphi(x) \in \mathrm{Frm}({\cal L})$ where
$x$ is a free variable, there is a constant symbol $c \in {\cal L}$ such that
$\exists x.\varphi(x) \to \varphi(c) \in \Gamma$.

******* Theta sentences
Given a language ${\cal L}'$ and an enumeration $\varphi_i(x_i)$ of formulas of ${\cal L}'$ in
which a variable $x_i$ occurs free.

Let $c_0$ be the first fresh constant symbol not in $\varphi_0(x_0)$, and $c_n$
the first fresh constant symbol not in $\theta_0,\dots,\theta_{n-1}, \varphi_n(x_n)$.

We define $\theta_n$ as $\exists x_n. \varphi_n(x_n) \to \varphi(c_n)$.

******* Extension of saturation
If $\Gamma$ is consistent, it can be extended to a saturated consistent set
$\Gamma'$.

******** Proof
Given ${\cal L}$, we get ${\cal L}'$, and then let using [[*Theta sentences][theta sentences]],

 * $\Gamma_0 = \Gamma$,
 * $\Gamma_{n+1} = \Gamma_n \cup \{\theta_n\}$,

then $\Gamma' = \bigcup \Gamma_n$ is saturated. If it were [[*Consistency][inconsistent]], empty could be
derived from a finite set of sentences, so some $\Gamma_n$ would be inconsistent.
We will show that each $\Gamma_n$ is consistent. If we had

 * $\Gamma_n \vdash \neg\{\theta_n\}$,

where $\theta_n$ is $\exists x_n.\varphi_n(x_n)$ then we would have

 * $\Gamma_n \vdash \exists x_n. \varphi_n(x_n)$,
 * $\Gamma_n \vdash \neg \varphi_n(c_n)$;

but as $c_n$ does not appear in $\Gamma_n$, $\Gamma_n \vdash \forall x.\neg \varphi_n(x)$ and then

\[
\forall x.\neg \varphi_n(x) \vdash \neg \exists x_{n}.\varphi_n(x)
\]

thus making $\Gamma_{n}$ inconsistent.

******* Complete, consistent and saturated sets
If $\Gamma$ is complete, consistent and saturated

 1. $\exists x.\varphi(x) \in \Gamma$ iff there exists $\varphi(t) \in \Gamma$, for some $t$;
 2. $\forall x.\varphi(x) \in \Gamma$ iff $\varphi(t) \in \Gamma$ for all closed $t$.

******** Proof
 1. By saturation we have $\exists x.\varphi(x) \to \varphi(c)$ for
    some $c$; then by completion, $\varphi(c) \in \Gamma$ or $\neg\varphi(c) \in \Gamma$;
    but only the first case allows consistency to be true.

    In the other direction, if $\varphi(t) \in \Gamma$, then by completion
    and consistency, $\exists x.\varphi(x) \in \Gamma$.

 2. If $\forall x.\varphi(x) \in \Gamma$, then for every $t$, by completion, we have $\varphi(t) \in \Gamma$
    or $\neg \varphi(t) \in \Gamma$; if we had $\neg\varphi(t)$, it would be inconsistent.

    In the other direction, by completion, if we had $\neg\forall x.\varphi(x) \in \Gamma$
    then we deduce $\exists x. \neg \varphi(x) \in \Gamma$, and by saturation and completion,
    again, $\neg \varphi(c) \in \Gamma$.

****** 8.5. Lindenbaum's lemma
******* Lindenbaum's lemma
Every consistent set $\Gamma'$ in a language ${\cal L}'$ can be extended to a complete
and consistent set $\Gamma^{\ast}$.

******** Proof
We take $\Gamma_0 = \Gamma'$ and we enumerate all formulas $\{\varphi_i\}$. At each step we
add $\Gamma_{n+1} = \Gamma_n \cup \{\varphi_{n}\}$ if it is consistent or $\Gamma_{n+1} = \Gamma_n \cup \{\neg\varphi_{n}\}$ 
otherwise. Let $\Gamma^{\ast} = \bigcup \Gamma_n$.

If both $\Gamma_n\cup \{\varphi_n\}$ and $\Gamma_n\cup \{\neg\varphi_n\}$ were inconsistent, $\Gamma_n$ would be
inconsistent. Thus, every subset of $\Gamma^{\ast}$ is consistent and it has to be
consistent.

****** 8.6. Construction of a model
******* Term model
Given $\Gamma^{\ast}$ complete, consistent and saturate; the *term model* $\model(\Gamma^{\ast})$ is
defined with

 1. domain $|\model(\Gamma^{\ast})|$ given by the set of closed terms;
 2. the interpretation of every constant as itself, $c^{\model(\Gamma^{\ast})} = c$;
 3. the function symbol is assigned to a function which returns the
    closed term of that function, $f^{\model(\Gamma^{\ast})}(t_1,\dots,t_n) = f(t_1,\dots,t_n)$;
 4. and if $R$ is an n-place symbol,
    \[
    \left\langle t_1,\dots,t_n \right\rangle \in R^{\model(\Gamma^{\ast})}
    \text{ iff }
    R(t_1,\dots,t_n) \in \Gamma^{\ast}.
    \]

******* TODO Term model and quantifiers
# Our model is covered

******* Truth lemma
If $\varphi$ does not contain $=$, then $\model(\Gamma^{\ast})\models \varphi$ iff $\varphi \in \Gamma^{\ast}$.

First-order logic for sets $\Gamma$ that do not contain $=$ is complete.

******** TODO Proof

****** 8.7. Identity
******* Factoring identity
Given $\Gamma^{\ast}$ a consistent and complete set in ${\cal L}$, the *relation* $\approx$ is
defined as $t \approx t'$ iff $t=t' \in \Gamma^{\ast}$.

******* TODO Properties of the new identity relation

******* Equivalence classes
Given $\Gamma^{\ast}$ a consistent and complete set in ${\cal L}$, then $t$ is a term and
$\approx$ as in the previous definition,

\[
[t]_{\approx} = \left\{ t' : t' \in \mathrm{Trm}({\cal L}), t \approx t' \right\};
\]

and $\mathrm{Trm}({\cal L})/_{\approx} = \left\{ [t]_{\approx} : t \in \mathrm{Trm}({\cal L}) \right\}$.

******* Representative term structure
****** 8.8. Completeness theorem
******* GÃ¶del's Completeness theorem
Let $\Gamma$ be a set of sentences; if it is consistent, it is satisfiable.

******** Proof
There is a saturated $\Gamma' \supseteq \Gamma$, and there is a $\Gamma^{\ast} \supseteq \Gamma'$ consistent and
complete; while it is also saturated. If $\Gamma$ contains $=$, then we compute
the quotient to have $\model/_{\approx}\models \varphi$ iff $\varphi \in \Gamma^{\ast}$.

******* Completeness theorem, second version
For all $\Gamma$ and $\varphi$, if $\Gamma\models\varphi$, then $\Gamma \vdash \varphi$.

******** Proof
If $\Gamma \models \varphi$, then $\Gamma \cup \{\neg\varphi\}$ is unsatisfiable; by completeness theorem,
it has to be inconsistent, so $\Gamma \vdash \varphi$.

****** TODO 8.9. Compactness theorem
****** TODO 8.10. A direct proof of the compactness theorem
****** TODO 8.11. The LÃ¶wenheim-Skolem theorem
****** TODO 8.12. Overspill
*** ZFC - Pablo Baeyens
# Estos apuntes han sido tomados durante el seminario de Pablo
# Baeyens para LibreIM sobre ZFC. El artÃ­culo que acompaÃ±a a
# este seminario puede leerse en
#
#  http://tux.ugr.es/libreim/blog/2017/03/25/zfc/
#
# Estos apuntes estÃ¡n licenciados bajo CC-BY-SA.
Utilizando lÃ³gica de primer orden.

\[
\wedge, \implies, \iff, \forall, \exists
\]

AdemÃ¡s de:

- Un conjunto infinito numerable de variables: $x_1,x_2,\dots$
- SÃ³lo puede cuantificarse sobre variables.
- Reglas para fÃ³rmulas bien formadas.

SÃ­mbolos propios de la teorÃ­a de conjuntos:

- $=, \in$

Se usa internamente la axiomÃ¡tica de la lÃ³gica de primer orden.

**** Axiomas de la igualdad
La igualdad es relaciÃ³n de equivalencia. Es

 1. Reflexiva, $\forall x: x=x$.
 2. Transitiva, $\forall x,y,z: x=y \wedge x=y \implies x=z$.
 3. SimÃ©trica, $\forall x,y : x=y \iff y=x$.
 4. SustituciÃ³n, $\forall x,y: x=y \implies \varphi \iff \varphi'$ donde $\varphi'$ sale de sustituir $x$.

**** NÃºmeros como conjuntos
Algunas teorÃ­as consideran elementos que no tienen elementos dentro.
En nuestro caso usaremos conjuntos para representar todos los objetos
matemÃ¡ticos.

***** NÃºmeros naturales
Definimos:

 - $0 = \varnothing$
 - $S(x) = x \cup \{x\}$

***** Otra construcciÃ³n posible
Definiendo:

 - $\varnothing$
 - $S(x) = \{x\}$

***** ConstrucciÃ³n de funciones
Para definir una funciÃ³n, usamos su grÃ¡fico:

\[\{(a,b) \in A \times B \mid f(a) = b\}\]

**** Restricciones
No podemos hablar todavÃ­a del conjunto de todos los conjuntos. Dentro
de ZFC no podemos hablar de cosas como $Obj(\mathtt{Set})$.

**** Axiomas de la teorÃ­a de conjuntos
Tomamos como ciertos:

***** 0. Axioma de existencia
$\exists x : x=x$

***** 1. Axioma de extensionalidad
$\forall x,y:(\forall z: z\in x \implies z \in y) \implies x = y$

De paso definimos la inclusiÃ³n:

\[
x \subseteq y := \forall z: z\in x \implies z \in y
\]

Y este axioma es equivalente a la antisimetrÃ­a de la inclusiÃ³n.

***** 2. Axioma de comprensiÃ³n
Exigiendo que $A$ no aparezca como variable libre en $\varphi$.

\[
\forall A: \exists B: \forall z:\left( \varphi(z) \vee z \in A\right) \iff z \in B
\]

Puede usarse para demostrar que existe el conjunto vacÃ­o. O que
no existe el conjunto universal.

****** Paradoja de Russell
Supongamos el universal $U$. PodrÃ­amos definir:

\[
R = \{x \in U \mid x \notin x\}
\]

Tanto $R \in R$ como $R \notin R$. Para evitarla podrÃ­amos haber usado
estratificaciÃ³n.

****** El nÃºmero de axiomas es numerable
Las fÃ³rmulas son sucesiones finitas de los sÃ­mbolos que hemos usado
antes. Al haber una cantidad numerable de sÃ­mbolos, las fÃ³rmulas son
numerables, y los axiomas que se generan en este esquema lo son.

****** NBG
En la teorÃ­a axiomÃ¡tica de Von-Neumann se usa un nÃºmero finito de
axiomas.

****** Diferencia de conjuntos
\[A-B = \{x \in A \mid x \notin B\}\]

****** IntersecciÃ³n de conjuntos
NÃ³tese que todavÃ­a no podemos definir la uniÃ³n arbitraria.

\[
\bigcap {\cal F} = \{x \in A \mid \forall y \in {\cal F} x \in y\}
\]

***** 3. Axioma del par
Dados dos conjuntos, tenemos uno que los contiene a los dos:

\[
\forall a,b : \exists z:
(a\in z \vee b \in z)
\]

****** Naturales
Con este axioma podemos construir los naturales en su segunda 
construcciÃ³n.

****** Hay conjuntos no vacÃ­os
Hasta ahora, los axiomas eran consistentes con que sÃ³lo existiera
el conjunto vacÃ­o.

****** Pares ordenados
Definimos un par ordenado:

\[
(a,b) := \{a , \{a,b\}\}
\]

***** 4. Axioma de la uniÃ³n
Para una colecciÃ³n de conjuntos, crearemos una uniÃ³n:

\[
\forall {\cal F}: \exists A: \forall Y,x: (x \in Y \wedge Y \in {\cal F}) \implies x \in A
\]

***** 5. Axioma del infinito
Construye directamente los nÃºmeros naturales:

\[
\exists I: \varnothing \in I \wedge \forall x: x\in I \implies S(x) \in I \longrightarrow \exists \mathbb{N}
\]

Lleva a la existencia de cardinales grandes.

****** Finitud
Hasta ahora, podrÃ­amos trabajar suponiendo todos los conjuntos finitos.

***** 6. Axioma del conjunto potencia
Existencia del conjunto potencia:

\[
\forall x: \exists y: \forall z: z \subseteq x \implies z \in y
\]

****** Producto cartesiano
Podemos definir el producto cartesiano de $A,B$. Tenemos que todos los
elementos $a\in A,b \in B$ llevan a $\{a,b\} \in {\cal P}(A,B)$.

\[
A \times B = \{ x \in {\cal P}{\cal P}(A \cup B) \mid x = (a,b), a \in A, b \in B\}
\]

NÃ³tese que asÃ­ sÃ³lo hemos creado el producto cartesiano finito.

****** Funciones
Con el producto cartesiano, podemos pasar a ver las funciones como su
grÃ¡fico.
***** 7. Axioma de reemplazamiento
Con estos seis axiomas, que constituyen la teorÃ­a de Zermelo, no
podemos definir el conjunto con:

\[
\mathbb{N}, {\cal P}\mathbb{N}, {\cal P}{\cal P}\mathbb{N}, \dots
\]

Un predicado funcional es una fÃ³rmula con variables libres $\varphi(x,y)$ 
que se comporta como una funciÃ³n:

\[
\forall x: \exists! y: \varphi(x,y)
\]

Tenemos entonces:

\[
\forall z: \exists \omega: \forall x,y: (x \in z \wedge \varphi(x,y) \implies y \in \omega)
\]

****** Definiendo el conjunto de potencias
Usando la funciÃ³n $n \mapsto {\cal P}^n(\mathbb{N})$.

****** Definiendo la imagen por un funcional
Definimos $\{x \in A \mid \phi(x)\}$ partiendo en el caso de que exista y 
de que no.

****** No existe el conjunto de todos los conjuntos.
Si existiera, podrÃ­amos usar todas las topologÃ­as triviales y desde ahÃ­
el conjunto universal.
***** 8. Axioma de elecciÃ³n
Para cualquier familia de conjuntos sin ninguno vacÃ­o,

\[
\forall {\cal F} : 
\left(
\forall x \in {\cal F} :\varnothing \neq x
\implies 
\exists f : {\cal F} \to \bigcup {\cal F}: f(a) \in A
\right)
\]

Siendo $f$ una funciÃ³n.

***** 9. Axioma de fundaciÃ³n
Para cualquier conjunto no vacÃ­o:

\[
\forall x: (x \neq \varnothing \implies \exists y: y \in x \wedge x \cap y = \varnothing)
\]
** Programming                                                                                           :programming:
*** Coq tactics
**** Difference Defined / Qed                                                                              :nodrill:
SCHEDULED: <2019-03-03 Sun>
:PROPERTIES:
:ID:       5cf0a7c8-363d-484d-a007-aa8105effeed
:DRILL_LAST_INTERVAL: 253.9188
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:00]
:END:
What is the difference between =Defined= and =Qed=?

***** Answer
=Defined= produces a *transparent definition* whereas =Qed= provides
an *opaque one*.

Reference: http://gallium.inria.fr/blog/coq-eval/

**** Tactics
***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2019-03-16 Sat>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       17c701dd-28d1-4e03-9b29-43cb7bc33359
:DRILL_LAST_INTERVAL: 266.5299
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:07]
:END:

The [exact] tactic [solves a goal by providing an inhabitant].

***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2019-06-13 Thu>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       5be5f1f6-d8ff-4e79-8032-11a14c6d193d
:DRILL_LAST_INTERVAL: 349.8314
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.125
:DRILL_EASE: 2.96
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:09]
:END:

The [split] tactic [introduces a conjunction].

***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2019-01-24 Thu>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       3ee44b74-7bfb-46fd-bdf6-9095a0c7ddd0
:DRILL_LAST_INTERVAL: 215.7602
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 9
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.62
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:11]
:END:

The [absurd] tactic [eliminates falsehood].

***** Coq Tactic                                                                                            :drill:
SCHEDULED: <2018-10-12 Fri>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       ef5486ca-801f-49cb-826b-db3e15806159
:DRILL_LAST_INTERVAL: 152.67
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 12
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 3.918
:DRILL_EASE: 3.1
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:45]
:END:

The [replace] tactic [substitutes all ocurrences of a term using a path].

****** Syntax
=replace {term} with {term2} by {tactic}=

where ={tactic}= solves ={term1 = term2}=.

***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2019-03-16 Sat>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       ef5486ca-801f-49cb-826b-db3e15806159
:DRILL_LAST_INTERVAL: 305.2936
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-15 Tue 23:14]
:END:

The [firstorder] tactic [automates first-order reasoning].

***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2019-04-04 Thu>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       ef5486ca-801f-49cb-826b-db3e15806159
:DRILL_LAST_INTERVAL: 286.3133
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:

The [set] tactic [defines a term that can be used later].

****** Syntax
set ({var} := {def}).
***** Coq Tactic                                                                                          :nodrill:
SCHEDULED: <2018-05-24 Thu>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       fbca0b6d-d8cd-4669-9f32-a2050dcf5824
:DRILL_LAST_INTERVAL: 107.9262
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.428
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-02-05 Mon 22:49]
:END:

The [f_equal] tactic [proves {f x = f' x'} with {f = f'} and {x = x'}].

***** Coq Tactic                                                                                            :drill:
SCHEDULED: <2019-04-09 Tue>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       b84bd40d-3cef-4b97-b8f8-4d3ecfc5ff4f
:DRILL_LAST_INTERVAL: 290.942
:DRILL_REPEATS_SINCE_FAIL: 6
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.8
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-22 Fri 22:05]
:END:

The [ring] tactic [solves equations on a ring structure].
*** Algorithms
**** Data structures
***** Binary search tree                                                                                    :drill:
SCHEDULED: <2018-08-04 Sat>
:PROPERTIES:
:ID:       df4a8afe-3bf6-4ff8-997c-6598e693ba8b
:DRILL_LAST_INTERVAL: 83.8998
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:41]
:END:
Definition of (unbalanced) Binary search tree (BST).

****** Definition

    8
   / \
  3   10
 /    / \
1    9   12

****** Complexity of searching in average and worst case
Searching is O(n) in the worst case (linear tree), but O(log n) in
average.

***** Red-black tree                                                                                      :nodrill:
SCHEDULED: <2018-05-19 Sat>
:PROPERTIES:
:ID:       83f6dfb9-a29a-400b-9ced-dff41732dae4
:DRILL_LAST_INTERVAL: 4.4178
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.8
:DRILL_EASE: 1.94
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-15 Tue 23:12]
:END:
Complexity of a red-black tree.

****** Complexity
Has O(log n) in worst case for searching, inserting and deleting.

****** Balancing properties                                                                                :extra:

 * It is a binary search tree.
 * Both children of a red node are black.
 * Every path to a leaf contains the same number of red and blacks.

These enforce that the path to the farthest leaf is no more than twice
the path to the nearest leaf.

**** Array sorting
***** Quicksort                                                                                             :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       ead02bcf-2b4c-412b-97da-485b069eb45a
:DRILL_LAST_INTERVAL: 12.2695
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.875
:DRILL_EASE: 2.76
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:11]
:END:
Complexity of quicksort.

****** Complexity
O(n^2) in the worst case, O(n log n) on average.

***** Mergesort                                                                                             :drill:
SCHEDULED: <2018-07-31 Tue>
:PROPERTIES:
:ID:       f8a18a27-422d-4fee-84a6-81b51a79fa4b
:DRILL_LAST_INTERVAL: 79.8424
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-12 Sat 17:43]
:END:
Complexity of mergesort.

****** Complexity
O(n log n) in all cases.

***** Heapsort
Complexity of heapsort.

****** Complexity
O(n log n) in all cases.
*** Monads for functional programming - Philip Wadler                                          :paper:monads:haskell:
:PROPERTIES:
:INTERLEAVE_PDF: ~/pdf/wadler_monads_for_functional_programming.pdf
:END:
**** Notes for page 12
:PROPERTIES:
:interleave_page_note: 12
:END:

Monads can be defined in terms of unit, join and map. They can also be defined
in terms of unit and bind.
*** Qiaochu Yuan - [[https://qchu.wordpress.com/2018/02/07/gradient-descent/][Gradient descent]]                                                                        :analysis:
**** Problem with gradient descent                                                                           :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       ccf30e30-56c9-407c-b2cb-c1b7d364c07c
:DRILL_LAST_INTERVAL: 4.1408
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 22:27]
:END:
Problem with gradient descent and units.

***** Answer
It is not dimensionally consistent.  If the parameters youâre
optimizing over have units of length, and the loss function is
dimensionless, then the derivatives youâre subtracting have units of
inverse length.

** Computability and complexity                                                             :computability:complexity:
*** P?=NP - Scott Aaronson                                                                         :complexity:paper:
* Notes
** Ecuaciones diferenciales II
*** Temas de teorÃ­a

 1. Resultados del trabajo.
 2. AcotaciÃ³n con funciones guÃ­a.
 3. Lema de Gronwall.
 4. Unicidad de Peano.

*** Prerrequisitos
**** Prehaces y dominios
Conociendo el concepto de prehaz, parece que tiene sentido escribir
siempre o casi siempre las funciones con su dominio, $f|_D$.

**** TODO Teorema de la funciÃ³n inversa
**** ResoluciÃ³n de ecuaciones lineales no homogÃ©neas
Dada una ecuaciÃ³n lineal $x' = xa(t) + b(t)$, la resolvemos con

\[
x = e^{A(t)} \left( k + \int_{t_0}^t b(s)e^{-A(s)}\,ds \right)
\]

donde $A$ es una primitiva de $a$. La constante $k$ ajustarÃ¡
la condiciÃ³n inicial

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-13 Fri>
:PROPERTIES:
:ID:       5d987c6b-f319-4ee8-81ab-0371d66af4a6
:DRILL_LAST_INTERVAL: 30.7501
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:13]
:END:
Resolver la ecuaciÃ³n diferencial $x' = xa(t) + b(t)$.

****** Answer

\[
x = e^{A(t)} \left( k + \int_{t_0}^t b(s)e^{-A(s)}\,ds \right)
\]

donde $A$ es una primitiva de $a$.

**** Jacobiana
La aplicaciÃ³n lineal jacobiana en un punto, $J_F(p)$, cumple que

\[
\lim_{\|x-p\| \to 0} \frac{\|F(x)-F(p) -J_F(p)(x-p)\|}{\|x-p\|} = 0.
\]

En el caso de espacios finitos tenemos la matriz jacobiana

\[\begin{pmatrix}
\pdv{y_1}{x_1} & \dots & \pdv{y_1}{x_n} \\
\vdots & \ddots & \vdots \\
\pdv{y_m}{x_1} & \dots & \pdv{y_m}{x_n} \\
\end{pmatrix}
\]

En ocasiones notaremos simplemente a la jacobiana como $F'$.

***** Card: Jacobiana                                                                                       :drill:
SCHEDULED: <2018-09-12 Wed>
:PROPERTIES:
:ID:       4183447e-4b69-4117-a3bd-9181de1bd672
:DRILL_LAST_INTERVAL: 75.4403
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:48]
:END:
Â¿QuÃ© propiedad define a la Jabobiana $\mathrm{Jac}(F)_p$ Ã³ $F'(p)$?

****** Propiedad
Es la aplicaciÃ³n lineal cumpliendo

\[
\lim_{\|x-p\| \to 0} \frac{\|F(x)-F(p) - \mathrm{Jac}(F)_p(x-p)\|}{\|x-p\|} = 0.
\]

que existe cuando $F$ es diferenciable.

***** Card: construcciÃ³n de la Jacobiana                                                                    :drill:
SCHEDULED: <2018-09-09 Sun>
:PROPERTIES:
:ID:       05dc6c9b-cb76-4ee7-85b5-a24f224aad1b
:DRILL_LAST_INTERVAL: 72.0671
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:55]
:END:
Â¿CÃ³mo construir la jacobiana en espacios finitos?

****** ConstrucciÃ³n

\[\begin{pmatrix}
\displaystyle\pdv{f_1}{x_1} & \dots & 
\displaystyle\pdv{f_1}{x_n} \\
\vdots & \ddots & \vdots \\
\displaystyle\pdv{f_m}{x_1} & \dots & 
\displaystyle\pdv{f_m}{x_n} \\
\end{pmatrix}
\]

**** Hessiana
:PROPERTIES:
:ID:       0cebe934-5d31-494d-bfd6-891d9c4c81ec
:END:
La hessiana existe cuando lo hacen todas las segundas derivadas
parciales de una funciÃ³n $f \colon \mathbb{R}^n \to \mathbb{R}$.

\[\begin{pmatrix}
\pdv[2]{f}{x_1} & \dots & \pdv{f}{x_1}{x_n} \\
\vdots & \ddots & \vdots \\
\pdv{f}{x_n}{x_1} & \dots & \pdv[2]{f}{x_n} \\
\end{pmatrix}
\]

Por el [[https://es.wikipedia.org/wiki/Teorema_de_Clairaut][teorema de Schwarz-Clairaut]], es simÃ©trica.

***** Card: definiciÃ³n                                                                                      :drill:
SCHEDULED: <2018-08-21 Tue>
:PROPERTIES:
:ID:       8d680bfb-66f0-4872-91bf-3011a3a19ec1
:DRILL_LAST_INTERVAL: 67.5636
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-14 Thu 11:49]
:END:
Â¿QuÃ© es la Hessiana?Â¿cuÃ¡ndo existe?

****** Hessiana

\[\begin{pmatrix}
\pdv[2]{f}{x_1} & \dots & \pdv{f}{x_1}{x_n} \\
\vdots & \ddots & \vdots \\
\pdv{f}{x_n}{x_1} & \dots & \pdv[2]{f}{x_n} \\
\end{pmatrix}
\]

existe cuando lo hacen todas las segundas derivadas parciales.

***** Card: simetrÃ­a                                                                                        :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       570c9701-1fb1-4c5c-b381-8bde318e961f
:DRILL_LAST_INTERVAL: 28.7981
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-19 Sat 11:01]
:END:
Â¿CuÃ¡ndo es simÃ©trica la Hessiana?Â¿por quÃ©?

****** Respuesta
Cuando las derivadas parciales cruzadas segundas son ademÃ¡s
continuas, por el [[https://es.wikipedia.org/wiki/Teorema_de_Clairaut][teorema de Schwarz-Clairaut]], es simÃ©trica.
**** Convexidad
La funciÃ³n $e^x$ es convexa.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       9bc614ac-a897-41af-be8f-c1035ab6c847
:DRILL_LAST_INTERVAL: 27.6005
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:45]
:END:
La funciÃ³n $e^x$ es Â¿cÃ³ncava o convexa?

****** Respuesta
Convexa.
**** Hessiana y convexidad
A continuous, twice differentiable function of several variables is
convex on a convex set if and only if its Hessian matrix of second
partial derivatives is positive semidefinite on the interior of the
convex set.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       beace7dc-3d4d-4adf-932f-2a369b60d6a6
:DRILL_LAST_INTERVAL: 23.3996
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:43]
:END:
CaracterizaciÃ³n de convexa de una funciÃ³n dos veces diferenciable de
varias variables en un conjunto convexo.

****** CaracterizaciÃ³n
Es convexa si y sÃ³lo si la Hessiana es semidefinida positiva.
NÃ³tese que eso no hace que tenga que ser forzosamente estrictamente
convexa y coerciva.

**** Wronskiana
La *wronskiana* no nula implica independencia lineal. NÃ³tese sin
embargo que puede anularse incluso cuando son independientes.
(Contraejemplo de [[id:8e9ccec0-c9de-4d00-a08e-3de44d448ca5][Peano]])

\[
W(f_1,\dots,f_n)(x) = \begin{vmatrix}
f_1(x) & \dots & f_n(x) \\
\vdots & \ddots & \vdots \\
f_1^{n-1)}(x) & \dots & f_n^{n-1)}(x) \\
\end{vmatrix}
\]

Puede usarse la [[https://en.wikipedia.org/wiki/Abel%2527s_identity][identidad de Abel]] para calcular la wronskiana incluso
cuando no se conocen las soluciones.
**** Nociones de continuidad y convergencia
***** Continuidad uniforme
$f \colon I \to \mathbb{R}^d$ es *uniformemente continua* si 

\[
\forall \varepsilon \colon \exists \delta \colon |t-s|<\delta \implies \|f(t)-f(s)\| \leq \varepsilon.
\]

***** Convergencia puntual
$f_n \overset{c.p.}\longrightarrow f$ si $\forall t \in I\colon \{\| f_n(t) - f(t)\|\}_{n \in \mathbb{N}} \to 0$.

***** Convergencia uniforme
$f_n \overset{c.u.}\longrightarrow f$ si $\forall \varepsilon > 0\colon \exists n_0\colon \forall n > n_0\colon \forall t \in I\colon \|f_n(t)-f(t)\| < \varepsilon$.

**** Cards: tipos de ecuaciones                                                                           :noexport:
***** Ecuaciones homogÃ©neas: forma                                                                          :drill:
SCHEDULED: <2018-08-26 Sun>
:PROPERTIES:
:ID:       87915c2b-ff33-4e9d-98c0-07e66a01ac68
:DRILL_LAST_INTERVAL: 97.434
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-21 Mon 14:54]
:END:
Forma de una ecuaciÃ³n homogÃ©nea.

****** Forma
\[
x' = f \left( \frac{x}{t} \right)
\]

***** Ecuaciones homogÃ©neas: resoluciÃ³n                                                                     :drill:
SCHEDULED: <2018-09-11 Tue>
:PROPERTIES:
:ID:       e558b5d7-7d81-4d91-9486-40352296fda2
:DRILL_LAST_INTERVAL: 90.6516
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.428
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:22]
:END:
CÃ³mo resolver una ecuaciÃ³n homogÃ©nea

\[
x' = f \left( \frac{x}{t} \right).
\]

****** ResoluciÃ³n
Aplicamos el cambio de variable $u = x/t$, que nos
lleva a 

\[u' = \frac{1}{t}(f(u) - u).\]

***** EcuaciÃ³n lineal escalar                                                                               :drill:
SCHEDULED: <2018-07-14 Sat>
:PROPERTIES:
:ID:       ead2dfed-95a4-41ad-b65e-d09cf22fbd8d
:DRILL_LAST_INTERVAL: 32.2866
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.715
:DRILL_EASE: 1.66
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:48]
:END:
CÃ³mo resolver, para $a,b \in {\cal C}(I,\mathbb{R})$;

\[
x' = a(t)x + b(t)
\]

****** ResoluciÃ³n
Una parte resuelve la homogÃ©nea, la otra es una soluciÃ³n
particular

\[
x = Ke^{(\int a)} + \left( \int_{t_0}^t b(z) e^{-(\int^{z} a)}\,dz \right) e^{(\int a)}
\]

Alternativamente, si $A$ es la primitiva de $a$, tenemos

\[x(t) = Ke^{A(t)} + \left( \int_{t_0}^t b(s) e^{-A(s)}\,ds \right) e^{A(t)}\]

***** EcuaciÃ³n de Bernoulli                                                                                 :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       c7712582-1d68-4dd0-9ebf-9b9a2afcd463
:DRILL_LAST_INTERVAL: 4.633
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.125
:DRILL_EASE: 2.28
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:41]
:END:
CÃ³mo resolver

\[
x' + ax + bx^n = 0.
\]

****** ResoluciÃ³n
Retiramos el caso trivial $x = 0$ y tomamos el cambio de
variable $u = x^{1-n}$, que la transforma en lineal

\[
u' + (1-n)au + (1-n)b = 0.
\]

***** EcuaciÃ³n de Ricatti                                                                                   :drill:
SCHEDULED: <2018-08-28 Tue>
:PROPERTIES:
:ID:       1113da24-b90d-4ab0-94d8-e65ae4710470
:DRILL_LAST_INTERVAL: 76.692
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.75
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:58]
:END:
CÃ³mo resolver

\[
x' + ax + bx^2 = f(x).
\]

****** ResoluciÃ³n
Conociendo una soluciÃ³n de la ecuaciÃ³n tomamos el cambio

\[
u = \frac{1}{x - \varphi}
\]

suele ser una elecciÃ³n algo de la forma $\varphi = -1/x$.

***** EcuaciÃ³n exacta                                                                                       :drill:
SCHEDULED: <2018-08-14 Tue>
:PROPERTIES:
:ID:       69ceda6b-32b1-4bc2-b8fd-71bc68c66072
:DRILL_LAST_INTERVAL: 61.6441
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 14
:DRILL_FAILURE_COUNT: 7
:DRILL_AVERAGE_QUALITY: 2.143
:DRILL_EASE: 2.28
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:01]
:END:
Dar la forma de una ecuaciÃ³n exacta, Â¿cuÃ¡l es la soluciÃ³n trivial?

****** ResoluciÃ³n
Una soluciÃ³n exacta es de la forma

\[
M + Nx' = 0
\]

donde son de la forma $M = \pdv{F}{t}$ y $N = \pdv{F}{x}$. 
Tiene soluciÃ³n trivial en $F = \mathrm{cte.}$
*** Tema 1. Existencia y unicidad. EcuaciÃ³n de Volterra
**** 1.1. Definiciones
:PROPERTIES:
:ID:       ba5ec2ba-b54a-455b-9b6c-9fca85940234
:END:
Una *ecuaciÃ³n diferencial ordinaria* (EDO) $x' = f(t,x)$ viene
dada por un dominio (abierto conexo) $D \subseteq \mathbb{R} \times \mathbb{R}^d$ y una $f \colon D \to \mathbb{R}^d$.
Llamamos

 * $t$, a la variable independiente o temporal;
 * $x$, a la incÃ³gnita o variable dependiente;
 * $f$, al campo de vectores;
 * $D$, al dominio de la ecuaciÃ³n.

Dado $I$ intervalo abierto, una soluciÃ³n $\varphi \colon I \to \mathbb{R}^d$ de una EDO cumple

 * $(t,\varphi(t)) \in D$, se queda en el dominio;
 * $\varphi \in {\cal C}^1(I,\mathbb{R}^d)$, es derivable;
 * $\varphi'(t) = f(t,\varphi(t))$, es soluciÃ³n.

Un *problema de valores iniciales* (PVI) estÃ¡ dado como
una EDO con una condiciÃ³n inicial,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

donde $(t_0,x_0) \in D$. Y una soluciÃ³n debe cumplir ademÃ¡s

 * $\varphi(t_0) = x_0$ con $t_0 \in I$.

**** 1.2. ProlongaciÃ³n de soluciones
***** SoluciÃ³n prolongable y maximal
La soluciÃ³n de un PVI $\varphi \colon I \to \mathbb{R}^d$ es *prolongable* si existe
$\phi \colon J \to \mathbb{R}^d$ soluciÃ³n tal que $I \subset J$ y al restringir, $\phi_{|I} = \varphi$.
Una soluciÃ³n es *maximal* si no es prolongable.

***** ConcatenaciÃ³n de soluciones
:PROPERTIES:
:ID:       078ac783-76bf-4445-ad46-3f91ea9d1ef6
:END:
Sean $\varphi_1 \colon I_1 \to \mathbb{R}^d$ y $\varphi_2 \colon I_2 \to \mathbb{R}^d$ soluciones de $x' = f(t,x)$.
Si $\varphi_1(\tau) = \varphi_2(\tau)$, entonces

\[\psi = \left\{\begin{aligned}
\varphi_1(t) &\ \text{ si } t \leq \tau \\
\varphi_2(t) &\ \text{ si } t > \tau \\
\end{aligned}\right.\]

es una soluciÃ³n.

****** Proof
Cumple

 * que se queda en el dominio trivialmente;
 * que es derivable en cada trozo por serlo las soluciones y
   derivable en el punto por coincidir sus dos derivadas laterales;
 * que coinciden porque ambas deben ser soluciones y en
   particular soluciones en el punto $\tau$, lo que hace a $\psi$ soluciÃ³n.

***** GarantÃ­as de maximalidad
Una soluciÃ³n $\varphi \colon (\alpha,\omega) \to \mathbb{R}^d$ es maximal cuando

 * $(\alpha,\omega) = (-\infty,\infty)$, *dominio infinito*;

 * $\omega < +\infty$ con $\lim_{t \to \omega} \|\varphi(t)\| = +\infty$, *explota en tiempo finito*;

 * $\omega < +\infty$ con $\lim_{t \to \omega}\varphi(t) = \xi \in \mathbb{R}^d$, pero $(\omega,\xi) \notin D$, *toca la frontera*.

 * $\omega < +\infty$ con $\not\exists \lim_{t \to \omega}\varphi(t)$, *ningÃºn punto la prolonga*.

AnÃ¡logamente con casos para $\alpha > -\infty$.

**** 1.3. Unicidad y soluciones maximales
***** Unicidad
Dado un problema de valores iniciales,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

hay

 * *Unicidad local* si, para todo par de soluciones, existe un
   intervalo abierto conteniendo al instante inicial donde son
   iguales.

 * *Unicidad en un intervalo* $J$, cuando todo par de soluciones
   $\varphi_1 \colon I_1 \to \mathbb{R}^d$ y $\varphi_2 \colon I_2 \to \mathbb{R}^d$ son iguales en $J \cap I_1 \cap I_2$;

 * *unicidad en el futuro* es unicidad en $[t_0,+\infty)$,

 * *unicidad en el pasado* es unicidad en $(-\infty,t_0]$,

 * *unicidad global*, unicidad en todo $\mathbb{R}$.

****** Card: unicidad local                                                                                :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       0261396e-1324-4bde-b01b-842f881f869e
:DRILL_LAST_INTERVAL: 10.5298
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:52]
:END:
Dado un problema de valores iniciales,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

define *unicidad local*.

******* Answer
Para todo par de soluciones existe un intervalo abierto conteniendo al
instante inicial donde son iguales.

******* Otra formulaciÃ³n
Para todo par de soluciones $\varphi_1 \colon I_1 \to \mathbb{R}^d$ y $\varphi_2 \colon I_2 \to \mathbb{R}^d$ existe
un intervalo *abierto* conteniendo al instante inicial, $t_0 \in J = \mathring{J}$,
donde $\varphi_1, \varphi_2$ son iguales.

****** Card: unicidad en un intervalo                                                                      :drill:
SCHEDULED: <2018-06-28 Thu>
:PROPERTIES:
:ID:       5024dcc3-fc4e-493b-a3d2-43a3c573855d
:DRILL_LAST_INTERVAL: 16.1373
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:38]
:END:
Dado un problema de valores iniciales,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

define *unicidad en un intervalo* $J$.

******* Answer
Todo par de soluciones coincide en los puntos de $J$ en los que estÃ¡n
definidas ambas.

******* Otra formulaciÃ³n
Cuando todo par de soluciones $\varphi_1 \colon I_1 \to \mathbb{R}^d$ y $\varphi_2 \colon I_2 \to \mathbb{R}^d$ son iguales
en $J \cap I_1 \cap I_2$.

****** Card: unicidad en el futuro                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       8bf16932-05b4-4fb2-810c-cc8f17c70743
:DRILL_LAST_INTERVAL: 11.4402
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:45]
:END:
Dado un problema de valores iniciales,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

define *unicidad en el futuro*.

******* Answer
Unicidad en el intervalo $[t_0,+\infty)$.

****** Card: unicidad global                                                                               :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       ecc0e77c-4708-4b2c-aac6-f44115513990
:DRILL_LAST_INTERVAL: 28.4551
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:15]
:END:
Dado un problema de valores iniciales,

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

define *unicidad global*.

******* Answer
Unicidad en el intervalo $\mathbb{R}^d$.

******* Otra formulaciÃ³n
Todo par de soluciones coincide en los puntos en los que estÃ¡n
definidas ambas.

***** Lema 1. Unicidad global desde unicidad local
:PROPERTIES:
:ID:       0b968d38-72b0-41e4-bdbd-a19fd33ecd59
:END:
Si para todo $(t_{0},x_0) \in D$, el problema de valores iniciales
cumple unicidad local, entonces para todo $(t_{0},x_0) \in D$ se
cumple la unicidad global.

****** Proof
Sean $\varphi_1|_{I_1}$ y $\varphi_2|_{I_2}$ soluciones. Como $I_1 \cap I_2$ es conexo, basta
probar $H = \left\{ t \in I_1 \cap I_2 \mid \varphi_1(t) = \varphi_2(t) \right\}$ abierto y cerrado.
Es cerrado porque es nÃºcleo de una continua. Es abierto respecto
a $I_1 \cap I_2$ porque dado $t_0 \in H \cap I_1 \cap I_2$, tomamos el problema con
el valor $x_0 = \varphi_1(t_0) = \varphi_2(t_0)$ y existirÃ¡ un intervalo $t_0 \in J \subset H$
por unicidad local.

***** Lema 2. Unicidad global da Ãºnica soluciÃ³n maximal
Si un PVI verifica unicidad global, tiene una Ãºnica soluciÃ³n maximal.
# Si no existen soluciones, la soluciÃ³n vacÃ­a es maximal, entiendo.

****** Proof
Sea $\Sigma(P)$ el conjunto de soluciones, tomamos el conjunto
siguiente que es intervalo por uniÃ³n de intervalos con
un punto comÃºn $(t_0,x_0),

\[
J = \bigcup_{\varphi|_I \in \Sigma(P)} I
\]

y definimos $\psi|_{J}(t)$ como el valor de una soluciÃ³n cualquiera en
$t$, que por unicidad global deberÃ¡ ser siempre igual. Esta funciÃ³n es
soluciÃ³n, se queda en el dominio, es derivable por ser localmente
(usando que es abierto) una soluciÃ³n y es soluciÃ³n trivialmente; es
maximal por definiciÃ³n y cualquier otra maximal deberÃ­a estar en el
mismo intervalo y por unicidad global, ser exactamente la misma.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-15 Sun>
:PROPERTIES:
:ID:       cc21bcb9-acde-4eec-aa64-945138de1035
:DRILL_LAST_INTERVAL: 33.3673
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:10]
:END:
Si existen soluciones, Â¿quÃ© implica la unicidad global de soluciÃ³n
respecto de las maximales?

******* Answer
Unicidad global implica que hay una Ãºnica soluciÃ³n maximal.

***** Unicidad local en ecuaciones de variables separadas
Dadas $a|_{J_1}$ y $g|_{J_{2}}$ continuas con $t_0 \in J_1$ y $x_0 \in J_2$, intervalos.

\[\left.\begin{aligned}
x' &= a(t)g(x) \\
x(t_0) &= x_0
\end{aligned}\right\} \mathit{(P)}
\]

Tenemos que

 1) el problema tiene soluciones;

 2) si $g(x_0) \neq 0$, hay unicidad local;

 3) si $g(x_0) = 0$, $a(t_0) \neq 0$, existe $G|_{J_2}$ derivable con $G(x_0) = 0$ 
    y ademÃ¡s $G'(x) = \frac{1}{g(x)}$ para todo $x$ en un semientorno de $x_0$; entonces
    no hay unicidad local.

****** Proof
Consideraremos los dos casos, $g(x_0) = 0$ y $g(x_0) \neq 0$.

 * Cuando $g(x_0) \neq 0$, tenemos un intervalo en el que el signo se
   conserva. Sea $A$ la antiderivada de $a$ con $A(t_0)=0$ y $G$ la
   antiderivada de $1/g(x)$ con $G(x_0)=0$. Cualquier soluciÃ³n $x$
   debe cumplir

   \[
   G(x(t)) =
   \int_{x(t_0)}^{x(t)} \frac{du}{g(u)} =
   \int_{t_0}^{t} \frac{x'(s)}{g(x(s))} \; ds =
   \int_{t_0}^{t} a(s) \; ds.
   \]

   Aplicamos Teorema de la FunciÃ³n Inversa para obtener $G^{-1}$ 
   derivable y definir $\varphi = G^{-1} \circ A$. Puede definirse localmente 
   dentro del dominio por continuidad, es derivable, es soluciÃ³n

   \[
   \varphi'(t) = 
   a(t) \cdot \left( G^{-1} \right)' \left( A(t) \right) =
   a(t) \cdot g \left( G^{-1} \left( A(t) \right)  \right) =
   a(t) \cdot g(\varphi(t)).
   \]

   y ademÃ¡s $\varphi(t_0) = G^{-1}(A(t_0)) = x_0$.

 * Cuando $g(x_0) = 0$, tenemos la soluciÃ³n trivial $\varphi(t) = x_0$. AdemÃ¡s,
   asumiendo que tenemos $G$ en con derivada dada en un semientorno
   superior, usamos que $A$ es continua para definir en algÃºn intervalo
   la siguiente soluciÃ³n.
   
   \[\varphi(t) = \left\{\begin{aligned}
   x_0 &\ \text{ si } t \leq t_0 \\
   G^{-1}(A(t)) &\ \text{ si } t > t_0 \\
   \end{aligned}\right.\]

   La soluciÃ³n es trivialmente continua en $t_0$. Es derivable porque
   ambas derivadas laterales son $0$. Esta soluciÃ³n es distinta de
   $x_0$ usando $a(t_0) \neq 0$ (?).

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       61153ecb-7cb0-4c5f-9ab3-1c9866b643b5
:DRILL_LAST_INTERVAL: 4.8253
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:40]
:END:
Â¿QuÃ© sabemos sobre existencia y unicidad en EDOs de variables separadas?

******* Answer
Para $x' = a(t)g(x)$.

 1) Existen soluciones.
 2) Si $g(x_0) \neq 0$; entonces hay unicidad local.
 3) Si $g(x_0) = 0$, $a(t_0) \neq 0$, y existe $G$ tal que $G(x_0) = 0$ y $G'(x) = 1/g(x)$;
    entonces no hay unicidad local.

**** 1.4. Teorema de Unicidad de Peano
:PROPERTIES:
:ID:       cc790c02-b622-4fc1-8bb1-ac3c05e73009
:END:
Sea un PVI dado por un campo continuo sobre un abierto $D \subset \mathbb{R} \times \mathbb{R}$,
al que llamamos $f \colon D \to \mathbb{R}$.

Para un PVI con $d=1$, es decir $f \colon D \subseteq \mathbb{R}\times \mathbb{R} \to \mathbb{R}$,

 1) si $\forall t \geq t_0$, $f(t,-)$ es decreciente, hay unicidad en el futuro;
 2) si $\forall t \leq t_0$, $f(t,-)$ es creciente, hay unicidad en el pasado.

***** Proof
En el caso de $f(t,x)$ decreciente, con dos soluciones $\varphi_1\colon I_1 \to \mathbb{R}$
y $\varphi_2 \colon I_2 \to \mathbb{R}$, definimos $h(t) = (\varphi_1(t)-\varphi_2(t))^2$ con derivada

\[\begin{aligned}
h'(t) &= 
2(\varphi_1(t)-\varphi_2(t))(\varphi_1'(t)-\varphi_2'(t)) \\&=
2(\varphi_1(t)-\varphi_2(t))(f(t,\varphi_1(t))-f(t,\varphi_2(t))) \leq 0,
\end{aligned}\]

que la hace decreciente. Pero $h(t) \geq 0$ en general y $h(t_0) = 0$, 
por lo que debe ser $0$ para $t \geq 0$. El segundo caso es anÃ¡logo,
siendo $h'(t)$ positiva y por tanto $h(t)$ creciente con $h(t_0) = 0$.

***** Card: enunciado                                                                                       :drill:
SCHEDULED: <2018-09-24 Mon>
:PROPERTIES:
:ID:       d6ac89c9-f053-4b0d-a8e0-c0233c137e87
:DRILL_LAST_INTERVAL: 102.8547
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.571
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:56]
:END:
Enuncia el teorema de Unicidad de Peano.

****** Enunciado
Para un PVI con $d=1$, es decir $f \colon \mathbb{R}\times \mathbb{R} \to \mathbb{R}$,

 1) si $\forall t \geq t_0$, $f(t,-)$ es decreciente, hay unicidad en el futuro;
 2) si $\forall t \leq t_0$, $f(t,-)$ es creciente, hay unicidad en el pasado.

***** Card: demostraciÃ³n                                                                                    :drill:
SCHEDULED: <2018-08-18 Sat>
:PROPERTIES:
:ID:       efae3e4f-a565-4023-bb0c-e1c762db3e1f
:DRILL_LAST_INTERVAL: 89.5744
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-20 Sun 23:25]
:END:
Â¿CuÃ¡l es la idea para demostrar el teorema de unicidad de Peano?
Supongamos $f(t,x)$ decreciente y dos soluciones $\varphi_1\colon I_1 \to \mathbb{R}$
y $\varphi_2 \colon I_2 \to \mathbb{R}$. Queremos ver unicidad en el futuro.

****** Idea
Definimos $h(t) = (\varphi_1(t)-\varphi_2(t))^2$ y calculamos su derivada para
ver que es decreciente.

**** 1.5. Problema dual en el tiempo
El problema dual en el tiempo hace una simetrÃ­a del problema respecto
al eje temporal. Si $x(t)$ es soluciÃ³n de uno, $y(t) = x(-t)$ es la soluciÃ³n
del otro, definida en los intervalos correspondientes.

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}\qquad
\left.\begin{aligned}
y' &= -f(-t,y) \\
y(-t_0) &= x_0
\end{aligned}\right\}
\]

**** 1.6. EcuaciÃ³n integral de Volterra
***** Operador integral de Volterra
Dada una EcuaciÃ³n de Volterra, fijamos el operador de Volterra,
que tiene las soluciones como puntos fijos

\[
V(y)(t) = x_0 + \int_{t_0}^t f(s,y(s))\;ds.
\]

Lo consideramos $V \colon E \to E$ donde 

\[
E = \overline{B}(x_0,b) \subset {\cal C}([t_0-a,t_0+a];\mathbb{R}^d)
\]

es un espacio mÃ©trico completo con la norma infinito.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       d632fc17-20f9-4797-929d-05f51c4cd2ea
:DRILL_LAST_INTERVAL: 26.2679
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.08
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:57]
:END:
Define el operador integral de Volterra y su dominio

******* DefiniciÃ³n

\[
V(y)(t) = x_0 + \int_{t_0}^t f(s,y(s))\;ds.
\]

EstÃ¡ en el dominio ${\cal C}([t_0-a,t_0+a];\mathbb{R}^d)$.
Puede probarse que es $V \colon E \to E$ donde $E = \overline{B_0}(x_0,b)$.

***** SoluciÃ³n de la ecuaciÃ³n de Volterra
:PROPERTIES:
:ID:       b43c8fe5-6fdb-487e-a609-5620610d23a0
:END:
Para $I$ intervalo, $\varphi \colon I \to \mathbb{R}$ es soluciÃ³n de la ecuaciÃ³n si

 1. $t_0 \in I$;
 2. $(t,\varphi(t)) \in D$, en el dominio;
 3. $\varphi$, es continua en $I$;
 4. $\varphi(t) = x_0 + \int^t_{t_0} f(s,\varphi(s))\; ds$, cumple la ecuaciÃ³n.

***** Lema de soluciones de Volterra
Cada PVI

\[\left.\begin{aligned}
x' &= f(t,x) \\
x(t_0) &= x_0
\end{aligned}\right\}
\]

tiene asociada la ecuaciÃ³n integral de Volterra

\[
x(t) = x_0 + \int_{t_0}^t f(s,x(s))\;ds.
\]

Sea $\varphi \colon I \to \mathbb{R}^d$ para $I$ intervalo abierto. Equivalen

 1. $\varphi$ es [[id:ba5ec2ba-b54a-455b-9b6c-9fca85940234][soluciÃ³n de un PVI]];
 2. $\varphi$ es [[id:b43c8fe5-6fdb-487e-a609-5620610d23a0][soluciÃ³n de la ecuaciÃ³n de Volterra]] asociada.
 
****** DemostraciÃ³n
1 a 2. Tenemos trivialmente las dos primeras condiciones. La
continudad surge de la derivabilidad y por regla de Barrow,
$\varphi(t) = \varphi(t_0) + \int_{t_0}^t f(s,\varphi(s))\;ds$.

2 a 1. Tenemos trivialmente las dos primeras condiciones y el
punto de evaluaciÃ³n. Por Teorema Fundamental del CÃ¡lculo, $\varphi$
es derivable con precisamente la derivada que la hace soluciÃ³n.

**** 1.7. Algunos resultados Ãºtiles de anÃ¡lisis
***** 1. Lema del primer instante
:PROPERTIES:
:ID:       a439476b-8c77-4994-a736-9062ee91e7d0
:END:
Para $f \in {\cal C}(I,\mathbb{R})$ con $f(t_0) > 0$, se da una de las alternativas.

 * $\forall t \geq t_0\colon f(t) > 0$.

 * $\exists \tau > t_0\colon \forall t \in [t_0,\tau)\colon  f(t) > 0,\ f(\tau) = 0$.

****** Proof
Si no se cumple la primera condiciÃ³n, el conjunto
$H = \left\{ t \in I \mid t > t_0, f(t) = 0 \right\}$ es no vacÃ­o por Bolzano,
cerrado y acotado inferiormente. Su mÃ­nimo es el $\tau$ buscado.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-22 Sun>
:PROPERTIES:
:ID:       b544c2e9-8d67-4c68-8587-26873148e533
:DRILL_LAST_INTERVAL: 62.8257
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-20 Sun 23:26]
:END:
Para $f \in {\cal C}(I,\mathbb{R})$ con $f(t_0) > 0$, enunciar el lema del primer
instante.

******* Enunciado
Se da una de las alternativas.

 * $\forall t \geq t_0\colon f(t) > 0$.

 * $\exists \tau > t_0\colon \forall t \in [t_0,\tau)\colon  f(t) > 0,\ f(\tau) = 0$.

***** 2. Teorema del valor medio real
:PROPERTIES:
:ID:       3efc40d6-3ca0-4a0d-ba1a-41e752bab364
:END:
Dada $f \colon \mathbb{R} \to \mathbb{R}$ con $f \in {\cal C}^1$, para $a,b \in \mathbb{R}$ se tiene que
existe $c \in [a,b]$ tal que 

\[
f(a) - f(b) = f'(c)(a-b).
\]

***** 2. Teorema del valor medio en funciones reales de varias variables
Dada $f \in {\cal C}^1(\mathbb{R}^n,\mathbb{R})$, para $a,b \in \mathbb{R}^n$, se tiene que existe $c \in [a,b]$ tal que

\[
f(b)-f(a) = \left\langle \grad f(c), b-a \right\rangle.
\]

****** Proof
Parametrizamos para obtener una funciÃ³n escalar ${\cal C}^1$

\[
h(t) = f((1-t)a + tb)
\]

para la que se por [[id:3efc40d6-3ca0-4a0d-ba1a-41e752bab364][Teorema del valor medio real]] existe $h(1)-h(0) = h'(d)$.
Derivando obtenemos, llamando $c = (1-d)a + db$,

\[
f(b) - f(a) = \left\langle \grad f(d) , b - a \right\rangle.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-22 Wed>
:PROPERTIES:
:ID:       deccb6c2-471e-4d1a-baff-b96dc400f410
:DRILL_LAST_INTERVAL: 70.769
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.143
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:58]
:END:
Para $f \in {\cal C}^1(\mathbb{R}^n,\mathbb{R})$ y $a,b \in \mathbb{R}^n$, enunciar el teorema del valor medio
para funciones *reales* de varias variables.

******* Enunciado
Existe $c \in [a,b]$ tal que

\[
f(b)-f(a) = \left\langle \grad f(c), b-a \right\rangle.
\]

***** 2. Teorema del valor medio multivariable (versiÃ³n apuntes)
Dadas $[x,y]\subseteq \Omega$,

\[
f(x) - f(y) = \left( \int_0^1 f'(sy + (1-s)x) \,ds \right) \cdot (y-x)
\]

****** Proof
Para $g(s) = f(sy + (1-s)x)$ se tiene $g'(s) = f'(sy+(1-s)x) \cdot (y-x)$.
Integrando y aplicando regla de Barrow se tiene la igualdad.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-25 Mon>
:PROPERTIES:
:ID:       1c375733-5f2b-460c-9595-83e32e399f0b
:DRILL_LAST_INTERVAL: 12.5516
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:37]
:END:
Enuncia la versiÃ³n del teorema del valor medio multivariable
que se sigue de la regla de Barrow.

******* Answer

\[
f(x) - f(y) = \left( \int_0^1 f'(sy + (1-s)x) \,ds \right) \cdot (y-x)
\]

O, de otra forma

\[
f(x) - f(y) = \left( \int_{[a,b]} f'(sy + (1-s)x) \,ds \right) \cdot (y-x)
\]

***** 3. Desigualdades integrales
Se tiene para cualquier funciÃ³n continua,

 * $\norm{\int_a^bf(s)\,ds} \leq \int_a^b\norm{f(s)}\,ds$

 * si $\norm{f(s)} \leq M$. entonces $\norm{\int_{t_0}^tf(s)\,ds} \leq M \abs{t-t_0}$

***** 4. Lema de Barbalat (versiÃ³n dÃ©bil)
Si $\lim_{t \to +\infty} f(t) = L \in \mathbb{R}$, entonces $\exists t_n \to +\infty$ con $f'(t_n) \to 0$.

****** Proof
Por [[id:3efc40d6-3ca0-4a0d-ba1a-41e752bab364][teorema del valor medio]], existen $t_n \in (n,n+1)$ tales
que $f'(t_n) = f(n+1) - f(n) \to 0$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       d2b43cd1-b482-4954-828c-4e94ca50e0c1
:DRILL_LAST_INTERVAL: 27.6024
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:14]
:END:
Enuncia el Lema de Barbalat (versiÃ³n dÃ©bil).

******* Answer
Si $\lim_{t \to +\infty} f(t) = L \in \mathbb{R}$, entonces $\exists t_n \to +\infty$ con $f'(t_n) \to 0$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-27 Wed>
:PROPERTIES:
:ID:       4a364dd0-415d-4b04-b73d-4c89d55664bf
:DRILL_LAST_INTERVAL: 15.054
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:37]
:END:
Demuestra el Lema de Barbalat. Si $\lim_{t \to +\infty} f(t) = L \in \mathbb{R}$,
entonces $\exists t_n \to +\infty$ con $f'(t_n) \to 0$.

******* Answer
Por [[id:3efc40d6-3ca0-4a0d-ba1a-41e752bab364][teorema del valor medio]], existen $t_n \in (n,n+1)$ tales
que $f'(t_n) = f(n+1) - f(n) \to 0$.

***** 4. Lema de Barbalat (versiÃ³n fuerte)
Si $\lim_{t \to +\infty}f(t) = L \in \mathbb{R}$, y ademÃ¡s $f'$ es uniformemente continua en
$(\alpha,\infty)$, entonces $\lim_{t \to +\infty}f'(t) = 0$.

****** TODO Proof                                                                                          :extra:
*** Tema 2. Lipschitzianidad. Picard-LindelÃ¶f
**** 2.1. Lipschitzianidad
***** Lipschiztianidad global
$f \colon \Omega \subset \mathbb{R}^n \to \mathbb{R}^m$ es *globalmente lipschitziana (GL)* si

\[
\exists L \geq 0 \colon
\forall x,y \in \Omega\colon\quad
\norm{f(x)-f(y)} \leq L\|x-y\|.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-12 Thu>
:PROPERTIES:
:ID:       e0db3e38-dc0f-42ff-9146-addb33fda380
:DRILL_LAST_INTERVAL: 30.1248
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:10]
:END:
DefiniciÃ³n globalmente lipschitziana.

******* DefiniciÃ³n
Hay una constante de Lipschitz que acota la distancia
entre dos imÃ¡genes por la distancia de los argumentos.

\[
\forall x,y \in \Omega\colon\quad
\norm{f(x)-f(y)} \leq L\|x-y\|.
\]

***** Lipschiztianidad global, caracterizaciÃ³n
:PROPERTIES:
:ID:       d89ce0be-e6df-4af0-a129-12b79008efc2
:END:
$f \colon \Omega \subset \mathbb{R}^n \to \mathbb{R}^m$ es globalmente lipschitziana si y sÃ³lo si no
existen sucesiones $x_n \neq y_{n}$ tales que

\[
\frac{\|f(x_n) - f(y_n)\|}{\|x_n-y_n\|} \to +\infty.
\]

****** Proof
Si existen las sucesiones, claramente no hay acotaciÃ³n. Si no hay
acotaciÃ³n posible, para cada $n$ existirÃ¡n $x_n \neq y_n$ tales que el
cociente sea mayor que $n$; teniÃ©ndose la divergencia.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       b77b3357-33e1-40bb-af13-6296280ddfa7
:DRILL_LAST_INTERVAL: 12.8477
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:59]
:END:
CaracterizaciÃ³n de lipschitzianidad global por sucesiones.

******* CaracterizaciÃ³n
No existen sucesiones tales que

\[
\frac{\|f(x_n) - f(y_n)\|}{\|x_n-y_n\|} \to +\infty.
\]

***** Lipschiztianidad global para derivada continua acotada
:PROPERTIES:
:ID:       884a32d3-15b9-4193-8104-c7ce01c555f9
:END:
Sea $I \subseteq \mathbb{R}$ intervalo. Para $f \in {\cal C}^1(I,\mathbb{R}^m)$ equivalen

 1) $f$ globalmente lipschitziana.
 2) $\exists L \geq 0: \forall t \in I: |f'(t)| \leq L$.

****** Proof
Cuando $f$ es globalmente lipschitziana basta tomar $y \to x$. Cuando
tenemos una cota, usamos regla de Barrow

\[ |f(x)-f(y)| = 
\left|\int_x^y f'(t) \;dt\right| = 
L|x-y|.
\]

***** Lipschiztianidad global para derivada continua acotada a trozos
Sea $I \subseteq \mathbb{R}$ intervalo. Para $f \in {\cal C}^1_T(I)$ equivalen

 1) $f$ globalmente lipschitziana.
 2) $\exists L \geq 0:\forall t \in I^{\ast}: |f'(t)| \leq L$.

****** Funciones de clase uno a trozos
Tenemos $f \in {\cal C}^1_T(I)$ cuando existe un recubrimiento finito de $I$ por
intervalos cerrados $\left\{ I_i \right\}$ tales que $f|_{I_i} \in {\cal C}^1(I_i)$.

La funciÃ³n serÃ¡ derivable excepto en un conjunto finito de puntos
llamados *nodos*, llamamos $I^{\ast}$ a los puntos que no son nodos.

****** Proof
Repetimos la demostraciÃ³n anterior con la Ãºnica diferencia
de que ahora la regla de Barrow deberÃ¡ aplicarse teniendo
en cuenta las propiedades de la integral. Podemos tomar un
recubrimiento finito de intervalos disjuntos de $[x,y]$.

\[ |f(x)-f(y)| =
\left|\int_x^y f'(t) \;dt\right| = 
\sum_{i=0}^n\left|\int_{I_i} f'(t) \;dt\right| = L\left| \sum_{i=0}^n \mu(I_i) \right| = L|x-y|.
\]

***** Lipschiztianidad global para jacobiana continua acotada
:PROPERTIES:
:ID:       0bd3f16f-5d05-4135-98b5-70d27b46afa9
:END:
En $\Omega$ abierto convexo con $f \in {\cal C}^1(\Omega;\mathbb{R}^m)$ equivalen

 1) $f$ es globalmente lipschiztiana.
 2) $\exists L \geq 0: \forall x \in \Omega: \vertiii{f'(x)} \leq L$.

/Esto generaliza al [[id:884a32d3-15b9-4193-8104-c7ce01c555f9][caso escalar]]./

****** Proof
Usamos crucialmente que $[x,y] \subset \Omega$. Tomando una norma matricial
compatible tenemos

\[
\norm{f'(sx + (1-s)y) \cdot (x - y)} \leq \vertiii{f'(sx + (1-s)y)}\norm{x-y}.
\]

Podemos acotarla usando su derivada como

\[\begin{aligned}
\norm{f(x) - f(y)} &=
\norm{\Big[ f(sx+(1-s)y) \Big]^1_0} &=
\int_0^1 \norm{f'(sx + (1-s)y) \cdot (x - y)} &\leq
L\norm{x-y}\abs{1-0}.
\end{aligned}\]

# En el otro sentido (?) Se tiene que en un entorno se deberÃ­a tener ese lÃ­mite.

****** Card: caracterizaciÃ³n de lipschitziana por el jacobiano                                             :drill:
SCHEDULED: <2018-08-04 Sat>
:PROPERTIES:
:ID:       4c53da16-5464-489d-8536-ddf06a659fd8
:DRILL_LAST_INTERVAL: 52.9818
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:12]
:END:
Caracteriza lipschitzianidad global en un abierto convexo $\Omega$
usando el jacobiano.

******* CaracterizaciÃ³n
Si $f \in {\cal C}^1(\Omega;\mathbb{R}^m)$ equivalen

 1) $f$ es globalmente lipschiztiana.
 2) $\exists L \geq 0: \forall x \in \Omega: \vertiii{f'(x)} \leq L$.

***** Lipschitzianidad global respecto de una variable
:PROPERTIES:
:ID:       c50d9301-e9db-40a1-b25f-b726327f37bb
:END:
$f \colon \Omega \to \mathbb{R}^m$ es *globalmente lipschitziana respecto* de $x \in \mathbb{R}^d$
cuando existe $L \geq 0$ tal que

\[
\| f(x_1,y) - f(x_2,y) \| \leq L \| x_1 - x_2 \|.
\]

Dicho de otra forma, las secciones $f(-,y)$ son lipschitzianas con una
constante de lipschitz comÃºn.

****** Card: definiciÃ³n                                                                                    :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       ab5dfd96-99d3-499a-ac32-bbfb645bbd4e
:DRILL_LAST_INTERVAL: 22.5254
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.714
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:56]
:END:
Una $f(x,y)$ definida en $\Omega \subseteq \mathbb{R}^d \times \mathbb{R}^n$ es [[id:c50d9301-e9db-40a1-b25f-b726327f37bb][globalmente lipschitziana respecto]]
de $x \in \mathbb{R}^d$ cuando

******* DefiniciÃ³n
Existe $L \geq 0$ tal que

\[
\| f(x,y) - f(x',y) \| \leq L \| x - x' \|.
\]

******* De otra forma
Las secciones $f(-,y)$ son lipschitzianas con una constante de
lipschitz comÃºn.

***** Lipschiztianidad local
:PROPERTIES:
:ID:       c9318e14-3e65-49d1-935a-f26e56c912cf
:END:
$f \colon \Omega \to \mathbb{R}^m$ es *localmente lipschitziana* (LL) si 
para cada punto hay un entorno $p \in U \subset \Omega$ con $f|_U$
globalmente lipschitziana.

/Nota: Â¡la constante de Lipschitz puede cambiar entre entornos!/

****** Card: localmente pero no globalmente lipschitz                                                      :drill:
SCHEDULED: <2018-09-08 Sat>
:PROPERTIES:
:ID:       9c7e258c-1ed7-45eb-bdc1-09c21a20bb94
:DRILL_LAST_INTERVAL: 87.0226
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:09]
:END:
Dar un ejemplo de funciÃ³n localmente lipschitzana pero no globalmente
lipschitziana.

******* Ejemplo
$x^2$ no tiene derivada acotada y podemos ir tomando sucesiones de
diferencias por encima de cualquier cota. Sin embargo, es ${\cal C}^1$
en cada entorno y ahÃ­ podemos acotar su derivada.

****** Card: lipschitzianidad local                                                                        :drill:
SCHEDULED: <2018-07-24 Tue>
:PROPERTIES:
:ID:       a57b3f38-2607-4a32-9ffd-d9a04d663100
:DRILL_LAST_INTERVAL: 42.9603
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:47]
:END:
DefiniciÃ³n de $f \colon \Omega \to \mathbb{R}^m$ *localmente lipschitziana*.

******* DefiniciÃ³n
Para cada punto hay un entorno $p \in U \subset \Omega$ con $f|_U$
globalmente lipschitziana.

******* Nota
La constante de Lipschitz puede cambiar entre entornos.

****** Card: no localmente lipschitz                                                                       :drill:
SCHEDULED: <2018-08-30 Thu>
:PROPERTIES:
:ID:       3eee05c0-75d5-4fa4-80be-dd99318ab95c
:DRILL_LAST_INTERVAL: 79.4168
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:23]
:END:
Dar un ejemplo de funciÃ³n no localmente lipschitz.

******* Ejemplo
$\sqrt{x}$ tiene una sucesiÃ³n contraviniendo la [[id:8e7d4d06-7b9f-4bbd-8e34-1e385dfdc439][caracterizaciÃ³n]] en $0$.

***** Lipschiztianidad local, anticaracterizaciÃ³n
:PROPERTIES:
:ID:       8e7d4d06-7b9f-4bbd-8e34-1e385dfdc439
:END:
$f \colon \Omega \to \mathbb{R}^m$ es localmente lipschitziana si y sÃ³lo si no
existen $x_n,y_n \in \Omega$ con $x_n\to p$ y $y_n \to p$ tales que

\[
\frac{\norm{f(x_n)-f(y_n)}}{\norm{x_n-y_n}} \to +\infty.
\]

****** Proof
Si es localmente lipschitziana, habrÃ¡ un entorno de $p$ con el
cociente acotado; pero si $x_n,y_n$ convergen a $p$ a partir de
algÃºn $n$ estarÃ¡n dentro de ese entorno.

Si no lo es, tomamos ${\cal U}_n = B(1/n,p) \cap \Omega$ y deben exisitr

\[
\frac{\norm{f(x_n) - f(y_n)}}{\norm{x_n-y_n}} \geq n,
\]

formando dos sucesiones que tienden a $p$ y con cociente
divergente.

****** Card: caracterizaciÃ³n                                                                               :drill:
SCHEDULED: <2018-08-03 Fri>
:PROPERTIES:
:ID:       5810c568-e044-447b-b6d4-0e1879deed84
:DRILL_LAST_INTERVAL: 52.4815
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:12]
:END:
CaracterizaciÃ³n por sucesiones de $f \colon \Omega \to \mathbb{R}^m$ localmente
lipschitziana.

******* CaracterizaciÃ³n
Es LL si y sÃ³lo si no existen $x_n,y_n \in \Omega$ con $x_n\to p$ y $y_n \to p$
tales que

\[
\frac{\norm{f(x_n)-f(y_n)}}{\norm{x_n-y_n}} \to +\infty.
\]

****** TODO Ejemplo: potencias localmente lipschitz                                                        :leech:
SCHEDULED: <2018-05-02 Wed>
:PROPERTIES:
:ID:       e84886df-626a-4e09-846b-572dabb83c40
:DRILL_LAST_INTERVAL: 3.95
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-04-28 Sat 16:00]
:END:
Â¿Para $p>0$, quÃ© debe cumplir $|x|^p$ para ser localmente lipschitz?
# Repasar

******* Debe cumplir
$p \notin (0,1)$

***** Lipschitzianidad local para derivada continua
En $\Omega$ abierto, cualquier $f \in {\cal C}^1(\Omega,\mathbb{R}^m)$ es localmente lipschitziana.

****** Proof
Cada punto tiene un entorno cerrado convexo en el que podemos aplicar
Weierstrass para acotar $\vertiii{f'(x)}$ y aplicar la caracterizaciÃ³n
[[id:0bd3f16f-5d05-4135-98b5-70d27b46afa9][global]] en un abierto dentro de Ã©l. 

****** Card: derivada continua y localmente lipschitz                                                      :drill: 
SCHEDULED: <2018-08-17 Fri>
:PROPERTIES:
:ID:       1d486c08-2683-48a2-8114-a6bd716eb758
:DRILL_LAST_INTERVAL: 65.7641
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:22]
:END:
CÃ³mo se relaciona la derivada continua con ser LL.

******* RelaciÃ³n
En $\Omega$ abierto, cualquier $f \in {\cal C}^1(\Omega,\mathbb{R}^m)$ es localmente lipschitziana.

***** Lipschitzianidad local respecto de una variable
$f \colon \Omega \subseteq \mathbb{R}^d \times \mathbb{R}^n \to \mathbb{R}^m$ es *localmente lipschitziana respecto* de $x \in \mathbb{R}^d$
si para cada punto existe un entorno $(p,q) \in U \subset \Omega$ tal que $f|_U$ es
globalmente lipschitziana respecto de $x \in \mathbb{R}^d$.

***** Lipschitzianidad local respecto de una variable para derivada continua
Si $\Omega$ abierto y $f_x$, derivada parcial respecto de $x$, es continua, entonces
$f$ es localmente lipschitziana respecto de la variable $x$.

****** Proof
Podemos tomar un entorno ${\cal U}$ donde $f_x$ estÃ¡ acotada, lo que hace que
todas las secciones $f(-,y)$ tengan derivada acotada uniformemente
respecto de $y$ y por la caracterizaciÃ³n [[id:0bd3f16f-5d05-4135-98b5-70d27b46afa9][global]], tengan una constante
de Lipschitz comÃºn.

***** Lipschitzianidad local es global en compactos
Una funciÃ³n localmente lipschitziana en un compacto es globalmente
lipschitziana.

****** Proof
Usamos la caracterizaciÃ³n, si no fuera globalmente lipschitziana,
[[id:d89ce0be-e6df-4af0-a129-12b79008efc2][existirÃ­an dos sucesiones]] donde el coeficiente crecerÃ­a. Como
estamos en un compacto, esas dos sucesiones deberÃ­an converger
y deberÃ­an converger al mismo punto porque la funciÃ³n estÃ¡
acotada en compactos por Weierstrass y la Ãºnica forma de crecer
del cociente es que se acerquen.

Ahora, en ese punto podemos aplicar la caracterizaciÃ³n de
[[id:c9318e14-3e65-49d1-935a-f26e56c912cf][lipschitzianidad local]].

https://math.stackexchange.com/questions/154721/if-locally-lipschitz-implies-lipschitz-on-compacts

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       1b4ecf37-b2e0-42b6-bc5b-cab6e044a9ea
:DRILL_LAST_INTERVAL: 13.0352
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:41]
:END:
Una funciÃ³n [localmente lipschitziana] en un [compacto] es
[globalmente lipschitziana].

***** Propiedades de lipschitzianidad global
Ejemplos:

 * Suma de lipschitz es lipschitz.
 * ComposiciÃ³n de lipscthiz es lipscthiz.
 * Lineal continua es lipschitz.
 * Valor absoluto es lipschitz.

Contraejemplos:

 * Producto de lipschitz no es lipschitz. $x^2$.


***** Propiedades de lipschitzianidad local
Ejemplos:

 * Suma de localmente lipschitz.
 * Producto (!) de localmente lipschitz.


**** 2.2. Lemas hacia Picard-LindelÃ¶f
***** Teorema del punto fijo de Banach
Sea $(E,d)$ un espacio mÃ©trico completo con $F \colon E \to E$ con un $\alpha$ cumpliendo
$d(F(x),F(y)) \leq \alpha d(x,y)$ para todo $x,y \in E$. Entonces,
 
  1. $\exists! x^{\ast}\in E: F(x^{\ast}) = x^{\ast}$;
  2. y si $x_0 \in E$, entonces $\left\{ F \circ \overset{n}{\dots} \circ F (x_0) \right\}_{n \in \mathbb{N}} \longrightarrow x^{\ast}$.

Es decir, toda funciÃ³n contractiva en un espacio mÃ©trico completo tiene
un punto fijo y todas sus aplicaciones sucesivas convergen a Ã©l.

****** Proof                                                                                               :extra:
***** Entornos y espacios para Picard-LindelÃ¶f
:PROPERTIES:
:ID:       44351076-d98f-4552-a229-a41405beb6b1
:END:
Dados $a,b \in \mathbb{R}^+$, definimos los siguientes entornos cerrados,
que pueden verse como la evoluciÃ³n de una bola cerrada dada en
un tiempo.

\[
R_{a,b}(t_0,x_0) = [t_0-a, t_0+a] \times {\overline B}(x_0,b).
\]

Definimos los siguientes espacios de funciones hacia los entornos,
que serÃ¡n precisamente las funciones cumpliendo $(t,\varphi(t)) \in R_{a,b}$.

\[
E_{a,b}(t_0,x_0) = 
\left\{ \varphi \in {\cal C}([t_{0} - a, t_{0} + a], \mathbb{R}^{d}) 
\mid \varphi(t) \in {\overline B}(x_0,b)
\right\}.
\]

En este espacio tenemos el operador de Volterra $V : E \to {\cal C}([t_0-a,t_0+a];\mathbb{R}^d)$
definido por

\[
V(\varphi) = \int_{t_0}^t f(s,\varphi(s))\,ds.
\]

Es un espacio completo por ser un cerrado del espacio mÃ©trico de
funciones continuas acotadas.

***** Lema 1 de funciÃ³n contractiva
:PROPERTIES:
:ID:       552dd347-bd4b-41e0-9ba7-0ebf9e42a26a
:END:
Sea $M\geq 0$ tal que

\[
\forall (t,x) \in R_{a,b}\colon \|f(t,x)\| \leq M,
\]

Para cualquier $\varphi \in E$ se tiene $\|V(\varphi)-x_0\|_{\infty} \leq Ma$.

****** Proof
Tenemos que
\[
\| V(\varphi)-x_0 \|_{\infty} \leq
\left\| \int_{t_0}^t f(s,\varphi(s))\,ds \right\|_{\infty} \leq
M\|t-t_0\|_{\infty} \leq Ma.
\]

NÃ³tese que usamos que $(t,\varphi(s)) \in R_{a,b}$.

***** Corolario 1 de funciÃ³n contractiva
:PROPERTIES:
:ID:       2b8c94be-2d60-45a0-aedf-715aa71fe67b
:END:
Sea $M \geq 0$ tal que

\[
\forall (t,x) \in R_{a,b}\colon \|f(t,x)\| \leq M.
\]

Si $aM \leq b$, entonces $V(E_{a,b}) \subset E_{a,b}$.

****** Proof
Aplicando el [[id:552dd347-bd4b-41e0-9ba7-0ebf9e42a26a][lema anterior]] para $\varphi \in E_{a,b}$ tenemos que $\|V(\varphi) - x_0\|_{\infty} \leq Ma \leq b$,
luego $V(\varphi) \in \overline{B}(x_0,b)$ y por [[id:44351076-d98f-4552-a229-a41405beb6b1][definiciÃ³n]] se tiene $V(\varphi) \in E_{a,b}$.

***** Lema 2 de funciÃ³n contractiva
:PROPERTIES:
:ID:       b3c86eea-1732-4df4-8f69-32b80a9510c0
:END:
Sea $M \geq 0$ tal que

\[
\forall (t,x) \in R_{a,b}\colon \|f(t,x)\| \leq M.
\]

Si $aM \leq b$, y $\varphi \colon [t_0-a,t_0+a] \to \mathbb{R}^d$ es soluciÃ³n de Volterra,
entonces $\varphi \in E_{a,b}$.

****** Proof
Afirmamos que $\|\varphi-x_0\|_{\infty} \leq b$. Si no fuera cierto, por el [[id:a439476b-8c77-4994-a736-9062ee91e7d0][Lema del primer instante]]
tendrÃ­amos un $b<\|\varphi(\tau) - x_0\|$ tal que $\forall t \in [t_0,\tau)\colon \|\varphi(t)-x_0\| \leq b$.
Aplicado en la direcciÃ³n contraria lo tendrÃ­amos para $(\tau,t_0]$.

Pero entonces, $\forall t \in [t_0,\tau)\colon (t,\varphi(t)) \in {\cal R}_{a,b}$ y

\[\begin{aligned}
b < \|\varphi(\tau)-x_0\| &\leq
\left\| \int_{t_0}^t f(s,\varphi(s))\,ds \right\| \leq
\int_{t_0}^t \left\| f(s,\varphi(s)) \right\|\,ds \\&\leq
M|t-t_0| \leq
Ma \leq b.
\end{aligned}\]

Llegando a contradicciÃ³n.

***** Lema 3 de funciÃ³n contractiva
:PROPERTIES:
:ID:       65fb9d51-29a8-4528-a9dc-eca1b77bf8e0
:END:
Sea $L \geq 0$ tal que

\[
\forall (t,x),(t,y) \in {\cal R}_{a,b}\colon \|f(t,x)-f(t,y)\| \leq L\|x-y\|.
\]

Si $\varphi,\psi \in E_{a,b}$, entonces $\|V(\varphi)-V(\psi)\|_{\infty} \leq aL\|\varphi-\psi\|_{\infty}$.

****** Proof
Para $t \in [t_0-a,t_0+a]$,

\[\begin{aligned}
\| V(\varphi)(t) - V(\psi)(t) \| &=
\left\| \int_{t_0}^t \left( f(s,\varphi(s)) - f(s,\psi(s)) \right)\,ds \right\| \\&\leq
\int_{t_0}^t \left\|  f(s,\varphi(s)) - f(s,\psi(s)) \right\|\,ds \\&\leq
\int_{t_0}^t L\left\| \varphi(s)-\psi(s) \right\|\,ds \\&\leq
La\|\varphi-\psi\|_{\infty}.
\end{aligned}\]

***** ProposiciÃ³n 1 de funciÃ³n contractiva
Sean $M,L \geq 0$ tales que

\[
\forall (t,x) \in R_{a,b}\colon \|f(t,x)\| \leq M,
\]

y que

\[
\forall (t,x),(t,y) \in {\cal R}_{a,b}\colon \|f(t,x)-f(t,y)\| \leq L\|x-y\|.
\]

Si $aM\leq b$ y $aL \leq 1$, entonces existe una Ãºnica $\varphi\colon [t_0-a,t_0+a] \to \mathbb{R}^d$
soluciÃ³n de Volterra.

****** Proof
Usando $M$ en el [[id:2b8c94be-2d60-45a0-aedf-715aa71fe67b][Corolario 1]] tenemos $V(E_{a,b}) \subset E_{a,b}$, y usando $L$ 
en el [[id:65fb9d51-29a8-4528-a9dc-eca1b77bf8e0][Lema 3]],

\[
\| V(\varphi) - V(\psi) \|_{\infty} \leq aL\|\varphi-\psi\|_{\infty} \leq \|\varphi - \psi\|_{\infty}.
\]

Por el Teorema del punto fijo de Banach, $\exists \varphi \in E\colon V(\varphi) = \varphi$, soluciÃ³n
de Volterra. Si hubiera otra soluciÃ³n $\psi$ en mismo intervalo usamos el
[[id:b3c86eea-1732-4df4-8f69-32b80a9510c0][Lema 2]] para tener $\psi \in E$ y por Teorema de Banach, $\varphi = \psi$.

# TODO: Â¿CÃ³mo sabemos que E es completo?

**** 2.3. Norma de Bielecki
***** Norma de Bielecki
En el espacio $E_{a,b}(t_0,x_0)$, dada una constante $R > 0$, consideramos

\[
\| \varphi \|_{B} = \max_{t \in [t_0-a,t_0+a]} e^{-R|t-t_0|}\|\varphi(t)\|.
\]

****** Bien definida
Sabemos que existe por Teorema de Weierstrass.

****** Card: definiciÃ³n                                                                                    :drill:
SCHEDULED: <2018-07-18 Wed>
:PROPERTIES:
:ID:       7748c216-94a7-4d91-a824-6fd195ffe10f
:DRILL_LAST_INTERVAL: 59.5744
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.2
:DRILL_EASE: 2.18
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-19 Sat 11:01]
:END:
Define la norma de Bielecki para una $R > 0$ dada.

******* DefiniciÃ³n
En $E_{a,b}(t_0,x_0)$, consideramos

\[
\| \varphi \|_{B} = \max_{t \in [t_0-a,t_0+a]} e^{-R|t-t_0|}\|\varphi(t)\|.
\]

***** Equivalencia con la norma infinito
La norma de Bielecki es equivalente a la norma infinito.

****** Proof
Tenemos las dos cotas dadas por $0 \leq |t - t_0| \leq a$.

\[
\max_{t \in [t_0-a,t_0+a]} e^{-R|t-t_0|}\|\varphi(t)\| \leq
\max_{t \in [t_0-a,t_0+a]} \|\varphi(t)\| 
\]

y

\[
\max_{t \in [t_0-a,t_0+a]} e^{-R|t-t_0|}\|\varphi(t)\| \geq
e^{-R|a|} \max_{t \in [t_0-a,t_0+a]} \|\varphi(t)\|.
\]

***** Lema 4 de funciÃ³n contractiva
:PROPERTIES:
:ID:       376ef232-7050-4306-abe7-b88843d13fcb
:END:
Sea $L \geq 0$ tal que

\[
\forall (t,x),(t,y) \in {\cal R}_{a,b}\colon \|f(t,x)-f(t,y)\| \leq L\|x-y\|.
\]

Si $\varphi , \psi \in E_{a,b}$ entonces $\|V(\varphi) - V(\psi)\|_B \leq \frac{L}{R} \| \varphi - \psi \|_{B}$.

****** Proof
# Comprobar, Â¿no hay que partir en caso negativo y positivo?

NÃ³tese primero que $\|V(\varphi)(t_0) - V(\psi)(t_0)\| = 0$. Fijado un $t \neq t_0$
usamos la cota de $L$ y la definiciÃ³n de la norma de Bielecki para tener

\[\begin{aligned}
\norm{V(\varphi)(t) - V(\psi)(t)} &=
\norm{\int_{t_0}^t f(s,\varphi(s)) - f(s,\psi(s))\,ds} \\&\leq
\int_{t_0}^t \norm{f(s,\varphi(s)) - f(s,\psi(s))}\,ds \\&\leq
L \int_{t_0}^t \norm{\varphi(s) - \psi(s)}\,ds \\&=
L \int_{t_0}^t \norm{\varphi(s) - \psi(s)} e^{-R\abs{s-t_0}}e^{R\abs{s-t_0}}\,ds \\&\leq
L \norm{\varphi - \psi}_{B} \int_{t_0}^t e^{R\abs{s-t_0}}\,ds \\&\leq
\frac{L}{R} \norm{\varphi - \psi}_{B} \left(e^{R\abs{t-t_0}} - 1\right) \\&\leq
\frac{L}{R} \norm{\varphi - \psi}_{B} e^{R\abs{t-t_0}}
\end{aligned}\]

ya que $e^{R|t-t_0|} > 0$. Dividiendo y uniendo ambos casos,
$\|V(\varphi) - V(\psi)\|_{\infty} \leq \frac{L}{R} \norm{\varphi - \psi}_{B} e^{R\abs{t-t_0}}$.

***** ProposiciÃ³n 2 de funciÃ³n contractiva
:PROPERTIES:
:ID:       51ed4957-d6c4-448f-bfe0-8a8d2afe218b
:END:
Sean $M,L \geq 0$ tales que

\[
\forall (t,x) \in R_{a,b}\colon \|f(t,x)\| \leq M,
\]

y que

\[
\forall (t,x),(t,y) \in {\cal R}_{a,b}\colon \|f(t,x)-f(t,y)\| \leq L\|x-y\|.
\]

Si $aM\leq b$, entonces existe una Ãºnica $\varphi\colon [t_0-a,t_0+a] \to \mathbb{R}^d$
soluciÃ³n de Volterra.

****** Proof
Si tomamos un $R > L$ podemos aplicar el [[id:376ef232-7050-4306-abe7-b88843d13fcb][lema anterior]] para obtener
que $V$ es contractiva con la norma de Bielecki, luego por Teorema
de Banach, existe un Ãºnico punto fijo de $V$.

**** 2.4. Picard-LindelÃ¶f
***** Teorema de Picard-LindelÃ¶f (versiÃ³n local)
:PROPERTIES:
:ID:       1bf1287f-59b3-4a74-9bfc-4c7de6004d50
:END:
Si $f$ es localmente lipschiztiana respecto de $x$ en un entorno de $(t_0,x_0)$,
el PVI tiene soluciÃ³n y es Ãºnica localmente.

****** Proof
Para algÃºn $(t_0,x_0) \in {\cal U} \subset D$,

\[
\forall (t,x),(t,y) \in {\cal U}\colon  \|f(t,x)-f(t,y)\| \leq L\|x-y\|.
\]

Tomamos ${\cal R}_{\overline{a},b}(t_0,x_0) \subset {\cal U}$ y $f|_{{\cal R}_{\overline{a},b}}$ serÃ¡ globalmente lipschitz luego continua
y podemos aplicar Teorema de Weierstrass para tener

\[
M = \max_{(t,x) \in {\cal R}_{a,b}} \|f(t,x)\|.
\]

Recortamos $a = \min\left\{\overline{a}, b/M, 1/L \right\}$ para aplicar la [[id:51ed4957-d6c4-448f-bfe0-8a8d2afe218b][proposiciÃ³n]] 
anterior, sabiendo ${\cal R}_{a,b} \subseteq {\cal R}_{\overline{a},b}$. Por Banach tenemos $\varphi \colon [t_0-a,t_0+a] \to \mathbb{R}^d$
soluciÃ³n de Volterra, que nos da $\varphi \colon (t_0-a,t_0+a) \to \mathbb{R}^d$
soluciÃ³n del PVI, con $\varphi \in E_{a,b}$.

Sea ahora $\psi \colon I \to \mathbb{R}^d$ otra soluciÃ³n; por continuidad
existe un $\delta < a$ tal que para $|t - t_0| < \delta$ se tiene $\|\psi(t) - x_0\| \leq b$.
TendrÃ­amos dos soluciones $\psi,\varphi$ en $E_{\delta,b}(t_0,x_0)$, donde $\delta M < b$, 
luego coincidirÃ­an en $[t_0-\delta,t_0+\delta]$.

***** Teorema de Picard-LindelÃ¶f (versiÃ³n global)
:PROPERTIES:
:ID:       bf38540e-3857-427e-9b27-e6c0bf0cfd73
:END:
Si $f$ es localmente lipschiztiana respecto de $x$ en todo el dominio,
el PVI tiene soluciÃ³n y es la Ãºnica soluciÃ³n maximal.

****** Proof
Cualquier $(t_0,x_0)$ tiene un entorno (el dominio) donde aplicar la
[[id:1bf1287f-59b3-4a74-9bfc-4c7de6004d50][versiÃ³n local]]. La unicidad local bajo cualquier condiciÃ³n inicial
[[id:0b968d38-72b0-41e4-bdbd-a19fd33ecd59][nos da]] la unicidad global bajo cualquier condiciÃ³n inicial.

***** Iterantes de Picard
Surgen como una aplicaciÃ³n repetida del operador de Volterra una vez que
sabemos que es contractivo.

\[
V(\varphi)(t) = x_0 + \int_{t_0}^t f(s,\varphi(s))\,ds.
\]

***** Contraejemplo de MÃ¼ller
Sea el PVI

\[\left\{\begin{array}{l}
x' = f(t,x) \\
x(0) = 0
\end{array}\right.\]

para $f \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ definida como

\[ f(t,x) = \left\{
\begin{array}{ll}
  0 & \mbox{ si } t \leq 0, \\
  2t & \mbox{ si } t > 0 \mbox{ y } x < 0, \\
  2t - \frac{4x}{t} & \mbox{ si } t > 0 \mbox{ y } 0 \leq x < t^2, \\
  -2t & \mbox{ si } t > 0 \mbox{ y } x > t^2. \\
\end{array}\right.\]

****** Unicidad
Sabemos que la soluciÃ³n al PVI es Ãºnica por tenerse unicidad
tanto en el futuro como en el pasado por [[id:cc790c02-b622-4fc1-8bb1-ac3c05e73009][
Teorema de Unicidad de Peano]].

La Ãºnica soluciÃ³n puede encontrarse buscando soluciÃ³n
a
\[
x' = 2t - \frac{4x}{t}.
\]
Que es una ecuaciÃ³n lineal escalar que se resolverÃ¡ como
$x = t^2/3 + A/t^4$ y a la que imponemos condiciones para
obtener $x = t^2/3$ soluciÃ³n cuando $t > 0$ y $x = 0$ cuando $t \leq 0$.

****** SucesiÃ³n de iterantes de Picard
La sucesiÃ³n de iterantes de Picard lleva a $0,t^2,-t^2,t^2,\dots$.
Tiene parciales convergentes pero ninguna a la soluciÃ³n.

*** Tema 3. Teorema de Cauchy-Peano y Teorema de ArzelÃ¡-Ascoli
**** 3.1. AcotaciÃ³n uniforme y equicontinuidad
***** SucesiÃ³n uniformemente acotada
:PROPERTIES:
:ID:       23715dfb-0085-4d8e-9b93-26465cbbee91
:END:
$f_n \colon I \to \mathbb{R}^{d}$ es *uniformemente acotada* si existe $\|f_n(t)\|\leq M$
para todo $n \in \mathbb{N}$ y $t \in I$.

****** No uniformemente acotada
$f_n \colon I \to \mathbb{R}^d$ no es *uniformemente acotada* si existe $\left\{ \| f_{m_n}(t_n) \| \right\}_{n \in \mathbb{N}} \to \infty$.
Por definiciÃ³n.

***** SucesiÃ³n equicontinua
$f_n \colon I \to \mathbb{R}^d$ es *equicontinua* si 

\[
\forall \varepsilon\colon \exists \delta\colon |t-s|<\delta \implies \|f_n(t) - f_n(s)\| \leq \varepsilon.
\]

***** CaracterizaciÃ³n de equicontinuidad
Una $f_n \colon I \to \mathbb{R}^d$ uniformemente continua no serÃ¡ equicontinua si
hay sucesiones $t_n$, $s_n$ tales que

\[
\exists (t_n,s_n)\colon \abs{ t_n-s_n} \to 0, \quad \|f_{m_n}(t_n)-f_{m_n}(s_n)\| \not\to 0.
\]

****** Proof
Sale al negar la definiciÃ³n.

****** TODO VariaciÃ³n
***** Lema 1. Derivada uniformemente acotada es equicontinua
:PROPERTIES:
:ID:       5b2c7b57-f17b-441f-a635-fddbb3e0c992
:END:
Si $f'_n$ es uniformemente acotada, $f_n \in {\cal C}^1(I,\mathbb{R}^d)$ es *equicontinua*.

****** Proof
Usamos teorema fundammental del cÃ¡lculo, que necesita $f'_n$ continua

\[
\|f_n(t)-f_n(s)\| = \left\| \int_s^t f'_n(z)\,dz \right\| \leq M|t-s|.
\]

****** Card: enunciado                                                                                     :drill:
SCHEDULED: <2018-07-14 Sat>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       91181bc4-99ca-4e68-99f3-f5c4467f3c16
:DRILL_LAST_INTERVAL: 32.266
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:16]
:END:
Si $f'_n$ es [uniformemente acotada], $f_n \in$ [${\cal C}^1(I,\mathbb{R}^d)$] es [ *equicontinua* ].

****** Card: idea de la demostraciÃ³n                                                                       :drill:
SCHEDULED: <2018-07-19 Thu>
:PROPERTIES:
:ID:       40e3f231-b6d2-4980-9fa0-37d6a98dc700
:DRILL_LAST_INTERVAL: 36.7182
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:21]
:END:
Â¿CÃ³mo se demuestra?

Si $f'_n$ es uniformemente acotada, $f_n \in$ ${\cal C}^1(I,\mathbb{R}^d)$ es *equicontinua*.

******* DemostraciÃ³n
Usamos teorema fundamental del cÃ¡lculo, que necesita $f'_n$ continua

\[
\|f_n(t)-f_n(s)\| = \left\| \int_s^t f'_n(z)\,dz \right\| \leq M|t-s|.
\]

***** Lema 2. Convergencia puntual de equicontinuas es uniformemente continua
:PROPERTIES:
:ID:       09edde13-8c3f-49d5-9571-76483e3c2cd3
:END:
Si $f_n \overset{c.p.}\longrightarrow f$ y $f_n \colon I \to \mathbb{R}^{d}$ equicontinua, $f$ es uniformemente continua.

****** Proof
Tenemos para $|t-s| < \delta$, $\norm{f_n(t) - f_n(s)} < \varepsilon$, tomando lÃ­mite en $n$,

\[\norm{f(t) - f(s)} \leq \varepsilon.\]

****** Card: enunciado                                                                                     :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       7be1e96f-5d20-4380-993f-cd23084f427e
:DRILL_LAST_INTERVAL: 24.3586
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:09]
:END:

Si [ $f_n \overset{c.p.}\longrightarrow f$ ] y $f_n\colon I \to \mathbb{R}^d$ [sucesiÃ³n equicontinua], $f$ es [uniformemente continua].

****** Card: idea de la demostraciÃ³n                                                                       :drill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:ID:       7805ecf8-032a-4fbf-ac0e-2eeab3dd2f48
:DRILL_LAST_INTERVAL: 25.3869
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:13]
:END:
Â¿CÃ³mo se demuestra?

Si $f_n \overset{c.p.}\longrightarrow f$ y $f_n$ equicontinua, $f$ es uniformemente continua.

******* DemostraciÃ³n
Tenemos para $|t-s| < \delta$, $\norm{f_n(t) - f_n(s)} < \varepsilon$, tomando lÃ­mite en $n$,

\[\norm{f(t) - f(s)} \leq \varepsilon.\]

***** Lema 3. Convergencia uniforme de continuas es continua
:PROPERTIES:
:ID:       0828ef4f-b005-4cd4-a036-e6aa535b27f0
:END:
Si $f_n \overset{c.u.}\longrightarrow f$ y $f_n \colon I \to \mathbb{R}^d$ son continuas, $f$ es continua.

****** Proof

\[
\norm{f(t) - f(s)} \leq 
\norm{f_n(t) - f(t)} + 
\norm{f_n(t) - f_n(s)} + 
\norm{f_n(s) - f(s)}
\]

Elegimos el $n$ que por convergencia uniforme dÃ© $\norm{f_n - f}_{\infty} \leq \varepsilon/3$.
Elegimos el $\delta$ que por continuidad de $f_n$ dÃ© $\norm{f_n(t)-f_n(s)} \leq \varepsilon/3$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       30c09570-1887-4a89-8c5e-e354d2fd18a9
:DRILL_LAST_INTERVAL: 12.9428
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:43]
:END:
La funciÃ³n a la que converge uniformemente una sucesiÃ³n de funciones
continuas es [continua].

***** Lema 4. Convergencia de sucesiones desde la convergencia uniforme
:PROPERTIES:
:ID:       d2bb5e99-e7e9-4fa2-a8e2-38dcda39917e
:END:
Sea $f_n \overset{c.u.}\longrightarrow f$ y $t_n \to t_{\ast}$. Si $f$ continua en $t_{\ast}$, entonces $f_n(t_n) \to f(t_{\ast})$.

****** Proof

\[
\norm{f_n(t_n) - f(t_{\ast})} \leq
\norm{f_n(t_n) - f(t_n)} + \norm{f(t_n) - f(t)} \leq \varepsilon
\]

Podemos usar la continuidad de $f$ y la convergencia uniforme para obtener
dos $n_0,n_1$ donde cada sumando estÃ¡ acotado. Tomamos el mÃ¡ximo de ambos.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       ab2c5732-be11-41e0-aeef-6a4cdb65bed2
:DRILL_LAST_INTERVAL: 27.3535
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:10]
:END:
Sea [ $f_n \overset{c.u.}\longrightarrow f$ ] y $t_n \to t_{\ast}$. Si [ $f$ continua en $t_{\ast}$ ], entonces [ $f_n(t_n) \to f(t_{\ast})$ ].

***** Corolario. Convergencia de sucesiones desde la convergencia uniforme de continuas
Sea $f_n \overset{c.u.}\longrightarrow f$ y $t_n \to t_{\ast}$. Si $f_n$ continuas, entonces $f_n(t_n) \to f(t_{\ast})$.

****** Proof
La [[id:0828ef4f-b005-4cd4-a036-e6aa535b27f0][convergencia uniforme de continuas es continua]], en particular,
continua en $t_{\ast}$. [[id:d2bb5e99-e7e9-4fa2-a8e2-38dcda39917e][Aplicamos el lema anterior]].

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-29 Fri>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       b3d5bdc9-f233-4873-ac2b-6dc5c066ff6a
:DRILL_LAST_INTERVAL: 16.5566
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:57]
:END:
Sea [ $f_n \overset{c.u.}\longrightarrow f$ ] y $t_n \to t_{\ast}$. Si $f_n$ continuas, entonces [ $f_n(t_n) \to f(t_{\ast})$].
****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       3c9e00f3-cbef-43fa-99bc-bef434ab5df5
:DRILL_CARD_TYPE: hide1cloze
:DRILL_LAST_INTERVAL: 11.5417
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:56]
:END:
Si $x_n \to x$ y tenemos [ $g_n \overset{c.u.}\longrightarrow g$ ], donde cada $g_n$ es continua,
entonces [ $g_n(x_n) \to g(x)$].

***** Ejemplos y contraejemplos
****** TODO FunciÃ³n continua no uniformemente continua
****** TODO SucesiÃ³n equicontinua
****** TODO SucesiÃ³n que converge puntualmente no uniformemente
****** SucesiÃ³n de uniformemente continuas no equicontinua
$f_n(t) = nt$ tiene cada funciÃ³n uniformemente continua (de hecho es lipschitz)
pero no es equicontinua porque

\[
\left\{ f_n\left(\frac{1}{n}\right) - f_n(0) \right\} \to 1.
\]

******* Card                                                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       49a7eaad-0bac-4e3f-b2d6-74e16b58bedc
:DRILL_LAST_INTERVAL: 11.1843
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:42]
:END:
Â¿Es equicontinua?

\[f_n(t) = nt\]

******** Answer                                                
:PROPERTIES:
:ID:       4aa943ea-e020-4f65-88d8-77aaedfe7bee
:END:
No. Se tiene

\[
\left\{ f_n\left(\frac{1}{n}\right) - f_n(0) \right\} \to 1.
\]

****** Derivada acotada uniformemente y equicontinua
La sucesiÃ³n $\sin(n + t)$ tiene sucesiÃ³n de derivadas $\cos(n+t)$
uniformemente acotada, luego es equicontinua.

******* Card                                                                                              :drill:
SCHEDULED: <2018-06-25 Mon>
:PROPERTIES:
:ID:       fcd52f7f-edcf-4c14-a859-44bdf5ea1ba5
:DRILL_LAST_INTERVAL: 10.5769
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-14 Thu 11:49]
:END:
Por quÃ© es $\sin(n + t)$ equicontinua.

******** Answer
Tiene sucesiÃ³n de derivadas $\cos(n+t)$ uniformemente acotada.

****** TODO Buscar ejemplos en matemapli
**** 3.2. Teorema de Ascoli-ArzelÃ¡
***** Teorema de Ascoli-ArzelÃ¡
:PROPERTIES:
:ID:       da2e68f3-95f5-4aa2-b265-234ab61750c5
:END:
Para $f_n \colon I \to \mathbb{R}^d$ uniformemente acotada y equicontinua, existe una
parcial $f_{\sigma(n)} \overset{c.p.}\longrightarrow f$; ademÃ¡s, $f$ es uniformemente continua.

****** DemostraciÃ³n
******* Paso 1. Convergencia en un numerable denso
Sea $D = \mathbb{Q} \cap I = \left\{ t_k \mid k \in \mathbb{N} \right\}$ numerable.

Para cada $k \in \mathbb{N}$, $\left\{f_n(t_k)\right\}_{n\in\mathbb{N}$ es acotada (por [[id:23715dfb-0085-4d8e-9b93-26465cbbee91][uniformemente acotada]]), 
y por Bolzano-Weierstrass inductivamente en $k$ tenemos $\sigma_k$ tal
que $f_{\sigma_1\circ\dots\circ\sigma_k(n)}(t_k)$ converge. Definimos en $D$,

\[
f(t_k) = \lim_{n \to \infty} f_{\sigma_1\circ \dots\circ \sigma_k(n)}(t_k).
\]

Ahora llamamos $\sigma(n) = \sigma_1\circ\dots\circ\sigma_n (n)$ y vemos que es creciente
por tenerse $\sigma_{n+1}(n+1) > n+1 > n$, luego $\sigma(n+1) > \sigma(n)$.
Comprobamos que $f_{\sigma(n)} \overset{c.p. \text{ en } D}\longrightarrow f$ porque para $t_k$, a partir de $k$,
la serie es una parcial de la que define $f$.

******* Paso 2. ExtensiÃ³n al intervalo
Dado $t \in I$ y $\varepsilon>0$.
Por equicontinuidad, $\exists \delta > 0\colon |s'-s|<\delta \implies \|f_n(s')-f_n(s)\| \leq \varepsilon/3$ dos veces.
Por densidad $\exists t_k\colon |t_k - t| < \delta$.
Por convergencia, $\forall p,q \geq n_0\colon \| f_{\sigma(p)}(t_k) - f_{\sigma(q)}(t_k) \| < \varepsilon/3$ una vez

uniendo todo con desigualdad triangular,

\[
\| f_{\sigma(p)}(t) - f_{\sigma(q)}(t)} \| < \epsilon.
\]

Y asÃ­, $f_{\sigma(n)}(t)$ es de Cauchy y converge a un punto que usamos para
definir $f(t)$. [[id:09edde13-8c3f-49d5-9571-76483e3c2cd3][Sabemos que]] la convergencia puntual de equicontinuas
es uniformemente continua.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-08 Wed>
:PROPERTIES:
:ID:       a2f54cc4-d74f-4e72-be6a-bba39aab74a7
:DRILL_CARD_TYPE: multisided
:DRILL_LAST_INTERVAL: 56.9093
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.18
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:19]
:END:
Enuncia el Teorema de Ascoli-ArzelÃ¡.

******* Enunciado
Para $f_n \colon I \to \mathbb{R}^d$ uniformemente acotada y equicontinua, existe una
parcial $f_{\sigma(n)} \overset{c.p.}\longrightarrow f$; ademÃ¡s, $f$ es uniformemente continua.

******* Idea de demostraciÃ³n
:PROPERTIES:
:ID:       80a088fa-84cb-44e7-9d9f-e6222a4a2eef
:END:
Tomamos $D = \mathbb{Q} \cap I = \left\{ t_k \mid k \in \mathbb{N} \right\}$. Bolzano-Weierstrass 
inductivamente nos da una convergente en $D$.

\[
f(t_k) = \lim_{n \to \infty} f_{\sigma_1\circ \dots\circ \sigma_k(n)}(t_k).
\]

Extendemos a todo el intervalo usando *equicontinuidad*,
*densidad* y *convergencia en D*.

***** Corolario a Ascoli-ArzelÃ¡
:PROPERTIES:
:ID:       859883bd-15a4-4efd-9d61-7d501b592918
:END:
En $I$ compacto, para $f_n \colon I \to \mathbb{R}^d$ uniformemente acotada y equicontinua,
existe una parcial tal que $f_{\sigma(n)} \overset{c.u.}\longrightarrow f$; ademÃ¡s, $f$ es uniformemente continua.

****** DemostraciÃ³n
Aplicamos [[id:da2e68f3-95f5-4aa2-b265-234ab61750c5][Teorema de Ascoli-ArzelÃ¡]] para obtener la $f$ uniformemente continua
y convergencia puntual hacia ella.

Por equicontinuidad, $\exists \delta_1\colon |t-s| < \delta_1 \implies \|f_n(t)-f_n(s)\| \leq \varepsilon/3$.
Por continuidad uniforme de $f$, $\exists \delta_2\colon |t-s| < \delta_2 \implies \|f(t) - f(s)\| \leq \varepsilon/3$.
Tomamos $\delta = \min \left\{ \delta_1,\delta_2 \right\}$.

Por compacidad,

\[
I = \bigcup_{t \in I} (t-\delta,t+\delta) = \bigcup_{i = 0}^k (t_k-\delta,t_k+\delta)
\]

Por convergencia puntual de cada $t_j$, $\exists n_j\colon \|f_{\sigma(n)}(t_j) - f(t_j)\| \leq \varepsilon/3$; y
tomamos $n_0 = \max\{n_1,\dots,n_k\}$. Finalmente, para $n \geq n_0$,

\[
\|f_{\sigma(n)}(t) - f(t)\| \leq 
\|f_{\sigma(n)}(t) - f_{\sigma(n)}(t_k)\| + 
\|f_{\sigma(n)}(t_k) - f(t_k)\| + 
\|f(t_k) - f(t)\| \leq \varepsilon.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-23 Thu>
:PROPERTIES:
:ID:       cda99124-7715-4d0b-8892-d901da3817a7
:DRILL_LAST_INTERVAL: 72.4154
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 15:01]
:END:
Â¿QuÃ© dice Ascoli-ArzelÃ¡ cuando se aplica sobre un compacto?

******* Corolario
En $I$ compacto, para $f_n \colon I \to \mathbb{R}^d$ uniformemente acotada y equicontinua,
existe una parcial tal que $f_{\sigma(n)} \overset{c.u.}\longrightarrow f$; ademÃ¡s, $f$ es uniformemente continua.

***** Herramienta para ejercicios                                                                           :extra:
Si $f_n$ es sucesiÃ³n de equicontinuas convergiendo puntualmente $f_n\to f$
a una $f$ uniformemente continua en un compacto $I$, entonces hay
convergencia uniforme.

****** Proof
Fijado un $\varepsilon$, hay por equicontinuidad y continuidad uniforme de $f$
un $\delta$ tal que tomamos bolas de radio $\delta$ en el compacto y creamos
un subrecubrimiento finito. Ahora tomamos un $n$ suficientemente grande
como para que todos los centros del subrecubrimiento $t^{\ast}_n$ estÃ©n mÃ¡s
cerca de $f(t^{\ast}_n)$ que  $\varepsilon/3$. Dado cualquier $t$, tiene a menos de $\delta$ a
un centro del subrecubrimiento. Aplicamos desigualdad triangular
para acotar la diferencia

\[
\norm{f_n(t) - f(t)} \leq
\norm{f_n(t) - f_n(t^{\ast})} +
\norm{f_n(t^{\ast}) - f(t^{\ast})} + 
\norm{f(t)-f(t^{\ast})}.
\]

**** 3.3. Convergencia e integraciÃ³n
***** Convergencia uniforme en integrales
Si $f_n \overset{c.u.}\longrightarrow f$ y $f_n$ continua en $[a,b]$, entonces $\int_a^b f_n \to \int_a^b f$.

****** Proof

\[
\norm{\int_a^b (f_n(s) - f(s))\,ds} \leq
\norm{f_n-f}_{\infty}(b-a) \to 0 
\]

***** Convergencia puntual desde acotada
Si $f_n \overset{c.p.}\longrightarrow f$ y $f_n$ continuas uniformemente acotadas, $\int_a^b f_n \to \int_a^b f$.

****** TODO Proof                                                                                          :extra:
Por el Teorema de convergencia dominada de Lebesgue. 

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       eac30143-b758-4577-a6e3-1d0347a4d8a9
:DRILL_LAST_INTERVAL: 4.6711
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.6
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:42]
:END:
Si $f_n \overset{c.p.}\longrightarrow f$ y [ademÃ¡s $f_n$ continuas uniformemente acotadas en $(a,b)$], entonces $\int_a^b f_n \to \int_a^b f$.

******* Extra: caso particular
Si $f_n \overset{c.u.}\longrightarrow f$ y $f_n$ continua en $[a,b]$, entonces $\int_a^b f_n \to \int_a^b f$.

En particular podemos pedir continuidad a todas las $f_n$.

**** 3.4. Teorema de Cauchy-Peano
***** Teorema de Stone-Weierstrass                                                                          :extra:
:PROPERTIES:
:ID:       a786fcd1-033d-477b-865e-a8a1f0aef252
:END:
****** Ãlgebras de Banach separando puntos y que no se desvanecen
Una familia de funciones reales ${\cal A}$ sobre un conjunto $X$ se dice
*Ã¡lgebra* si es cerrada para la suma y producto de funciones y
para el producto por escalares. 

Si consideramos el conjunto de funciones $X \to \mathbb{R}$ como un
espacio mÃ©trico con la norma del supremo, la convergencia es la
convergencia uniforme. En el caso en el que todas las funciones
de ${\cal A}$ estÃ¡n acotadas, podemos comprobar que la clausura bajo esta
norma de ${\cal A}$ es de nuevo un *Ã¡lgebra*; de hecho, tenemos que para
$\lambda \in \mathbb{R}$ y $p_n,q_n \in {\cal A}$ con $p_n \overset{c.u.}\longrightarrow p$ y $q_n \overset{c.u.}\longrightarrow q$,

 * $p_n + q_n \overset{c.u.}\longrightarrow p + q$, ya que
   \[
   \norm{(p_n + q_n) - (p + q)}_{\infty} \leq
   \norm{p_n - p}_{\infty} + \norm{q_n - q}_{\infty} \longrightarrow 0;
   \]

 * $p_nq_n \overset{c.u.}\longrightarrow pq$ usando que todas ellas estarÃ¡n acotadas y
   que $\norm{p}_{\infty} \leq \sup \left\{ \norm{p_n}_{\infty} \right\}$, que existe por la convergencia,
   \[
   \norm{p_nq_n - pq}_{\infty} \leq
   \norm{p_n - p}_{\infty}\norm{q_n}_{\infty} + 
   \sup \left\{ \|p_n\|_{\infty} \right\} \norm{q_n-q}_{\infty} \longrightarrow 0;
   \]

 * y $\lambda p_n \overset{c.u.}\longrightarrow \lambda p$, ya que
   \[
   \norm{\lambda p_n}_{\infty} \leq \abs{\lambda}\norm{p_{\infty}} \longrightarrow 0.
   \]

Decimos que el Ã¡lgebra *separa puntos* si para cada par $x,y \in X$
existe $p \in {\cal A}$ tal que $p(x) \neq p(y)$. Decimos que no se *desvanece*
en ningÃºn punto si para cada $x \in X$ hay una $p \in {\cal A}$ tal que $p(x) \neq 0$.
Cuando un Ã¡lgebra separa puntos y no se devanece, dados dos reales
$\lambda,\mu \in \mathbb{R}$ y dos puntos $x,y \in X$ existe una $p \in {\cal A}$ que separa
los puntos y dos $q_x,q_y \in {\cal A}$, con $q_x(x) \neq 0$ y $q_y(y) \neq 0$; entonces
podemos definir

\[
a = pq_x - p(y)q_x, \quad
b = pq_y - p(x)q_y, 
\]

que cumplen $a(x) \neq 0$ y $b(y) \neq 0$, $b(x) = a(y) = 0$,; asÃ­, existe
siempre una funciÃ³n $r$ que vale exactamente $\lambda$ en $x$ y $\mu$ en $y$,
definida como

\[
r = \frac{\lambda b}{b(x)} + \frac{\mu a}{a(y)}.
\]

****** Teorema de Stone-Weierstrass
Sea ${\cal A}$ un Ã¡lgebra de funciones continuas sobre un espacio
compacto $K$ que separa puntos y no se desvanece en ninguno.
La clausura bajo la norma del supremo es entonces ${\cal C}(K,\mathbb{R})$,
el conjunto de todas las funciones continuas reales sobre $K$.

******* Proof
******** Paso 1. AproximaciÃ³n de la raÃ­z cuadrada
Nuestro primer paso serÃ¡ aproximar uniformemente la raÃ­z cuadrada
por una sucesiÃ³n de polinomios. La serie de Taylor de la raÃ­z
cuadrada en $t = 1$,
\[
\sum_{n\geq 0} \lambda_n(t-1)^n =
\sum_{n\geq 0} \frac{(t - 1)^n}{n!} \prod_{k=1}^n \left( \frac{3}{2} - k \right),
\]

tiene coeficientes $\lambda_n$ tales que
\[
\abs{\frac{\lambda_{n+1}}{\lambda_n}} = 
\abs{\frac{1/2 - n}{n+1}} \to 1,
\qquad
n \left( 1 - \abs{\frac{\lambda_{n+1}}{\lambda_n}} \right) \to 3/2 > 1;
\]

por lo que sabemos por criterio de Raabe que $\sum_{n \geq 1} \lambda_n$ es absolutamente
convergente y la suma converge para $t \in [0,1]$. Podemos definir
\[
\psi(t) = \sum_{n=0}^{\infty}\lambda_n(t-1)^n, \quad\mbox{ para } t \in [0,1];
\]

a la que convergen uniformemente las parciales por tenerse
\[
\abs{\psi(t) - \sum_{n=0}^{m} \lambda_n(t-1)^n} \leq
\sum_{n=m+1}^{\infty} \abs{\lambda_n},
\quad\mbox{ para } t \in [0,1].
\]

Si ahora consideramos sus derivadas, usando convergencia uniforme para
intercambiar derivada y suma infinita, tenemos la ecuaciÃ³n diferencial
\[\begin{aligned}
\psi'(t) &=
\sum_{n=0}^\infty n\lambda_n(t-1)^{n-1} =
\sum_{n=1}^\infty \left(\frac{1}{2} - (n-1)\right)\lambda_{n-1}(t-1)^{n-1} \\&=
\frac{1}{2}\psi(t) - (t-1)\psi'(t),
\end{aligned}\]

con $\psi(1) = 1$, que se resuelve como $\psi(t) = \sqrt{t}$.

******** Paso 2. Estructura de retÃ­culo
Dada $p \in \overline{{\cal A}}$, veamos que $\abs{p} \in {\overline{\cal A}}$. Al ser $K$ compacto, $p$ estÃ¡ acotado.
Dado un $\varepsilon > 0$, como $p(t)/\norm{p}_{\infty} \leq 1$ usamos la aproximaciÃ³n por polinomios
de la raÃ­z cuadrada para existir algÃºn $n$ tal que
\[
\abs{\sqrt{\left(\frac{p(x)}{\norm{p}_{\infty}}\right)^2} -
\sum_{i=1}^n \lambda_i \left(\frac{p(x)}{\norm{p}_{\infty}}-1\right)^i} < \varepsilon;
\quad
\mbox{ para } x \in K.
\]

Y desde aquÃ­ deducimos que $\abs{p}/\norm{p}_{\infty} \in \overline{{\cal A}}$ por poder aproximarse uniformemente
por sumas de funciones en el Ã¡lgebra, luego $\abs{p} \in \overline{\cal A}$.

El ser cerrada para el valor absoluto hace que la clausura
sea un retÃ­culo con el orden parcial inducido punto a punto
desde los reales. En otras palabras, dados $p,q\in \overline{{\cal A}}$,
\[
\min(p,q) = \frac{p + q}{2} - \frac{\abs{p-q}}{2},\quad\text{y}\quad
\max(p,q) = \frac{p + q}{2} + \frac{\abs{p-q}}{2},
\]

pertenecen a $\overline{\cal A}$.

******** Paso 3. AproximaciÃ³n
Dada $f \in {\cal C}(K,\mathbb{R})$, un punto $y \in K$, y un $\varepsilon > 0$, probaremos que
existe una $r_y \in {\overline{\cal A}}$ tal que $r_y(y) = f(y)$ y $\forall x \neq y \in K\colon r_{y}(x) > f(x) - \varepsilon$.
Por ser ${\cal A}$ separable y no desvanecerse, tenemos para cada $z \in K$ una
funciÃ³n $s_z \in {\cal A}$ tal que $s_{z}(y) = f(y)$ y ademÃ¡s, $s_{z}(z) = f(z)$.

Por continuidad de $s_z$, existe un abierto $z \in J_z$ tal que
\[
\forall x \in J_z\colon\quad s_z(x) > f(x) - \varepsilon.
\]

Como $K = \bigcup_{z \in K} J_z$ es compacto, existen $z_1,\dots,z_n$ tales que
$K = J_{z_1} \cup \dots \cup J_{z_n}$; y podemos tomar $r_y = \max(s_{z_1},\dots,s_{z_n})$.

De nuevo, por continuidad de $r_y$, existe un abierto $y \in H_y$
tal que
\[
\forall x \in H_y\colon\quad r_y(x) < f(x) + \varepsilon;
\]

de nuevo por compacidad, existen $y_1,\dots,y_n$ con $K = H_{y_1} \cup \dots \cup H_{y_m}$;
y de nuevo podemos tomar $h = \min(r_{y_1},\dots,r_{y_m}) \in \overline{{\cal A}}$; que cumple
que $f(x) - \varepsilon < h(x) < f(x) + \varepsilon$, y por tanto $\norm{f - h}_{\infty} < \varepsilon$.

***** Lema de aproximaciÃ³n por localmente lipschitzianas
:PROPERTIES:
:ID:       6fb8c0d7-7c28-4d3f-a6ae-c781ac55cb4e
:END:
Sea $f \colon D = \mathring{D} \to \mathbb{R}^d$ continua y $M \geq 0$ tal que

 \[\forall (t,x) \in {\cal R}_{a,b}\colon \|f(t,x)\| \leq M,\]

entonces existen $f_n_{|_{R_{a,b}}} \overset{c.u.}\longrightarrow f_{|_{R_{a,b}}}$ localmente lipschitzianas y uniformemente
acotadas por

\[\forall (t,x) \in {\cal R}_{a,b}\colon 
\| f_n_{|_{R_{a,b}}}(t,x) \| \leq M.
\]

****** Proof
Aplicamos el Teorema de [[id:a786fcd1-033d-477b-865e-a8a1f0aef252][Stone-Weierstrass]] en el compacto ${\cal R}_{a,b}$, al
Ã¡lgebra de los polinomios en varias variables
\[
  {\cal A} =
  \left\{ \sum_{(i_1,\dots,i_n) \in \mathbb{N}^{d+1}}
    \left(\lambda_{i_1,\dots,i_n} \prod_{j=1}^n x_j^{i_j}\right)
    \midd
    \lambda_{i_1,\dots,i_n} \in \mathbb{R}
  \right\},
\]
que es cerrada para la suma, el producto por escalares y el producto
de funciones y que ademÃ¡s contiene funciones que separan puntos (con las
lineales basta) y funciones que no se desvanecen en cada uno de ellos.

Podemos aproximar tanto como queramos las $d$ componentes de la
funciÃ³n $f$ por polinomios y, por tanto, podemos aproximar la $f$ uniformemente
por una funciÃ³n polinomial, asÃ­ que tomamos funciones polinomiales $\set{p_n}$
tales que $\norm{p_n - f}_\infty < 1/n$.
NÃ³tese que cumplen $\norm{p_n}_\infty \leq M + 1/n$, por lo que podemos definir la
siguiente sucesiÃ³n de funciones, que son localmente lipschitzianas por
ser polinomiales y por tanto de clase ${\cal C}^1$,
\[
  f_n = \frac{M}{M+1/n}p_n;\quad\mbox{ cumpliendo }\quad
  \norm{f_n}_\infty \leq M.
\]
Y tenemos finalmente la siguiente aproximaciÃ³n uniforme,
\[
  \norm{\frac{M}{M + 1/n} p_n - f}_\infty \leq
  \norm{\frac{1/n}{M + 1/n}p_n}_\infty + \norm{p_n - f}_\infty \leq
  \frac{1}{n} + \frac{1}{n} \to 0.\qedhere
\]

# ConvoluciÃ³n, polinomios de Bessel o Bernstein

# http://www.mast.queensu.ca/~speicher/Section14.pdf <- Por bernstein
# https://math.stackexchange.com/questions/437145/lipschitz-continuous-polynomials <- polinomios son lipschitz
# http://lsec.cc.ac.cn/~xuzq/lecture%201.pdf <- Por convoluciÃ³n

***** ProposiciÃ³n de soluciones de Volterra
:PROPERTIES:
:ID:       cccfae5e-60f8-4f15-ba11-c14af22e54d8
:END:
Si $\|f_{|{\cal R}_{a,b}}(t,x)\| \leq M$ y $Ma \leq b$, entonces existe una $\varphi \colon [t_0-a,t_0+a] \to \mathbb{R}^d$ que
es soluciÃ³n de la ecuaciÃ³n de Volterra asociada.

****** Proof
Tomamos $f_n$ del lema [[id:6fb8c0d7-7c28-4d3f-a6ae-c781ac55cb4e][de aproximaciÃ³n por localmente lipschitzianas]], que son
lipschitzianas en ${\cal R}_{a,b}$ por ser compacto. Las Volterra asociadas tienen
soluciÃ³n $\varphi_n$ en $E_{a,b}$ por la [[id:51ed4957-d6c4-448f-bfe0-8a8d2afe218b][proposiciÃ³n a Picard-LindelÃ¶f]]. Las $\varphi_n$ estÃ¡n uniformemente
acotadas allÃ­ por $\|x_0\| + b$ y las derivadas estÃ¡n uniformemente acotadas
$\|\varphi_n(t)\| = \|f_n(t,x)\| \leq M$, [[id:5b2c7b57-f17b-441f-a635-fddbb3e0c992][por lo que es]] sucesiÃ³n equicontinua.

Aplicando el [[id:859883bd-15a4-4efd-9d61-7d501b592918][corolario a Ascoli-ArzelÃ¡]], tenemos $\varphi_{\sigma(n)} \overset{c.u.}\longrightarrow \varphi$ y podemos
usar convergencia dominada de Lebesgue y que $f_{\sigma(n)}(s,\varphi_{\sigma(n)}(s)) \to f(s,\varphi(s))$
[[id:d2bb5e99-e7e9-4fa2-a8e2-38dcda39917e][por ser convergencia uniforme]] para tener
\[\begin{aligned}
\varphi(t) &=
\lim_{n \to \infty} \left(x_0 + \int_{t_0}^t f_n(s,\varphi_n(s))\,ds\right) \\&=
x_0 + \int_{t_0}^t \lim_{n \to \infty} f_n(s,\varphi_n(s))\,ds =
x_0 + \int_{t_0}^t f(s,\varphi(s))\,ds,
\end{aligned}\]
soluciÃ³n continua por ser lÃ­mite uniforme de continuas.

***** Teorema de Cauchy-Peano
:PROPERTIES:
:ID:       076910d4-3fd8-4447-9712-20c4e6cd2251
:END:
Todo PVI con campo continuo sobre abierto tiene soluciÃ³n.

Para $D = \mathring{D} \subset \mathbb{R} \times \mathbb{R}^d$ y $f \colon D \to \mathbb{R}^{d}$ continua con $(t_0,x_0) \in D$,
el PVI siguiente tiene soluciÃ³n

\[\left\{\begin{array}{c}
x' = f(t,x) \\
x(t_0) = x_0
\end{array}\right.\]

****** DemostraciÃ³n
Con $D$ abierto, existe ${\cal R}_{\overline{a},b} \subseteq D$. Por Weierstrass,

\[
\exists M\colon \forall (t,x) \in {\cal R}_{\overline{a},b}\colon \norm{f(t,x)} \leq M.
\]

Tomamos $a = \min \{b/M,\overline{a}\}$ y por la [[id:cccfae5e-60f8-4f15-ba11-c14af22e54d8][proposiciÃ³n anterior]] tenemos
soluciÃ³n de Volterra en intervalo $[t_0-a,t_0+a]$, soluciÃ³n al PVI
en $(t_0-a,t_0+a)$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-02 Thu>
:PROPERTIES:
:ID:       dd272b8d-7028-46f1-93f3-f77b1f4ceb5e
:DRILL_LAST_INTERVAL: 51.4563
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:57]
:END:
Enuncia el Teorema de Cauchy-Peano.

******* Enunciado
Para $f \colon D = \mathring{D} \subset \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^{d}$ continua con $(t_0,x_0) \in D$,
el PVI siguiente tiene soluciÃ³n

\[\left\{\begin{array}{c}
x' = f(t,x) \\
x(t_0) = x_0
\end{array}\right.\]

*** Tema 4. ProlongaciÃ³n y acotaciÃ³n de soluciones
**** 4.1. Comportamiento de soluciones maximales
***** Contexto
Consideramos para este tema
 
 * $D \subset \mathbb{R} \times \mathbb{R}^d$, abierto,
 * $f \colon D \to \mathbb{R}^{d}$, continua,
 * $\varphi \colon (\alpha,\omega)\to\mathbb{R}^d$ soluciÃ³n maximal de $x' = f(t,x)$.
  
Estudiaremos resultados para $\omega$ que se traducirÃ¡n directamente para $\alpha$.

***** Lema 1. El lÃ­mite no infinito toca el borde
:PROPERTIES:
:ID:       304d100c-6e59-4ed7-b623-4714c361a3dc
:END:
Si $\omega < \infty$ y $\exists \lim_{t \to \omega} \varphi(t) = \xi \in \mathbb{R}^d$, entonces $(\omega,\xi) \in \partial D$.

****** Proof
Sabemos $\varphi$ [[id:ba5ec2ba-b54a-455b-9b6c-9fca85940234][soluciÃ³n]], luego $\forall t \in (\alpha,\omega): (t,\varphi(t)) \in D$, y tomando lÃ­mite
obtendrÃ­amos $(\omega,\xi)\in \overline{D}$. Si se tuviera $(\omega,\xi)\in D$, entonces tomarÃ­amos
el PVI con $x(\omega) = \xi$, y por [[id:076910d4-3fd8-4447-9712-20c4e6cd2251][Cauchy-Peano]] tendrÃ­a una soluciÃ³n en entorno

\[
\psi \colon (\omega-a,\omega+a) \to \mathbb{R}^d.
\]

Pero entonces podrÃ­amos crear una soluciÃ³n mayor $\widetilde \varphi$ [[id:078ac783-76bf-4445-ad46-3f91ea9d1ef6][concatenando]] ambas,
lo que contraviene maximalidad.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-25 Mon>
:PROPERTIES:
:ID:       2de33c5f-62ce-4194-92a8-c6cd3f13b629
:DRILL_LAST_INTERVAL: 14.1176
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:41]
:END:
Si $\omega < \infty$ y $\exists \lim_{t \to \omega} \varphi(t) = \xi \in \mathbb{R}^d$, entonces [ $(\omega,\xi) \in \partial D$ ].

***** Lema 2. El lÃ­mite de una sucesiÃ³n toca el borde
:PROPERTIES:
:ID:       36ad0e3b-9430-43b5-9dcd-45f042f63000
:END:
Si $\omega < \infty$ y tenemos $t_n \in (\alpha,\omega)$ con $t_n \to \omega$ y $\varphi(t_n)\to \xi \in \mathbb{R}^d$, entonces
$(\omega,\xi)\in \partial D$.

****** Proof
Sabemos $\varphi$ [[id:ba5ec2ba-b54a-455b-9b6c-9fca85940234][soluciÃ³n]], luego $\forall t \in (\alpha,\omega)\colon (t,\varphi(t)) \in D$, y tomando lÃ­mite
obtendrÃ­amos $(\omega,\xi) \in \overline{D}$. Si se tuviera $(\omega,\xi) \in D$ abierto, entonces fijamos
$\varepsilon > 0$ y existe $R_{\overline{a},b}(\omega,\xi)\subseteq D$ con $b < \varepsilon$ por ser [[id:44351076-d98f-4552-a229-a41405beb6b1][base de entornos]].
Tomamos ademÃ¡s un mÃ¡ximo por Teorema de Weierstrass

\[
M = \max_{(t,x) \in R_{\overline{a},b}(\omega,\xi)} \| f(t,x) \|,
\]

y tomamos $a < \overline{a}$ tal que $aM \leq b/2$.

Usando convergencia, podemos encontrar $N \in \mathbb{N}$ tal que

\[
\forall n > N\colon\quad t_n \in (\omega-a,\omega), \quad\mbox{ y }\quad 
\|\varphi(t_n) - \xi \| \leq b/2.
\]

Si no fuera verdad que $\forall t \in [t_N,\omega]\colon \varphi(t) \in B(\xi,b)$, por el
[[id:a439476b-8c77-4994-a736-9062ee91e7d0][Lema del primer instante]] se llegarÃ­a a

\[
\exists \tau \in (t_N,\omega)\colon \|\varphi(t) - \xi\| < b,\qquad \|\varphi(\tau)-\xi\| = b,
\]

pero entonces, 

\[\begin{aligned}
b &\leq 
\|\varphi(\tau)-\varphi(t_n)\| + \| \varphi(t_n) - \xi \| \\&<
\left\| \int_{t_N}^{\tau} \varphi'(s)\,ds \right\| + \frac{b}{2} \\&=
\left\| \int_{t_n}^{\tau}f(s,\varphi(s))\,ds \right\| + \frac{b}{2} \\&\leq
M(\tau - t_n) + \frac{b}{2} \\&<
Ma + b/2 \leq b.
\end{aligned}\]

Llegando a contradicciÃ³n. AsÃ­, tenemos el lÃ­mite $\lim_{t \to \omega} \varphi(t) = \xi \in \mathbb{R}^d$,
y podemos aplicar el [[id:304d100c-6e59-4ed7-b623-4714c361a3dc][Lema anterior]] para tener $(\omega,\xi) \notin D$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       38226179-5572-4011-9c27-1bc2e7f2bb63
:DRILL_LAST_INTERVAL: 8.7389
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:46]
:END:
Si $\omega < \infty$ y tenemos $t_n \in (\alpha,\omega)$ con $t_n \to \omega$ y $\varphi(t_n)\to \xi \in \mathbb{R}^d$,
entonces [ $(\omega,\xi)\in \partial D$ ].

***** Alternativas de comportamiento en el extremo superior
:PROPERTIES:
:ID:       435b13ac-d20b-469c-bb0a-4915735b6c62
:END:
Si $\omega < +\infty$ se verifica una de las siguientes alternativas.

 * $\lim_{t \to \omega}\| \varphi(t)\| = \infty$, explota en tiempo finito;

 * $\exists (t_n,\varphi(t_n)) \to (\omega,\xi) \in \partial D$, tiende a tocar la frontera.

****** Proof
Supongamos que no se tiene la primera opciÃ³n: debe existir un $R_0$
y una sucesiÃ³n de $\tau_{n}$ tal que $\abs{\tau_n - \omega} < 1/n$ pero sin embargo
$\norm{\varphi(\tau_n)} \leq R_0$, rompiendo la convergencia.

Como $\varphi(\tau_n)$ es acotada, por Bolzano-Weierstrass, $\varphi(\tau_{\sigma(n)}) \to \xi \in \mathbb{R}^d$.
Pero ahora, $t_n = \tau_{\sigma(n)}$ cumple que $t_n \to \omega$ mientras que $\varphi(t_n) \to \xi$.
Estamos en las condiciones del [[id:36ad0e3b-9430-43b5-9dcd-45f042f63000][lema anterior]].

****** Card: Â¿CuÃ¡les son las tres alternativas?                                                            :drill:
SCHEDULED: <2018-08-07 Tue>
:PROPERTIES:
:ID:       44d857b1-d184-4611-a6b5-6df45fac65b0
:DRILL_LAST_INTERVAL: 79.9369
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-19 Sat 11:00]
:END:
Â¿CuÃ¡les son las tres alternativas de comportamiento
para el $\omega$ de una soluciÃ³n maximal?

******* Alternativas

 * $\omega = +\infty$, continÃºa indefinidamente;

 * $\lim_{t \to \omega}\| \varphi(t)\| = \infty$, explota en tiempo finito;

 * $\exists (t_n,\varphi(t_n)) \to (\omega,\xi) \in \partial D$, tiende a tocar la frontera.

***** Corolario 1
Si $\omega < \infty$ y el dominio no tiene borde, $D = \mathbb{R} \times \mathbb{R}^{d}$,
entonces $\lim_{t \to \infty} \|\varphi(t)\| = \infty$.

****** Proof
Al ser $\partial D = \varnothing$, el [[id:435b13ac-d20b-469c-bb0a-4915735b6c62][teorema anterior]] sÃ³lo da la primera alternativa.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-26 Tue>
:PROPERTIES:
:ID:       c5c0c521-3a1f-408e-a1c8-466aa6097c61
:DRILL_LAST_INTERVAL: 14.1292
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:43]
:END:
Si $\omega < \infty$ y el dominio no tiene borde, $D = \mathbb{R} \times \mathbb{R}^{d}$,
entonces [ $\lim_{t \to \infty} \|\varphi(t)\| = \infty$ ].

***** Corolario 2
Si $\omega < b < \infty$ y $D = (a,b) \times \mathbb{R}^d$, entonces $(\omega,\xi) \notin \partial D$ y $\lim_{t \to \omega} \|\varphi(t)\| = \infty$.

****** Proof
No se extiende indefinidamente y no puede tocar la frontera porque
$\omega < b$, debe explotar en tiempo finito.

***** Corolario 3
Si $\omega < \infty$ y $D = \mathbb{R} \times B$ para $B$ acotado, entonces
$\exists (t_n,\varphi(t_n)) \to (\omega,\xi) \in \partial D = \mathbb{R} \times \partial B$.

****** Proof
No se extiende indefinidamente y, como $B$ estÃ¡ acotado, no puede
explotar en tiempo finito.

**** 4.2. Crecimimento a lo sumo lineal
***** Lema de Gronwall (versiÃ³n dÃ©bil)
:PROPERTIES:
:ID:       23749801-6cfb-4efe-a808-aef93e33bc9a
:END:
Sea $y \in {\cal C}([t_0,\omega))$ con $C \in \mathbb{R}$ y $R \in \mathbb{R}^+$ tales que

\[
y(t) \leq
C + R \int_{t_0}^t y(s)\,ds.
\]

para todo $t \in [t_0,\omega[$. Entonces $y(t) \leq C e^{R(t-t_0)}$ para todo $t \in [t_0,\omega[$.

****** Proof
Caso $R = 0$ trivial. Si $R > 0$, tenemos por Teorema fundamental del
cÃ¡lculo que $Y'(t) \leq C + RY(t)$ para $Y(t) = \int_{t_0}^t y(s)\,ds$. Calculamos

\[
\dv{}{t} \left( e^{-R(t-t_0)}Y(t) \right) = 
-Re^{-R(t-t_0)}Y(t) + e^{-R(t-t_0)}Y'(t) \leq
Ce^{-R(t-t_0)},
\]

e integramos a ambos lados usando $Y(t_0) = 0$ y $R > 0$,

\[
e^{-R(t-t_0)}Y(t) \leq \int_{t_0}^t Ce^{-R(t-t_0)}\,ds = \frac{C}{-R}(e^{-R(t-t_0)} - 1)
\]

\[
Y(t) \leq \frac{C}{R}(e^{R(t-t_0)} - 1)
\]

Y aplicamos en la desigualdad original,

\[
y(t) \leq C + R\frac{C}{R} \left( e^{R(t-t_0)} -1 \right) = Ce^{R(t-t_0)}.
\]

****** Card: Enunciar el Lema de Gronwall                                                                  :drill:
SCHEDULED: <2018-08-30 Thu>
:PROPERTIES:
:ID:       c6e5ae82-baf7-4eea-a47e-5d9f5dbb1ec3
:DRILL_LAST_INTERVAL: 78.5209
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.6
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:43]
:END:
Enunciar el Lema de Gronwall, que acota una funciÃ³n cuando podemos
acotarla por su integral.

******* Enunciado
Sea $y \in {\cal C}([t_0,\omega),\mathbb{R})$ con $C \in \mathbb{R}$ y $R \in \mathbb{R}^+$ tales que

\[
\forall t \in [t_0,\omega)\colon
y(t) \leq
C + R \int_{t_0}^t y(s)\,ds.
\]

Entonces

\[
\forall t \in [t_0,\omega)\colon y(t) \leq C e^{R(t-t_0)}.
\]
 
***** Lema de Gronwall (fuerte)                                                                             :extra:
***** Crecimiento a lo sumo lineal
$f \colon (a,+\infty) \times \mathbb{R}^d = D \to \mathbb{R}^{d}$ tiene *crecimiento a lo sumo lineal*
si existen $m,n \in (a,+\infty) \to \mathbb{R}^+_0$ continuas tales que

\[
\forall (t,x) \in D \colon \norm{f(t,x)} \leq m(t)\norm{x} + n(t).
\]

****** Ejemplos

 * Las $f(t,x)$ globalmente lipschitzianas respecto $x$ son CSL.
 * Las $f(t,x)$ acotadas son CSL.
 * Las $f(t,x)$ lineales respecto de $x$ son CSL.

******* Lineales
******* AutÃ³nomas
****** Card                                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       d38e7263-005a-4230-97d1-d1ac4568b375
:DRILL_LAST_INTERVAL: 9.8053
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:47]
:END:
Â¿CuÃ¡ndo tiene $f\colon (a,+\infty)\times\mathbb{R}^d\to\mathbb{R}^{d}$ crecimiento a lo sumo
lineal?

******* DefiniciÃ³n
Existen $m,n \in (a,+\infty) \to \mathbb{R}^+_0$ *continuas* tales que

\[
\forall (t,x) \in D \colon \norm{f(t,x)} \leq m(t)\norm{x} + n(t).
\]

***** Teorema de crecimiento a lo sumo lineal
Si $f \colon (a, +\infty)\times\mathbb{R}^d \to \mathbb{R}^d$ tiene crecimiento a lo sumo lineal,
entonces $\omega = +\infty$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       76564c59-0fa4-421c-a3e7-efd830498553
:DRILL_LAST_INTERVAL: 10.9182
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:45]
:END:
Â¿QuÃ© dice el teorema de crecimiento a lo sumo lineal?

******* Answer
Que si un campo sin fronteras en el futuro $f \colon (a,+\infty) \times \mathbb{R}^d \to \mathbb{R}^d$
tiene crecimiento a lo sumo lineal, entonces $\omega = +\infty$.

****** Proof
Si $\omega < +\infty$, por [[id:435b13ac-d20b-469c-bb0a-4915735b6c62][alternativa de comportamiento]] y sabiendo que no
puede tocar la frontera, debe explotar en tiempo finito.
$\lim_{t \to \omega} \|\varphi(t)\| = +\infty$. Aplicamos Weierstrass para tener $M = \max_{t_0 \leq t \leq \omega} m(t)$
y $N = \max_{t_0 \leq t \leq \omega} n(t)$. Ahora

\[\begin{aligned}
y(t) &= \norm{\varphi(t)} =
\norm{x_0 + \int_{t_0}^tf(s,\varphi(s))\,ds} \\&\leq
\norm{x_0} + \int_{t_0}^t\norm{f(s,\varphi(s))}\,ds \\&\leq
\norm{x_0}  + N(\omega-t_0) + M\int_{t_0}^ty(s)\,ds.
\end{aligned}\]

Y por [[id:23749801-6cfb-4efe-a808-aef93e33bc9a][Lema de Gronwall]], acotamos por una constante

\[
y(t) \leq \left( \|x_0\| + N(\omega-t_0) \right) e^{M(t-t_0)} \leq (\|x_0\| + N(\omega-t_0)) e^{M(\omega-t_0)}.
\]

Llegando a contradicciÃ³n con la hipÃ³tesis de que explota en tiempo
finito.

***** Teorema de crecimiento a lo sumo lineal (hipÃ³tesis debilitada)
Si $f \colon D \to \mathbb{R}^d$ tiene crecimiento a lo sumo lineal en un $\Omega$
cerrado $[t_0,+\infty) \times \Omega \subset D$ con $\varphi(t) \in \Omega$ para cualquier $t \in [t_0,\omega)$,
entonces $\omega = +\infty$.

****** TODO Proof
**** 4.3. AcotaciÃ³n de soluciones. Funciones guÃ­a
***** Acotada en el futuro y acotada en el pasado
Decimos $\varphi$ *acotada en el futuro* si $\omega =+\infty$ y $\exists M\colon\forall t \geq t_0\colon \|\varphi(t)\| \leq M$.
Decimos $\varphi$ *acotada en el pasado* si $\omega = +\infty$ $\exists M\colon\forall t \leq t_0\colon \|\varphi(t)\| \leq M$.
Decimos $\varphi$ *acotada* si lo estÃ¡ en el futuro y en el pasado.

***** Funciones coercivas
:PROPERTIES:
:ID:       37022c48-c27d-4e11-aa9d-85c998a4b917
:END:
Decimos $V \in {\cal C}(\mathbb{R}^d, \mathbb{R})$ *coerciva* si todo conjunto de subnivel $\Omega_r$
es compacto.

****** Conjuntos de nivel y conjuntos de subnivel
El *conjunto de nivel* $r$ es $S_r = V^{-1}(r)$.
El *conjunto de subnivel* $r$ es $\Omega_r = V^{-1}([-\infty,r])$.

****** CaracterizaciÃ³n por lÃ­mite
Ser coerciva equivale a tenerse $\lim_{\|x\| \to +\infty}V(x) = +\infty$.

******* Proof
Sea $V$ coerciva, dado $K$, $V^{-1}(]-\infty,K])$ es compacto, luego
acotado y para $\|x\|$ suficientemente grande, serÃ¡ $V(x) > K$.

Si tenemos el lÃ­mite, $V^{-1}(]-\infty,K])$ es cerrado por continuidad
y estÃ¡ acotado porque si no lo estuviera podrÃ­amos tomar una
sucesiÃ³n que divergiera pero con sus imÃ¡genes acotadas por $K$.

****** MÃ­nimo global
NÃ³tese que una coerciva tiene mÃ­nimo global por Weierstrass sobre
un compacto para alguna cota.

****** Card: funciÃ³n coerciva                                                                              :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       6e44a52f-a72d-4288-85b7-3c7daa3ca1d4
:DRILL_LAST_INTERVAL: 35.3401
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.167
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-13 Sun 22:08]
:END:
DefiniciÃ³n de funciÃ³n coerciva.

******* DefiniciÃ³n
Todo conjunto de subnivel $\Omega_r$ es compacto.

****** Card: caracterizaciÃ³n                                                                               :drill:
SCHEDULED: <2018-07-26 Thu>
:PROPERTIES:
:ID:       37d1fd0f-a7af-4a52-8be0-626e6968b6c2
:DRILL_LAST_INTERVAL: 43.8661
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 18:16]
:END:
Caracteriza por lÃ­mites una funciÃ³n coerciva

******* CaracterizaciÃ³n
Ser coerciva equivale a

\[\lim_{\|x\| \to +\infty}V(x) = +\infty\]
***** Derivada a lo largo de un campo
La *derivada* de $V \in {\cal C}^1(\mathbb{R}^d,\mathbb{R})$ a lo largo de $f$ se define

\[
\bV(t,x) = \left\langle \nabla V(x),f(t,x) \right\rangle.
\]

NÃ³tese en particular $\dv{}{t} \left[ V(\varphi(t)) \right]_{|_t} = \overset{\bullet}{V}(t,\varphi(t))$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-19 Sun>
:PROPERTIES:
:ID:       170edc4f-ef66-4b1e-a3ef-3b1312feaeb5
:DRILL_LAST_INTERVAL: 66.5458
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.08
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:29]
:END:
Â¿CÃ³mo se define la derivada a lo largo de un campo $f$?

$\bV(t,x) = \dots$

******* DefiniciÃ³n
Para $V \in {\cal C}^1(\mathbb{R}^d,\mathbb{R})$ se define

\[
\bV(t,x) = \left\langle \nabla V(x),f(t,x) \right\rangle.
\]
***** ProposiciÃ³n de acotaciÃ³n por coercivas
:PROPERTIES:
:ID:       82dab245-c6b0-4bd6-8746-98af9e2892c6
:END:
Sea $V \in {\cal C}^1(\mathbb{R}^d, \mathbb{R})$ coerciva. Si $S_r$ es un conjunto de nivel para
algÃºn $r$, cumpliendo que $\bV(t,x) < 0$ para todo $(t,x) \in S_r$ y ademÃ¡s,
$x_0 \in \Omega_r$, entonces $\forall t \geq t_0\colon \varphi(t) \in \Omega_r$, y estÃ¡ acotada en el futuro.

****** Proof
Partimos en casos segÃºn $V(x_0) < r$ Ã³ $V(x_0) = r$.

******* Caso 1
Sea $V(x_0) < r$. Definimos $y = V \circ \varphi \in {\cal C}^1$ y tenemos $y'(t) = \bV(t,\varphi(t))$.
Si no se tuviera $y(t) < r$, por [[id:a439476b-8c77-4994-a736-9062ee91e7d0][Lema del primer instante]] se tendrÃ­a
$y(\tau) = r$ y $\forall t \in [t_0,\tau[\colon y(t) < r$ para algÃºn $\tau > t_0$. AhÃ­ tendrÃ­amos
$y'(\tau) \geq 0$ pero $\bV(\varphi(\tau)) < 0$, llegando a contradicciÃ³n.

Sabemos asÃ­ que $\varphi([t_0,\omega[) \subset \Omega_r$, que es compacto por ser $V$ [[id:37022c48-c27d-4e11-aa9d-85c998a4b917][coerciva]],
luego estÃ¡ acotada.

******* Caso 2
Sea $V(x_0) = r$, tendrÃ­amos $y(t_0) = r$ y $y'(t_0) < 0$. Por definiciÃ³n de
derivada, para algÃºn $\varepsilon$ tendremos $\forall t \in (t_0,t_0+\varepsilon)\colon y(t) < r$. Aplicamos
el primer caso en algÃºn punto de ese intervalo.

***** Teorema. AcotaciÃ³n mediante funciones guÃ­a
:PROPERTIES:
:ID:       b1ed10c4-862c-4722-b481-6f83bf18c479
:END:
Si existe $V \in {\cal C}^1(\mathbb{R}^d)$ coerciva tal que $\bV(t,x)\leq 0$ para
cualesquiera $(t,x) \in D$ con $t \geq t_0$; entonces las soluciones
maximales de $x' = f(t,x)$, $x(t_0)=x_0$ estÃ¡n acotadas en el futuro.

AnÃ¡logamente, si $\bV(t,x) \geq 0$ cuando $t \leq t_0$, estarÃ¡n acotadas en
el pasado, y si $\bV(t,x) = 0$ siempre, estarÃ¡n acotadas.

****** Proof
Definimos $y = V \circ \varphi \in {\cal C}^1$, decreciente por $y'(t) = \bV(t,\varphi(t)) \leq 0$.
Luego, por la [[id:82dab245-c6b0-4bd6-8746-98af9e2892c6][proposiciÃ³n anterior]], $\varphi([t_0,\omega[) \subset \Omega_{V(x_0)}$, compacto y
por tanto acotado, por ser $V$ [[id:37022c48-c27d-4e11-aa9d-85c998a4b917][coerciva]].

****** TODO Proof (independiente)
****** Card: enunciado                                                                                     :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       fbcbd5af-72f0-45e7-aeea-916ae98bc48f
:DRILL_LAST_INTERVAL: 11.6797
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:54]
:END:
Enunciado del teorema de acotaciÃ³n mediante funciones guÃ­a.

******* Enunciado
Si existe $V \in {\cal C}^1(\mathbb{R}^d)$ coerciva tal que $\bV(t,x)\leq 0$ para
cualesquiera $(t,x) \in D$ con $t \geq t_0$; entonces las soluciones
maximales de $x' = f(t,x)$, $x(t_0)=x_0$ estÃ¡n acotadas en el futuro.

AnÃ¡logamente, si $\bV(t,x) \geq 0$ cuando $t \leq t_0$, estarÃ¡n acotadas en
el pasado, y si $\bV(t,x) = 0$ siempre, estarÃ¡n acotadas.

***** AcotaciÃ³n mediante funciones guÃ­a: corolario
Si existe $V \in {\cal C}^1(\mathbb{R}^d,\mathbb{R})$ coerciva tal que $\forall (t,x) \in D\colon \bV(t,x) = 0$,
las soluciones maximales estÃ¡n acotadas, teniÃ©ndose $\varphi(t) \in \Omega_{V(x_0)}$.

****** Proof
Aplicando los dos casos del [[id:b1ed10c4-862c-4722-b481-6f83bf18c479][teorema]] anterior.
***** AcotaciÃ³n mediante funciones guÃ­a: debilitaciÃ³n
Podemos aplicar dos debilitaciones a las hipÃ³tesis del teorema
de [[id:b1ed10c4-862c-4722-b481-6f83bf18c479][acotaciÃ³n mediante funciones guÃ­a]],

 * sÃ³lo necesitamos $\bV(t,x)\leq 0$ en $(t_0,+\infty) \times \left(\mathbb{R}^d \setminus B(p,r)\right)$ para
   algunos $p \in \mathbb{R}^d$ y $r > 0$;
 * podemos prescinidir de $V$ coerciva y pedir sÃ³lo que las
   componentes conexas de los $\Omega_r$ sean acotadas.

****** TODO Proof
***** ElecciÃ³n de funciones guÃ­a
Para $V,W\colon \mathbb{R}^d \to \mathbb{R}$ y $V_1,\dots,V_d \colon \mathbb{R} \to \mathbb{R}$ continuas se tiene

 * $V(x_1,\dots,x_d) = V_1(x_1) + \dots + V_d(x_d)$ coerciva ssi lo son las $V_i$;

 * $V(x) \geq W(x)$ coerciva si lo es $W$;

 * $V$ cuadrÃ¡tica es coerciva ssi $\mathrm{Hess}(V)$ es definida o semidefinida positiva;

 * $V$ coerciva si $V \in {\cal C}^1(\mathbb{R}^d)$ es convexa con Ãºnico punto crÃ­tico.

Suelen funcionar el cuadrado de la norma euclÃ­dea, variables separadas
y, en el peor de los casos, una forma cuadrÃ¡tica definida positiva.

****** Card: cuadrÃ¡ticas coercivas                                                                         :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       c495ad82-f560-46dd-b9ab-677edf45aeee
:DRILL_LAST_INTERVAL: 7.1098
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:38]
:END:
En el caso particular de una forma cuadrÃ¡tica. Â¿Cuando es una forma
cuadrÃ¡tica coerciva?

******* Answer
Ssi la parte cuadrÃ¡tica (coincide con la hessiana) es definida
positiva.

****** Card: Ãºltima condiciÃ³n                                                                              :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       4134c0bb-2847-4c1c-b3ce-55807475a84b
:DRILL_LAST_INTERVAL: 11.1451
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:38]
:END:
Si $V \in {\cal C}^1(\mathbb{R}^d)$ es [convexa] con [un Ãºnico punto crÃ­tico]; entonces [ $V$ es coerciva ]

*** Seminario 1. Ecuaciones autÃ³nomas ([[~/pdf/alonso_seminario_i_de_ecuaciones_diferenciales.pdf][PDF]])
**** SoluciÃ³n general
Trabajamos en una ecuaciÃ³n autÃ³noma real con un campo localmente
lipschitziano definido en $(a,b)$. Llamamos $X(t;t_0,x_0)$ a la
*soluciÃ³n general*, la Ãºnica soluciÃ³n maximal del PVI en esas
condiciones iniciales.

***** Proof
Sabemos que la soluciÃ³n maximal es Ãºnica por [[id:bf38540e-3857-427e-9b27-e6c0bf0cfd73][Picard-LindelÃ¶f]].
**** Puntos de equilibrio
Los ceros del campo, $Z_f = \left\{ p \in (a,b) \mid f(p) = 0 \right\}$, dan soluciones
Ãºnicas triviales llamadas *puntos de equilibrio*.

**** Propiedad 1. Soluciones no constantes son estrictamente monÃ³tonas
En una autÃ³noma escalar, las soluciones no constantes son estrictamente
monÃ³tonas.

***** Proof
Si $\varphi(t) = X(t;t_0,x_0)$ no fuera monÃ³tona, se tendrÃ­a algÃºn
$\varphi'(\tau)=0$, y en ese punto se tendrÃ­a $\varphi(\tau) \in Z_f$ y se romperÃ­a
la unicidad en el punto de equilibrio.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       c289aec0-a546-4295-a571-556977c38e06
:DRILL_LAST_INTERVAL: 11.6107
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:44]
:END:
En una ecuaciÃ³n autÃ³noma escalar, Â¿por quÃ© las soluciones no
constantes son estrictamente monÃ³tonas?

****** Answer
Si no lo fuera, en algÃºn punto se tendrÃ­a derivada nula.
Ese punto serÃ­a un punto de equilibrio y se romperÃ­a la
unicidad con la soluciÃ³n constante en ese mismo punto.

**** Propiedad 2,3. Si hay lÃ­mite, es punto de equilibrio y estÃ¡ en infinito
En una autÃ³noma escalar, la soluciÃ³n maximal cumple que
si $\lim_{t \to \omega} \varphi(t) = L \in (a,b)$, entonces $L \in Z_f$ y ademÃ¡s $\omega = +\infty$.

En una autÃ³noma escalar, la soluciÃ³n maximal cumple que
si $\lim_{t \to \alpha} \varphi(t) = L \in (a,b)$, entonces $L \in Z_f$ y ademÃ¡s $\alpha = -\infty$.

De otra forma, si $\lim_{t \to \omega(t_0,x_0)} X(t;t_0,x_0) = L \in (a,b)$, entonces
$L \in Z_f$ y ademÃ¡s $\omega(t_0,x_0) = +\infty$.

***** Proof
Aplicamos [[id:435b13ac-d20b-469c-bb0a-4915735b6c62][alternativas de comportamiento]] y descartamos que pueda
explotar en tiempo finito (tiene lÃ­mite finito) y que pueda tocar
la frontera, que supondrÃ­a $L \in \partial (a,b) = \left\{ a,b \right\}$. AsÃ­, $\omega = +\infty$.

Siendo $\varphi(t) = X(t;t_0,x_0)$, por Barbalat tenemos una sucesiÃ³n
$f(L) \gets f(\varphi(t_n)) = \varphi'(t_n) \to 0$.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       707d89c3-fd94-45a3-8898-959fd2fbccd2
:DRILL_LAST_INTERVAL: 8.1577
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-14 Thu 11:49]
:END:
Para una soluciÃ³n maximal de ecuaciÃ³n autÃ³noma escalar,
si $\lim_{t \to \omega} \varphi(t) = L \in (a,b)$, entonces $L \in Z_f$ y ademÃ¡s $\omega = +\infty$.
Â¿Por quÃ©?

****** Proof
La Ãºnica alternativa de comportamiento positble es es
$\omega = +\infty$. Por Barbalat, $f(L) \gets f(\varphi(t_n)) = \varphi'(t_n) \to 0$.

**** Propiedad 4,5. Los ceros son barreras superiores
En una autÃ³noma escalar, si $x_0 < p \in Z_f$, entonces la soluciÃ³n
maximal empezando en $x_0$ queda acotada, $\varphi(t) < p$.

En una autÃ³noma escalar, si $x_0 > p \in Z_f$, entonces la soluciÃ³n
maximal empezando en $x_0$ queda acotada, $\varphi(t) > p$.


***** Proof
Si no se tuviera, por Bolzano tendrÃ­amos algÃºn punto donde
$\varphi(\tau) = p$, y ahÃ­ se romperÃ­a la unicidad.

**** Propiedad 6,7. Sigmoides
Sea $f(p) = f(q) = 0$ con $p < q$ y con $f(x) > 0$ en $(p,q)$.
Se tiene para la soluciÃ³n empezando en $x_0 \in (p,q)$ que

 * queda atrapada en $(p,q)$,
 * es estrictamente creciente,
 * se extiende a $+\infty$ con lÃ­mite $q$,
 * se exitende a $-\infty$ con lÃ­mite $p$.

Si en esta misma situaciÃ³n se tiene $f(x) < 0$, la soluciÃ³n

 * queda atrapada en $(p,q)$,
 * es estrictamente decreciente,
 * se extiende a $+\infty$ con lÃ­mite $p$,
 * se exitende a $-\infty$ con lÃ­mite $q$.
 
**** Propiedad 8,9. Comportamiento en los bordes
**** Ãrbitas
La Ã³rbita de una ecuaciÃ³n escalar autÃ³noma en el punto $x_0$ es la
imagen de la Ãºnica soluciÃ³n maximal de un PVI desde $x_0$.

**** Propiedad 10. Ãrbitas posibles
Se tiene ${\cal O}(x_0) \subset (a,b)$. Si $f(x_0) = 0$ entonces ${\cal O}(x_0) = \left\{ x_0 \right\}$
y si $f(x_0) \neq 0$, entonces ${\cal O}(x_0)$ es un intervalo abierto.

**** Propiedad 11. Ãrbitas coincidentes
Cuando $f(x) \neq 0$ en $[x_0,x_1]$, tenemos Ã³rbitas coincidentes
${\cal O}(x_0) = {\cal O}(x_1)$ y existe algÃºn desfase $X(t;t_1,x_1) = X(t + \tau,t_0,x_0)$.

***** Proof
La coincidencia de Ã³rbitas es porque no hay cero entre ellas
y el desfase se obtiene de la unicidad de soluciÃ³n.

**** Diagrama de fases
Algoritmo:

 1. Calcular puntos de equilibrio.
 2. Usar signo del campo para las direcciones del diagrama.
 3. Estudiar la infinitud de definiciÃ³n en algunos casos.

**** Propiedad 12. Â¿Tocan la frontera?
Si $\lim_{x \to c} f(x) = L \neq 0$ para $c$ real en la frontera y $L \in \mathbb{R} \cup \left\{ \infty,-\infty \right\}$;
entonces 

 * si $\lim_{t \to \omega} \varphi(t) = c$, se tiene $\omega < +\infty$.
 * si $\lim_{t \to \alpha}\varphi(t) = c$, se tiene $\alpha > -\infty$.

***** Proof
Si $\omega = +\infty$ y $\lim_{t \to \infty} \varphi(t) = c$, por Barbalat tendrÃ­amos
$0 \gets \varphi'(t_n) = f(\varphi(t_n)) \to L$.

**** Propiedad 13. Â¿Explotan en tiemepo finito? Orden de infinitud
Si $b = +\infty$, con $\lim_{t \to \omega} X(t;t_0,x_0) = +\infty$ y teniÃ©ndose

\[
\lim_{x \to {}+\infty} \frac{f(x)}{x^p} = L \in [0,+\infty]
\]

para $p \geq 1$.

 * Si $p = 1$ y $L \in \mathbb{R}$, entonces $\omega = +\infty$.
 * Si $p > 1$ y $L > 0$, entonces $\omega < +\infty$.
 
***** TODO Proof
**** Propiedad 14. RegiÃ³n de atracciÃ³n
Para un punto de equilibrio $p \in Z_f$, la regiÃ³n de atracciÃ³n es

\[
{\cal R}(p) = \left\{ x_0 \in (a,b) \mid \lim_{t \to \omega} X(t;t_0,x_0) = p \right\}.
\]

Sabemos que

 * $p \in {\cal R}(p)$,

 * $x_0 \in {\cal R}(p)$ implica $\omega(0,x_0) =+\infty$,

 * ${\cal R}(p)$ intervalo. 

**** Propiedad 15. Atractores
Un punto es *atractor* si estÃ¡ en el interior de su regiÃ³n de
atracciÃ³n. Es *atractor global* si su regiÃ³n de atracciÃ³n es
todo $(a,b)$.

Si $f$ es derivable en $p \in Z_f$.

 1. $f'(p)<0$ nos da $p$ atractor.
 2. $f'(p) > 0$ nos da $p$ no atractor.

*** Seminario 2. Ecuaciones diferenciales de segundo orden ([[~/pdf/alonso_seminario_ii_de_ecuaciones_diferenciales.pdf][PDF]])
**** 1. Ecuaciones sin rozamiento
***** DefiniciÃ³n
Llamamos *ecuaciÃ³n sin rozamiento* a una de la forma

$mx'' + g(x) = 0$

donde $m > 0$, $g$ continua.  Equivale a 

\[\left\{\begin{array}{l}
x'_1 = x_2 \\
x'_2 = -\frac{1}{m}g(x_1)
\end{aligned}\right.\]

Cuando $g$ es lipschitziana, existe soluciÃ³n Ãºnica por
Picard-LindelÃ¶f.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       9fb6c5d6-730d-4297-a0fe-212c2b8f2b76
:DRILL_LAST_INTERVAL: 10.0479
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:52]
:END:
Forma de una ecuaciÃ³n sin rozamiento.

******* Answer

$mx'' + g(x) = 0$

donde $m > 0$, $g$ continua.

***** AcotaciÃ³n de soluciones, mÃ©todo de la energÃ­a
Llamamos *energÃ­a* a

\[
V(x_1,x_2) = V_1(x_1) + V_2(x_2) = G(x_1) + \frac{1}{2}mx^2_2
\]

Se tiene $\bV(x_1,x_2) = 0$.  Cuando $G$, primitiva de $g$ es coerciva,
todas las soluciones estÃ¡n acotadas en el futuro y en el pasado.

Esto es un principio de *conservaciÃ³n de la energÃ­a*.

**** 2. Ecuaciones con rozamiento
***** DefiniciÃ³n
Llamamos *ecuaciÃ³n con rozamiento* a una de la forma

\[
mx'' + cx' + g(x) = 0
\]

donde $m,c > 0$, $g \in {\cal C}(\Omega,\mathbb{R}^d)$.  Equivale a

\[\left\{\begin{array}{l}
x'_1 = x_2 \\
x'_2 = -\frac{c}{m}x_2 - \frac{1}{m}g(x_1)
\end{aligned}\right.\]

Cuando $g$ es localmente lipschitziana, existe soluciÃ³n Ãºnica
por Picard-LindelÃ¶f.

***** AcotaciÃ³n de soluciones, mÃ©todo de la energÃ­a
Llamamos *energÃ­a* a

\[
V(x_1,x_2) = V_1(x_1) + V_2(x_2) = G(x_1) + \frac{1}{2}mx^2_2
\]

y ahora se tiene $\bV(x_1,x_2) \leq 0$. Cuando $G$ es coerciva, todas
las soluciones estÃ¡n acotadas en el futuro y la energÃ­a se
va perdiendo.

****** Card: ejercicio                                                                                     :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       7969a4bb-c897-4560-bd1b-984118b0b9f9
:DRILL_LAST_INTERVAL: 11.909
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:44]
:END:
Demuestra que las soluciones de

\[
x'' + 2x' + x\abs{x} = 0
\]

estÃ¡n acotadas en el futuro.

******* Answer
Tenemos una ecuaciÃ³n con rozamiento $mx'' + cx' + g(x) = 0$ con 
$m = 1$, $c = 2$ y $g(x) = x\abs{x}$ donde la primitiva $G(x) = \frac{1}{3}\abs{x}^3$
es coerciva.
**** 4. EcuaciÃ³n de LiÃ©nard
***** Planteamiento
Llamamos ecuaciÃ³n de LiÃ©nard a

\[
x'' + f(x)x' + g(x) = 0
\]

donde $f,g \in {\cal C}(\mathbb{R})$ son localmente lipschitzianas.  Tomamos el cambio
de variable $x'_1 = x_2 - F(x_1)$ para llegar a

\[\left\{\begin{array}{l}
x'_1 = x_2 - F(x_1) \\
x'_2 = - g(x_1)
\end{aligned}\right.\]

donde $F$ es una primitiva de $f$.

****** Card: ecuaciÃ³n                                                                                    :nodrill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       b30bc986-968e-4035-a702-5d5dd943c680
:DRILL_LAST_INTERVAL: 9.8018
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:52]
:END:
Forma de la ecuaciÃ³n de LiÃ©nard.

******* Answer

\[
x'' + f(x)x' + g(x) = 0
\]

donde $f,g \in {\cal C}(\mathbb{R})$ son localmente lipschitzianas

****** Card: cambio variable                                                                               :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       bdde7129-5d4d-4c8a-9d5c-db27651f4746
:DRILL_LAST_INTERVAL: 4.786
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.6
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 15:06]
:END:
Â¿QuÃ© cambio de variable debe hacerse en la ecuaciÃ³n de LiÃ©nard
para plantearlo como un sistema de primer orden?

\[
x'' + f(x)x' + g(x) = 0
\]

******* Answer

\[\left\{\begin{array}{l}
x'_1 = x_2 - F(x_1) \\
x'_2 = - g(x_1)
\end{aligned}\right.\]

***** FunciÃ³n guÃ­a
Cuando $G$, primitiva de $g$ es coerciva y ademÃ¡s $g$ tiene el mismo signo
que $F$, la primitiva de $f$, hay una funciÃ³n guÃ­a en sumandos separados
que nos da /acotaciÃ³n en el futuro/.

Puede comprobarse simplemente buscando la funciÃ³n guÃ­a.

****** Card: criterio                                                                                      :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       6b663f7a-48f4-4c30-aa68-e9ed409c278e
:DRILL_LAST_INTERVAL: 4.5718
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 15:03]
:END:
CuÃ¡l es el criterio para tener *acotaciÃ³n en el futuro* en una
EcuaciÃ³n de LiÃ©nard.

\[
x'' + f(x)x' + g(x) = 0
\]

******* Answer
Cuando $G$, primitiva de $g$, es coerciva y ademÃ¡s $g$ tiene el mismo
signo que $F$, primitiva de $f$.

****** Card: en Lienard                                                                                    :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       17cab3fb-5c21-409e-a827-a67ac309dcd1
:DRILL_LAST_INTERVAL: 4.1578
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:30]
:END:
Supongamos que tenemos una ecuaciÃ³n de LiÃ©nard

\[
x'' + f(x)x' + g(x) = 0
\]

y sabemos que $G$, primitiva de $g$, es coerciva y ademÃ¡s $g$ tiene el
mismo signo que $F$, primitiva de $f$. Â¿QuÃ© sabemos?

******* Answer
Que podemos encontrar una funciÃ³n guÃ­a que nos darÃ¡ acotaciÃ³n
en el futuro.

**** 3. Ecuaciones forzadas
Llamamos *ecuaciÃ³n forzada* a una de la forma

\[
mx'' + cx' + g(x) = p(t)
\]

donde $m>0$, $c \geq 0$, $g \in {\cal C}(\Omega,\mathbb{R}^d)$ es localmente lipschitziana
y $p \in {\cal C}(\mathbb{R},\mathbb{R}^d)$.  Equivale a

\[\left\{\begin{array}{l}
x'_1 = x_2 \\
x'_2 = \frac{p(t)}{m} -\frac{c}{m}x_2 - \frac{1}{m}g(x_1)
\end{aligned}\right.\]

La funciÃ³n guÃ­a debe buscarse. 

*** Tema 5. Continuidad y diferenciabilidad de la soluciÃ³n
**** 5.1. Entornos tubulares
***** Entorno tubular
Para $J \subset \mathbb{R}$ intervalo, $\varphi \colon {\cal C}(J, \mathbb{R})$ y $\rho > 0$,

\[
T_{\rho}(J,\varphi) = \left\{ (t,x) \in J \times \mathbb{R}^d 
\;\middle|\;
\norm{x - \varphi(t)} \leq \rho \right\}.
\]

****** Entornos para Picard-LindelÃ¶f
Los [[id:44351076-d98f-4552-a229-a41405beb6b1][entornos]] de Picard-LindelÃ¶f, son entornos tubulares sobre
una funciÃ³n constante.

\[
{\cal R}_{a,b}(t_0,x_0) = T_b([t_0-a,t_0+a],x_0).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-17 Tue>
:PROPERTIES:
:ID:       0645fdf9-3254-44f3-9354-a00edd4af6a9
:DRILL_LAST_INTERVAL: 35.0078
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-12 Tue 14:39]
:END:
Entorno tubular, $T_{\rho}(J,\varphi)$.

******* DefiniciÃ³n
Para $J \subset \mathbb{R}$ intervalo, $\varphi \colon {\cal C}(J, \mathbb{R})$ y $\rho > 0$,

\[
T_{\rho}(J,\varphi) = \left\{ (t,x) \in J \times \mathbb{R}^d 
\;\middle|\;
\norm{x - \varphi(t)} \leq \rho \right\}.
\]

****** Card: abiertos o cerrados                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       59fa2dbc-7796-4cd4-91db-cc295bcec020
:DRILL_LAST_INTERVAL: 32.2101
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-21 Mon 14:53]
:END:
Â¿Los entornos tubulares son abiertos o cerrados?

******* Respuesta
Cerrados

\[
T_{\rho}(J,\varphi) = \left\{ (t,x) \in J \times \mathbb{R}^d 
\;\middle|\;
\norm{x - \varphi(t)} \leq \rho \right\}.
\]

***** Lema 1. Homomorfismo del entorno tubular

\[
T_{\rho}(J,\varphi) \cong J \times \overline{B}(0,\rho).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-09-12 Wed>
:PROPERTIES:
:ID:       b0afd4f3-2e7a-4222-8273-435a3ab7f163
:DRILL_LAST_INTERVAL: 74.9219
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:53]
:END:
Â¿A quÃ© espacio mÃ¡s sencillo es homeomorfo el
entorno tubular $T_p(J,\varphi)$?

******* Respuesta

\[
T_{\rho}(J,\varphi) \cong J \times \overline{B}(0,\rho)
\]

***** Lema 2. Entorno tubular dentro de un dominio
Para $D \subset \mathbb{R}^d \times \mathbb{R}$ abierto con $(t,\varphi(t)) \in D$ y para $J$ compacto, existe
$\rho>0$ tal que $T_p(J,\varphi) \subset D$.

****** TODO Proof
***** Lema 3. Localmente lipschitziana en compacto es globalmente lipschitziana
Una funciÃ³n localmente lipschitziana en un compacto es globalmente
lipschitizana en el compacto.

****** Proof
Si no fuera globalmente lipschitziana podemos aplicar
Bolzano-Weierstrass para obtener sucesiones que hicieran que no
pudiera ser localmente lipschitziana.

***** TODO Lema 4. Globalmente lipschitziana en un entorno tubular
**** 5.2. Convergencia uniforme en compactos
***** Convergencia uniforme en compactos
Una sucesiÃ³n de funciones definidas en intervalos, $f_n \colon I_n \to \mathbb{R}^d$;
*converge uniformemente en compactos* a $f \colon I \to \mathbb{R}$ cuando para
todo $[a,b] \subset I$, a partir de un $n_0$,

\[
[a,b] \subseteq I_n, \mbox{ y ademÃ¡s }\quad
f_n_{|[a,b]} \overset{c.u.}\longrightarrow f_{|[a,b]}.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-06 Mon>
:PROPERTIES:
:ID:       8b8c5241-89f5-4f08-b039-7e62f401a15f
:DRILL_LAST_INTERVAL: 53.9366
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:28]
:END:
Convergencia uniforme en compactos.

******* DefiniciÃ³n
Tenemos $f_n_{|I_n} \overset{cuc}\longrightarrow f_{|I}$ si para todo $[a,b] \subset I$, a partir de un $n_0$,

 * $[a,b] \subseteq I_n$,

 * $f_n_{|[a,b]} \overset{cu}\longrightarrow f_{|[a,b]}$.

****** Card: convergencia uniforme en compactos pero no uniforme                                           :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       c69612d6-af02-41c5-82b5-abb4085409db
:DRILL_LAST_INTERVAL: 29.2476
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-21 Mon 14:53]
:END:
Ejemplo de $f_n$ que convergen uniformemente en compactos pero
no uniformemente.

******* Ejemplo
En $f_n \colon \mathbb{R} \to \mathbb{R}$, trivialmente no convergen uniformemente pero
sÃ­ convergen en cualquier compacto.

\[
f_n(t) = \frac{1}{n}t
\]

****** Card: convergencia puntual no uniforme en compactos                                                 :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       a23dd345-50e1-4c73-a77a-9aa9abf4a93a
:DRILL_LAST_INTERVAL: 29.5063
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-11 Mon 21:42]
:END:
Ejemplo de $f_n$ que converge puntual pero no uniformemente en
compactos.

******* Ejemplo
En $[0,1]$, las funciones $f_n = x^n$ convergen puntualmente a una
funciÃ³n no continua.

******* Ejemplo
En $[0,1]$, podemos tomar

\[
f_n(t) = n \left( t + \frac{1}{n} \right)_+ - n(t)_+
\]

que crean un escalÃ³n entre $[1/n,0]$, haciendo que no haya convergencia
uniforme en cualquier intervalo conteniendo a $0$.

***** Lema 5. Continuidad del lÃ­mite de convergencia uniforme en compactos
Si $f_n \in {\cal C}(I_n)$ y $f_n \overset{cuc}\longrightarrow f$, entonces $f \in {\cal C}(I)$.

****** TODO Proof
# Por desigualdad triangular.
****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       c62d7140-be30-4f57-8c75-d10ff98b7c00
:DRILL_LAST_INTERVAL: 29.2661
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-20 Sun 23:27]
:END:
Para una secuencia $f_n \in {\cal C}(I_n)$, quÃ© tipos de convergencia preservan
la continuidad.

******* Respuesta
Basta $f_n \overset{cuc}\longrightarrow f$ para tener $f \in {\cal C}(I)$. TambiÃ©n se cumple con la
convergencia uniforme, que es mÃ¡s fuerte; pero no se cumple con
la puntual.

***** Lema 6. Convergencia de aplicaciones en convergencia uniforme
Si $f_n \overset{cuc}\longrightarrow f$, sabemos $t_n \in I_n$ con $t_n \to t_{\ast}$, y $f$ es continua en $t$,
entonces $f_n(t_n) \to f(t_{\ast})$.

**** 5.3. Continuidad de la soluciÃ³n general
Consideramos dominio abierto $D \subset \mathbb{R} \times \mathbb{R}^d$ y campo $f$ continuo y
localmente lipschitziano respecto $x$.

***** ProposiciÃ³n 1.
Sea ${\cal R}_{a,b}(t_0,x_0) \subset D$ donde $\norm{f(t,x)} \leq M$. Existe $\mu_0$ tal que
$\forall\mu \in (0,\mu_0)$, si $\overline{\varphi} \colon I \to \mathbb{R}^d$ es una soluciÃ³n maximal con condiciÃ³n inicial
$(t_1,x_1) \in {\cal R}_{\mu,\mu}(t_0,x_0)$; entonces $(t,\overline{\varphi}(t)) \in {\cal R}_{a,b}(t_0,x_0)$ para
$t \in [t_0-\mu,t_0+\mu]$ y ademÃ¡s

\[
\norm{\overline{\varphi}(t)-x_0} \leq
\norm{x_1 - x_0} + M\abs{t-t_1} \leq
(1 + 2M)\mu.
\]

En particular, $\norm{\overline{\varphi}(t_0) - x_0} \leq (1 + M)\mu$.

****** Proof                                                                                               :extra:
***** Lema 7. Desigualdad fundamental
Sean $\varphi_i \colon I_i \to \mathbb{R}^{d}$ dos soluciones de $x' = f(t,x)$ tales que

\[
\forall t \in I_1\cap I_2 \colon\qquad
\norm{f(t,\varphi_1(t)) - f(t,\varphi_2(t))} \leq
L\norm{\varphi_1(t) - \varphi_2(t)},
\]

entonces para $t_0 \in I_1 \cap I_2$ se tiene

\[
\forall t \in I_1\cap I_2 \colon\qquad
\norm{\varphi_1(t) -\varphi_2(t)} \leq
\norm{\varphi_1(t_0) - \varphi_2(t_0)} e^{L\abs{t-t_0}}.
\]

****** TODO Proof
Volterra y Gronwall.
****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       d4e16b44-c476-4276-bccc-0a2d15a20555
:DRILL_LAST_INTERVAL: 10.8859
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:50]
:END:
Enunciar la *desigualdad fundamental*, que controla la diferencia entre
dos soluciones a una ecuaciÃ³n diferencial. Â¿QuÃ© condiciÃ³n necesita?

******* Answer
Para $\varphi_i \colon I_i \to \mathbb{R}^{d}$ soluciones de $x' = f(t,x)$ tales que

\[
\forall t \in I_1\cap I_2 \colon\qquad
\norm{f(t,\varphi_1(t)) - f(t,\varphi_2(t))} \leq
L\norm{\varphi_1(t) - \varphi_2(t)},
\]

se tiene 

\[
\forall t \in I_1\cap I_2 \colon\qquad
\norm{\varphi_1(t) -\varphi_2(t)} \leq
\norm{\varphi_1(t_0) - \varphi_2(t_0)} e^{L\abs{t-t_0}}.
\]

para $t_0 \in I_1 \cap I_2$ se tiene.

***** ProposiciÃ³n 2. Acotar una soluciÃ³n dentro de un entorno tubular
Para $(t_0,x_0) \in D$, sean $\alpha(t_0,x_0) < a < t_0 < b < \omega(t_0,x_0)$ y sea
$T_{\rho}([a,b],\varphi) \subset D$.  Entonces existe $\delta > 0$ tal que
$\forall (t_1,x_1) \in [t_0-\delta,t_0+\delta] \times \overline{B}(x_0,\delta)$, la soluciÃ³n desde
ellos estÃ¡ definida en $[a,b]$ y ademÃ¡s

\[
(t,X(t;t_1,x_1)) \in T_{\rho},\quad\forall t\in [a,b].
\]

****** TODO Proof
# Usando la proposiciÃ³n 1 y la desigualdad fundamental.

***** Teorema 1. Continuidad de la soluciÃ³n general
El conjunto

\[
\Omega = \left\{ (t,t_0,x_0) \mid (t_0,x_0) \in D, \alpha(t_0,x_0) < t < \omega(t_0,x_0) \right\}
\]

es abierto y la soluciÃ³n general $X \colon \Omega \to \mathbb{R}^d$ es continua.

****** Card: conjunto                                                                                      :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       fb86730f-4593-425c-b828-234da37f0a0f
:DRILL_LAST_INTERVAL: 4.4024
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:07]
:END:
Describe el conjunto en el que estÃ¡ definida la soluciÃ³n general, Â¿quÃ©
propiedad topolÃ³gica conocemos de Ã©l?

******* Answer
El conjunto

\[
\Omega = \left\{ (t,t_0,x_0) \mid (t_0,x_0) \in D, \alpha(t_0,x_0) < t < \omega(t_0,x_0) \right\}
\]

es abierto. Sobre este conjunto la soluciÃ³n general es continua.

****** TODO Proof

***** Corolario 1. Convergencia uniforme en compactos respecto condiciones iniciales
Para $(t_0,x_0) \in D$ consideramos la soluciÃ³n maximal $\varphi \colon (\alpha,\omega)\to \mathbb{R}^d$
y sucesiones $t_{0n}\to t_0$ y $x_{0n}\to x_0$. Si $\varphi_n$ es la soluciÃ³n con condiciones
iniciales $(t_{0n},x_{0n})$, hay convergencia uniforme en compactos $\varphi_n \overset{cuc}\longrightarrow \varphi$
en el intervalo $(\alpha,\omega)$.

***** Algoritmo con el corolario 1.
Si tenemos una sucesiÃ³n de problemas $x' = f(t,x)$ con valores
iniciales $x(t_{0n}) = x_{0n}$, con soluciones $\varphi_n \colon (\alpha_n,\omega_n) \to \mathbb{R}^d$;
podemos calcular

$\varphi_n(\tau_n) \to \varphi(\lim \tau_n)$

donde $\varphi$ es al problema inicial con $x(\lim t_{0n}) = \lim x_{0n}$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       dcb13d3c-67e0-4358-8a27-cab413ae9e95
:DRILL_LAST_INTERVAL: 4.0802
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:59]
:END:
Calcular $\lim_{n \to \infty} \varphi_n(\tau_n)$ donde $\tau_n = (1 + 1/n)^n$ y donde $\varphi_n$ es la
soluciÃ³n de

\[\left.\begin{aligned}
x' &= (x+t)x^2 \\
x(1 + 1/n) &= 1/n^2
\end{aligned}\right\}\]

******* Answer
Podemos usar convergencia uniforme en compactos de las soluciones respecto
de condiciones iniciales para buscar la soluciÃ³n $\varphi$ al sistema lÃ­mite

\[\left.\begin{aligned}
x' &= (x+t)x^2 \\
x(1) &= 0
\end{aligned}\right\}\]

que en este caso es trivial, y aplicarla en $\varphi(e) = 0$, el lÃ­mite
del los $\tau_n$.

**** 5.4. Dependencia continua respecto de parÃ¡metros
Consideramos dominio abierto $D \subset \mathbb{R} \times \mathbb{R}^d$ y campo $f$ continuo y
localmente lipschitziano respecto $x$ y un parÃ¡metro $\lambda$.

***** Teorema 2. Continuidad de la soluciÃ³n general respecto de parÃ¡metros
Si llamamos $X(t;t_0,x_0,\lambda_0)$ a la soluciÃ³n general del problema con
parÃ¡metros dado por $x' = f(t,x,\lambda_0)$ y por $x(t_0) = x_0$; definida
en el conjunto

\[
\Omega = \left\{ (t;t_0,x_0,\lambda_0) \mid 
((t_0,x_0),\lambda) \in D \times P,\ \alpha(t_0,x_0,\lambda_0) < t < \omega(t_0,x_0,\lambda_0) \right\}.
\]

Entonces el conjunto $\Omega$ es abierto y $X$ es *continua*. Hay ademÃ¡s
convergencia uniforme en compactos $X(-;t_{0n},x_{0n},\lambda_{0n}) \overset{cuc}\longrightarrow X(-;t_0,x_0,\lambda_0)$
en el intervalo $(\alpha(t_0,x_0,\lambda_0),\omega(t_0,x_0,\lambda_0))$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       f5867c8f-f14d-4f5d-9ca2-32c213da4ab3
:DRILL_LAST_INTERVAL: 4.9074
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:23]
:END:
Enunciar la continuidad de la soluciÃ³n general respecto de parÃ¡metros.

******* Answer
$X$ es continua sobre $\Omega$, que es abierto. AdemÃ¡s,

$X(-;t_{0n},x_{0n},\lambda_{0n}) \overset{cuc}\longrightarrow X(-;t_0,x_0,\lambda_0)$

***** TODO Ejemplo. ConservaciÃ³n de signo
**** 5.5. Diferenciabilidad de la soluciÃ³n general
***** HipÃ³tesis de regularidad
Consideramos

\[ \pdv{f}{x}\colon D \to {\cal M}_d \]

continua y bien definida. Esto nos da $f$ localmente lipschitziana
respecto de $x$.

****** Card: hipÃ³tesis de regularidad                                                                      :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       3c751cbd-8623-4e89-91ed-b6ef58b20aa0
:DRILL_LAST_INTERVAL: 5.0362
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:39]
:END:
Â¿QuÃ© es la hipÃ³tesis de regularidad?Â¿QuÃ© nos da?

******* Answer
Es tener $f \colon D \to \mathbb{R}^d$ continua con

\[ \pdv{f}{x}\colon D \to {\cal M}_d \]

continua y bien definida. Esto nos da $f$ localmente lipschitziana
respecto de $x$.

***** Teorema 3. Derivabilidad respecto condiciones iniciales
Con la hipÃ³tesis de regularidad.

 1. Se tiene $X \in {\cal C}^1(\Omega)$.
 2. Existen y son continuas

    \[\pdv[2]{X}{t}{t_0},\quad
    \pdv[2]{X}{t}{x_0},\quad
    \pdv[2]{X}{t_0}{t},\quad
    \pdv[2]{X}{x_0}{t}.
    \]

Debemos notar que las soluciones quedan determinadas por

\[
\pdv{X}{t} (t;t_0,x_0) = f(t,X(t;t_0,x_0)),\quad X(t_0,t_0,x_0) = x_0,
\]

y desde ellas usando [[https://es.wikipedia.org/wiki/Teorema_de_Clairaut][Schwarz-Clairaut]] podemos obtener una
*ecuaciÃ³n variacional* sobre la derivada parcial que queramos
calcular.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       7f78d326-04fd-4995-8167-ba24ad1b683b
:DRILL_LAST_INTERVAL: 3.7189
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:21]
:END:
CuÃ¡l es la hipÃ³tesis de regularidad sobre $f \colon D \to \mathbb{R}^d$ que debemos comprobar
antes de aplicar derivabilidad respecto de condiciones iniciales.

******* Answer
Pedimos $f \colon D \to \mathbb{R}^d$ continua con

\[ \pdv{f}{x}\colon D \to {\cal M}_d \]

continua y bien definida.

***** Niveles de diferenciabilidad
Para $f \colon D \to \mathbb{R}^m$ con $D \subset \mathbb{R}^m$ abierto y $a \in D$, consideramos
distintos niveles de diferenciabilidad, cada uno implicando
los anteriores.

 1. Existen las *derivadas parciales*,

    \[
    \pdv{f}{x_i} (a) = \lim_{h \to 0} \frac{f(a+he_i) - f(a)}{h} \in \mathbb{R}^m
    \]

 2. Existen todas las *derivadas direccionales*

    \[
    D_vf(a) = \lim_{h \to 0} \frac{f(a+hv) - f(a)}{h} \in \mathbb{R}^m
    \]

 3. *Diferenciabilidad de Gateaux*: $v \mapsto D_vf(a)$ es lineal y
    continua. $D_vf(a) = Tv$ para alguna $T \in {\cal M}_{m\times n}$.

 4. *Diferenciabilidad de Frechet*: tenemos

    \[
    \lim_{v \to 0} \frac{\norm{f(a+v) - f(a) - D_vf(a)}}{\norm{v}} = 0, \qquad \forall v \in \mathbb{R}^n
    \]

Si existen todas las derivadas parciales de $f$ y son continuas,
entonces $f$ es diferenciable en sentido de Frechet.

****** TODO Contraejemplo: Gateaux pero no Frechet
****** Card: Gateaux                                                                                       :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       7594c6e6-90c4-44ce-91e7-8cc0892f465c
:DRILL_LAST_INTERVAL: 4.0024
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:54]
:END:
Define diferenciabilidad de Gateaux.

******* Answer
La aplicaciÃ³n de las derivadas direccionales, $v \mapsto D_vf(a)$ es
lineal y continua. $D_vf(a) = Tv$ para alguna $T \in {\cal M}_{m\times n}$.

****** Card: Frechet                                                                                       :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       a7a6a587-2c1b-40d5-899c-acf77bd561a6
:DRILL_LAST_INTERVAL: 4.1818
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:33]
:END:
Define diferenciabilidad de Frechet.

******* Answer
Cuando tenemos

\[
\lim_{v \to 0} \frac{\norm{f(a+v) - f(a) - D_vf(a)}}{\norm{v}} = 0, \qquad \forall v \in \mathbb{R}^n
\]
***** Regla de Barrow multivariable
Para $G \in {\cal C}^{1}(A,\mathbb{R}^m)$ con $[p,q] \subset A \subset \mathbb{R}^n$,

\[
G(p) - G(q) = \left( \int_0^1 G'(sp + (1-s)q)\,ds \right)(p-q).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       648d37c6-b4bd-41ab-a818-fd358b61a789
:DRILL_LAST_INTERVAL: 4.6646
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:41]
:END:
Enunciar la regla de Barrow multivariable para $G \in {\cal C}^{1}(A,\mathbb{R}^m)$
con $[p,q] \subset A \subset \mathbb{R}^n$. Esto es,

\[
G(p) - G(q) = \dots
\]

******* Answer

\[
G(p) - G(q) = \left( \int_0^1 G'(sp + (1-s)q)\,ds \right)(p-q).
\]

***** Teorema 4. Derivabilidad respecto condiciones iniciales y parÃ¡metros
Sea $f \colon D \times P \to \mathbb{R}^d$ dependiente de un parÃ¡metro y continua con la hipÃ³tesis
de regularidad, es decir,

\[
\pdv{f}{x}\colon D \times P \to {\cal M}_d,\qquad
\pdv{f}{\lambda}\colon D \times P \to {\cal M}_{d\times k},
\]

son continuas y bien definidas.

 1. Se tiene $X \in {\cal C}^1(\Omega)$.
 2. Existen y son continuas

    \[\pdv[2]{X}{t}{t_0},\quad
    \pdv[2]{X}{t}{x_0},\quad
    \pdv[2]{X}{t_0}{t},\quad
    \pdv[2]{X}{x_0}{t},\quad
    \pdv[2]{X}{t}{\lambda},\quad
    \pdv[2]{X}{\lambda}{t}.
    \]
 
***** TODO Ejemplo de cÃ¡lculo y diagonalizaciÃ³n
*** Tema 6. Estabilidad en el sentido de Lyapunov
Fijamos $D \subset \mathbb{R} \times \mathbb{R}^d$ dominio abierto y $f$ campo continuo y localmente
lipschitziano respecto de $x$.

**** 6.0. Estabilidad
***** Estable
Una soluciÃ³n $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$ es *estable* si para cada $\varepsilon > 0$
existe un $\delta > 0$ tal que si $\norm{x_0 - \varphi(t_0)} < \delta$, entonces

 - $\omega(t_0,x_0) = \infty$, la soluciÃ³n llega a infinito,

 - para $t \geq t_0$, se tiene $\norm{X(t;t_0,x_0) - \varphi(t))} < \varepsilon$, estÃ¡ siempre
   suficientemente cerca de la original.

Dicho de otra forma, para todo entorno tubular de tamaÃ±o $\varepsilon$, existe
un $\delta$ tal que si la partÃ­cula empieza a menos distancia de $\delta$ de la
original, entonces la soluciÃ³n se mantiene en ese entorno tubular.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       fe0c06f5-3cd4-4b6b-b0f1-6d53475a34eb
:DRILL_LAST_INTERVAL: 11.3386
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:46]
:END:
DefiniciÃ³n de soluciÃ³n estable $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$.

******* Answer
Si $\norm{x_0 - \varphi(t_0)} < \delta$,

 * $\omega(t_0,x_0) = \infty$,
 * $\norm{X(t;t_0,x_0) - \varphi(t))} < \varepsilon$.

******* Detallado
Para cada $\varepsilon > 0$ existe un $\delta > 0$ tal que si $\norm{x_0 - \varphi(t_0)} < \delta$,
entonces

 - $\omega(t_0,x_0) = \infty$, la soluciÃ³n llega a infinito,

 - para $t \geq t_0$, se tiene $\norm{X(t;t_0,x_0) - \varphi(t))} < \varepsilon$, la soluciÃ³n
   estÃ¡ cerca de la original.

***** Inestable
Una soluciÃ³n $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$ es *inestable* si existe un $\varepsilon_0 > 0$
tal que podemos crear $x_n \to \varphi(t_0)$ cumpliendo que

 - o bien $\omega(t_0,x_n) < \infty$, la soluciÃ³n se corta,

 - o bien $\exists t_n \in [t_0,+\infty)\colon \norm{\varphi(t_n) - X(t_n;t_0,x_n)} \geq \varepsilon_0$ queda por
   encima del umbral en algÃºn punto.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       660bb6be-01eb-44a3-9265-8621a73aca16
:DRILL_LAST_INTERVAL: 10.0376
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:50]
:END:
DefiniciÃ³n de soluciÃ³n inestable $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$.

******* Answer
Existe algÃºn umbral $\varepsilon_0 > 0$ tal que podemos crear $x_n \to \varphi(t_0)$ que

 - $\omega(t_0,x_n) < \infty$, la soluciÃ³n se corta, o

 - existe algÃºn punto $t_n \in [t_0,+\infty)$ para el que $\norm{\varphi(t_n) - X(t_n;t_0,x_n)} \geq \varepsilon_0$.

***** Atractor
Una soluciÃ³n $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$ es un *atractor* (local) si existe
$\mu > 0$ tal que $\norm{x_0-\varphi(t_0)} < \mu$ implica

 - que $\omega(t_0,x_0) = \infty$,

 - $\lim_{t \to \infty} X(t;t_0,x_0) - \varphi(t) = 0$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       c865d6ca-f89e-477d-92d6-0e46cbe041c6
:DRILL_LAST_INTERVAL: 9.2669
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:48]
:END:
DefiniciÃ³n de soluciÃ³n atractor $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$.

******* Answer
Existe $\mu > 0$ tal que $\norm{x_0-\varphi(t_0)} < \mu$ implica

 - que $\omega(t_0,x_0) = \infty$, continÃºa hasta infinito;

 - $\lim_{t \to \infty} X(t;t_0,x_0) - \varphi(t) = 0$, se acaba acercando a la
   soluciÃ³n.

***** AsintÃ³ticamente estable
Una soluciÃ³n es *asintÃ³ticamente estable* si es un atractor estable.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       0953e8b1-7f90-4df8-b9b6-c060bd52acf9
:DRILL_LAST_INTERVAL: 9.3206
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:44]
:END:
DefiniciÃ³n de asintÃ³ticamente estable.

******* Answer
Atractor estable.

*Atractor*
Para $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$ hay algÃºn $\mu$ tal que $\norm{x_0-\varphi(t_0)} < \mu$ nos da

 - que $\omega(t_0,x_0) = \infty$, todas las soluciones cerca de el punto inicial
   estÃ¡n definidas;

 - $\lim_{t \to \infty} X(t;t_0,x_0) - \varphi(t) = 0$, todas las soluciones tienden en
   el futuro hacia la soluciÃ³n.

*Estable*
Para $\varphi \colon (\alpha,+\infty) \to \mathbb{R}^d$, y para cada $\varepsilon > 0$ existe un $\delta > 0$ tal que
si $\norm{x_0 - \varphi(t_0)} < \delta$, entonces

 - $\omega(t_0,x_0) = \infty$,

 - para $t \geq t_0$, se tiene $\norm{X(t;t_0,x_0) - \varphi(t))} < \varepsilon$.

**** 6.1. Estabilidad de ecuaciones lineales
***** ProposiciÃ³n 1. Estabilidad en ecuaciones lineales
:PROPERTIES:
:ID:       9a67a179-4e81-44c6-853e-8f54dea11422
:END:
Sea la ecuaciÃ³n lineal $x' = A(t)x + b(t)$ para $A,b$ continuas.
Equivalen,

 1. todas las soluciones de la general son estables;
 2. la general tiene una soluciÃ³n estable;
 3. las soluciones de la homogÃ©nea estÃ¡n acotadas en el futuro;
 4. la homogÃ©nea tiene una matriz fundamental acotada en el futuro.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       33092476-ce17-4d97-8c10-71d893194990
:DRILL_LAST_INTERVAL: 10.0192
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:51]
:END:
Estudiamos *estabilidad* en ecuaciones lineales. Para
$A,b$ continuas en la ecuaciÃ³n $x' = A(t)x + b(t)$, Â¿quÃ© 4
equivalencias hay?

******* Answer

 1. todas las soluciones de la general son estables;
 2. la general tiene una soluciÃ³n estable;
 3. las soluciones de la homogÃ©nea estÃ¡n acotadas en el futuro;
 4. la homogÃ©nea tiene una matriz fundamental acotada en el futuro.

****** Proof 1 to 2
Si todas las soluciones de la general son estables, sabiendo que
alguna existe, serÃ¡ estable.

****** Proof 2 to 3
Sea $\varphi$ soluciÃ³n de la general, $\psi$ soluciÃ³n de la homogÃ©nea. Tomamos
$\varepsilon = 1$ en la estabilidad y fijamos un $\lambda \norm{\psi(t_0)} < \delta$.  Tenemos que
$\varphi + \lambda \psi$ es soluciÃ³n de la general y que tiene el valor inicial dentro
de los valores de estabilidad. Desde aquÃ­ $\norm{\psi(t)} < 1/\lambda$ acotada.

****** Proof 3 to 4
Tomamos un sistema fundamental de soluciones. Todas las componentes de
la matriz fundamental estarÃ¡n acotadas en el futuro, luego la matriz
lo estarÃ¡.

****** Proof 4 to 1
SerÃ¡ $\vertiii{\Psi} < M$ una matriz fundamental, y $\vertiii{\Phi(t)} \leq \vertiii{\Psi(t)}\vertiii{\Psi(t_0)^{-1}}$
la matriz fundamental en $t_0$. Si tomamos

\[
M \vertiii{\Psi(t_0)^{-1}} \delta < \varepsilon
\]

tenemos que una soluciÃ³n de la general debe ser
$X(t;t_0,x_0) = \varphi(t) + \Phi(t)(x_0-\varphi(t_0))$, y que por tanto,

\[
\norm{X(t;t_0,x_0) - \varphi(t)} = \norm{\Phi(t)(x_0-\varphi(t_0))} \leq \vertiii{\Psi(t)}\vertiii{\Psi(t_0)^{-1}}\norm{x_0-\varphi(t_0)} \leq \varepsilon.
\]

***** ProposiciÃ³n 2. Estabilidad asintÃ³tica en ecuaciones lineales
:PROPERTIES:
:ID:       6293d367-8f82-4677-b692-2484008b7464
:END:
Sea la ecuaciÃ³n lineal $x' = A(t)x + b(t)$ para $a,b$ continuas.
Equivalen,

 1. todas las soluciones de la general son asintÃ³ticamente estables;
 2. la general tiene una soluciÃ³n atractor;
 3. las soluciones de la homogÃ©nea convergen a $0$ cuando $t \to \infty$;
 4. la homogÃ©nea tiene una matriz fundamental que converge a $0$ cuando $t \to \infty$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       c22f1984-5237-424f-b646-1c1562d7b6d1
:DRILL_LAST_INTERVAL: 3.9808
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:23]
:END:
Estudiamos *estabilidad asintÃ³tica* en ecuaciones lineales. Para
$A,b$ continuas en la ecuaciÃ³n $x' = A(t)x + b(t)$, Â¿quÃ© 4
equivalencias hay?

******* Answer

 1. todas las soluciones de la general son asintÃ³ticamente estables;
 2. la general tiene una soluciÃ³n atractor;
 3. las soluciones de la homogÃ©nea convergen a $0$ cuando $t \to \infty$;
 4. la homogÃ©nea tiene una matriz fundamental que converge a $0$ en el futuro.

******* CompÃ¡rese con estabilidad, donde equivalen

 1. todas las soluciones de la general son estables;
 2. la general tiene una soluciÃ³n estable;
 3. las soluciones de la homogÃ©nea estÃ¡n acotadas en el futuro;
 4. la homogÃ©nea tiene una matriz fundamental acotada en el futuro.

****** Proof                                                                                               :extra:
**** 6.2. Estabilidad de ecuaciones lineales con coeficientes constantes
***** Bloques de Jordan
Para $A \in {\cal M}_d(\mathbb{R})$ tenemos

 * $\sigma(A) = \left\{ \lambda_1,\dots,\lambda_d \right\}$, el *espectro* de valores propios $\lambda_i \in \mathbb{C}$, cada uno con
   su multiplicidad;

 * $\sigma_0(A) = \left\{ \lambda \in \sigma(A) \mid \mathrm{Re}(\lambda) = 0 \right\}$, el *espectro imaginario*;

 * $\nu(\lambda)$, el *Ã­ndice* (tamaÃ±o) del mayor bloque de Jordan asociado a un valor
   propio; llamamos $\gamma(\lambda) = \nu(\lambda) - 1$ al nÃºmero de unos supradiagonales.

NÃ³tese que toda matriz puede diagonalizarse en forma normal de Jordan $A = PJP^{-1}$
y que $e^{At} = P e^{Jt} P^{-1}$. AdemÃ¡s, la exponencial se calcula por bloques, siendo de
la forma siguiente.

\[
e^{J_it} = \begin{pmatrix}
e^{\lambda_it} & te^{\lambda_it} & \dots & \dots & \frac{t^{k-1}}{(k-1)!}e^{\lambda_it} \\
0 & e^{\lambda_it} & te^{\lambda_it} & \dots & \dots \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & \dots & \dots & \dots & e^{\lambda_it} \\
\end{pmatrix}
\]

****** Card: Ã­ndice                                                                                        :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       02081431-5e2f-4fd1-8d3e-074080a59980
:DRILL_LAST_INTERVAL: 4.7525
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:31]
:END:
Define el Ã­ndice $\nu$ y $\gamma$ de un valor propio $\lambda$.

******* Answer
$\nu(\lambda)$, es el tamaÃ±o del mayor bloque de Jordan asociado a un valor propio;
llamamos $\gamma(\lambda) = \nu(\lambda) - 1$ al nÃºmero de unos supradiagonales.

****** Card: exponencial de un bloque de Jordan                                                            :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       330f0ad8-0d64-4d61-9911-30d343144ab9
:DRILL_LAST_INTERVAL: 3.5297
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:41]
:END:
Calcular la exponencial de un bloque de Jordan.

******* Answer

\[
e^{J_it} = \begin{pmatrix}
e^{\lambda_it} & te^{\lambda_it} & \dots & \dots & \frac{t^{k-1}}{(k-1)!}e^{\lambda_it} \\
0 & e^{\lambda_it} & te^{\lambda_it} & \dots & \dots \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & \dots & \dots & \dots & e^{\lambda_it} \\
\end{pmatrix}
\]
***** Teorema. Estabilidad para lineales con coeficientes constantes
Para el sistema $x' = Ax$, sea $\mu = \max\left\{ \mathrm{Re}(\lambda) \mid \lambda \in \sigma(A) \right\}$, entonces

 * si $\mu < 0$, todas las soluciones son /asintÃ³ticamente estables/;

 * si $\mu = 0$ y $\forall \lambda \in \sigma_0(A) \colon \nu(\lambda) = 1$, todas las soluciones son /estables/ 
   pero no /asintÃ³ticamente estables/;

 * si $\mu = 0$ y $\exists \lambda \in \sigma_0(A)\colon \nu(\lambda) > 1$, todas las soluciones son /inestables/;

 * si $\mu > 0$, todas las soluciones son /inestables/.

****** Proof
Podemos usar norma matricial de la suma (todas las normas son equivalentes)
sobre $e^{Jt}$ para ver si es una matriz fundamental de homogÃ©nea acotada en el futuro.

Por casos podremos aplicar [[id:9a67a179-4e81-44c6-853e-8f54dea11422][ProposiciÃ³n 1]] y [[id:6293d367-8f82-4677-b692-2484008b7464][ProposiciÃ³n 2]].

****** Card: teorema de estabilidad                                                                        :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       dc1978bc-4a68-4ae3-86ec-b84a8313cb22
:DRILL_LAST_INTERVAL: 4.0677
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:06]
:END:
Enuncia el Teorema de estabilidad para ecuaciones lineales con
coeficientes constantes aplicado sobre $x' = Ax$.

******* Answer
Sea $\mu = \max\left\{ \mathrm{Re}(\lambda) \mid \lambda \in \sigma(A) \right\}$, entonces

 * si $\mu < 0$, todas las soluciones son /asintÃ³ticamente estables/;

 * si $\mu = 0$ y $\forall \lambda \in \sigma_0(A) \colon \nu(\lambda) = 1$, todas las soluciones son /estables/ 
   pero no /asintÃ³ticamente estables/;

 * si $\mu = 0$ y $\exists \lambda \in \sigma_0(A)\colon \nu(\lambda) > 1$, todas las soluciones son /inestables/;

 * si $\mu > 0$, todas las soluciones son /inestables/.
****** Card: teorema de estabilidad en el caso de parte real nula                                          :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       f5d44679-e729-4478-a90d-be61e7b713cd
:DRILL_LAST_INTERVAL: 7.0929
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:52]
:END:
Caso particular. Â¿QuÃ© dice el Teorema de estabilidad para ecuaciones lineales
cuando $\mu = \max\left\{ \mathrm{Re}(\lambda) \mid \lambda \in \sigma(A) \right\} = 0$?

******* Answer
Lo que diferencia es el Ã­ndice de los valores propios del espectro
imaginario.

 * si $\mu = 0$ y $\forall \lambda \in \sigma_0(A) \colon \nu(\lambda) = 1$, todas las soluciones son /estables/ 
   pero no /asintÃ³ticamente estables/;

 * si $\mu = 0$ y $\exists \lambda \in \sigma_0(A)\colon \nu(\lambda) > 1$, todas las soluciones son /inestables/;
***** Corolario. Estabilidad en el caso bidimensional en tÃ©rminos de traza y determinante
Para el sistema no nulo $x' = Ax$, bidimensional $A \in {\cal M}_2(\mathbb{R})$,

 1. si $\mathrm{det}(A) < 0$, es /inestable/;

 2. si $\mathrm{det}(A)>0$,

    1. si $\mathrm{traza}(A) < 0$, es /asintÃ³ticamente estables/;

    2. si $\mathrm{traza}(A) > 0$, es /inestable/;

    3. si $\mathrm{traza}(A) = 0$, es /estable/ no /asintÃ³ticamente estable/;

 3. si $\mathrm{det}(A) = 0$,

    1. si $\mathrm{traza}(A) > 0$, es /inestable/;

    2. si $\mathrm{traza}(A)<0$, es /estable/ no /asintÃ³ticamente estable/;

    3. si $\mathrm{traza}(A) = 0$, es /inestable/.

Podemos resumir en una tabla.

   - /Valores propios reales/.
     
     |----+----+-------+-----+------------------------------------|
     | Î»â | Î»â | Traza | Det | ClasificaciÃ³n                      |
     |----+----+-------+-----+------------------------------------|
     | -  | -  | -     | +   | AsintÃ³ticamente estable            |
     | +  | -  | ?     | -   | Inestable                          |
     | +  | +  | +     | +   | Inestable                          |
     | 0  | -  | -     | 0   | Estable no asintÃ³ticamente estable |
     | +  | 0  | +     | 0   | Inestable                          |
     | 0  | 0  | 0     | 0   | Inestable                          |
     |----+----+-------+-----+------------------------------------|

   - /Valores propios complejos/, deberÃ¡n ser conjugados.

     |--------+-------+-----+------------------------------------|
     | Re(Î»â) | Traza | Det | ClasificaciÃ³n                      |
     |--------+-------+-----+------------------------------------|
     | -      | -     | +   | AsintÃ³ticamente estable            |
     | +      | +     | +   | Inestable                          |
     | 0      | 0     | +   | Estable no asintÃ³ticamente estable |
     |--------+-------+-----+------------------------------------|

****** Card: tabla de valores reales                                                                       :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       5139a49b-be48-4693-b4df-ab0a77cab754
:DRILL_LAST_INTERVAL: 4.8548
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:40]
:END:
Enuncia la tabla de estabilidad con valores propios *reales* para el sistema
bidimiensional lineal con coeficientes constantes $x' = Ax$, para $A \in {\cal M}_2(\mathbb{R})$.

******* Answer

|----+----+-------+-----+------------------------------------|
| Î»â | Î»â | Traza | Det | ClasificaciÃ³n                      |
|----+----+-------+-----+------------------------------------|
| -  | -  | -     | +   | AsintÃ³ticamente estable            |
| +  | -  | ?     | -   | Inestable                          |
| +  | +  | +     | +   | Inestable                          |
| 0  | -  | -     | 0   | Estable no asintÃ³ticamente estable |
| +  | 0  | +     | 0   | Inestable                          |
| 0  | 0  | 0     | 0   | Inestable                          |
|----+----+-------+-----+------------------------------------|

****** Card: tabla de valores complejos                                                                    :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       915f92a9-00ac-46c6-a20f-c6b073a4ad80
:DRILL_LAST_INTERVAL: 3.6296
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:04]
:END:
Enuncia la tabla de estabilidad con valores propios *complejos* para el sistema
bidimiensional lineal con coeficientes constantes $x' = Ax$, para $A \in {\cal M}_2(\mathbb{R})$.

******* Answer

|--------+-------+-----+------------------------------------|
| Re(Î»â) | Traza | Det | ClasificaciÃ³n                      |
|--------+-------+-----+------------------------------------|
| -      | -     | +   | AsintÃ³ticamente estable            |
| +      | +     | +   | Inestable                          |
| 0      | 0     | +   | Estable no asintÃ³ticamente estable |
|--------+-------+-----+------------------------------------|

***** Diagrama de PoincarÃ©
Se puede hacer representaciÃ³n de diagramas de fases planos a papel.

**** 6.3. Primer mÃ©todo de Lyapunov
El primer mÃ©todo de Lyapunov estudia la estabilidad de las soluciones
constantes de ecuaciones escalares autÃ³nomas.

***** Test de la primera aproximaciÃ³n
Sea un campo $f \in {\cal C}^1(D)$ en un abierto $D \subset \mathbb{R}^d$, con $p \in D$ tal que $f(p)=0$;
sea

\[
\mu = \max\left\{ \mathrm{Re}(\lambda) \mid \lambda \in \sigma(f'(p)) \right\}
\]

entonces

 1. si $\mu <0$, la soluciÃ³n constantemente $p$ es /asintÃ³ticamente estable/;

 2. si $\mu > 0$, la soluciÃ³n constantemente $p$ es /inestable/;

 3. si $\mu = 0$, no podemos afirmar nada en general.

****** Proof                                                                                               :extra:
Se puede deducir de los teoremas de estabilidad.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       049ee33e-2bb5-4274-ba8c-66a684a2179d
:DRILL_LAST_INTERVAL: 4.2029
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:38]
:END:
Aplicar el primer mÃ©todo de Lyapunov (test de la primera aproximaciÃ³n)
a un punto de equilibrio $f(p) = 0$ con un campo $f \in {\cal C}^1(D)$.

******* Answer
Siendo $\mu = \max\left\{ \mathrm{Re}(\lambda) \mid \lambda \in \sigma(f'(p)) \right\}$ entonces la soluciÃ³n constantemente $p$ es

 1. si $\mu <0$, /asintÃ³ticamente estable/;

 2. si $\mu > 0$, /inestable/;

 3. si $\mu = 0$, no podemos afirmar nada en general.
***** Corolario. Primera aproximaciÃ³n en el caso bidimensional
Sea un campo $f \in {\cal C}^1(\Omega)$, con $f(p)=0$, entonces la soluciÃ³n constantemente $p$ es

 1. si $\mathrm{det}(f'(p)) > 0$ y $\mathrm{traza}(f'(p)) < 0$, /asintÃ³ticamente estable/;
 2. si $\mathrm{det}(f'(p)) < 0$ o $\mathrm{traza}(f'(p))>0$, /inestable/.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       f5ad6236-bb1b-462a-8140-7eb72f774ff8
:DRILL_LAST_INTERVAL: 4.4073
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:54]
:END:
Aplicar el primer mÃ©todo de Lyapunov (test de la primera aproximaciÃ³n)
a un punto de equilibrio $f(p) = 0$ con un campo $f \in {\cal C}^1(D)$.
*EN EL CASO BIDIMENSIONAL*, $d=2$, de un sistema plano.

En tÃ©rminos de traza y determinante.

******* Answer

 1. si $\mathrm{det}(f'(p)) > 0$ y $\mathrm{traza}(f'(p)) < 0$, es /asintÃ³ticamente estable/;

 2. si $\mathrm{det}(f'(p)) < 0$ o $\mathrm{traza}(f'(p))>0$, es /inestable/.

***** Comentario sobre diagramas de PoincarÃ©
La estructura del diagrama de fases se mantiene cuando se realiza una
perturbaciÃ³n pequeÃ±a en el entorno de un punto de equilibrio.  Excepto
en un caso, cuando hay $\mu = 0$ y un centro en $p$, la perturbaciÃ³n
puede romper las Ã³rbitas.

Hay casos de Ã³rbitas interesantes como

 - *Ã³rbitas heteroclinas*, conectando dos puntos de equilibrio;
 - *Ã³rbitas homoclinas*, conectando un punto de equilibrio consigo mismo;
 - *ciclos lÃ­mite*, tal que otras Ã³rbitas se enrollan en torno a
   ellos.

***** TODO Ejemplo del pÃ©ndulo
**** 6.4. Segundo mÃ©todo de Lyapunov
Tomamos $f \colon D \to \mathbb{R}^{d}$ campo localmente lipschitziano en un abierto.
Suponemos un punto de equilibrio $f(p)=0$.

***** Teorema 1. Estabilidad de Lyapunov
:PROPERTIES:
:ID:       ee768b41-dd65-4f94-a6bb-a8804729ec36
:END:
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ¡ximo local;

entonces $p$ es punto /estable/ de la autÃ³noma $x' = f(x)$.

****** Proof
Nos quedamos en una bola suficientemente pequeÃ±a donde se cumplan
todas las propiedades con radio $\varepsilon$ o menor.  Por Weiestrass, $V$ tiene
mÃ­nimo $m$ en el borde de este disco. Tomamos un disco menor de radio
$\delta$ donde $V$ sea siempre menor que $m$.

Tomamos $x_0$ en el disco y una soluciÃ³n $\varphi$. Afirmamos $\varphi(t) \in B(p,\varepsilon)$
porque, si no, por Primer Instante tendrÃ­amos $T > 0$ con $\varphi(T)$ en
el borde del disco por primera vez. Entonces $y(t) = V(\varphi(t))$ serÃ­a
derivable y decreciente, pero entonces llegamos a contradicciÃ³n

\[
m \leq V(\varphi(T)) = y(T) \leq y(0) = V(x_0) < m.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       ad47708f-d84f-471a-99b2-3090ab5d71a7
:DRILL_LAST_INTERVAL: 4.1708
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:57]
:END:
Enuncia el Teorema de estabilidad de Lyapunov para la autÃ³noma $x' = f(x)$ con
$f \in {\cal C}^1(D)$ localmente lipschitziana y con $f(p)=0$.

******* Answer
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ¡ximo local;

entonces $p$ es punto estable.

***** Corolario de estabilidad no asintÃ³tica de Lyapunov
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f = 0$ en $D$;

entonces $p$ es un punto /estable no asintÃ³ticamente estable/ de
la autÃ³noma $x' = f(x)$.

****** Proof
Sabemos que es estable.  Si fuera asintÃ³ticamente estable, habrÃ­a
un $\mu$ tal que en un entorno las soluciones tenderÃ­an a $p$.  Pero
ahora $y$ es derivable y constante y entonces por ser asintÃ³ticamente
estable se llegarÃ­a a contradicciÃ³n,

\[
V(p) < V(x_0) = y(0) = y(t) = \lim_{t \to \infty} V(X(t,x_0)) = V(p).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       a7ac04a2-3db7-44d1-87d6-0901935e0037
:DRILL_LAST_INTERVAL: 3.998
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:55]
:END:
Si para un punto de equilibrio de un campo localmente lipschitziano en un
abierto tenemos que existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f = 0$ en $D$;

Â¿quÃ© sabemos?

******* Answer
$p$ es un punto /estable no asintÃ³ticamente estable/ de
la autÃ³noma $x' = f(x)$.
***** Lema hacia estabilidad asintÃ³tica de Lyapunov
:PROPERTIES:
:ID:       93519983-9322-405b-94ef-a471d42b6537
:END:
Si $\varphi \colon (\alpha,+\infty)\to\mathbb{R}^d$ soluciÃ³n y

 1. queda en una bola $\varphi(t) \in \overline{B}(p,R)$, para $t \geq t_0$;
 2. existe $V \in {\cal C}(\overline{B}(p,R))$ con mÃ­nimo estricto en $p$;
 3. $\lim_{t \to \infty} V(\varphi(t)) = V(p)$.

Entonces $\lim_{t \to \infty} \varphi(t) = p$.

****** Proof
Suponemos $\varepsilon < R$. Tomamos la corona $C(p,\varepsilon,R) = \left\{ x \in \mathbb{R}^d\mid \varepsilon \leq \norm{x-p} \leq R \right\}$,
donde $V(x) > V(p)$ y por Weiestrass tomamos mÃ­nimo $m > V(p)$. Por definiciÃ³n
de lÃ­mite $V(\varphi(t)) < m$ a partir de algÃºn $t \geq T$, luego $\varphi(t)$ no estarÃ¡ en
la corona. Esto funciona para $\varepsilon$ arbitrario, dÃ¡ndonos el lÃ­mite.

***** Teorema 2. Estabilidad asintÃ³tica de Lyapunov
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ¡ximo local *estricto*;

entonces $p$ es punto /asintÃ³ticamente estable/ de la autÃ³noma $x' = f(x)$.

****** Proof
Tomamos una bola suficientemente pequeÃ±a para aplicar el [[id:ee768b41-dd65-4f94-a6bb-a8804729ec36][Teorema de
Estabilidad]], que nos darÃ¡ un $\mu$ donde podemos tomar soluciones y tener
estabilidad. Tomamos una soluciÃ³n allÃ­ $\varphi(t) = X(t,x_0)$, veremos que
$\lim_{t \to \infty} V(\varphi(t)) = V(p)$ y aplicaremos el [[id:93519983-9322-405b-94ef-a471d42b6537][lema previo]].

Tenemos $y(t) = V(\varphi(t))$ derivable decreciente y acotada inferiormente
por $V(p)$, luego tiene lÃ­mite $L \geq V(p)$. Si no fuera exactamente $V(p)$,
tendrÃ­amos una bola alrededor de $p$ donde $V$ serÃ­a menos que $L$. De nuevo
tomamos la corona $C(p,\delta,R) = \left\{ x \in \mathbb{R}^d\mid \delta \leq \norm{x-p} \leq R \right\} \subset D$ y
como $\bV$ es continua tiene un mÃ¡ximo negativo $-M$ en la corona. La soluciÃ³n
se quedarÃ­a en la corona.

Pero entonces $y(t)-y(0) \leq -Mt$ integrando, y $y(t) \leq V(x_0)-Mt$, una
funciÃ³n acotada inferiormente queda por debajo de una recta de pendiente
negativa. Imposible.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       e007d816-29c8-48d2-99d0-cb2cd3d34f4e
:DRILL_LAST_INTERVAL: 3.6459
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:55]
:END:
Enuncia el Teorema de estabilidad *asintÃ³tica* de Lyapunov para la
autÃ³noma $x' = f(x)$ con $f \in {\cal C}^1(D)$ localmente lipschitziana y con $f(p)=0$.

NÃ³tese que es distinto del Teorema de estabilidad de Lyapunov (!).

******* Answer
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ¡ximo local *estricto*;

entonces $p$ es punto /asintÃ³ticamnete estable/ de la
autÃ³noma $x' = f(x)$.
***** Teorema 3. Inestabilidad de Lyapunov
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ­nimo local *estricto*;

entonces $p$ es un punto /inestable/ de la
autÃ³noma $x' = f(x)$.

****** Proof
Tomamos un disco donde se cumplan las condiciones.  Si $p$ es
estable habrÃ­a un $\mu$ tal que si $x_0 \in B(p,\mu)$, entonces
$\omega = +\infty$. Tomamos un $x_0 \in B(p,R) - \left\{ p \right\}$, y como $V(x_0)>V(p)$,
hay un $\delta > 0$ tal que $V(x)<r$ en el disco dado por ese $\delta$.
Definimos $y(t) = V(\varphi(t)) > 0$, creciente y llegamos a que
$X(t,x_0) \in B(p,\delta)$, luego estÃ¡ en la corona $C(p,\delta,R)$.

La corona es compacta y tomamos $M$ mÃ¡ximo de $V$ y $m$ mÃ­nimo
de $\bV$. Integrando llegamos a $M \geq r + mt$, donde estarÃ­amos
acotando una recta con pendiente positiva.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       852b6852-ecc7-4f88-8fa5-f84211725c7c
:DRILL_LAST_INTERVAL: 5.3094
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 10:24]
:END:
Enuncia el Teorema de *Inestabilidad de Lyapunov* para la
autÃ³noma $x' = f(x)$ con $f \in {\cal C}^1(D)$ localmente lipschitziana
y con $f(p)=0$.

******* Answer
Si existe $V \in {\cal C}^1(D)$ tal que

 1. $V$ alcanza en $p$ un mÃ­nimo local *estricto*,
 2. $\bV_f$ alcanza en $p$ un mÃ­nimo local *estricto*;

entonces $p$ es un punto /inestable/ de la
autÃ³noma $x' = f(x)$.

***** Teorema 4. Inestabilidad de Chetaev
Si existe $V \in {\cal C}^1(D)$ tal que

 1. existe $x_n \to p$ tal que $V(x_n) > V(p)$,

 2. $\bV_f$ alcanza en $p$ mÃ­nimo local *estricto*.

Entonces $p$ es un punto /inestable/ de la autÃ³noma $x' = f(x)$.

****** TODO Proof
****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       f10ab18a-f000-46ba-b74a-dfd64261caae
:DRILL_LAST_INTERVAL: 3.8485
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 1.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 19:37]
:END:
Enuncia el Teorema de *Inestabilidad de Chetaev* para la
autÃ³noma $x' = f(x)$ con $f \in {\cal C}^1(D)$ localmente lipschitziana
y con $f(p)=0$.

******* Answer
Si existe $V \in {\cal C}^1(D)$ tal que

 1. existe $x_n \to p$ tal que $V(x_n) > V(p)$,

 2. $\bV_f$ alcanza en $p$ mÃ­nimo local *estricto* .

Entonces $p$ es un punto /inestable/ de la autÃ³noma $x' = f(x)$.
***** ElecciÃ³n de funciones de Lyapunov

 - Buscar funciones con variables separadas. $V(x) = V(x_1) + \dots + V(x_n)$.
 - Usar funcionales energÃ­a en ecuaciones que provienen de EDOs
   segundo orden.
 - $V(x,y) = x^2+y^2$ para Lyapunov.
 - $V(x,y) = x^2-y^2$,  $V(x,y) = -x^2+y^2$, $V(x,y) = xy$ para Chetaev.
 - Forma cuadrÃ¡tica mÃ¡s general $V(x,y) = ax^2 + bxy + cy^2$.
 - Distancia a un punto, $V(x) = \norm{x-p}^2$.
***** Ejemplos
****** Ejemplo: ecuaciÃ³n de Duffing en el segundo mÃ©todo
****** Ejemplo de examen
*** Ejercicios
# La mayorÃ­a en papel

**** Matemapli
***** 19 de marzo                                                                                           :drill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:ID:       dc14004e-7a0a-422f-88f7-d97989f41784
:DRILL_LAST_INTERVAL: 7.6901
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:48]
:END:
La sucesiÃ³n $f_n \colon \mathbb{R} \to \mathbb{R}$ dada por

\[
f_n(t) = \frac{1}{n}\cos(n^2t)
\]

Â¿tiene alguna parcial que converja puntualmente hacia una funciÃ³n
continua $f \colon \mathbb{R} \to \mathbb{R}$?

****** Answer
SÃ­, trivialmente se calcula punto a punto como la constante $0$.

***** 20 de marzo                                                                                           :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       e18b2038-e1a0-4770-aa02-0e7554a4ffc4
:DRILL_LAST_INTERVAL: 4.8236
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:30]
:END:
Por quÃ© es cierto

\[
\lim_{n \to \infty} \int_0^1 \frac{x}{e^{x/n} +x^2}\,dx = 
\int_0^1 \frac{x}{1 +x^2}\,dx
\]

****** Answer
Hay convergencia puntual de la sucesiÃ³n y estÃ¡n uniformemente
acotadas, eso basta para asegurar que conmuta lÃ­mite e integral.

***** 16 de abril                                                                                           :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       8ce0d643-faa5-4e08-9fbd-58c157ca5429
:DRILL_LAST_INTERVAL: 4.2214
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 09:58]
:END:
CuÃ¡ntas Ã³rbitas tiene

\[
x' = (x^2 + 2)(x-1)(x-3)
\]

****** Answer
5. Nota que el polinomio tiene dos raÃ­ces.

***** 24 de abril                                                                                           :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       c2a28d7e-b184-4bb5-8cc9-0f7b646b8404
:DRILL_LAST_INTERVAL: 4.1906
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-13 Wed 11:28]
:END:
Por quÃ© sabemos que todas las soluciones de

\[
x'' + x^2x' + \mathrm{senh}(x) = 0
\]

Â¿estÃ¡n acotadas en el futuro?

****** Answer
Sabemos que $\mathrm{cosh}$ es coerciva y tenemos que $x^3/3$
es del mismo signo que $\mathrm{senh}$.

**** RelaciÃ³n 1 [6/6]
***** DONE Ejercicio 1
#+begin_statement
Calcula una soluciÃ³n maximal del PVI

\[\left.\begin{aligned}
x' &= x^2 \\
x(t_0) &= x_0
\end{aligned}\right\}
\]
#+end_statement

Calculamos usando [[*Condiciones para la unicidad local en variables separadas][variables separadas]].

 * En el caso $x_0 = 0$, tenemos la soluciÃ³n constante $0$ en $\mathbb{R}$.
   Es trivialmente maximal por estar definida en todo $\mathbb{R}$.

 * En el caso $x_0 \neq 0$, calculamos
   
   \[
   A(t) = \int_{t_0}^tds = t - t_0, \qquad
   G(t) = \int_{x_0}^x \frac{1}{z^2} \;dz = \frac{-1}{x} + \frac{1}{x_0},
   \]
   
   e igualando obtenemos

   \[
   x(t) = \frac{1}{\frac{1}{x_0} + t_0 - t}
   \]
  
   que sÃ³lo da problemas de definiciÃ³n en $t = 1/x_0 + t_0$. Cuando
   $x_0 > 0$, tenemos $t_0 \in (-\infty, 1/x_0 + t_0)$; y cuando $x_0 < 0$, tenemos
   que $t_0 \in (1/x_0 + t_0, +\infty)$. Elegimos el intervalo adecuado en cada
   caso. Tenemos maximalidad porque la funciÃ³n explota en
   \[
   \lim_{t \to (1/x_0 + t_0)} \varphi(t) = +\infty.
   \]
***** DONE Ejercicio 2
***** DONE Ejercicio 3
***** DONE Ejercicio 4
***** DONE Ejercicio 6
***** DONE Ejercicio 7
**** RelaciÃ³n 2 [4/5]
***** DONE Ejercicio 1
***** DONE Ejercicio 2
***** TODO Ejercicio 3
***** DONE Ejercicio 4
***** DONE Ejercicio 5
**** RelaciÃ³n 3 [4/6]
***** DONE Ejercicio 1
***** DONE Ejercicio 2
***** TODO Ejercicio 3
***** DONE Ejercicio 4
***** DONE Ejercicio 5
***** TODO Ejercicio 6
**** RelaciÃ³n 4 [4/5]
***** DONE Ejercicio 1
***** TODO Ejercicio 2
***** DONE Ejercicio 3
***** DONE Ejercicio 4
***** DONE Ejercicio 5
**** RelaciÃ³n 5 [2/3]
***** DONE Ejercicio 1 (crecimiento supralineal)
Si $\varphi$ es soluciÃ³n maximal y existe $f(t,x) \geq mx^p$ para $m,p,x_0 >0$;
entonces explota en tiempo finito (crecimiento supralineal).

Si $\varphi$ es soluciÃ³n maximal y existe $f(t,x) \leq -m\abs{x}^p$ con $m,p>0$
pero $x_0 < 0$; entonces explota en tiempo finito.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       da96001f-dc6f-4219-b3b4-c05bd10b7273
:DRILL_LAST_INTERVAL: 10.1514
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.75
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:46]
:END:
QuÃ© es y quÃ© nos da el crecimiento supralineal de una ecuaciÃ³n
diferencial.

******* Answer
Si $f(t,x) \geq mx^p$ para $m,p >0$; entonces la soluciÃ³n maximal
empezando en $x_0 > 0$ explota en tiempo finito.

***** DONE Ejercicio 2
***** TODO Ejercicio 3
**** RelaciÃ³n 6 [6/8]
***** DONE Ejercicio 1.a
***** TODO Ejercicio 1.b
***** TODO Ejercicio 2
***** DONE Ejercicio 3
***** DONE Ejercicio 4
***** DONE Ejercicio 5
***** DONE Ejercicio 6
***** DONE Ejercicio 7
** Modelos matemÃ¡ticos II
http://www.ugr.es/~jjmnieto/docencia.html

*** 0. Preliminares
**** Identidades trigonomÃ©tricas
Las identidades trigonomÃ©tricas

\[\left\{\begin{array}{l}
\sin(x)^2 + \cos(x)^2 = 1 \\
\sin(a+b) = \sin(a)\cos(b) + \cos(a)\sin(b) \\
\cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b)
\end{array}\right\}\]

son [[id:893398fe-a78e-45a4-b0dd-d4e36bc98d78][suficientes]] para caracterizar al seno y al coseno.

**** FÃ³rmula de DeMoivre
Sabemos que

\[
\left( \cos x + i \sin x \right)^n = \cos(nx) + i\sin(nx)
\]

puede ser expresada usando que

\[
e^{i\theta} = \cos \theta + i \sin \theta
\]

**** MÃ©todo de variaciÃ³n de constantes
**** MÃ©todo de coeficientes indeterminados
**** EcuaciÃ³n diferencial de Euler
**** Teorema de Fubini
**** Teorema de Tonelli
**** Norma en Lp                                                                                             :drill:
SCHEDULED: <2018-07-19 Thu>
:PROPERTIES:
:ID:       ceb95e11-f3e9-4928-9690-7f625454045f
:DRILL_LAST_INTERVAL: 42.6818
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:13]
:END:
Describe la norma en $L^p(S)$,

\[
\norm{f}_p
\]

***** Norma

\[
\norm{f}_p = \left(\int_S \abs{f^p} \,d\mu \right)^{1/p} < \infty
\]

**** Lp cuando 0 < p < 1                                                                                     :drill:
SCHEDULED: <2018-06-27 Wed>
:PROPERTIES:
:ID:       c6c30efb-81ce-4d04-8926-767c1685ca97
:DRILL_LAST_INTERVAL: 33.5873
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:46]
:END:
Â¿QuÃ© ocurre con $L^p$ cuando $0 < p < 1$? 

***** Respuesta
El operador

\[
N_p(f) = \int_S \abs{f}^p \,d\mu < \infty
\]

no es una norma, es sÃ³lo una quasinorma.

**** Teorema de Riesz
Sea $H$ un espacio de Hilbert. Para todo $f \in H^{\ast}$ existe un
Ãºnico $v \in H$ tal que $f(x) = \langle x,v \rangle$ para todo $x \in H$.
AdemÃ¡s, $\|f\| = \|v\|$.

**** Teorema de Lax-Milgram
Sea $H$ un espacio de Hilbert y sea $a \colon H \times H \to \mathbb{R}$ una
forma bilineal tal que
 
 1. es /continua/, existe $C$ tal que $| a(x,y) | \leq C\|x\|\|y\|$;
 2. y es /coerciva/, existe $\alpha$ tal que $|a(x,x)| \geq \alpha\|x\|^{2}$.

Entonces, para todo $f \in H^{\ast}$ existe un Ãºnico $y_0\in H$ tal que
$a(x,y_0) = f(x)$ para cualquier $x \in H$.

**** Tipos de ecuaciÃ³n diferencial ordinaria
Nombres de las ecuaciones

 * Son *ordinarias* aquellas que no involucran derivadas parciales.

 * Son *autÃ³nomas* aquellas que no dependen de $x$ directamente.

 * Son *lineales* si la derivada mayor depende linealmente de las demÃ¡s.
   $a_0(x) y(x) + \dots + a_n(x) y^{(n)}(x) + b(x) = 0$. Dentro de las lineales, las
   lineales con coeficientes constantes se resuelven fÃ¡cilmente.

Tipos explÃ­citos de ecuaciÃ³n

 * Lineal de primer orden, homogÃ©nea: $y' + P(x)y = 0$.

 * Lineal de primer orden, no homogÃ©nea: $y' + P(x)y = Q(x)$.

 * [[https://en.wikipedia.org/wiki/Cauchy%25E2%2580%2593Euler_equation][EcuaciÃ³n de Euler]]: $a_nx^ny^{(n)}(x) + \dots + a_0y(x) = 0$.

 * [[https://en.wikipedia.org/wiki/Bernoulli_differential_equation][EcuaciÃ³n de Bernoulli]]: $y' + P(x)y = Q(x)y^n$

**** Cambio de variables en varias variables

\[
\int_{\varphi(U)}f(x)\,dx = \int_{U} f(\varphi(x)) \abs{\mathrm{Jac}_{\varphi}(x)}\,dx
\]

**** IntegraciÃ³n en polares
:PROPERTIES:
:ID:       8b7c51f7-5499-4cd3-a1c6-cab635bd0015
:END:
Para integrar en una bola vectores $x \in \mathbb{R}^N$, podemos tomar
el cambio de variables

 * $r = |x|$

 * $\theta = x/|x|$

 * $dx = r^{N-1}\,drd\theta$

**** TODO Lineales
***** Forma autoadjunta de una ecuaciÃ³n lineal
Una ecuaciÃ³n lineal de orden 2 para $[x_0,x_1]$ es de la forma

\[
y''(x) + a(x)y'(x) + b(x)y(x) = c(x).
\]

Si multiplicamos por $P(x) = e^{\int_{x_0}^x a(s) \; ds}$, obtenemos la ecuaciÃ³n

\[
P(x)y''(x) + P(x)a(x)y'(x) + P(x)b(x)y(x) = P(x)c(x)
\]

que puede escribirse uniendo los dos primeros sumandos como

\[
\left( P(x)y'(x) \right)' + Q(x)y(x) = R(x)
\]

lo que llamamos su *forma autoadjunta*.

***** Forma variacional
El funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} \frac{P(y')^2}{2} - Q\frac{y^2}{2} + Ry \;dx
\]

tiene como ecuaciÃ³n de Euler-Lagrange asociada a la forma
autoadjunta de una ecuaciÃ³n lineal

\[
(Py')' + (-Qy + R) = 0.
\]

***** De problema de contorno a coordenadas homogÃ©neas
Sea un problema de contorno y condiciones de contorno Dirichlet no
homogÃ©neas.

\[\left\{\begin{array}{l}
(Py')' + Qy = R \\
y(x_0) = y_0 \\
y(x_1) = y_1
\end{array}\]

Podemos obtener un problema de contorno con coordenadas homogÃ©neas.

****** ConstrucciÃ³n
Proponemos un cambio de variables $z(x) = y(x) - (Ax + B)$ y buscamos
$A$, $B$ y $\widetilde R$ tales que

\[\left\{\begin{array}{l}
(Pz')' + Qz = {\widetilde R} \\
z(x_0) = 0 \\
z(x_1) = 0
\end{array}\]

Lo que nos da resolviendo el sistema lineal dado por las condiciones
de contorno

\[
B = \frac{y_1x_0-y_0x_1}{x_0-x_1}
\]

\[
A = \frac{y_1-y_0}{x_1-x_0}
\]

Luego el cambio de variable correcto es

\[
z(x) = y(x) + \left( \frac{y_1(x-x_0) + y_0(x-x_1)}{x_0-x_1} \right).
\]

Que nos lleva al sustituir en la ecuaciÃ³n inicial a tener
${\widetilde R} = R - P' \cdot A - Q \cdot (Ax + B)$.

**** CARDS
***** EcuaciÃ³n de Euler: forma                                                                              :drill:
SCHEDULED: <2018-08-05 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: twosided
:ID:       d8bb9dc5-ce37-4775-a7d6-5faae2b8da03
:DRILL_LAST_INTERVAL: 59.9578
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:19]
:END:

Forma y tÃ©cnica de resoluciÃ³n de una EcuaciÃ³n de Euler.

****** Forma

\[ a_nx^ny^{(n)}(x) + \dots + a_0y(x) = 0
\]

****** TÃ©cnica de resoluciÃ³n
Se puede aplicar sustituciÃ³n $x =e^s$, $z(s) =y(x)$ para reducirla a
una lineal con coeficientes constantes, teniendo cuidado con los
coeficientes, que cambian.

***** Integral del seno al cuadrado                                                                         :drill:
SCHEDULED: <2018-08-05 Sun>
:PROPERTIES:
:ID:       a5c02ef6-79c6-4b32-943b-dd064732749e
:DRILL_LAST_INTERVAL: 83.7282
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.75
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-13 Sun 22:21]
:END:
Encontrar y comprobar primitivas de

\[
\int \sin^2(x)\,dx
\]

****** Primitiva

\[
\frac{1}{2}(x - \sin(x)\cos(x))
\]

Puede obtenerse usando

\[
\sin^2x = \frac{1-\cos(2x)}{2}.
\]

*** 1. MotivaciÃ³n y fundamentos del cÃ¡lculo variacional
*** 2. CÃ¡lculo de variaciones
**** 2.1. Teorema 1. Lema fundamental del cÃ¡lculo de variaciones
:PROPERTIES:
:ID:       652bdddb-0853-4a9a-8060-28a730928816
:END:
Para $f \colon {\cal C}([a,b], \mathbb{R})$ equivalen

 1. $f \equiv 0$,
 2. $\forall \phi \in {\cal C}^1_0(a,b)\colon \int_a^b f\phi = 0$.

***** Proof
Si tuviÃ©ramos en algÃºn punto $f(x_0) > 0$; por conservaciÃ³n del
signo, tenemos un entorno de $x_0$ contenido en $(a,b)$ donde la
funciÃ³n es estrictamente positiva, sea $(x_0-\varepsilon,x_0+\varepsilon)$. Tomamos

\[\phi(x) = \left\{\begin{array}{lr}
e^{\frac{1}{(x-x_0-\varepsilon)(x-x_0+\varepsilon)}}, & 
\mbox{ si } x \in (x_0-\varepsilon, x_0 + \varepsilon), \\
0, & \mbox{ en otro caso.}
\end{array}\right.\]

Esta funciÃ³n es no analÃ­tica pero continua con derivadas continuas.
Por contradicciÃ³n, $f = 0$.

***** Card: enunciado                                                                                       :drill:
SCHEDULED: <2018-08-13 Mon>
:PROPERTIES:
:ID:       5184345f-67cf-402b-8732-6baf557e13b6
:DRILL_LAST_INTERVAL: 87.1379
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:14]
:END:
Enuncia el lema fundamental del cÃ¡lculo de variaciones para una
$f \colon [a,b] \to \mathbb{R}$. Â¿CuÃ¡l es la idea de la demostraciÃ³n?

****** Enunciado
Para $f \colon [a,b] \to \mathbb{R}$ *continua* equivalen

 1. $f(x) = 0$ para todo $x \in (a,b)$;
 2. $\int_a^b f(x)\phi(x)\,dx = 0$ para toda $\phi \in {\cal C}_0^\infty(a,b)$.

****** Idea de demostraciÃ³n
La funciÃ³n siguiente ese infinitamente diferenciable pero
nos permite aprovechar que un punto de $f$ no fuera cero
para integrar por conservaciÃ³n de signo sÃ³lo esa zona.

\[
\phi(x) = e^{\frac{1}{(x-x_0-\varepsilon)(x-x_0+\varepsilon)}}
\]

**** 2.2. EcuaciÃ³n de Euler-Lagrange
***** Teorema 2. EcuaciÃ³n de Euler-Lagrange
:PROPERTIES:
:ID:       a197ab8c-e2fa-4327-a265-1ece45f66c23
:END:
Para $F(x,y,p) \colon [x_0,x_1] \times \Omega \to \mathbb{R}$, con $\Omega \subseteq \mathbb{R}^2$ dominio, dos veces
derivable en $p$ y en $y$, consideramos

\[
{\cal F}[y] = \int_{x_0}^{x_1} F(x,y(x),y'(x))\,dx,
\]

sobre

\[
{\cal D} = \left\{ \begin{array}{c} 
y \in {\cal C}^1(x_0,x_1), \\
y(x_0) = y_0, \\
y(x_1) = y_1, \\
\forall x \in [x_0,x_1]: (y(x),y'(x)) \in \Omega.
\end{array}\right\}
\]

Cualquier $\overline{y} \in {\cal D} \cap {\cal C}^2(x_0,x_1)$ (*extremal*) mÃ­nimo de ${\cal F}$ sobre ${\cal D}$
verifica

\[
F_y - \dv{}{x}F_p = 0.
\]

****** Proof
:PROPERTIES:
:ID:       d1ff00ca-9b61-4a83-9016-35f8fca5866a
:END:
Dado $y \in {\cal D}$, para cualquier $\phi \in {\cal C}^1_0(x_0,x_1)$ sabemos por compacidad
de $(x_0,x_1)$ que $\phi$ estÃ¡ acotado y podemos tomar un $s$ suficientemente
pequeÃ±o para que $y + s\phi \in {\cal D}$, cumpliendo condiciones de contorno.

Definimos $g(s) = {\cal F}[\oy + s\phi]$ y usando que $\oy$ es mÃ­nimo y su
derivabilidad,

\[\begin{aligned}
0 = g'(0) &= 
\dv{}{s} \int_{x_0}^{x_1}F(x,\oy + s\phi(x), \oy'(x)+s\phi'(x))\,dx \\&=
\int_{x_0}^{x_1} \dv{}{s} F(x,\oy + s\phi(x), \oy'(x)+s\phi'(x))\,dx \\&=
\int_{x_0}^{x_1} F_y(\dots)\phi(x) + F_p(\dots)\phi'(x) \,dx \\&=
\int_{x_0}^{x_1} F_y(\dots)\phi(x) \,dx + \int_{x_0}^{x_1}F_p(\dots)\phi'(x) \,dx \\&=
\int_{x_0}^{x_1} F_y(\dots)\phi(x) \,dx + \cancel{\left[ \phi(x) F_p(\dots) \right]^{x_1}_{x_0}} - \int_{x_0}^{x_1} \dv{}{x}F_p(\dots)\phi(x) \,dx
\end{aligned}\]

Habiendo usado la integrabilidad de $F$ y $\dv{}{s}F$, la regla de la cadena
multivariable, el hecho de que $\phi$ se anula en $x_0,x_1$ e integraciÃ³n por
partes. AquÃ­ aplicamos el [[id:652bdddb-0853-4a9a-8060-28a730928816][lema fundamental]].

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       86b89cab-3163-477b-bdad-49a3d129ef5e
:DRILL_LAST_INTERVAL: 11.2281
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:21]
:END:
EcuaciÃ³n de Euler-Lagrange.

******* Answer
Todo $y$ extremal del funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} F(x,y(x),y'(x))\,dx,
\]

verifica

\[
F_y - \dv{}{x}F_p = 0.
\]

****** Card: derivabilidad para Euler-Lagrange                                                             :drill:
SCHEDULED: <2018-09-08 Sat>
:PROPERTIES:
:ID:       cd2cc1ba-0d03-4f95-871f-a4b27229bf39
:DRILL_LAST_INTERVAL: 106.823
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.875
:DRILL_EASE: 3.0
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:57]
:END:
Condiciones de derivabilidad de la $F$ para Euler-Lagrange.

******* CaracterÃ­sticas
Dos veces derivable en $p$ y en $y$.

****** Card: Dominio de Euler-Lagrange                                                                     :leech:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       7c4a6835-9dea-4a2a-a944-4ed5540545ee
:DRILL_LAST_INTERVAL: 66.2476
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.6
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-05 Sat 20:07]
:END:
${\cal D}$ del funcional al que se aplica Euler-Lagrange.

******* Dominio

\[{\cal D} = \left\{ \begin{array}{c} 
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y_0 \\
y(x_1) = y_1 \\
\forall x \in [x_0,x_1]: (y(x),y'(x)) \in \Omega
\end{array}\right\}\]

***** Teorema 3. Condiciones para conjuntos sin condiciones de contorno
:PROPERTIES:
:ID:       094690c8-7f15-424a-89aa-126400e3d0c6
:END:
Si no tenemos condiciones de contorno, el extremal cumple

\[
F_p(x_0,\oy(x_0),\oy'(x_0)) = 0 = F_p(x_1,\oy(x_1),\oy'(x_1).
\]

Cuando sÃ³lo una viene fijada, la otra puede fijarse tambiÃ©n asÃ­

****** Proof
La [[id:d1ff00ca-9b61-4a83-9016-35f8fca5866a][demostraciÃ³n]] de Euler-Lagrange puede repetirse para cualquier
$\phi \in {\cal C}^1(x_0,x_1)$ para obtener

\[
0 = 
\int_{x_0}^{x_1} F_y(\dots)\phi(x) \,dx + 
\left[ \phi(x) F_p(\dots) \right]^{x_1}_{x_0} - 
\int_{x_0}^{x_1} \dv{}{x}F_p(\dots)\phi(x) \,dx.
\]

Como esto se cumple en particular para $\phi \in {\cal C}^1_0(x_0,x_1)$, estamos en
las condiciones del teorema anterior y se tiene la ecuaciÃ³n de
Euler-Lagrange. Pero entonces

\[\left[ \phi(x) F_p(\dots) \right]^{x_1}_{x_0} = 0\]

y tomando en particular funciones cumpliendo $\phi(x_j) = \delta_{ij}$ para
$i,j = 0,1$ tenemos las condiciones de contorno buscadas.

****** Card: Condiciones de contorno para problemas sin ellas                                              :drill:
SCHEDULED: <2018-07-30 Mon>
:PROPERTIES:
:ID:       742721a0-1edb-47c7-aabc-bb203354d4bc
:DRILL_LAST_INTERVAL: 81.1591
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.833
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-10 Thu 23:44]
:END:
Â¿QuÃ© condiciones de contorno verifica un extremal si no vienen dadas?

******* Condiciones de contorno

\[
F_p(x_0,\oy(x_0),\oy'(x_0)) = 0 = F_p(x_1,\oy(x_1),\oy'(x_1).
\]

******* Extra
Si viene dada solo una de ellas, puede fijarse la otra asÃ­.

***** Teorema 4. Euler-Lagrange para F independiente de y
En [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]], si $F_y = 0$, el extremal cumple

\[
F_p = \mathrm{cte.}
\]

****** Proof
Trivialmente integrando en la ecuaciÃ³n de Euler-Lagrange.

****** Card: Euler-Lagrange para F independiente de y                                                      :drill:
SCHEDULED: <2018-08-01 Wed>
:PROPERTIES:
:ID:       b5ac284c-b322-4cf6-996f-c70db55e891b
:DRILL_LAST_INTERVAL: 79.9878
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.571
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-13 Sun 22:19]
:END:
Reescribe Euler-Lagrange sabiendo $F_y = 0$.

******* Euler-Lagrange

\[
F_p(x,\oy'(x)) = \mathrm{cte.}
\]

***** Teorema 5. Euler-Lagrange para F independiente de x
En [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]], si $F_ x= 0$, el extremal cumple

\[
F - y'F_p = \mathrm{cte.}
\] 

****** Proof
Tenemos que, aplicando [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]]

\[
\dv{}{x}\left( F - y'F_p \right) = y'\left(F_y - \dv{}{x}F_p\right) = 0.
\]

****** Card: Euler-Lagrange para F independiente de x                                                      :drill:
SCHEDULED: <2018-07-13 Fri>
:PROPERTIES:
:ID:       22e8b4d8-c306-4927-9a7f-159725999cbd
:DRILL_LAST_INTERVAL: 63.7384
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 10
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.65
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-10 Thu 23:44]
:END:
Reescribe Euler-Lagrange sabiendo $F_x = 0$.

******* Euler-Lagrange

\[
F - y'F_p = \mathrm{cte.}
\]

**** 2.3. Ejemplos
***** Oscilador armÃ³nico
***** Longitud mÃ­nima
***** Longitud mÃ­nima usando un caso particular
***** Superficie minimal de revoluciÃ³n
****** Card                                                                                                :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       3c7fbd05-0ff4-4169-b50b-a0cf2e328b71
:DRILL_LAST_INTERVAL: 32.3061
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:18]
:END:
SoluciÃ³n de 

\[
y(x) = c \sqrt{1 + y'(x)^2}
\]

******* SoluciÃ³n
\[
y(x) = c_0 \cosh \left( \frac{x-k}{c_0}  \right)
\]

**** 2.4. Convexidad
***** Convexidad
$\Omega \subseteq V$ es *convexo* cuando

\[
\forall x,y \in \Omega: \forall 0 \leq t \leq 1:\quad tx + (1-t)y \in \Omega.
\]

$f \colon \Omega \to \mathbb{R}$ es *convexa* cuando cumple la desigualdad de Jensen,

\[\forall x,y \in \Omega: \forall 0 \leq t \leq 1:\quad
f(tx + (1-t)y) \leq tf(x) + (1-t)f(y).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-23 Mon>
:PROPERTIES:
:ID:       a8a0d061-c728-48f3-a08b-93a1b1b625f8
:DRILL_LAST_INTERVAL: 46.8681
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:07]
:END:
Desigualdad de Jensen

******* Desigualdad

\[
f(tx + (1-t)y) \leq tf(x) + (1-t)f(y).
\]

***** Teorema 6. Existencia de mÃ­nimo con convexidad
:PROPERTIES:
:ID:       89c9e4a0-cc09-4962-b3e5-8fc1cfbb0c4e
:END:
En [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]], si ${\cal D}$ y ${\cal F}$ convexos, extremal es mÃ­nimo global.

****** Proof
Dado $z \in {\cal D}$, llamamos $\phi =z-y$ y tenemos

\[
{\cal F}[\oy - t\phi] \leq t{\cal F}[z] + (1-t){\cal F}[\oy].
\]

Tomando lÃ­mites y usando que es extremal,

\[
0 = \dv{t} \left( {\cal F}[\oy + t\phi] \right) =
\lim_{t \to 0}\frac{{\cal F}[\oy-t\phi] - {\cal F}[\oy]}{t} \leq
{\cal F}[z] - {\cal F}[y].
\]

***** Teorema 7. Los mÃ­nimos locales son globales con convexidad
:PROPERTIES:
:ID:       c46055cc-5d8b-4772-8af3-48057d7e45a4
:END:
En [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]], si ${\cal F}$ y ${\cal D}$ convexos, un mÃ­nimo local (estricto) 
es mÃ­nimo global (estricto).

****** Proof
Sea $\oy$ mÃ­nimo local, $\forall \|z - \oy\| \leq \varepsilon\colon {\cal F}[\oy] \leq {\cal F}[z]$. Para $v \in {\cal D}$ tomamos $t_0 = \frac{\varepsilon}{\| v - \oy\|}$

\[
{\cal F}[\oy] \leq
{\cal F}[t_0v + (1 - t_0)\oy] \leq t_0{\cal F}[v] + (1-t_0){\cal F}[\oy],
\]

teniendo $t_0 {\cal F}[\oy] \leq t_0{\cal F}[v]$.

***** Teorema 8. CondiciÃ³n de convexidad
:PROPERTIES:
:ID:       4d623017-26e1-4e3e-82ec-325f4b6c0b38
:END:
En [[id:a197ab8c-e2fa-4327-a265-1ece45f66c23][Euler-Lagrange]], si $F$ convexo en $\Omega$ convexo, ${\cal F}$ convexo.

****** TODO No es condiciÃ³n necesaria
Tenemos $F = y^2(1-p)$ no convexa, pero su ${\cal F}$ es convexo.

****** Proof
Usando linealidad y acotaciÃ³n de la integral tenemos

\[\begin{aligned}
{\cal F}[\lambda y + (1-\lambda) z] &=
\int_{x_0}^{x_1} F\Big(x,\lambda y + (1-\lambda)z, \lambda y' + (1-\lambda)z'\Big)\,dx \\&\leq
\lambda \int_{x_0}^{x_1} F\Big(x,y,y'\Big)\,dx + (1 - \lambda)\int_{x_0}^{x_1} F\Big(x,z,z'\Big)\,dx.
\end{aligned}\]

*** 3. CÃ¡lculo de extremales: problemas de contorno
**** TODO Euler-Lagrange con segunda derivada
:PROPERTIES:
:ID:       1d1082d8-cb7a-4e0c-868c-5d734cd8949b
:END:
**** 3.1. Existencia y unicidad
***** Problema de contorno
Un *problema de contorno* consiste encontrar una soluciÃ³n
de una EDO (o EDP) en un intervalo $[x_0,x_1]$ (o dominio $\Omega$)
verificando condiciones en $x_0,x_1$ (o en $\partial\Omega$).

***** TODO Ejemplos
**** 3.2. Tipos de condiciones de contorno
Tipos de condiciones de contorno

 * Dirichlet:            $y(x_0) = y_0$, $y(x_1) = y_1$.

 * Dirichlet homogÃ©neas: $y(x_0) = 0$, $y(x_1) = 0$.

 * Newmann:              $y'(x_0) = z_0$, $y'(x_1) = z_1$.

 * Newmann homogÃ©neas:   $y'(x_0) = 0$, $y'(x_1) = 0$.

 * PeriÃ³dicas:           $y(x_0) = y(x_1)$, $y'(x_0) = y'(x_1)$.

**** 3.3. El problema de la viga
***** EcuaciÃ³n de la viga
:PROPERTIES:
:ID:       afb30645-46b6-4087-bf43-bf765d53719b
:END:
La *ecuaciÃ³n de la viga*, para $M > 0$, constante $N$ y
densidad $f(x)$ es

\[
Mu'''' + Nu'' = f.
\]

****** Tipos de sujecciÃ³n de una viga
Existen los siguientes tipos de sujecciÃ³n:

 * Empotramiento:   $u(0)=0$,  $u'(0) = 0$.
 * Apoyo:           $u(0)=0$,  $u''(0) = 0$.
 * Voladizo:        $u''(0) = 0$, $u'''(0) = 0$.

****** Card: ecuaciÃ³n                                                                                      :drill:
SCHEDULED: <2018-08-09 Thu>
:PROPERTIES:
:ID:       d370b290-0054-4558-ac81-0da896782fbe
:DRILL_LAST_INTERVAL: 76.8573
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:54]
:END:
CuÃ¡l es la ecuaciÃ³n diferencial del problema de la viga.

******* EcuaciÃ³n diferencial

\[
Mu''''(x) + Nu''(x) = f(x)
\]

constantes $M,N$ y funciÃ³n $f$. Se complementa con cuatro condiciones
de contorno, segÃºn sujeciÃ³n. Con otra notaciÃ³n,

\[
Mu'''' + Nu'' = f
\]

***** Problema variacional de la viga

\[
{\cal F}[u] = \int_0^L \left( \frac{M}{2}(u''(x))^2 - \frac{N}{2}(u'(x))^2 - f(x)u(x) \right)\,dx.
\]

Aplicando [[id:1d1082d8-cb7a-4e0c-868c-5d734cd8949b][Euler-Lagrange en la segunda derivada]] se llega a la
[[id:afb30645-46b6-4087-bf43-bf765d53719b][ecuaciÃ³n de la viga]].

****** Card: forma variacional                                                                             :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       6d3c077e-63c9-4376-b35a-4b5158236b39
:DRILL_LAST_INTERVAL: 28.0398
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 12
:DRILL_FAILURE_COUNT: 6
:DRILL_AVERAGE_QUALITY: 2.75
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:46]
:END:
Da el funcional a minimizar en el problema de la viga.

******* Funcional

\[
{\cal F}[u] = \int_0^L \left(\frac{M}{2}(u'')^2 - \frac{N}{2}(u')^2 - f(x) \right)\,dx,
\]

con cuatro condiciones de contorno para la $u$.

****** Card: obtenciÃ³n de la ecuaciÃ³n                                                                    :nodrill:
SCHEDULED: <2018-06-11 Mon>
:PROPERTIES:
:ID:       df2d59fc-26e4-43e7-b955-e1251a820cdb
:DRILL_LAST_INTERVAL: 29.0223
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 9
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 3.111
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-13 Sun 22:20]
:END:
En el problema de la viga, dar $F$ y Euler-Lagrange para segunda
derivada.

******* FunciÃ³n
\[
F(x,y,p,q) = \frac{M}{2}q^2 - \frac{N}{2}p^2 - f(x)y.
\]

******* Euler-Lagrange en segunda derivada

\[
F_y - 
\dv{}{x}F_p + 
\dv[2]{}{x}F_{q} = 0.
\]

******* EcuaciÃ³n final

\[
Mu''''(x) + Nu''(x) = f(x).
\]

**** 3.4. Forma autoadjunta de algunos problemas de contorno
Cualquier EDO lineal de segundo orden

\[
y''(x) + a(x)y'(x) + b(x)y(x) = c(x)
\]

puede escribirse en *forma autoadjunta* o *forma de Sturm-Liouville*

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = R(x),
\]

haciendo el cambio

 * $P(x) = \exp \left( \int_{x_0}^xa(z)\,dz \right)$

 * $Q(x) = P(x)b(x)$

 * $R(x) = P(x)c(x)$

****** Card: paso a forma autoadjunta                                                                      :drill:
SCHEDULED: <2018-08-19 Sun>
:PROPERTIES:
:ID:       a1c6ef44-0fb4-43aa-9b09-10427e5ba538
:DRILL_LAST_INTERVAL: 52.1526
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:09]
:END:
Â¿CÃ³mo pasar una EDO lineal de segundo orden a forma autoadjunta?

\[
y''(x) + a(x)y'(x) + b(x)y(x) = c(x)
\]

******* Respuesta
Puede escribirse en *forma autoadjunta* o *forma de Sturm-Liouville*

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = R(x),
\]

haciendo el cambio

 * $P(x) = \exp \left( \int_{x_0}^xa(z)\,dz \right)$

 * $Q(x) = P(x)b(x)$

 * $R(x) = P(x)c(x)$

**** 3.5. RelaciÃ³n con cÃ¡lculo de variaciones
***** Paso a problema variacional
Dada una EDO en forma autoadjunta

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = R(x),
\]

existe un funcional que la tiene como Euler-Lagrange asociada

\[
{\cal F}[y] = 
\int_{x_0}^{x_1} \left( 
P(x)\frac{y'(x)^2}{2} - Q(x)\frac{y(x)^2}{2} + R(x)y(x)
\right)\,dx,
\]

con

\[
F(x,y,p) = P\frac{p^2}{2} - Q\frac{y^2}{2} + Ry.
\]

****** Card: paso a forma variacional                                                                      :drill:
SCHEDULED: <2018-08-09 Thu>
:PROPERTIES:
:ID:       a8c3472f-302b-42e0-be27-cdf7bcb96c3d
:DRILL_LAST_INTERVAL: 63.7746
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:17]
:END:
Â¿CÃ³mo pasar una EDO en forma autoadjunta a un problema
variacional?

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = R(x),
\]

******* Funcional
Existe un funcional que la tiene como Euler-Lagrange asociada

\[
{\cal F}[y] = 
\int_{x_0}^{x_1} \left( 
P\frac{(y')^2}{2} - Q\frac{y^2}{2} + Ry
\right)\,dx.
\]

con

\[
F(x,y,p) = P\frac{p^2}{2} - Q\frac{y^2}{2} + Ry.
\]

***** Teorema 9. CondiciÃ³n de unicidad para el problema variacional
Si $P,Q,R \in {\cal C}[x_0,x_1]$ con $P > 0$, $Q < 0$, entonces ${\cal F}$ es convexo.
Bajo *condiciones quasi-Dirichlet*,

\[\left.\begin{array}{l}
a_0y(x_0) + b_0y'(x_0) = c_0 \\
a_1y(x_1) + b_1y'(x_1) = c_1
\end{array}\right\}\quad
\mbox{ con }|a_0| + |a_1| > 0 \mbox{ y con } a_0b_0 \geq 0, a_1b_1 \geq 0\]

existe una Ãºnica extremal soluciÃ³n del problema de contorno y
mÃ­nimo de ${\cal F}$.

****** Proof
******* Convexidad
La [[id:0cebe934-5d31-494d-bfd6-891d9c4c81ec][hessiana]] de $F(x,y,p) = \frac{P}{2}p^2 - \frac{Q}{2}y^2 + Ry$ en sus dos primeras
variables es definida positiva, luego $F$ es convexa

\[\mathrm{Hess}(F) = \begin{pmatrix}
-Q & 0 \\
0 & P
\end{pmatrix}.\]

Por [[id:4d623017-26e1-4e3e-82ec-325f4b6c0b38][Teorema 8]], ${\cal F}$ es convexa. Si existe extremal serÃ­a Ãºnica.

******* Extremal Ãºnica y soluciÃ³n (caso Dirichlet)
Por [[id:568bcd38-4589-424c-8f17-71f3c2df75d0][Alternativa de Fredholm]], hay extremal (soluciÃ³n de la ecuaciÃ³n
completa) Ãºnica, cuando sÃ³lo existe la soluciÃ³n trivial al sistema
homogÃ©neo

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = 0.
\]

Si hubiera soluciÃ³n distinta de $0$, por unicidad del PVI asociado,
deberÃ­a tenerse $y'(x_0) = 0 > 0$, luego $y(x) > 0$ en algÃºn entorno a
la derecha de $x_0$. Como $y(x_1) = 0$, debe haber un mÃ¡ximo interno y
un punto crÃ­tico donde llegarÃ­amos a contradicciÃ³n.

******* Extremal Ãºnica y soluciÃ³n (caso quasi-Dirichlet)                                                  :extra:
****** Card: condiciones quasi-Dirichlet                                                                 :nodrill:
SCHEDULED: <2018-08-02 Thu>
:PROPERTIES:
:ID:       49074f9d-fd25-49ae-a5f3-f04ed9d610c1
:DRILL_LAST_INTERVAL: 35.8713
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 9
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.556
:DRILL_EASE: 1.8
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:23]
:END:
*Describe las condiciones de contorno quasi-Dirichlet* que se usan
para asegurar unicidad en un problema variacional obtenido desde
una EDO autoadjunta

\[
{\cal F}[y] = 
\int_{x_0}^{x_1} \left( 
P(x)\frac{y'(x)^2}{2} - Q(x)\frac{y(x)^2}{2} + R(x)y(x)
\right)\,dx,
\]

******* Condiciones quasi-Dirichlet

\[\left.\begin{array}{l}
a_0y(x_0) + b_0y'(x_0) = c_0 \\
a_1y(x_1) + b_1y'(x_1) = c_1
\end{array}\right\}\quad
\mbox{ con }|a_0| + |a_1| > 0 \mbox{ y con } a_0b_0 \geq 0, a_1b_1 \geq 0\]

****** Card: enunciar el Teorema 9                                                                       :nodrill:
SCHEDULED: <2018-06-13 Wed>
:PROPERTIES:
:ID:       1506e11a-fd0e-41db-b339-bdce782014bb
:DRILL_LAST_INTERVAL: 19.6194
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.571
:DRILL_EASE: 1.94
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:45]
:END:
Enuncia el Teorema 9 sobre unicidad de extremales en un funcional

\[
{\cal F}[y] = 
\int_{x_0}^{x_1} \left( 
P(x)\frac{y'(x)^2}{2} - Q(x)\frac{y(x)^2}{2} + R(x)y(x)
\right)\,dx,
\]

obtenido del PC de una EDO autoadjunta

\[
\Big( P(x)y' \Big)' + Q(x)y(x) = R(x),
\]

******* Enunciado
Si $P,Q,R \in {\cal C}[x_0,x_1]$ con $P > 0$, $Q < 0$, entonces ${\cal F}$ es convexo.
Bajo *condiciones quasi-Dirichlet*, existe una Ãºnica extremal soluciÃ³n
del problema de contorno y mÃ­nimo de ${\cal
F}$.

******* Condiciones quasi-Dirichlet

\[\left.\begin{array}{l}
a_0y(x_0) + b_0y'(x_0) = c_0 \\
a_1y(x_1) + b_1y'(x_1) = c_1
\end{array}\right\}\quad
\mbox{ con }|a_0| + |a_1| > 0 \mbox{ y con } a_0b_0 \geq 0, a_1b_1 \geq 0\]

***** Condiciones de contorno
Tenemos condiciones Newmann homogÃ©neas sin imponer nada en el problema
variacional. Las condiciones Dirichlet sÃ­ deben ser aÃ±adidas.

****** Proof
En conjuntos [[id:094690c8-7f15-424a-89aa-126400e3d0c6][sin condiciones de contorno]] tendrÃ­amos

\[\begin{aligned}
0 = F_p(x_{0},y(x_0),y'(x_0)) = y'(x_0)P(x_0),\\
0 = F_p(x_1,y(x_1),y'(x_1)) = y'(x_1)P(x_1),
\end{aligned}\]

y como $P > 0$, se tienen las condiciones.

***** Teorema 10. Paso a Dirichlet homogÃ©neas
Dada una ecuaciÃ³n en forma autoadjunta con condiciones Dirichlet
no homogÃ©neas,

\[\left.\begin{array}{l}
\Big( P(x)y'(x) \Big)' + Q(x)y(x) = R(x),\ x \in [x_0,x_1] \\
y(x_0) = y_0, \\
y(x_1) = y_1.
\end{array}\right\}\]

podemos aplicar el cambio $z(x) = y(x) - (ax+b)$, con
$a = (y_1-y_0)/(x_1-x_0)$, con $b = (y_0x_1-y_1x_0)/(x_1-x_0)$ y con
${\tilde R} = R - aP' - (ax + b)Q$ para llegar a

\[\left.\begin{array}{l}
\Big( P(x)z'(x) \Big)' + Q(x)z(x) = {\tilde R}(x),\ x \in [x_0,x_1] \\
z(x_0) = 0, \\
z(x_1) = 0.
\end{array}\right\}\]

****** Proof
Calculando se verifica la equivalencia. NÃ³tese que hemos
tomado la recta que anula la $y$ en justo esos puntos.

***** Ejemplo 14. Paso a Newmann homogÃ©neas
Realizado en el [[id:de1af79c-3a0b-410e-8cca-d26c92021ec8][Ejercicio 4]].

**** 3.6. Soluciones de un PC en forma autoadjunta
***** Caso homogÃ©neo
Las soluciones de la *ecuaciÃ³n homogÃ©nea*

\[
\left( P(x)y'(x) \right)' + Q(x)y(x) = 0
\]

forman un espacio vectorial.

***** Caso no homogÃ©neo: definiciones
Llamamos *ecuaciÃ³n completa* a

\[
\left( P(x)y'(x) \right)' + Q(x)y(x) = R(x).
\]

*Condiciones de contorno separadas*

\[\left.\begin{array}{l}
a_0y(x_0) + b_0y'(x_0) = 0 \\
a_1y(x_1) + b_1y'(x_1) = 0
\end{array}\right\}\quad\]

y *condiciones de contorno periÃ³dicas*

\[\left.\begin{array}{l}
y(x_0) = y(x_1) \\
y'(x_0) = y'(x_1)
\end{array}\right\}\quad\]

****** Card: condiciones de contorno                                                                       :nodrill:
SCHEDULED: <2018-06-14 Thu>
:PROPERTIES:
:ID:       833872e8-5496-40d0-a969-d24dc2a5ad00
:DRILL_LAST_INTERVAL: 20.7715
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.6
:DRILL_EASE: 1.94
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:45]
:END:
Bajo quÃ© tipos de condiciones de contorno se puede aplicar
Alternativa de Fredholm a

\[
\left( P(x)y'(x) \right)' + Q(x)y(x) = R(x).
\]

******* Condiciones
*Condiciones de contorno separadas homogÃ©neas*

\[\left.\begin{array}{l}
a_0y(x_0) + b_0y'(x_0) = 0 \\
a_1y(x_1) + b_1y'(x_1) = 0
\end{array}\right\}\quad\]

y *condiciones de contorno periÃ³dicas*

\[\left.\begin{array}{l}
y(x_0) = y(x_1) \\
y'(x_0) = y'(x_1)
\end{array}\right\}\quad\]

***** Teorema 11. Alternativa de Fredholm
:PROPERTIES:
:ID:       568bcd38-4589-424c-8f17-71f3c2df75d0
:END:
En un PC con condiciones de contorno separadas homogÃ©neas o
periÃ³dicas, necesariamente se verifica una de las siguientes
alternativas

 1. El problema homogÃ©neo sÃ³lo admite la soluciÃ³n trivial, y
    entonces el completo sÃ³lo admite una soluciÃ³n Ãºnica.

 2. El problema homogÃ©neo admite varias soluciones $y_h$ y el
    completo admitirÃ¡ tantas como Ã©l si y sÃ³lo si para toda
    soluciÃ³n,

    \[
    \int_{x_0}^{x_1}R(x)y_h(x)\,dx = 0.
    \]

****** Proof                                                                                               :extra:
Sea $\left\{ \phi_1,\phi_2 \right\}$ sistema fundamental de soluciones de la homogÃ©nea.
Por fÃ³rmula de variaciÃ³n de constantes, cualquier soluciÃ³n de la
completa es

\[
y = y_h + y_p = A\phi_1 + B\phi_2 + \int_{x_0}^x \frac{R(z)}{W(x_0)P(x_0)}
\left( \phi_2(x)\phi_1(z) - \phi_2(z)\phi_1(x) \right)\,dz
\]

Usando la fÃ³rmula de variaciÃ³n de constantes y aplicando
propiedades del Wronskiano.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-04 Sat>
:PROPERTIES:
:ID:       a683d1ef-c516-4d88-aa81-89961fd3780c
:DRILL_LAST_INTERVAL: 58.9299
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.18
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:20]
:END:

Enuncia la Alternativa de Fredholm para

\[
\left( P(x)y'(x) \right)' + Q(x)y(x) = R(x).
\]

con condiciones separadas homogÃ©neas o periÃ³dicas.

******* Alternativa de Fredholm
Se cumple una de dos alternativas

 1. El problema homogÃ©neo sÃ³lo admite la soluciÃ³n trivial, y
    entonces el completo sÃ³lo admite una soluciÃ³n Ãºnica.

 2. El problema homogÃ©neo admite varias soluciones $y_h$ y el
    completo admitirÃ¡ tantas como Ã©l si y sÃ³lo si para toda
    soluciÃ³n,

    \[
    \int_{x_0}^{x_1}R(x)y_h(x)\,dx = 0.
    \]

*** 4. Ligaduras, extremos condicionados
**** 4.1. Teorema 12. Euler-Lagrange para funcionales de varias funciones
:PROPERTIES:
:ID:       cf95308e-1cfe-4d08-af55-b2fb3956f471
:END:
Para $F \colon [x_0,x_1] \times \Omega \to \mathbb{R}$ continua, $\Omega \subseteq \mathbb{R}^{2n}$ dominio, minimizar

\[
{\cal F}(y) = \int_{x_0}^{x_1} F(x,y_1(x),\dots,y_n(x),y'_1(x), \dots, y'_n(x))\,dx.
\]

sobre

\[
{\cal D} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega
\end{array}\right\}
\]

Para $\oy \in {\cal D} \cap \left( {\cal C}^2[x_0,x_1] \right)^n$ extremal se tiene Euler-Lagrange

\[
F_{y_i} - \dv{}{x}\Big( F_{p_i} \Big) = 0, \quad i = 1,\dots,n.
\]

***** Proof
Seguimos un razonamiento similar al Teorema 2, tomando $g(s) = {\cal F}[y +s\phi]$
con $\phi = (\phi_1,\dots,\phi_n)$. Llegamos a

\[
0 = \sum_{i=1}^n \int_{x_0}^{x_1} \left( F_{y_i} - \dv{}{x} F_{p_i} \right) \phi_i(x)\,dx
\]

y tomamos funciones test en cada componente para obtener las ecuaciones.

***** Card: ecuaciones                                                                                      :drill:
SCHEDULED: <2018-09-15 Sat>
:PROPERTIES:
:ID:       c03c7dca-f9ba-4bc7-9dcd-2a1ea5dd1bcb
:DRILL_LAST_INTERVAL: 78.2524
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:56]
:END:
Ecuaciones de Euler-Lagrange para funcionales dependientes de
varias funciones.

****** Ecuaciones

\[
F_{y_i} - \dv{}{x}\Big( F_{p_i} \Big) = 0, \quad \mbox{ para }i = 1,\dots,n.
\]

***** TODO Card: dominio                                                                                    :leech:
SCHEDULED: <2018-05-07 Mon>
:PROPERTIES:
:ID:       2303fc9e-f13d-4b4f-90e1-78e0bcc5fca3
:DRILL_LAST_INTERVAL: 4.1868
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-03 Thu 20:18]
:END:
Dominio de un funcional dependiente de varias funciones.

\[
{\cal F}(y) = \int_{x_0}^{x_1} F(x,y_1(x),\dots,y_n(x),y'_1(x), \dots, y'_n(x))\,dx.
\]

****** Dominio

\[
{\cal D} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega
\end{array}\right\}
\]

**** 4.2. Teorema 13. Ligaduras algebraico-diferenciales
:PROPERTIES:
:ID:       2e02f30e-7a39-472d-bbec-47c949e5b641
:END:
${\cal F}$ en [[id:cf95308e-1cfe-4d08-af55-b2fb3956f471][varias funciones]], con $m < n$, sobre

\[
{\cal D}_{\varphi} = {\cal D} \cap \left\{\begin{array}{c}
\varphi_1(x,y_1,\dots,y_n,y_1',\dots,y_n') = 0\\
\vdots\\
\varphi_m(x,y_1,\dots,y_n,y_1',\dots,y_n') = 0\\
\end{array}\right\}
\]

para $\oy \in {\cal D}_{\varphi} \cap \left( {\cal C}^2[x_0,x_1] \right)^n$ extremal, existen *multiplicadores* $\lambda_j(x)$,

\[
F^{\ast} = F - \sum_{j=1}^m \lambda_j(x) \varphi_j(x,y_1,\dots,y_n,y_1',\dots,y_n')
\]

con $(\oy,\lambda_1,\dots,\lambda_n)$ extremales de ${\cal F}^{\ast}[y,\lambda] = \int_{x_0}^{x_1} F^{\ast}\,dx$.  Por Euler-Lagrange,

\[\left\{\begin{array}{l}
F^{\ast}_{y_i} - \dv{}{x} \left( F^{\ast}_{p_i}\right) = 0,\quad i = 1,\dots,n; \\
\varphi_j = 0, \quad j = 1,\dots,m.
\end{array}\]

***** Proof                                                                                                 :extra:
# DemostraciÃ³n en el libro de Elsgoltz.

***** Card: dominio en ligaduras algebraicas                                                                :drill:
SCHEDULED: <2018-07-18 Wed>
:PROPERTIES:
:ID:       84993bca-f28f-4487-bd43-a16b443f2c51
:DRILL_LAST_INTERVAL: 43.82
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:19]
:END:
Describe un dominio con ligaduras algebraicas.

****** Dominio

\[
{\cal D} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega \\
\varphi_1(x,y_1,\dots,y_n) = 0\\
\vdots\\
\varphi_m(x,y_1,\dots,y_n) = 0\\
\end{array}\right\}
\]

***** Card: ligaduras algebraicas                                                                           :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       71146515-8346-4743-829e-487b2e4ef9d0
:DRILL_LAST_INTERVAL: 31.8129
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:02]
:END:
CÃ³mo buscar el mÃ­nimo de un funcional de varias variables ${\cal F}$

\[
{\cal F}(y) = \int_{x_0}^{x_1} F(x,y_1(x),\dots,y_n(x),y'_1(x), \dots, y'_n(x))\,dx.
\]

En un conjunto con ligaduras algebraicas

\[
{\cal D} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega \\
\varphi_1(x,y_1,\dots,y_n) = 0\\
\vdots\\
\varphi_m(x,y_1,\dots,y_n) = 0\\
\end{array}\right\}
\]

****** MÃ©todo
Definimos

\[
F^{\ast} = F - \sum_{j=1}^m \lambda_j(x) \varphi_j(x,y_1,\dots,y_n)
\]

Buscamos $(\oy,\lambda_1,\dots,\lambda_n)$ extremales de ${\cal F}^{\ast}[y,\lambda] = \int_{x_0}^{x_1} F^{\ast}\,dx$ sobre
un conjunto sin restricciones. En estos puntos tenemos las ecuaciones
de Euler Lagrange

\[\left\{\begin{array}{l}
F^{\ast}_{y_i} - \dv{}{x} \left( F^{\ast}_{p_i}\right) = 0,\quad i = 1,\dots,n; \\
\varphi_j = 0, \quad j = 1,\dots,m.
\end{array}\]

***** Card: dominio en ligaduras algebro-diferenciales                                                    :nodrill:
SCHEDULED: <2018-06-12 Tue>
:PROPERTIES:
:ID:       e8794a70-7d98-469e-907d-16b566f467ba
:DRILL_LAST_INTERVAL: 25.1192
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:01]
:END:
Describe un dominio con ligaduras algebro-diferenciales.

****** Dominio

\[
{\cal D}'_{\varphi} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega \\
\varphi_1(x,y_1,\dots,y_n,y'_1,\dots,y'_n) = 0\\
\vdots\\
\varphi_m(x,y_1,\dots,y_n,y'_1,\dots,y'_n) = 0\\
\end{array}\right\}
\]

***** Card: completo                                                                                        :drill:
SCHEDULED: <2018-08-29 Wed>
:PROPERTIES:
:ID:       9165da91-53b4-4f99-a4bd-aedd91cab3c5
:DRILL_LAST_INTERVAL: 62.7939
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:16]
:END:
CÃ³mo buscar el mÃ­nimo de un funcional sobre varias funciones ${\cal F}$
sobre un conjunto con ligaduras algebro-diferenciales.

****** MÃ©todo
Definimos

\[
F^{\ast} = F - \sum_{j=1}^m \lambda_j(x)\varphi_j(x,y_1,\dots,y_n,y'_1,\dots,y'_n)
\]

y le buscamos extremales sin restricciones con Euler-Lagrange

\[\left\{\begin{array}{l}
F^{\ast}_{y_i} - \dv{}{x} \left( F^{\ast}_{p_i}\right) = 0,\quad i = 1,\dots,n; \\
\varphi_j = 0, \quad j = 1,\dots,m.
\end{array}\]

**** 4.3. Teorema 14. Ligaduras integrales
Sea ${\cal F}$ en [[id:cf95308e-1cfe-4d08-af55-b2fb3956f471][varias funciones]], con $m < n$ o $m \geq n$, sobre

\[
{\cal D}_{\varphi} = {\cal D} \cap \left\{\begin{array}{c}
\int_{x_0}^{x_1} G_1(x,y_1,\dots,y_n,y_1',\dots,y_n') = L_1\\
\vdots\\
\int_{x_0}^{x_1} G_m(x,y_1,\dots,y_n,y_1',\dots,y_n') = L_m\\
\end{array}\right\}
\]

para $\oy \in {\cal D}_{\varphi} \cap \left( {\cal C}^2[x_0,x_1] \right)^n$ extremal, hay *multiplicadores constantes* $\lambda_j$,

\[
F^{\ast} = F - \sum_{j=1}^m \lambda_j G_j,
\]

cumpliendo por Euler-Lagrange,

\[
F^{\ast}_{y_i} - \dv{}{x} \Big( F^{\ast}_{p_i} \Big) = 0, \qquad i=1,\dots,n.
\]

***** Principio de reciprocidad
La $F^{\ast}$ se mantiene cuando multiplicamos todo por $\mu \in \mathbb{R}$ y 
cambiamos $F$ por alguna de las $G_j$.

El *principio de reciprocidad* dice que equivale maximiar el Ã¡rea
fijada una longitud y minimizar la longitud fijada un Ã¡rea.

***** Proof
Tomamos $\tilde{\cal F}[y,z_1,\dots,z_m] = {\cal F}[y]$ con restricciones que sirven para
fijar las $z_j$ como primitivas, pero que son tipo algebraico-diferencial,

\[
{\cal D}_{\varphi}' = \left\{\begin{array}{c}
\vec{y}(x) \in {\cal D} \\
z_i \in {\cal C}^1[x_0,x_1] \\
\vec{z}(x_0) = 0 \\
\vec{z}(x_1) = \vec{L} \\
G_1(x,y_1,\dots,y_n,y'_1,\dots,y'_n) - z_1'(x) = 0 \\
\vdots\\
G_m(x,y_1,\dots,y_n,y'_1,\dots,y'_n) - z_m'(x) = 0
\end{array}\right\}
\]

y aplicando el teorema sobre [[id:2e02f30e-7a39-472d-bbec-47c949e5b641][ligaduras diferenciales]], tenemos

\[
\tilde{F}^{\ast} = \tilde{F} - \sum_{j=1}\lambda_j(x)(G_j - z_j')
\]

y llegamos a

\[\left\{\begin{aligned}
\tilde{F}^{\ast}_{y_i} - \dv{}{x} \left( \tilde{F}^{\ast}_{p_i} \right) = 0, \\
\dv{}{x} \left( \lambda_j(x) \right) = 0,\\
\int_{x_0}^{x_1}G_j\,dx = L_j
\end{aligned}\right.\]

que se corresponden a las ligaduras que marca el enunciado.

***** Card: dominio en ligaduras integrales                                                                 :drill:
SCHEDULED: <2018-07-14 Sat>
:PROPERTIES:
:ID:       d6f0bf2b-96a3-4aa7-97cf-a10986b4ca1b
:DRILL_LAST_INTERVAL: 38.0072
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:06]
:END:
Describe un dominio con ligaduras integrales.

****** Dominio

\[
{\cal D}_{G} = \left\{\begin{array}{c}
\vec{y}(x) = (y_1(x),\dots,y_n(x)) \\
y_i \in {\cal C}^1[x_0,x_1] \\
\vec{y}(x_0) = \vec{y}_0 \\
\vec{y}(x_1) = \vec{y}_1 \\
\forall x \in [x_0,x_1]\colon (\vec{y}(x),\vec{y}'(x)) \in \Omega \\
\int_{x_0}^{x_1} G_1(x,y_1,\dots,y_n,y'_1,\dots,y'_n)\, dx = L_1 \\
\vdots\\
\int_{x_0}^{x_1} G_m(x,y_1,\dots,y_n,y'_1,\dots,y'_n)\, dx = L_m
\end{array}\right\}
\]

****** Detalle importante
AquÃ­ los $L_i$ no tienen por quÃ© ser $0$.
***** TODO Card                                                                                             :drill:
:PROPERTIES:
:ID:       2b3f9246-bbdd-45b3-9684-6ee284f3bbc4
:END:
**** 4.4. Ejemplos
***** Ejemplo: Problema de Dido
Queremos maximizar el Ã¡rea que encierra una curva cerrada de longitud
finita.

****** Planteamiento
Consideramos $\Gamma$ la traza de $\alpha \colon [0,1] \to \mathbb{R}^2$ en sentido antihorario llamando 
$\alpha(t) = (x(t),y(t))$, y cumpliendo que $\alpha(0) = \alpha(1)$. El Ã¡rea que
encierra $\alpha$ es

\[
A(x,y) = \frac{1}{2}\int^1_0 x(t)y'(t) - x'(t)y(t)\,dt.
\]

Y queremos minimizar este funcional sobre el siguiente conjunto.

\[
{\cal D}_{G} = \left\{\begin{array}{c}
x,y \in {\cal C}^1[x_0,x_1] \\
x_0=x_1 \\
y_0=y_1 \\
\int_0^1 \sqrt{(x'(t))^2 + (y'(t))^2} \,dt = L
\end{array}\right\}
\]

Tenemos por tanto $n = 2$ incÃ³gnitas y $m = 1$ ligaduras integrales.
Tenemos definidas las funciones

 * $F(t,x,y,p,q) = \frac{1}{2}(xq - yp)$

 * $G(x,y,p,q) = \sqrt{p^2 + q^2}$

****** Lagrangiana
Teniendo la lagrangiana

\[
F^{\ast} = F - \lambda G = \frac{1}{2}(xq-yp) - \lambda \sqrt{p^2 + q^2}
\]

podemos aplicar el [[id:e37d89ff-11da-493c-ab46-2000fe221da9][Teorema 14]] sobre ligaduras integrales para tener
las siguientes ecuaciones asociadas.

\[\pdv{F^{\ast}}{x} - \dv{}{t}\pdv{F^{\ast}}{p} = 0\]

\[
\pdv{F^{\ast}}{y} - \dv{}{t}\pdv{F^{\ast}}{p} = 0
\]

Desde ellas podemos obtener dos constantes

\[
y + \lambda \frac{x'}{\sqrt{x'^2 + y'^2}} = y_0, \qquad
x + \lambda \frac{y'}{\sqrt{x'^2 + y'^2}} = x_0;
\]

que dan como soluciÃ³n una circunferencia de centro $(x_0,y_0)$ y radio $|\lambda|$.

***** Ejemplo: Catenaria
Queremos hallar la forma de una cuerda de longitud $L > 1$ colgando de
dos postes a altura $h$ separados a $1$ unidad de distancia. 

****** Planteamiento
Minimizaremos

\[
{\cal F}[y] = \int_0^1 y(x) \sqrt{1 + (y'(x))^2}\,dx
\]

sujeto a

\[
{\cal D}_{G} = \left\{\begin{array}{c}
y \in {\cal C}^1[0,1] \\
y(0) = y(1) = h,\\
\int_0^1 \sqrt{1 + (y'(x))^2} \,dx = L
\end{array}\right\}
\]

Tenemos por tanto $n = 1$ incÃ³gnitas y $m = 1$ ligaduras integrales.
Tenemos definidas las funciones

 * $F(x,y,p) = y\sqrt{1 + p^2}$,

 * $G(x,y,p) = \sqrt{1 + p^2}$.

****** Lagrangiana
Tenemos la lagrangiana

\[
F^{\ast} = (y - \lambda)\sqrt{1 + p^2}
\]

que al no depender de $x$ directamente, simplifica su condiciÃ³n de
Euler-Lagrange a $F - y'F_p = \mathrm{cte.} = C$. Resolviendo obtenemos

\[
y = \lambda + C\sqrt{1+y'^2}.
\]

Esta es una ecuaciÃ³n diferencial que hemos tratado anteriormente
con soluciÃ³n

\[
y(x) = \lambda + C \cosh \left( \frac{x-k}{C} \right).
\]

****** DeterminaciÃ³n de las constantes
Imponiendo la ligadura $y(0) = y(1)$ obtenemos $k = 1/2$. Imponiendo
la ligadura integral tenemos

\[
L = \int_0^1 \sqrt{1 + \sinh(\dots)^2}\,dx = \int_0^1 \cosh(\dots)\,dx = 2C \sinh\frac{1}{2C}
\]

que determina $C$ de forma Ãºnica cuando $L > 1$.

****** Dato extra
Si ademÃ¡s imponemos $y(0) = h$, entonces

\[
\lambda = h - C_0 \cosh \left( \frac{1}{2C_0} \right)
\] 

y podemos demostrar que

\[
\lambda = h - \frac{1}{2}\sqrt{4C_0^2 + L}.
\]

*** 5. Funcionales dependientes de funciones de varias variables
**** 5.0. EcuaciÃ³n de Euler-Lagrange dependiente de varias variables
***** EcuaciÃ³n de Euler-Lagrange con funciones de varias variables
Para

\[
{\cal F}[u] = \int_{\Omega} F \left( x_1,x_2,u(x),u_{x_1}(x),u_{x_2}(x) \right) \,dx,
\]

actuando en ${\cal D} = \left\{ u \in {\cal C}^1(\overline{\Omega}) \;\middle|\; \forall x \in \partial \Omega\colon u(x) = \gamma(x)  \right\}$. Los extremales
$u \in {\cal C}^2(\Omega)$ cumplen

\[
F_u - \dv{}{x_1} F_p - \dv{}{x_2}F_q = 0.
\]

****** Card: caso considerado                                                                              :drill:
SCHEDULED: <2018-08-05 Sun>
:PROPERTIES:
:ID:       d3ca9936-b4c5-4eba-a576-293beefb4226
:DRILL_LAST_INTERVAL: 59.746
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:19]
:END:
Da la ecuaciÃ³n de Euler-Lagrange para un funcional

\[
{\cal F}[u] = \int_{\Omega} F \left( x_1,x_2,u(x),u_{x_1}(x),u_{x_2}(x) \right) \,dx,
\]

******* EcuaciÃ³n

\[
F_u - \dv{}{x_1} F_p - \dv{}{x_2}F_q = 0.
\]

****** Card: caso con tres variables                                                                       :drill:
SCHEDULED: <2018-06-27 Wed>
:PROPERTIES:
:ID:       a66957ab-1847-466f-b93c-813e5cb671b3
:DRILL_LAST_INTERVAL: 40.3114
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:07]
:END:
Da la ecuaciÃ³n de Euler-Lagrange para un funcional

\[
{\cal F}[u] = \int_{\Omega} F(t,x,y,u,u_t,u_x,u_y) \,dt\,dx\,dy
\]

******* EcuaciÃ³n

\[
F_u - \dv{}{t}F_r - \dv{}{x}F_p - \dv{}{y}F_q
\]

***** Teoremas anÃ¡logos de convexidad
Se tienen en estas condiciones

 * el [[id:89c9e4a0-cc09-4962-b3e5-8fc1cfbb0c4e][Teorema 6]],
 * el [[id:c46055cc-5d8b-4772-8af3-48057d7e45a4][Teorema 7]], y
 * el [[id:4d623017-26e1-4e3e-82ec-325f4b6c0b38][Teorema 8]].

**** 5.1. La membrana vibrante
***** Laplaciana
:PROPERTIES:
:ID:       1ef43534-3455-4e69-bd5f-c9cb72a1eb85
:END:
La *laplaciana* es la suma de todas las segundas derivadas
sin mezclar.

\[
\Delta f = \sum_{i=1}^n{\pdv[2]{f}{x_i}}.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-29 Wed>
:PROPERTIES:
:ID:       a5ac910f-d07f-4938-afb0-2eba7a25e7c9
:DRILL_LAST_INTERVAL: 63.3234
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:20]
:END:
Da la definiciÃ³n de laplaciana y distÃ­nguela de la definiciÃ³n
de gradiente.

******* Laplaciana
La *laplaciana* es la suma de todas las segundas derivadas
sin mezclar:

\[
\Delta f = \sum_{i=1}^n{\pdv[2]{f}{x_i}}.
\]

******* Gradiente
El *gradiente* es el vector de las primeras derivadas, una
particularizaciÃ³n del jacobiano.

\[
(\nabla f)(p) = \left( \pdv{f}{x_1}(p), \dots, \pdv{f}{x_n}(p) \right) = 
(f_{x_1}(p), \dots,f_{x_n}(p)).
\]

***** Divergencia
La *divergencia* es la suma de todas las primeras derivadas en
cada una de las componentes de un campo vectorial $\mathbb{R}^n \to \mathbb{R}^n$,

\[
\mathrm{div}(f) = \sum_{i=1}^n \pdv{f_i}{x_i}
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       9ceb6b3f-39fd-45f8-8d13-6d6c857f5ada
:DRILL_LAST_INTERVAL: 30.3428
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:14]
:END:
Define la *divergencia* en cÃ¡lculo vectorial, $\mathrm{div}(f)$.

******* Divergencia

\[
\mathrm{div}(f) = \sum_{i=1}^n \pdv{f_i}{x_i}
\]

***** Marco del problema
Consideramos una membrana sujeta por el borde, llamando $u(t,x)$ al
a la *altura* de un punto de la membrana, que estarÃ¡ en $\Omega \subseteq \mathbb{R}^N$.
Llamamos $\varphi(x) \in {\cal C}^1(\Omega)$ a la *posiciÃ³n de reposo o equilibrio* de la
que parte, que usualmente se toma constante cero.

****** Modelo inicial

\[\begin{aligned}
{\cal F}[u] =
C(\varphi) &+ \frac{1}{2} \int_0^{\infty}\int_{\Omega} m \abs{\pdv{u}{t}}^2 \,d(x,y)dt 
\\& + \int_0^{\infty}\int_{\Omega} -\sigma 
  \sqrt{1 + \abs{\nabla u}^2} + f(x)u - \frac{\alpha}{2}(u - \varphi)^2\,dxdt
\\& + \int_0^{\infty} \int_{\partial\Omega} g_1(x)u - \frac{\alpha_1}{2}(u - \varphi)^2\,dSdt
\end{aligned}\]

Normalmente sÃ³lo aparecerÃ¡ este funcional simplificado.

****** SimplificaciÃ³n 1: equilibrio nulo.
Asumimos la posiciÃ³n de equilibrio $\varphi(x) = 0$ horizontal.

****** SimplificaciÃ³n 2: deformaciones pequeÃ±as (Taylor).
Utilizamos el desarrollo de Taylor

\[
\sqrt{1 + s^2} = 1 + \frac{s^2}{2} - \frac{s^4}{8} + {\cal O}(s^6).
\]

para simplificar el tÃ©rmino $\sqrt{1 + \abs{\nabla u}^2}$ despreciando ${\cal O}(\abs{\nabla u}^4)$
para aproximar

\[
\int_{\Omega}\sigma \sqrt{1 + \abs{\nabla u}^2 \,dx} \sim C + \frac{1}{2}\int_{\Omega} \sigma \abs{\nabla u}^2\,dx.
\]

****** SimplificaciÃ³n 3: caso estacionario.
La $u$ es independiente del tiempo.

***** Modelo de la membrana simplificado
:PROPERTIES:
:ID:       f0ac6c21-d9c3-4298-b48e-8602ca0948a5
:END:
Si asumimos todas las simplificaciones posibles,

\[
{\cal F}[u] = \int_{\Omega} \left( 
\frac{\sigma(x)}{2}\abs{\nabla u}^2 + \frac{\alpha}{2}u^2 - f(x)u
\right)\,dx
\]

sobre $u \in {\cal C}^1_0(\Omega)$. Si no asumimos borde nulo tendremos $u \in {\cal C}^1(\overline{\Omega})$
con $u = \gamma = \varphi$ en $\partial\Omega$.

****** EcuaciÃ³n
Desde el que llegamos a

\[
-\mathrm{div}(\sigma\Delta_x u) = f - \alpha u.
\]

# Comprobar, en los apuntes usa la divergencia y no parece que salga
# el mismo resultado ni lo que sale haciendo Euler-Lagrange cuando 
# la sigma no es constante.

Suponiendo $\sigma$ constante llegamos a

\[
-\sigma \Delta_{x} = f - \alpha u
\]

******* Laplaciana en variables espaciales
NÃ³tese que en este caso nos interesa sÃ³lo la [[id:1ef43534-3455-4e69-bd5f-c9cb72a1eb85][laplaciana]] respecto de
las variables no temporales.

\[
\Delta_xu := \sum_{i=1}^n \pdv[2]{u}{x_i}
\]

NÃ³tese que $u$ no depende del tiempo por las simplificaciones.

****** Proof
La $F$ asociada estÃ¡ en los apuntes. La ecuaciÃ³n se obtiene
usando la definiciÃ³n de la laplaciana $\Delta u$.
****** Card: modelo simplificado de la membrana                                                            :leech:
SCHEDULED: <2018-05-07 Mon>
:PROPERTIES:
:ID:       41429f6b-f4a0-496b-9f2b-d3c27f11c3eb
:DRILL_LAST_INTERVAL: 3.7493
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-03 Thu 12:42]
:END:
Enuncia el funcional del modelo simplificado de la membrana.

******* Funcional

\[
{\cal F}[u] = \int_{\Omega} \left( 
\frac{\sigma(x)}{2}\abs{\nabla u}^2 + \frac{\alpha}{2}u^2 - f(x)u
\right)\,dx
\]

****** Card: ecuaciÃ³n de la membrana                                                                       :drill:
:PROPERTIES:
:ID:       87e0aead-2a8b-4392-a283-3030da911c37
:END:
***** EcuaciÃ³n de Poisson
Tomando $\sigma = \mathrm{cte}$ (nula serÃ­a trivial) y $\alpha = 0$ en el modelo de la
[[id:f0ac6c21-d9c3-4298-b48e-8602ca0948a5][membrana]] tenemos la *EcuaciÃ³n de Poisson*.

\[
-\Delta_x u = f/\sigma.
\]

****** TODO Card: ecuaciÃ³n de Poisson                                                                      :leech:
SCHEDULED: <2018-05-28 Mon>
:PROPERTIES:
:ID:       72b375cf-bcc8-4ca6-aeea-8e220904f72f
:DRILL_LAST_INTERVAL: 9.7942
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.167
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:10]
:END:
Enuncia la EcuaciÃ³n de Poisson.

******* EcuaciÃ³n

\[
-\Delta_x u = f/\sigma.
\]

****** Card: proviene de                                                                                   :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       2a969466-4703-47e1-974b-eb5ea28a3bd0
:DRILL_LAST_INTERVAL: 30.885
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:10]
:END:
Â¿De dÃ³nde proviene la ecuaciÃ³n de Poisson?

\[
-\Delta_x u = f/\sigma.
\]

******* Respuesta
Del modelo simplificado de la membrana.

\[
{\cal F}[u] = \int_{\Omega} \left( 
\frac{\sigma(x)}{2}\abs{\nabla u}^2 + \frac{\alpha}{2}u^2 - f(x)u
\right)\,dx
\]

Tomando $\sigma$ constante tenemos Euler-Lagrange

\[
-\sigma \Delta_{x}u = f - \alpha u
\]

Tomando $\alpha = 0$ tenemos la EcuaciÃ³n de Poisson.

***** Modos de vibraciÃ³n                                                                                    :extra:
***** Problema de Dirichlet
:PROPERTIES:
:ID:       8e9ccec0-c9de-4d00-a08e-3de44d448ca5
:END:
Tomando $\alpha = f = 0$, $\sigma = \mathrm{cte.}$ en el modelo de la [[id:8e9ccec0-c9de-4d00-a08e-3de44d448ca5][membrana]], y no
asumiendo borde fijo llamamos *funcional de Dirichlet* al

\[
{\cal F}[u] = \int_{\Omega} \abs{\nabla u}^2\,dx.
\]

que lleva a

\[\left\{\begin{array}{ll}
\Delta_x u(x) = 0 &\mbox{ si } x \in \Omega\\
u(x) = \gamma(x) &\mbox{ si } x \in \partial\Omega
\end{array}\right.\]

y podemos comprobar por convexidad que la soluciÃ³n es Ãºnica.

****** Proof
Tenemos

$F = \sum_i p_i^2$

convexa, [[id:4d623017-26e1-4e3e-82ec-325f4b6c0b38][luego]] ${\cal F}$ convexa y los [[id:89c9e4a0-cc09-4962-b3e5-8fc1cfbb0c4e][extremales]] son mÃ­nimos globales.

***** TODO Superficie minimal
***** EcuaciÃ³n de ondas
:PROPERTIES:
:ID:       6412aa5f-6f69-4458-9832-134ab22707b3
:END:
La *ecuaciÃ³n de ondas* sin el caso estacionario. Tomando $\alpha$ y $\sigma$ constantes,

\[
{\cal F}[u] = \int_0^{\infty}\int_{\Omega}
\left(\frac{(\partial_t u)^2}{2} - \frac{\sigma}{2}\abs{\nabla u}^2 + fu - \alpha\frac{u^2}{2}\right)
\,dx\,dt.
\]

Sobre

\[{\cal D} = \left\{\begin{array}{lr}
u \in {\cal C}^1([0,T] \times \Omega), &\\
u(0,x) = \varphi(x),& x\in \Omega \\
u(T,x) = \mathrm{dado},& x \in \Omega \\
u(t,x) = \varphi(x), & t \geq 0, x \in \partial\Omega
\end{array}\right\}\]

Luego

\[
F = \frac{1}{2}
\left( r^2 - \sigma(p^2+q^2) + 2fu - \alpha u^2 \right).
\]

Y la condiciÃ³n de Euler-Lagrange se llama *ecuaciÃ³n de ondas*.

\[
\partial^2_{tt}u = f(x) - \alpha u  + \sigma \Delta_x u.
\]

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-08-26 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: multisided
:ID:       d2737759-5b77-4e0f-8860-769f0dae8897
:DRILL_LAST_INTERVAL: 58.9259
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 1.94
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:13]
:END:
EcuaciÃ³n de ondas.

******* EcuaciÃ³n

\[
u_{tt} = f - \alpha u + \sigma \Delta_x u.
\]

******* Funcional

\[
{\cal F}[u] = \int_0^{\infty}\int_{\Omega}
\left(\frac{u^2_t}{2} - \frac{\sigma}{2}\abs{\nabla u}^2 + fu - \alpha\frac{u^2}{2}\right)
\,dx\,dt.
\]

******* Dominio del funcional

\[{\cal D} = \left\{\begin{array}{lr}
u \in {\cal C}^1([0,T] \times \Omega), &\\
u(0,x) = \varphi(x),& x\in \Omega \\
u(T,x) = \mathrm{dado},& x \in \Omega \\
u(t,x) = \varphi(x), & t \geq 0, x \in \partial\Omega
\end{array}\right\}\]

**** 5.2. ResoluciÃ³n: separaciÃ³n de variables y necesidad del desarrollo en serie
***** EcuaciÃ³n de ondas unidimensional simplificada
:PROPERTIES:
:ID:       e3c0e5df-d8b2-495c-9993-bd2b93d4782e
:END:
Sea [[id:6412aa5f-6f69-4458-9832-134ab22707b3][ecuaciÃ³n de ondas]] unidimensional con $\sigma = c^2$ y $\alpha = f = 0$,

\[
{\cal F}[u] = 
\frac{1}{2}\int_0^{\infty}\int_{0}^L
\left( u_t^2 - c^2 u_x^2 \right)\,dxdt.
\]

Obteniendo

\[
u_{tt} = c^2 u_{xx}.
\]

Compensando el no fijar condiciÃ³n en $T = \infty$, fijamos derivada inicial.

\[\left\{\begin{array}{lr}
u(0,x) = u_0(x) \in {\cal C}^2, & x \in [0,L], \\
u_t(0,x) = v_0(x) \in {\cal C}^1, & x \in [0,L], \\
u(t,0) = u(t,L) = 0, & t \geq 0.
\end{array}
\right\}\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-02 Mon>
:PROPERTIES:
:DRILL_CARD_TYPE: multisided
:ID:       5c25baff-dce3-4454-b5e5-baf0ac520cdc
:DRILL_LAST_INTERVAL: 27.7362
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 3.125
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:06]
:END:
EcuaciÃ³n de ondas unidimensional simplificada.

******* Funcional

\[
{\cal F}[u] = 
\frac{1}{2}\int_0^{\infty}\int_{0}^L
\left( u_t^2 - c^2 u_x^2 \right)\,dx\,dt.
\]

******* EcuaciÃ³n

\[
u_{tt} = c^2 u_{xx}.
\]

******* Dominio del funcional

\[{\cal D} = \left\{\begin{array}{lr}
u(0,x) = u_0(x) \in {\cal C}^2, & x \in [0,L], \\
u_t(0,x) = v_0(x) \in {\cal C}^1, & x \in [0,L], \\
u(t,0) = u(t,L) = 0, & t \geq 0.
\end{array}
\right\}\]

***** SoluciÃ³n de la ecuaciÃ³n de ondas
La Ãºnica soluciÃ³n de la [[id:e3c0e5df-d8b2-495c-9993-bd2b93d4782e][ecuaciÃ³n de ondas simplificada]] es la serie

\[
\sum_{n \geq 1} \left( 
a_n\cos\left(\frac{nc\pi t}{L}\right) +
\frac{b_nL}{nc\pi} \sin \left( \frac{nc\pi t}{L} \right)
\right)
\sin \left( \frac{n\pi x}{L} \right)
\]

donde hemos desarrollado

\[
u_0(x) = \sum_{n \geq 1} a_n \sin \left( \frac{n\pi x}{L} \right),
\qquad
v_0(x) = \sum_{n \geq 1} b_n \sin \left( \frac{n\pi x}{L} \right).
\]

****** Proof: separaciÃ³n de variables y principio de superposiciÃ³n
Buscamos una soluciÃ³n de la forma $u(t,x) = T(t)W(x)$, que
si es soluciÃ³n debe cumplir

\[
T''(t)W(x) = c^2T(t)W''(x),
\]

por lo que, se tienen las dos funciones siguientes independientes
de la otra variable y constantes $T''(t)/T(t) = c^2W''(x)/W(x) = -\lambda$.

Si existieran deberÃ­an de cumplir

 * $T'(t) + \lambda T(t) = 0$,

 * $W'(t) + \frac{\lambda}{c^2} W(t) = 0$,

donde ademÃ¡s $W(0) = W(L) = 0$ si $T$ no es nula. El problema de resolver
una ecuaciÃ³n de este tipo es un problema de [[id:1e2009c1-db10-4490-af5c-db57a46dab0f][Sturm-Liouville]] que hemos
[[id:e15aa2f8-76b4-4aa6-82bb-c0b424bda884][tratado ya]] y con soluciones $\lambda_n = (nc\pi/L)^2$ y $W_n(x) = \sin(n\pi x/L)$.

Ahora, al resolver la primera ecuaciÃ³n para $T$ obtendrÃ­amos

\[
T_n(t) = A_n \cos(\frac{nc\pi t}{L}) + B_n \sin(\frac{nc\pi t}{L})
\]

dÃ¡ndonos finalmente

\[
u_n(t,x) = 
\sin(\frac{n\pi x}{L}) 
\left( A_n\cos(\frac{nc\pi t}{L}) + B_n\sin(\frac{nc\pi t}{L}) \right).
\]

El problema es que al imponer las condiciones de contorno que tenÃ­amos
llegamos a un punto en el que no estÃ¡ claro cÃ³mo seguir.

  * $u_n(0,x) = A_n\sin(\frac{n\pi x}{L}) = u_0(x)$,

  * $\partial_t u_n(0,x) = B_n \frac{nc\pi}{L}\sin(\frac{n \pi x}{L}) = v_0(x)$.

Para solucionar esto usaremos que la suma de dos soluciones cualesquiera
de [[id:e3c0e5df-d8b2-495c-9993-bd2b93d4782e][nuestro problema]] vuelve a ser soluciÃ³n, y que toda funciÃ³n con cierta
regularidad y con $u_0(0) = u_0(L) = 0$ serÃ¡ una serie de la forma

\[
u_0(x) = \sum_{n=1}^{\infty} a_n\sin(\frac{n\pi x}{L}).
\]

Si ajustamos los coeficientes llegamos a la soluciÃ³n buscada.

****** Card: procedimiento                                                                                 :drill:
SCHEDULED: <2018-09-20 Thu>
:PROPERTIES:
:ID:       8be7d27b-0a8a-43b6-a7a1-3086967a27e5
:DRILL_LAST_INTERVAL: 82.9064
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:56]
:END:
Â¿CuÃ¡l es el procedimiento de resoluciÃ³n de la ecuaciÃ³n de ondas
simplificada?

\[
u_{tt} = c^2 u_{xx}.
\]

\[\left\{\begin{array}{lr}
u(0,x) = u_0(x) \in {\cal C}^2, & x \in [0,L], \\
u_t(0,x) = v_0(x) \in {\cal C}^1, & x \in [0,L], \\
u(t,0) = u(t,L) = 0, & t \geq 0.
\end{array}
\right\}\]

******* Procedimiento

1. Buscamos soluciÃ³n $u(t,x) = T(t)W(x)$.

2. Llegamos a un Sturm-Liouville en $W$ que nos da
   $\lambda_n = (nc\pi / L)^2$ y $W_n(x) = \sin(n\pi x/L)$. Resolvemos
   para $T$.

3. Desarrollamos en serie de senos $u_0$ y $v_0$.

4. Por principio de superposiciÃ³n ajustamos coeficientes para
   llegar a la soluciÃ³n.

**** 5.3. Problemas de Sturm-Liouville
***** Problema de Sturm-Liouville
:PROPERTIES:
:ID:       1e2009c1-db10-4490-af5c-db57a46dab0f
:END:
Un *problema de Sturm-Liouville* consiste en encontrar las
$\lambda \in \mathbb{R}$ donde el PC tiene soluciones no nulas.

\[\left.\begin{array}{c}
\forall x \in [x_0,x_1]\colon\qquad (Py')' + (Q + \lambda S) y = 0 \\
a_0y(x_0) + b_0y'(x_0) = 0,\\
a_1y(x_1) + b_1y'(x_1) = 0.
\end{array}\right\}\]

Con $Q,S \in {\cal C}^0$, $P \in {\cal C}^1$ positivas $P > 0$, y $S > 0$;
y condiciones de contorno no triviales,
$|a_0| + |b_0| > 0$ y $|a_1|+|b_1|> 0$.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-06-08 Fri>
:PROPERTIES:
:DRILL_CARD_TYPE: multisided
:ID:       dac10bbc-2e51-40da-a9e4-4a7e80bb50cf
:DRILL_LAST_INTERVAL: 4.0623
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.833
:DRILL_EASE: 2.08
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:20]
:END:
Â¿QuÃ© es un problema de Sturm-Liuville?

******* Problema
Encontrar $\lambda \in \mathbb{R}$ para los cuales la siguiente
ecuaciÃ³n tiene soluciones no nulas.

\[\left.\begin{array}{c}
\forall x \in [x_0,x_1]\colon\qquad (Py')' + (Q + \lambda S) y = 0 \\
a_0y(x_0) + b_0y'(x_0) = 0,\\
a_1y(x_1) + b_1y'(x_1) = 0.
\end{array}\right\}\]

Se asumen ademÃ¡s ciertas condiciones.

******* Condiciones

 1. $Q,S$ continuas, $P \in {\cal C}^1$,

 2. $P > 0$ y $S > 0$,

 3. las condiciones de contorno no son triviales, teniÃ©ndose
    $|a_0| + |b_0| > 0$ y $|a_1|+|b_1|> 0$

****** Card                                                                                                :drill:
:PROPERTIES:
:ID:       8f362b9c-11c5-4954-b4f1-c4384b474a29
:END:
EcuaciÃ³n de un Sturm-Liouville.

******* Answer
Buscamos lambdas para que la siguiente ecuaciÃ³n tenga soluciÃ³n no nula.

\[
(Py')' + (Q + \lambda S) y = 0
\]

***** Teorema 15. TeorÃ­a de Sturm-Liouville
Los valores propios de un problema de Sturm-Liouville forman una
sucesiÃ³n creciente y divergente $\lambda_1< \lambda_2< \dots\to \infty$. Sus funciones
propias normalizadas $\|y_n\|_{L^2_S} = 1$ 

 * son Ãºnicas salvo signo

 * $y_n$ tiene $n-1$ ceros en $(x_0,x_1)$,

 * son ortogonales, para $n \neq m$

   \[
   \pair{y_n,y_m} = \int_{x_0}^{x_1} y_n(x)y_m(x) S(x)\,dx = 0;
   \]

 * cualquier funciÃ³n $y \in L^2_S(x_0,x_1)$ puede ser desarrollada en serie
   de funciones propias

   \[
   y(x) = \sum_{n=1}^{\infty}\pair{y,y_n}y_n.
   \]

En el caso de condiciones periÃ³dicas en el problema, un mismo valor
propio puede tener dos funciones propias y la ortogonalidad no se
tiene directamente.

****** Proof                                                                                               :extra:
# Fuera de los objetivos del curso.
****** Card: los valores propios                                                                           :drill:
SCHEDULED: <2018-09-01 Sat>
:PROPERTIES:
:ID:       3458d9e8-3283-4f9c-908d-52ac8c553a42
:DRILL_LAST_INTERVAL: 64.7041
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:09]
:END:
Los valores propios de un Sturm-Liouville $\lambda_i$ forman
una sucesiÃ³n [creciente y divergente $\lambda_1< \lambda_2< \dots\to \infty$].

****** Card: funciones propias                                                                             :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       ea61c9ca-13ef-4a74-8c5e-14ced1b5a060
:DRILL_LAST_INTERVAL: 32.4906
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:13]
:END:
QuÃ© cuatro propiedades verifican las funciones propias $f_n$ de un
Sturm-Liouville.

******* Propiedades
Cuando las *normalizamos*, $\|y_n\|_{L^2_S} = 1$, verifican

 * su normalizada es Ãºnica salvo signo,

 * la $y_n$ tiene $n-1$ ceros en $(x_0,x_1)$

 * son ortogonales,

 * toda funciÃ³n $L^2_S(x_0,x_1)$ se desarrolla en serie de funciones
   propias.

****** Card: el caso periÃ³dico                                                                           :nodrill:
SCHEDULED: <2018-06-08 Fri>
:PROPERTIES:
:ID:       d398f6f5-55e9-4dfc-98ad-ef48ad0d3039
:DRILL_LAST_INTERVAL: 20.9843
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:04]
:END:
Â¿QuÃ© cambia en la teorÃ­a de Sturm-Liouville en el caso periÃ³dico?

******* Respuesta
Cada valor propio puede tener asociadas dos funciones y la
ortogonalidad no se tiene directamente.

***** CÃ¡lculo variacional en Sturm-Liouville
:PROPERTIES:
:ID:       99594c9a-d729-4e87-b886-c4ee6899d0d7
:END:
Dado el funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} \Big( P(x)(y'(x))^2 - Q(x)y(x)^2 \Big)\,dx
\]

en

\[{\cal D}_1 = \left\{\begin{array}{c}
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y(x_1) = 0 \\
\|y\|^2_{L^2_S} = \int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right\}
\]

podemos aplicar el [[id:e37d89ff-11da-493c-ab46-2000fe221da9][Teorema 14]] para obtener que un mÃ­nimo debe cumplir

\[\left\{\begin{array}{c}
(Py')' + (Q + \lambda S)y = 0, \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right.\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-16 Sat>
:PROPERTIES:
:ID:       78956ad4-f53c-484e-b5c2-59b62ad5f391
:DRILL_LAST_INTERVAL: 22.662
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.2
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:48]
:END:
Â¿QuÃ© funcional bajo quÃ© condiciones nos da un problema de
Sturm-Liouville?

\[\left\{\begin{array}{c}
(Py')' + (Q + \lambda S)y = 0, \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right.\]

******* Funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} \Big( P(x)(y'(x))^2 - Q(x)y(x)^2 \Big)\,dx
\]

******* Condiciones

\[{\cal D}_1 = \left\{\begin{array}{c}
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y(x_1) = 0 \\
\|y\|^2_{L^2_S} = \int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right\}
\]

***** Teorema 16: problemas variacionales con Sturm-Liouville
En las condiciones de un problema de Sturm-Liouville, las funciones
propias $y_n$ se obtienen al minimizar el ${\cal F}$ [[id:99594c9a-d729-4e87-b886-c4ee6899d0d7][anterior]] sobre

\[
{\cal D}_n = \left\{\begin{array}{c}
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1 \\
\int_{x_0}^{x_1} y(x)y_1(x)S(x)\,dx = 0 \\
\vdots\\
\int_{x_0}^{x_1} y(x)y_{n-1}(x)S(x)\,dx = 0
\end{array}\right\}
\]

teniÃ©ndose ademÃ¡s que ${\cal F}[y_n] = \lambda_n$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: multisided
:ID:       b29a89a5-093b-41e9-84d3-c47685ee9ad9
:DRILL_LAST_INTERVAL: 34.2454
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 11
:DRILL_FAILURE_COUNT: 4
:DRILL_AVERAGE_QUALITY: 2.455
:DRILL_EASE: 1.8
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:11]
:END:

Sturm-Liouville y problemas variacionales

******* Funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} \Big( P(x)(y'(x))^2 - Q(x)y(x)^2 \Big)\,dx
\]

sobr el dominio

\[{\cal D}_1 = \left\{\begin{array}{c}
y \in {\cal C}^0(x_0,x_1) \\
y(x_0) = y(x_1) = 0 \\
\|y\|^2_{L^2_S} = \int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right\}
\]

******* Problema asociado

\[\left\{\begin{array}{c}
(Py')' + (Q + \lambda S)y = 0, \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1.
\end{array}\right.\]

******* Teorema de caracterizaciÃ³n de valores propios
Las funciones propias $y_n$ se obtienen al minimizar el ${\cal F}$ [[id:99594c9a-d729-4e87-b886-c4ee6899d0d7][anterior]]
sobre

\[
{\cal D}_n = \left\{\begin{array}{c}
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1 \\
\int_{x_0}^{x_1} y(x)y_1(x)S(x)\,dx = 0 \\
\vdots\\
\int_{x_0}^{x_1} y(x)y_{n-1}(x)S(x)\,dx = 0
\end{array}\right\}
\]

teniÃ©ndose ademÃ¡s que ${\cal F}[y_n] = \lambda_n$.

****** Card: enunciado                                                                                     :drill:
SCHEDULED: <2018-08-29 Wed>
:PROPERTIES:
:ID:       35437f0b-383e-45ab-ba01-c030fbeb1d98
:DRILL_LAST_INTERVAL: 63.493
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:20]
:END:
Enuncia el Teorema de caracterizaciÃ³n variacional de funciones
y valores propios de un Sturm-Liouville.

******* Enunciado
En las condiciones de un problema de Sturm-Liouville, las funciones
propias $y_n$ se obtienen al minimizar el ${\cal F}$ [[id:99594c9a-d729-4e87-b886-c4ee6899d0d7][anterior]] sobre

\[
{\cal D}_n = \left\{\begin{array}{c}
y \in {\cal C}^1(x_0,x_1) \\
y(x_0) = y(x_1) = 0, \\
\int_{x_0}^{x_1} y^2(x)S(x)\,dx = 1 \\
\int_{x_0}^{x_1} y(x)y_1(x)S(x)\,dx = 0 \\
\vdots\\
\int_{x_0}^{x_1} y(x)y_{n-1}(x)S(x)\,dx = 0
\end{array}\right\}
\]

teniÃ©ndose ademÃ¡s que ${\cal F}[y_n] = \lambda_n$.

******* Funcional

\[
{\cal F}[y] = \int_{x_0}^{x_1} \Big( P(x)(y'(x))^2 - Q(x)y(x)^2 \Big)\,dx
\]

*** 5b. Series de Fourier
**** 5.4. Fundamentos del anÃ¡lisis funcional                                                                 :extra:
***** Espacio considerado
Consideramos el espacio de Hilbert $L^2(I)$ con producto escalar

\[
\pair{u,v} = \int_I u(x)v(x)\,dx.
\]

***** Base hilbertiana
Un *sistema ortonormal* es una sucesiÃ³n $\left\{ y_n \right\}_{n \in \mathbb{N}}$ tales que

\[
\pair{y_n,y_m} = \delta_{n,m}.
\]

Se le llama *base hilbertiana* si genera un subespacio denso.

\[
\overline{L(y_1,y_2,\dots)} = L^2(I).
\]

***** Teorema 17. ProyecciÃ³n ortogonal y mejor aproximaciÃ³n
Dado un espacio de Hilbert $H$ y un sistema ortonormal $\left\{ y_n \right\}$,
si llamamos $Y_n = L(y_1,\dots,y_{N})$, tenemos la *proyecciÃ³n ortogonal*
de $v$ sobre $Y_N$ dada por

\[
P_N(v) = \sum_{i=1}^N \pair{v,y_n}y_n.
\]

La proyecciÃ³n se caracteriza por

 * $v - P_N(v)$ es ortogonal a $Y_n$,
 * $\| v - P_N(v)\|_{H} \leq \|v - y\|_H$ para todo $y \in Y_n$.

Si ademÃ¡s el sistema es base hilbertiana se tiene

\[
\lim_{N \to \infty} \|v - P_N(v)\|_H = 0.
\]

Llamamos *serie de Fourier* de $v$ relativa al sistema ortonormal
a la que define el lÃ­mite anterior

\[
\sum_{i=1}^{\infty} \pair{v,y_n} y_n = v.
\]

***** Teorema 18. Desigualdad de Bessel e Igualdad de Bessel
Para un espacio de Hilbert $H$ y un sistema ortonormal $\left\{ y_n \right\}$ se
tiene la *Desigualdad de Bessel*: para cualquier $v \in H$,

\[
\sum_{i=0}^{\infty} |\pair{v,y_n}^2| \leq \|v\|^2_H.
\]

El sistema es completo (la base es Hilbertiana) si y sÃ³lo si
se tiene la *identidad de Bessel*: para cualquier $v \in H$,

\[
\sum_{i=0}^{\infty} |\pair{v,y_n}^2| = \|v\|^2_H.
\]

***** TODO Nota 19. Caracterizaciones de base hilbertiana

***** Teorema 19. Series de Fourier
Dado un espacio de Hilbert $L^2_S(x_0,x_1)$, un sistema ortonormal
completo $\left\{ y_n(x) \right\}$, una funciÃ³n $y \in L^2_S(x_0,x_1)$ y su serie de Fourier
$P_N(x)$, se cumplen.

 1. para $y$ continua y $P_N(y)$ uniformemente convergente, la suma
    de la serie es continua;

 2. si $P_N(y)$ es uniformemente convergente entonces la integral
    conmuta con la suma

    \[
    \int_{x_0}^{x_1}\sum_{n=1}^{\infty} c_ny_n(x)\,dx =
    \sum_{n=1}^{\infty} c_n\int_{x_0}^{x_1} y_n(x)\,dx.
    \]

    NÃ³tese que si $(x_0,x_1)$ es infinito, hay que pedir integrabilidad
    de $y_n$.

 3. Si las $y_n(x)$ son derivables, $P_N(x)$ converge puntualmente y $P'_N(x)$
    converge uniformemente, entonces $y$ es derivable y la derivada
    conmuta con la suma.

    \[
    y'(x) = \sum_{n=1}^{\infty}c_ny'_n(x).
    \]

***** TODO Test de Cauchy de convergencia uniforme
***** TODO Test de Weierstrass de convergencia uniforme
**** 5.5. Series trigonomÃ©tricas de Fourier
***** Teorema 20. de Riesz-Fischer
:PROPERTIES:
:ID:       a1f0e87e-9bd8-468d-ad43-7e7fb16c1b67
:END:
El conjunto de funciones trigonomÃ©tricas $\left\{ y_n \right\}^{\infty}_{n =0}$ dado por

\[
y_0(x) = \frac{1}{\sqrt{T}}, \quad 
y_{2n}(x) = \sqrt{\frac{2}{T}}\cos \left( \frac{2n\pi x}{T} \right), \quad
y_{2n-1}(x) = \sqrt{\frac{2}{T}}\sen \left( \frac{2n\pi x}{T} \right)
\]

forma un sistema ortonormal completo de $L^2(0,T)$. NÃ³tese que la
convergencia es cpd en este espacio.

****** Proof                                                                                            :exercise:
Se debe comprobar que es base normal y que son ortornormales en el
espacio $L^2(0,T)$ integrando y usando identidades trigonomÃ©tricas.

****** Card                                                                                                :leech:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       5b0b3868-b27b-4e52-baa0-a215080509fa
:DRILL_LAST_INTERVAL: 30.0495
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:06]
:END:
Dar la base ortonormal de $L^2(0,T)$ del Teorema de Riesz-Fischer.

******* Base

\[
y_0(x) = \frac{1}{\sqrt{T}}, \quad 
y_{2n}(x) = \sqrt{\frac{2}{T}}\cos \left( \frac{2n\pi x}{T} \right), \quad
y_{2n-1}(x) = \sqrt{\frac{2}{T}}\sen \left( \frac{2n\pi x}{T} \right)
\]

***** Desarrollo de una funciÃ³n en Serie de Fourier
Es habitual escribir el desarrollo de una funciÃ³n $y \in L^2(0,T)$ como

\[
y(x) = \frac{a_0}{2} + 
\sum^{\infty}_{n=1} 
a_n\cos \left( \frac{2\pi nx}{T} \right) + 
b_n\sin \left( \frac{2\pi nx}{T} \right).
\]

donde

 * \[a_0 = \frac{2}{T} \int_0^T y(x)\,dx \]

 * \[a_n = \frac{2}{T} \int_0^T y(x) \cos \left( \frac{2n\pi x}{T} \right)\,dx \]

 * \[b_n = \frac{2}{T} \int_0^T y(x) \sin \left( \frac{2n\pi x}{T} \right)\,dx \]

# TODO: Comprobar que este es efectivamente el desarrollo y calcular
# el valor de los coeficientes a_n para comprobar que cuadran con los
# dados en los apuntes.

# TODO: Comprobar que es un sistema ortonormal usando identidades
# trigonomÃ©tricas.

***** Convergencia puntual: condiciÃ³n necesaria
Todas las $y_n$ son periÃ³dicas, asÃ­ que para tener convergencia
puntual habrÃ¡ que tener periodicidad en la funciÃ³n aproximada.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       31ad5eda-567d-4b54-b25f-e2e95e24a0b3
:DRILL_LAST_INTERVAL: 31.9059
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:07]
:END:
Por Riesz-Fischer tenemos convergencia casi por doquier de la serie de
Fourier.  Â¿CuÃ¡l es la primera condiciÃ³n necesaria para tener
convergencia en todo punto?

******* CondiciÃ³n necesaria
La periodicidad de la funciÃ³n aproximada.

***** Convergencia puntual: a trozos
Para una funciÃ³n continua a trozos $y(x)$, la serie converge en todo punto
de continuidad y en los puntos de discontinuidad $a_i$ se tiene el
*fenÃ³meno de Gibbs*,

\[
\lim_{N\to\infty} P_N(a_i) = \frac{f(a_i^+) - f(a_i^-)}{2}.
\] 

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-06-11 Mon>
:PROPERTIES:
:ID:       e1a64429-139b-4b76-a77e-dbf1c41fcd9c
:DRILL_LAST_INTERVAL: 24.0119
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:03]
:END:
Convergencia de la serie de Fourier de una funciÃ³n continua a trozos.
Â¿QuÃ© ocurre en los puntos de discontinuidad?

******* Convergencia
Tenemos convergencia en todo punto de continuidad, y en los puntos
de discontinuidad se produce el fenÃ³meno de Gibbs con

\[
\lim_{N\to\infty} P_N(a_i) = \frac{f(a_i^+) - f(a_i^-)}{2}.
\] 

***** Convergencia uniforme
Para $y \in {\cal C}$ con $y(0) = y(L)$ y $y'$ continua a trozos, la serie de
Fourier converge uniformemente.

****** Proof                                                                                               :extra:
:PROPERTIES:
:ID:       55291685-9a8a-4c19-9338-570d037dd93c
:END:
Usando criterio de Weierstrass.

http://math.ucr.edu/~res/math153/fourier-uniform.pdf

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-06-08 Fri>
:PROPERTIES:
:ID:       f7a7e291-c6f6-40dd-9008-3c54fd86d012
:DRILL_LAST_INTERVAL: 4.1707
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:20]
:END:
Â¿CuÃ¡ndo converge uniformemente la serie de Fourier?

******* Respuesta 
Basta tener $y \in {\cal C}$, periÃ³dica $y(0) = y(L)$ y $y'$ continua a trozos.
Es decir,

 1. continua,
 2. periÃ³dica,
 3. derivada continua a trozos.

****** TODO Ejemplo de convergencia uniforme

***** Teorema 21. Convergencia de derivadas superiores
Si $y \in {\cal C}^k[0,T]$, tiene periodicidad $y^{n)}(0) = y^{n)}(T)$ para $n = 0,\dots,k$
y la derivada $k+1$ es continua a trozos, entonces las derivadas hasta
la $k$ convergen uniformemente.

****** Proof
En este caso, iterando el resultado, tendremos que

\[
\sum_{n \geq 1} n^k(|a_n| + |b_n|),
\]

converge uniformemente para $a_n$ y $b_n$ coeficientes de Fourier.
Se aplicarÃ¡ un razonamiento similar al aplicado [[id:55291685-9a8a-4c19-9338-570d037dd93c][anteriormente]].

****** Card                                                                                                :drill:
SCHEDULED: <2018-09-08 Sat>
:PROPERTIES:
:ID:       3566f799-f086-4e6f-9b1c-6be4a775beae
:DRILL_LAST_INTERVAL: 70.7902
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.6
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:50]
:END:
Enunciar el Teorema 21 sobre convergencia de derivadas superiores
en series de Fourier.

******* Enunciado
Si $y \in {\cal C}^k[0,T]$, tiene periodicidad $y^{n)}(0) = y^{n)}(T)$ para $n = 0,\dots,k$
y la derivada $k+1$ es continua a trozos, entonces las derivadas hasta
la $k$ convergen uniformemente.

***** Convergencia en media
La convergencia en media la da la igualdad de Bessel

\[
\frac{2}{T}\int_0^T y(x)^2\,dx = \frac{a_0^2}{2} + \sum_{n=1}^{\infty}a^2_n + b^2_n.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       7fd53585-d7a3-4da3-8087-77d8ff760eb6
:DRILL_LAST_INTERVAL: 31.0459
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.875
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:03]
:END:
Enuncia la igualdad de Bessel que da la convergencia en
media de la serie de Fourier.

******* Igualdad

\[
\frac{2}{T}\int_0^T y(x)^2\,dx = \frac{a_0^2}{2} + \sum_{n=1}^{\infty}a^2_n + b^2_n.
\]

***** TODO Cambio de intervalo
***** Teorema 22. Serie de Fourier en el intervalo (-T,T)
:PROPERTIES:
:ID:       b0018ca3-1e81-42e1-8938-727ad480fc40
:END:
Para $y \in L^2(-T,T)$ podemos desarrollar

\[
y(x) = \frac{a_0}{2} +
\sum_{n=1}^{\infty} 
a_n\cos \left( \frac{n\pi x}{T} \right) +
b_n\sin \left( \frac{n\pi x}{T} \right)
\]

siendo

 * \[a_0 = \frac{1}{T}\int_{-T}^T y(x) \,dx \]

 * \[a_n = \frac{1}{T}\int_{-T}^T y(x)\cos \left( \frac{n\pi x}{T} \right) \,dx \]

 * \[b_n = \frac{1}{T}\int_{-T}^T y(x)\sin \left( \frac{n\pi x}{T} \right) \,dx \]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       f254979e-2fbc-4b1d-928b-336f633977b4
:DRILL_LAST_INTERVAL: 32.7988
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:16]
:END:
Desarrollo en Serie de Fourier de $y \in L^2(-T,T)$.
(Coeficientes de ayuda).

******* Desarrollo

\[
y(x) = \frac{a_0}{2} +
\sum_{n=1}^{\infty} 
a_n\cos \left( \frac{n\pi x}{T} \right) +
b_n\sin \left( \frac{n\pi x}{T} \right)
\]

******* Coeficientes

 * \[a_0 = \frac{1}{T}\int_{-T}^T y(x) \,dx \]

 * \[a_n = \frac{1}{T}\int_{-T}^T y(x)\cos \left( \frac{n\pi x}{T} \right) \,dx \]

 * \[b_n = \frac{1}{T}\int_{-T}^T y(x)\sin \left( \frac{n\pi x}{T} \right) \,dx \]

****** Card: coeficientes                                                                                  :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       da80a0c6-bbfb-4437-8438-29c5af8dd817
:DRILL_LAST_INTERVAL: 30.269
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.4
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:49]
:END:
Determina los coeficientes del desarrollo en Serie de Fourier
en el intervalo $y \in L^2(-T,T)$.

\[
y(x) = \frac{a_0}{2} +
\sum_{n=1}^{\infty} 
a_n\cos \left( \frac{n\pi x}{T} \right) +
b_n\sin \left( \frac{n\pi x}{T} \right)
\]

******* Coeficientes

 * \[a_0 = \frac{1}{T}\int_{-T}^T y(x) \,dx \]

 * \[a_n = \frac{1}{T}\int_{-T}^T y(x)\cos \left( \frac{n\pi x}{T} \right) \,dx \]

 * \[b_n = \frac{1}{T}\int_{-T}^T y(x)\sin \left( \frac{n\pi x}{T} \right) \,dx \]

***** Teorema 23. Serie de cosenos/senos
Para $y \in {\cal C}^1(0,T)$, su serie de cosenos converge absoluta y
uniformemente (extensiÃ³n par)

\[
\frac{a_0}{2} + \sum_{n=1}^{\infty}a_n\cos \left( \frac{n\pi x}{T} \right)
\]

si ademÃ¡s $y(0) = y(T)$, su serie de senos converge absoluta y
uniformemente (extensiÃ³n impar)

\[
\sum_{n=1}^{\infty} b_n \sin \left( \frac{n\pi x}{T} \right).
\]

Los coeficientes en estos casos son

 * \[a_0 = \frac{2}{T}\int_{0}^T y(x) \,dx \]

 * \[a_n = \frac{2}{T}\int_{0}^T y(x)\cos \left( \frac{n\pi x}{T} \right) \,dx \]

 * \[b_n = \frac{2}{T}\int_{0}^T y(x)\sin \left( \frac{n\pi x}{T} \right) \,dx \]


****** Extensiones par e impar
Definimos las extensiones par e impar de $y \in {\cal C}[0,T]$ al
intervalo $[-T,T]$. NÃ³tese que en el desarrollo de una funciÃ³n
impar no aparecerÃ¡n cosenos y en el desarrollo de una funciÃ³n
par no aparecerÃ¡n senos.

La extensiÃ³n par es siempre continua, pero la extensiÃ³n
impar requiere $y(0) = y(T) = 0$. Aplicando el desarrollo en
[[id:b0018ca3-1e81-42e1-8938-727ad480fc40][serie de Fourier]] a estas extensiones es como obtenemos el
desarrollo en senos y cosenos.

**** 5.6. SoluciÃ³n de la ecuaciÃ³n de ondas
***** Teorema 24. SoluciÃ³n de la ecuaciÃ³n de ondas
Para $u_0 \in {\cal C}^2(0,L)$ con $u_0'''$ continua a trozos y $v_0 \in {\cal C}^1(0,L)$ con $v_0''$
continua a trozos, anulÃ¡ndose $u_0,u_0'',v_0$ en los extremos, el problema
de la ecuaciÃ³n de ondas simplificada

\[\left\{\begin{array}{lr}
u_{tt} = c^2 u_{xx} & t \geq 0, x \in [0,L] \\
u(0,x) = u_0(x) \in {\cal C}^2, & x \in [0,L], \\
u_t(0,x) = v_0(x) \in {\cal C}^1, & x \in [0,L], \\
u(t,0) = u(t,L) = 0, & t \geq 0.
\end{array}
\right\}\]

Tiene candidato a soluciÃ³n

\[
u(t,x) = \sum_{n \geq 1} 
\left( b_n\cos \left( \frac{nc\pi t}{L} \right) +
\frac{b'_nL}{nc\pi} \sin \left( \frac{n c \pi t}{L} \right) \right)
\sin \left( \frac{n\pi x}{L} \right)
\]

donde tenemos los desarrollos en senos

 * \[u_0(x) = \sum_{n \geq 1} b_n\sin \left( \frac{n \pi x}{L} \right),\quad b_n = \frac{2}{L}\int_0^L u_0(x) \sin \left( \frac{n \pi x}{L} \right),\] 

 * \[v_0(x) = \sum_{n \geq 1} b_n'\sin \left( \frac{n \pi x}{L} \right),\quad b_n' = \frac{2}{L}\int_0^L v_0(x) \sin \left( \frac{n \pi x}{L} \right).\] 

Tenemos convergencia uniforme hasta la segunda derivada.

***** 5.6.1. Unicidad y mÃ©todo de la energÃ­a
La unicidad de soluciÃ³n se demuestra comprobando que la *energÃ­a*
es constante en soluciones y la diferencia de soluciones debe ser
nula porque su energÃ­a lo es.

\[
E[u](t) = \frac{1}{2}\int_0^L \left( u_t^2 + c^2u_x^2 \right)\,dx = 0.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       4f851785-c8d4-4153-a88c-f6460f3c860b
:DRILL_LAST_INTERVAL: 25.1088
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:50]
:END:
Enuncia la energÃ­a en la ecuaciÃ³n de ondas.

******* EnergÃ­a

\[
E[u](t) = \frac{1}{2}\int_0^L \left( u_t^2 + c^2u_x^2 \right)\,dx.
\]

***** 5.6.2. Soluciones de la ecuaciÃ³n de ondas en R, fÃ³rmula de D'Alembert
La *ecuaciÃ³n de ondas* unidimensional,

\[\left\{ \begin{array}{lr}
u_{tt} = c^2u_{xx}, & t > 0, x \in \mathbb{R}, \\
u(0,x) = u_0(x) \in {\cal C}^2(\mathbb{R}), \\
u_t(0,x) = v_0(x) \in {\cal C}^1(\mathbb{R}),
\end{aligned}\right.\]

se resuelve con la FÃ³rmula de D'Alembert.

\[
u(t,x) = \frac{u_0(x+ct) - u_0(x-ct)}{2} + \frac{1}{2c}\int_{x-ct}^{x+ct}v_0(y)\,dy.
\]

****** ResoluciÃ³n
Tomamos $\xi = x + ct$, $\eta = x - ct$, por lo que $x = \frac{\xi + \eta}{2}$ y $t = \frac{\xi - \eta}{2c}$.
Definimos $U(\xi,\eta) = u \left( \frac{\xi + \eta}{2} , \frac{\xi - \eta}{2} \right)$, luego $u(x,t) = U(x + ct, x -ct)$.
Calculamos, evaluando $u$ en $(x,t)$ y $U$ en $(\xi,\eta)$,

\[ \left\{\begin{array}{l}
u_{tt} = c^2U_{\xi\xi} - c^2U_{\xi\eta} - c^2U_{\eta\xi} + c^2U_{\eta\eta} \\
u_{xx} = U_{\xi\xi} + U_{\xi\eta} + U_{\eta\xi} + U_{\eta\eta}
\right.\end{array}\]

Por lo que $0 = u_{tt} - c^2u_{xx} = -4U_{\xi\eta}$. El que esta derivada sea nula
implica que $U_{\xi},U_{\eta}$ dependen de una sola de las variables, e integrando
y pasando constantes tendremos que $U$ es de la forma

\[
U(\xi,\eta) = g(\xi) + h(\eta).
\]

Deshaciendo el cambio de variables, tenemos las condiciones iniciales

\[\left\{\begin{array}{l}
u_0(x) = g(x) + h(x), \\
v_0(x) = c(g'(x) - h'(x)).
\end{array}\right.\]

Resolviendo e integrando,

\[\left\{\begin{aligned}
g(x) = K_1 + \frac{u_0(x)}{2} + \frac{1}{2c}\int_0^x v(y)}\,dy\\
h(x) = K_2 + \frac{u_0(x)}{2} - \frac{1}{2c}\int_0^x v(y)}\,dy
\end{aligned}\right.\]

y para que se cumpla la condiciÃ³n, $K_1 + K_2 = 0$. Sustituyendo obtenemos
la fÃ³rmula de d'Alembert.

\[
u(t,x) = \frac{u_0(x+ct) - u_0(x-ct)}{2} + \frac{1}{2c}\int_{x-ct}^{x+ct}v_0(y)\,dy.
\]

****** Comentario: cono de luz
**** 5.7. SoluciÃ³n de la ecuaciÃ³n de Dirichlet
***** Laplaciano en polares
Aplicando el cambio de variable $U(\rho,\theta) = u(\rho\cos(\theta), \rho\sin(\theta))$, tenemos
el laplaciano en polares

\[
\left( U_{\rho\rho} + \frac{U_{\theta\theta}}{\rho^2} + \frac{U_{\rho}}{\rho} \right)(\rho,\theta) =
(\Delta_x u)(\rho\cos(\theta),\rho\sin(\theta)).
\]

***** EcuaciÃ³n de Dirichlet en el disco
Viendo la [[id:8e9ccec0-c9de-4d00-a08e-3de44d448ca5][ecuaciÃ³n de Dirichlet]] en el disco $D$ tenemos $\mathbb{S}^1 = \partial D$
y definimos $\overline{\gamma}(\theta) = \gamma(\cos(\theta),\sin(\theta))$. El problema de Dirichlet queda
como

\[\left\{\begin{array}{ll}
U_{\rho\rho} + \frac{U_{\theta\theta}}{\rho^2} + \frac{U_{\rho}}{\rho} = 0, &
\forall(\rho,\theta) \in (0,1] \times [0,2\pi],\\
U(1,\theta) = \overline{\gamma}(\theta), & \theta \in [0,2\pi], \\
U(\rho,0) = U(\rho,2\pi), & \rho \in (0,1], \\
\end{array}\right.\]

La resoluciÃ³n se hace por separaciÃ³n de variables $U(\rho,\theta) = v(p)w(\theta)$.

***** EcuaciÃ³n de Dirichlet en el rectÃ¡ngulo
*** 6. FormulaciÃ³n dÃ©bil
**** 6.1. Derivada generalizada y dÃ©bil
***** FÃ³rmula de Green
Para $u \in {\cal C}^1(\overline{\Omega})$, $v \in {\cal C}^1_0(\overline{\Omega})$, la derivada la caracteriza la
*FÃ³rmula de Green*

\[
\int_{\Omega} u_{x_i}(x) v(x) \,dx = 
-\int_{\Omega} u(x)v_{x_i}(x)\,dx.
\]

****** Clave 1: derivadas caracterizadas
:PROPERTIES:
:ID:       893398fe-a78e-45a4-b0dd-d4e36bc98d78
:END:
Dados $u \in {\cal C}^1(\overline{\Omega})$ y $g \in {\cal C}^0(\overline{\Omega})$, si para cualquier $v \in {\cal C}^1_0(\Omega)$

\[
\int_{\Omega} g(x)v(x)\,dx = - \int_{\Omega} u(x)v_{x_i}(x)\,dx,
\]

entonces $u_{x_i} = g$.

******* Proof
Consecuencia directa del [[id:652bdddb-0853-4a9a-8060-28a730928816][lema fundamental]].
****** Clave 2: operador lineal de Green
:PROPERTIES:
:ID:       c34bbc56-aa81-404b-a6ea-0670000f306b
:END:
Para $u \in L_{loc}^1(\Omega)$ (integrable sobre compactos), se define un
operador lineal $L \colon {\cal C}^1_0(\Omega) \to \mathbb{R}$ como

\[
L_u[v] = - \int_{\Omega} u(x)(\partial_{x_i}v(x))\,dx.
\]

Que caracteriza propiedades de la derivada sin necesitar
derivabilidad.

****** Caso general                                                                                        :extra:
Dadas $u,v \in {\cal C}^1(\overline{\Omega})$, para $\Omega \subset \mathbb{R}^N$, si llamamos $n_i(x)$ a la
componente del vector $n(x)$ normal a $\partial\Omega$ en $x$, tenemos la
siguiente generalizaciÃ³n de la regla de la cadena.

\[\int_{\Omega} u(x) v_{x_i}(x) \,dx = 
-\int_{\Omega} u_{x_i}(x)v(x)\,dx +
\int_{\partial\Omega} u(x)v(x)n_i(x)\,dS.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-02 Thu>
:PROPERTIES:
:ID:       2f8ab0e2-d672-4822-8957-da14137f3350
:DRILL_LAST_INTERVAL: 70.427
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.857
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:53]
:END:
Enunciar la fÃ³rmula de Green para $u \in {\cal C}^1(\overline{\Omega})$ y $v \in {\cal C}^1_0(\overline{\Omega})$; es decir,
para el caso de frontera nula.

******* Enunciado
La fÃ³rmula de Green es un anÃ¡logo a la integraciÃ³n por partes para
varias variables. Normalmente tiene un tÃ©rmino mÃ¡s, pero cuando
$v$ se anula en la frontera,

\[
\int_{\Omega} u(x) (\partial_{x_i}v(x)) \,dx = 
-\int_{\Omega} (\partial_{x_i}u(x))v(x)\,dx.
\]

***** Derivada generalizada o distribucional
Para $u \in L_{loc}^1(\Omega)$, la *derivada generalizada* $\pair{u_{x_i},-} \colon {\cal C}^{\infty}_0(\Omega) \to \mathbb{R}$
se define

\[
\left\langle u_{x_i}, v \right\rangle =
-\left\langle u, v_{x_i} \right\rangle =
-\int_{\Omega} u(x)v_{x_i}(x)\,dx.
\]

***** Derivada dÃ©bil
Para $u \in L_{loc}^1(\Omega)$, decimos $u_{x_i} = g \in L^1_{loc}(\Omega)$ en sentido *dÃ©bil* si

\[
\forall v \in {\cal C}^{\infty}_0(\Omega)\colon\quad \pair{u_{x_i},v} = \pair{g,v}.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       29496071-ee41-4f3e-94f0-15f048942995
:DRILL_LAST_INTERVAL: 16.9877
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:07]
:END:
Decimos $u_{x_i} = g \in L^1_{loc}(\Omega)$ en sentido *dÃ©bil* si

******* Respuesta

\[
\forall v \in {\cal C}^{\infty}_0(\Omega)\colon\quad 
\pair{u_{x_i},v} := -\pair{u,v_{x_i}} = \pair{g,v}.
\]

***** FunciÃ³n Heaviside y Delta de Dirac
La *funciÃ³n de Heaviside* $H(x) = \max\left\{ 0,x/|x| \right\}$ estÃ¡ definida
c.p.d. Su derivada generalizada puede calcularse sabiendo $v \in {\cal C}^{\infty}_0(\mathbb{R})$
como

\[\left\langle H',v \right\rangle = 
-\int_{\mathbb{R}} H(x)v'(x)\,dx =
-[v(x)]^{\infty}_0 = v(0).\]

Es decir, su derivada dÃ©bil es la *Delta de Dirac*, $\delta_0(v) = v(0)$.

**** 6.1.1. Espacios de Sobolev
***** Espacio de Sobolev
Para $\Omega \subseteq \mathbb{R}^N$ dominio, $p \in [1,\infty]$, se define el *espacio de Sobolev*

\[
W^{m,p}(\Omega) = \left\{ u \in L^p(\Omega) \;\middle|\;
\pdv[\abs{\alpha}]{u}{x} \in L^p(\Omega),\, \forall 0 \leq \alpha \leq m \in \mathbb{N} \right\}.
\]

/El espacio de las m-derivadas p-integrables./

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:ID:       e5522b23-1312-4980-b0ac-2d736e86d2e7
:DRILL_LAST_INTERVAL: 44.4198
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:56]
:END:
Para $\Omega \subseteq \mathbb{R}^N$ dominio, define el *espacio de Sobolev* $W^{m,p}(\Omega)$.

******* Espacio de Sobolev
El espacio de las m-derivadas p-integrables.

\[
W^{m,p}(\Omega) = \left\{ u \;\middle|\;
\pdv[\abs{\alpha}]{u}{x} \in L^p(\Omega),\, \forall 0 \leq \alpha \leq m \right\}.
\]

***** Norma de espacios de Sobolev
:PROPERTIES:
:ID:       192a4ee0-0514-42cb-96e6-e4eb68c3ab41
:END:
El espacio $W^{m,p}(\Omega)$ con la norma

\[
\|u\|_{W^{m,p}(\Omega)} = 
\left( \|u\|^{p}_p +
\sum_{0 \leq \alpha \leq m} \norm{\pdv[\abs{\alpha}]{u}{x}}^p_p \right)^{1/p}
\]

es un espacio de Banach. En el caso $p=2$ notamos como $H^m(\Omega) = W^{m,2}(\Omega)$
y es un espacio de Hilbert. En particular

\[
H^1(\Omega) = \left\{ u \in L^2(\Omega) \mid \pdv{u}{x_i} \in L^2(\Omega), \forall i = 1,\dots,N \right\}
\]

teniendo el producto escalar

\[
\pair{u,v}_{H^1(\Omega)} = 
\int_{\Omega} (uv + \nabla u \cdot \nabla v)\,dx.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       99ab3e47-4a1e-4f57-bb0c-f498dd36cc38
:DRILL_LAST_INTERVAL: 24.2486
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:46]
:END:
Â¿CuÃ¡l es el producto escalar en $H^1(\Omega)$?

******* Respuesta

\[
\pair{u,v}_{H^1(\Omega)} = 
\int_{\Omega} (uv + \nabla u \cdot \nabla v)\,dx.
\]

***** Ejemplo 32. Espacios de integrabilidad
Para $\beta \in \mathbb{R}$ y $\overline{B} \subset \mathbb{R}^{N}$ una bola centrada en el origen

 * $\frac{1}{\abs{x}^{\beta}} \in L^p(\overline{B})$ si y sÃ³lo si $\beta p < N$,

 * $\frac{1}{\abs{x}^{\beta}} \in L^q(\mathbb{R}^n - \overline{B})$ si y sÃ³lo si $\beta q > N$, con $q < +\infty$.

****** Proof
Integramos usando la fÃ³rmula de [[id:8b7c51f7-5499-4cd3-a1c6-cab635bd0015][integraciÃ³n en polares]] y
llegaremos hasta una integral que es finita sÃ³lo cuando
$\beta p + 1 - N < 1$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-27 Fri>
:PROPERTIES:
:ID:       62ba0268-3776-4f16-9fdd-0dab96753381
:DRILL_LAST_INTERVAL: 51.0746
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:12]
:END:
Estudia integrabilidad de $1/|x|^{\beta}$ en $\overline{B}$ y en $\mathbb{R}^N-\overline{B}$.

******* Respuesta

 * \[\frac{1}{\abs{x}^{\beta}} \in L^p(\overline{B}) \iff \beta p < N\]

 * \[\frac{1}{\abs{x}^{\beta}} \in L^q(\mathbb{R}^n - \overline{B}) \iff \beta q > N\], 

con $q < +\infty$.

**** 6.2. Repaso de anÃ¡lisis funcional
***** Teorema fundamental del cÃ¡lculo
Dada $f \in L_{loc}^1(a,b)$ y un $c \in (a,b)$, definimos la *primitiva* de $f$
que se anula en $c$ como

\[
F(x) = \int_c^x f(s)\,ds.
\]

Sabemos que $F$ es continua y tiene derivada dÃ©bil en $(a,b)$ que
es igual a $f$ c.p.d.

****** Proof
Por definiciÃ³n, partiendo la integral,

\[
\left\langle F',\phi \right\rangle =
\int_a^c\int_x^c f(s)\phi'(x)\,dsdx -
\int_c^b\int_c^x f(s)\phi'(x)\,dsdx.
\]

Aplicamos Teorema de Fubini independientemente en las dos
variabes, sabiendo que $f\phi' \in L_{loc}^1$,

\[
\left\langle F',\phi \right\rangle =
\int_a^c f(s) \int_a^s \phi'(x)\,dxds -
\int_c^b f(s) \int_s^b \phi'(x)\,dxds.
\]

Usamos Barrow dos veces en la integral interna para tener,
sabiendo que $\phi(a)=\phi(b)=0$,

\[
\left\langle F',\phi \right\rangle =
\int_a^c f(s)\phi(s)\,ds -
\int_c^b f(s)(-\phi(s))\,ds = \left\langle f,\varphi \right\rangle.
\]

En un caso mÃ¡s general del [[id:893398fe-a78e-45a4-b0dd-d4e36bc98d78][comentado anteriormente]], esto
nos da que $f = F'$ c.p.d.

# Tenemos que no son continuas, sÃ³lo integrables. CÃ³mo se generaliza a
# este caso no queda del todo claro, porque usÃ¡bamos crucialmente antes
# la conservaciÃ³n del signo de funciones continuas.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-15 Wed>
:PROPERTIES:
:ID:       48323a75-8295-4d00-af02-7a57721d5269
:DRILL_LAST_INTERVAL: 89.3989
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 8
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.625
:DRILL_EASE: 2.76
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:13]
:END:
Enunciar el teorema fundamental del cÃ¡lculo en su versiÃ³n dÃ©bil
para una $f \in L_{loc}^1(a,b)$ y un $c \in (a,b)$.

******* Enunciado
La funciÃ³n

\[
\int_c^x f(s)\,ds
\]

 * es continua
 * y tiene derivada dÃ©bil en $(a,b)$ igual a $f$ c.p.d.

***** Desigualdad de PoincarÃ©
:PROPERTIES:
:ID:       9d2c746b-11e7-4055-9608-3d1e6b0cc136
:END:
Para $\Omega$ dominio acotado y $u \in H_0^1(\Omega)$ existe una constante $C$
dependiente solo de $\Omega$ tal que

\[
\int_{\Omega} u^2\,dx \leq C \int_{\Omega} |\nabla u|^2\,dx.
\]

****** Proof
:PROPERTIES:
:ID:       7d0050e4-8d08-4e26-9aef-f6a4572748a9
:END:
En una variable, si minimizamos la integral de la derivada
al cuadrado fijando el mÃ³dulo al cuadrado, llegamos a una
[[id:e15aa2f8-76b4-4aa6-82bb-c0b424bda884][Sturm-Liouville]] que hemos resuelto. Aplicamos esa minimizaciÃ³n
a cualquier funcional normalizado para tener la desigualdad
con la cota

\[
C = \left(\frac{x_1-x_0}{n\pi}\right)^2.
\]

Todo esto lo prueba para funciones ${\cal C}_0^{\infty}$, pero podemos
usar que $H_0^1 = \overline{{\cal C}_0^{\infty}}^{H^1}$ para tener una convergencia $\norm{u_n-u}^2_H \to 0$,
que nos da, sabiendo la [[id:7d0050e4-8d08-4e26-9aef-f6a4572748a9][norma]] de este Hilbert

\[
\int_{x_0}^{x_1} \left( u'_n-u' \right)^2\,dx + \int_{x_0}^{x_1} \left( u_n -u \right)^2\,dx \to 0.
\]

luego $\norm{u_n' - u'}_{L^2} \to 0$ y $\norm{u_n - u}_{L^2} \to 0$, por lo que

\[
\int_{x_0}^{x_1} u_n^2\,dx \to \int_{x_0}^{x_1} u^2\,dx.
\]

\[
\int_{x_0}^{x_1} u_n'^2\,dx \to \int_{x_0}^{x_1} u'^2\,dx.
\]

y tomando lÃ­mites en el caso ${\cal C}_0^\infty$, se tiene lo buscado.sss

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-18 Sat>
:PROPERTIES:
:ID:       132abd02-1210-4329-b150-e7bdbd5fe70b
:DRILL_LAST_INTERVAL: 50.3547
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:51]
:END:
Enuncia la desigualdad de PoincarÃ© para $u \in H^1_0(\Omega)$.

******* Enunciado

\[
\int_{\Omega} u^2\,dx \leq C \int_{\Omega} |\nabla u|^2\,dx.
\]

donde $C$ depende de $\Omega$.

****** Card                                                                                                :drill:
:PROPERTIES:
:ID:       ec463032-b0df-40eb-8423-4758b7d57763
:END:
Idea de la demostraciÃ³n de la desigualdad de PoincarÃ©.

\[
\int_{\Omega} u^2\,dx \leq C \int_{\Omega} |\nabla u|^2\,dx.
\]


******* Idea
En una variable, si minimizamos la integral de la derivada
al cuadrado fijando el mÃ³dulo al cuadrado, llegamos a una
[[id:e15aa2f8-76b4-4aa6-82bb-c0b424bda884][Sturm-Liouville]] que hemos resuelto.

Todo esto lo prueba para funciones ${\cal C}_0^{\infty}$, pero podemos
usar que $H_0^1 = \overline{{\cal C}_0^{\infty}}^{H^1}$ para tener una convergencia $\norm{u_n-u}^2_H \to 0$,
que nos implica $\norm{u_n' - u'}_{L^2} \to 0$ y $\norm{u_n - u}_{L^2} \to 0$.

***** Norma equivalente en el Sobolev de Hilbert
Para $\Omega$ acotado, 

\[
\vertiii{u} = \sqrt{ \int_{\Omega} \abs{\nabla u}^2\,dx }
\]

es una norma de $H_0^1(\Omega) \leq H^1(\Omega)$, equivalente a la heredada.

****** Proof
Por [[id:9d2c746b-11e7-4055-9608-3d1e6b0cc136][desigualdad de PoincarÃ©]].

***** Teorema de Riesz
Para $H$ hilbert, toda lineal continua viene representada por un
Ãºnico elemento con su misma norma.

***** Propiedades de los Hilbert
 * Para $A \subset H$, se tiene $A^{\bot}^{\bot} = \overline{\pair{A}}$.
 * Para $V \leq H$ denso, $V^{\bot} = \left\{ 0 \right\}$.

**** 6.3. Teorema de Lax-Milgram
***** Ejemplo de la membrana
El problema de la [[id:f0ac6c21-d9c3-4298-b48e-8602ca0948a5][membrana simplificado]] puede escribirse como 
tres problemas equivalentes.

****** Problema variacional
Buscar $u \in H_0^1(\Omega)$ que minimice

\[
{\cal E}[u] = \int_{\Omega} \left( 
\frac{\sigma(x)}{2}\abs{\nabla u}^2 + \frac{\alpha}{2}u^2 - f(x)u
\right)\,dx.
\]

****** EcuaciÃ³n diferencial
Encontrar extremales $u \in H_0^1(\Omega)$ que resuelvan

\[
\sigma \Delta_x u = \alpha u  - f.
\]

****** FormulaciÃ³n dÃ©bil
Buscar $u \in H^1_0$ tal que el razonamiento que lleva a Euler-Lagrange
sea cierto para todo $\phi \in H_0^1$.

\[
\int_{\Omega} \left(
\sigma\nabla u \cdot \nabla \phi + \alpha u \phi
\right)\,dx - 
\int_{\Omega} \left(
f \phi
\right)\,dx = 0
\]

Podemos observar una parte cuadrÃ¡tica $\alpha(u,\phi)$ en el primer sumando
y una parte lineal $\tf(\phi)$ en el segundo. Es decir,

\[
a(u,\phi) - \tf(\phi) = 0.
\]

Si consideramos $J(u) = \frac{1}{2}a(u,u) - \tf(u)$, minimizar este operador
es de nuevo el problema variacional.

***** TODO Tipos de soluciÃ³n

 * ClÃ¡sica.
 * Distribucional. $L^1_{loc}(\Omega)$ $a(u,v) = \tf(v)$.
 * DÃ©bil. $H_0^1(\Omega)$, $a(u,v) = \tf(v)$.
 * Variacional. $H^1_0(\Omega)$ con $J(u) \leq J(v)$.

***** Teorema 30. Teorema de Lax-Milgram
$a \colon H \times H \to \mathbb{R}$ bilineal continua coerciva $\exists \alpha \colon \forall v \in H\colon  \alpha \norm{v}^2_H \leq a(v,v)$
sobre un Hilbert. Sea $\tf \colon H \to \mathbb{R}$ lineal continua. Hay soluciÃ³n Ãºnica de

\[(\text{\textit{FormulaciÃ³n dÃ©bil}})\left\{\begin{array}{l}
\overline{u} \in H, \\
\forall v \in H\colon\ a(\overline{u},v) = {\tilde f}(v).
\end{aligned}\right.\]

Y la soluciÃ³n cumple,

\[
\norm{u_{\tf}}_H \leq \frac{1}{\alpha}\norm{\tilde f}_{H'},
\]

por lo que $\tf\mapsto u_{\tf}$ es continua.

Cuando $a$ simÃ©trica, $u_\tf$ es la Ãºnica soluciÃ³n de

\[(\text{\textit{Problema variacional}})\left\{\begin{array}{l}
\overline{u} \in H, \\
\forall v \in H\colon\ J(\overline{u}) \leq J(v),
\end{aligned}\right.\]

donde $J(u) = \frac{1}{2} a(u,u) - \tf(u)$.

****** DemostraciÃ³n: parte 1
Fijado $u \in H$, por Riesz sobre $a(u,-) \colon H \to \mathbb{R}$, lineal continua,
tenemos $Au$, lineal, con $\pair{Au,v} = a(u,v)$ para cualquier $v \in H$.
Por Riesz y por continuidad de $a$ tenemos

\[
\|Au\|_H = \|a(u,-)\|_{H'} = \sup \frac{|a(u,v)|}{\|v\|_H} \leq
\sup \frac{C\norm{u}\norm{v}}{\|v\|} \leq C \|u\|
\]

y $A$ es *continua*. Tenemos *inyectividad* de $A$ por coercividad

\[
\alpha \norm{v}^2_H \leq
a(v,v) = \pair{Av,v} \leq
\norm{Av}_H\norm{v}_H,\quad
\|v\|_H \leq \frac{1}{\alpha}\|Av\|_H
\]

Tenemos $A(H)$ *denso* de nuevo por coercividad, porque si $v_0 \in A(H)^{\bot}$,
$a(v_0,v_0) = 0$ y $A(H)^{\bot} = 0$; esto da densidad en un Hilbert.
Tenemos $A(H)$ *cerrado* porque la inversa de $A$ en su imagen
es continua por la desigualdad anterior, luego $H \cong A(H)$ y $H$ es completo.
AsÃ­ $H = A(H)^{\bot^{\bot}} = \overline{A(H)} = A(H)$ y es *sobreyectiva*.

TambiÃ©n por Riesz tenemos $\tilde{f} = \pair{f,v}$, el PD es $Au = f$, con soluciÃ³n
Ãºnica por biyectividad. La desigualdad se tiene por Riesz,

\[
\norm{u_f} =
\norm{A^{-1}(f)}_H \leq
\frac{1}{\alpha}\norm{f}_H =
\frac{1}{\alpha}\norm{\tilde f}_{H'}.
\]

****** DemostraciÃ³n: parte 2
Cuando $a$ es simÃ©trica tenemos

\[
J(u+v) = J(u) + \left( a(u,v)-{\tilde{f}}(v) \right) + \frac{1}{2}a(v,v)
\]

y cuando $u_f$ es soluciÃ³n del PD, aplicamos coercividad para
obtener el Ãºnico mÃ­nimo global

\[
J(u_f+v) = J(u_f) + \frac{1}{2}a(v,v) \geq J(u_f) + \frac{\alpha}{2}\norm{v}_H > J(u_f).
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       54bc4de7-db60-426c-bcbb-08e80dbc2824
:DRILL_LAST_INTERVAL: 31.8409
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-18 Fri 22:05]
:END:
Enuncia Lax-Milgram

******* Enunciado
$a \colon H \times H \to \mathbb{R}$ bilineal continua coerciva, $\exists \alpha \colon \forall v \in H\colon  \alpha \norm{v}^2_H \leq a(v,v)$,
sobre un Hilbert. Sea $\tf \colon H \to \mathbb{R}$ lineal continua. Hay soluciÃ³n Ãºnica de

\[(\text{\textit{FormulaciÃ³n dÃ©bil}})\left\{\begin{array}{l}
\overline{u} \in H, \\
\forall v \in H\colon\ a(\overline{u},v) = {\tilde f}(v).
\end{aligned}\right.\]

Y la soluciÃ³n cumple,

\[
\norm{u_{\tf}}_H \leq \frac{1}{\alpha}\norm{\tilde f}_{H'},
\]

por lo que $\tf\mapsto u_{\tf}$ es continua.

Cuando $a$ simÃ©trica, $u_\tf$ es la Ãºnica soluciÃ³n de

\[(\text{\textit{Problema variacional}})\left\{\begin{array}{l}
\overline{u} \in H, \\
\forall v \in H\colon\ J(\overline{u}) \leq J(v),
\end{aligned}\right.\]

donde $J(u) = \frac{1}{2} a(u,u) - \tf(u)$.

*** 7. Ley de acciÃ³n de masas
**** 7.1. Ley de acciÃ³n de masas
***** FormulaciÃ³n
La *ley de acciÃ³n de masas* dice que, en una reacciÃ³n

\[
A+B \overset{k}\longrightarrow C
\]

donde las concentraciones de material se escriben como $a,b,c$, la
velocidad de reacciÃ³n es proporcional al producto de las
concentraciones de las sustancias que la originan.

\[
c'(t) = ka(t)b(t)
\]

***** FormulaciÃ³n en reacciones bidireccionales
Dada una fÃ³rmula bidireccional,

\[A+B \xrightleftharpoons[k_2]{k_1} C\]

la ley de acciÃ³n de masas se aplica sobre cada una
de las reacciones, cada una con una constante

 * $a'(t) = k_2c(t) - k_1a(t)b(t)$,

 * $b'(t) = k_2c(t) - k_1a(t)b(t)$,

 * $c'(t) = k_1a(t)b(t) - k_2c(t)$.

Existe una *constante de equilibrio* entre concentraciones sobre la
que no se mueve la reacciÃ³n.

\[
k_{eq} := \frac{k_2}{k_1} = \frac{[A]_{eq}[B]_{eq}}{[C]_{eq}}
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-28 Sat>
:PROPERTIES:
:ID:       0ff2edea-e9e8-4899-bc7a-5483b34ca087
:DRILL_LAST_INTERVAL: 28.5434
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 10:23]
:END:
Aplica la ley de acciÃ³n de masas sobre la fÃ³rmula.

\[A+B \xrightleftharpoons[k_2]{k_1} C\]

******* Answer

  $a' = k_2c - k_1ab$,

  $b' = k_2c - k_1ab$,

  $c' = k_1ab - k_2c$.

**** 7.2. Modelo de Michaelis-Menten
***** Modelo inicial de Michaelis-Menten
El modelo de Michaelis-Merten queda determinado por

 * $S+E \xrightleftharpoons[k_{-1}]{k_1} SE$

 * $SE \overset{k_2}\longrightarrow P + E$

Llamamos en este caso

 * $s(t) = [S]$, nutrientes,
 * $e(t) = [E]$, receptores libres,
 * $c(t) = [SE]$, receptores ocupados,
 * $p(t) = [P]$, producto.

Con condiciones iniciales

 * $s(0) = s_0$, alimento inicial,
 * $e(0) = e_0$, enzimas iniciales,
 * $c(0) = 0$, sin enzimas llenas iniciales,
 * $p(0) = 0$, sin producto inicial.

Llegamos a las ecuaciones

 * $s' = -k_1se + k_{-1}c$
 * $e' = -k_1se + k_{-1}c + k_2c$
 * $c' = k_1se - k_{-1}c - k_2c$
 * $p' = k_2c$

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-23 Mon>
:PROPERTIES:
:ID:       521ee441-1af7-44a6-a91f-f16440121990
:DRILL_LAST_INTERVAL: 23.8294
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:43]
:END:
Reacciones del modelo de Michaelis-Merten.

******* Reacciones

 * $S+E \xrightleftharpoons[k_{-1}]{k_1} SE$

 * $SE \overset{k_2}\longrightarrow P + E$

***** Modelo simplificado de Michaelis-Menten
Simplificamos.

 * La $p$ estÃ¡ desacoplada, resolvemos en funciÃ³n de $c$

   \[
   p(t) = \int_0^tk_2c(s)\,ds.
   \]

 * La suma de enzimas libres y ocupadas no varÃ­a

     \[
   \dv{c}{t} + \dv{e}{t} = 0,\qquad
   e(t) = e_0 - c(t).
   \]

Obtenemos finalmente,

 * $s' = -k_1e_0s + (k_1s + k_{-1})c$,
 * $c' = k_1e_0s - (k_1s + k_{-1} + k_2)c$.

Con condiciones iniciales, $s(0)=s_0$ y $c(0) = 0$.

***** AdimensionalizaciÃ³n de Michaelis-Menten
Consideramos unidades de tiempo $\brck{T}$ y unidades de concentraciÃ³n $\brck{C}$.
Calculamos

 * $\brck{k_1} = \brck{T}^{-1}\brck{C}^{-1}$,
 * $\brck{k_{-1}} = \brck{T}^{-1}$,
 * $\brck{k_{2}} = \brck{T}^{-1}$.

Definimos el tiempo adimensional $\tau = k_1e_0t$ y tomamos.

\[
u(\tau) = \frac{1}{s_0}s \left( \frac{\tau}{k_1e_0} \right)
\]

\[
v(\tau) = \frac{1}{e_0}c \left( \frac{\tau}{k_1e_0} \right)
\]

Derivando y tomando $\lambda = \frac{k_2}{k_1s_0}$, $k=\frac{k_{-1}+k_2}{k_1s_0}$ y $\epsilon = \frac{e_0}{s_0}$,

\[
u' = -u + (u+k-\lambda)v
\]

\[
\epsilon v' = u - (u + k)v
\]

con condiciones $u(0)=1$ y $v(0)=0$. Podemos probar que estÃ¡n acotadas
con $0 \leq u \leq 1$ y con $0 \leq v < 1/\epsilon$. Concluimos $\omega = +\infty$. Por Picard-LindelÃ¶f
hay soluciÃ³n Ãºnica.

****** TODO Proof

***** Positividad y acotaciÃ³n en Michaelis-Menten
Puede comprobarse 

 - *unicidad* con Picard-LindelÃ¶f,
 - *positividad* con lema de primer instante,
 - *acotaciÃ³n* definiendo $h = u+\varepsilon v$, que tiene $h' \leq 0$,
 - *comportamiento en infinito*, $u,v \to 0$ cuando $t \to \infty$ por Barbalat.

*** 8. DinÃ¡mica de poblaciones y Ley de Fick
**** 8.1. Modelo
***** Teorema de la divergencia de Gauss
:PROPERTIES:
:ID:       c87531f2-0004-425a-9b15-058eb2854731
:END:
Dada $F \colon \Omega \to \mathbb{R}^N$ con $\partial\Omega$ suficientemente regular, tenemos

\[
\int_{\Omega} \mathrm{div}(F)\,dx = \int_{\partial \Omega} F \cdot n \,dS,
\]

donde el operador de *divergencia* estÃ¡ definido como

\[
\mathrm{div}(F) = \pdv{F_1}{x_1} + \dots + \pdv{F_N}{x_N}.
\]

****** Card: divergencia                                                                                   :drill:
SCHEDULED: <2018-07-26 Thu>
:PROPERTIES:
:ID:       42fe30e9-4824-4592-8874-d687ea2a6ac7
:DRILL_LAST_INTERVAL: 26.8781
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:52]
:END:
Define el operador de divergencia

\[
\mathrm{div}(F) = \dots
\]

******* Answer

\[
\mathrm{div}(F) = \pdv{F_1}{x_1} + \dots + \pdv{F_N}{x_N}.
\]

****** Card: teorema                                                                                       :drill:
SCHEDULED: <2018-07-30 Mon>
:PROPERTIES:
:ID:       4fcc3170-b015-4315-a725-6cedd121e585
:DRILL_LAST_INTERVAL: 31.476
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:53]
:END:
Enuncia el teorema de la divergencia de Gauss.

******* Answer
Dada $F \colon \Omega \to \mathbb{R}^N$ con $\partial\Omega$ suficientemente regular, tenemos

\[
\int_{\Omega} \mathrm{div}(F)\,dx = \int_{\partial \Omega} F \cdot n \,dS,
\]

***** Modelo
Dada una *densidad de poblaciÃ³n* $u$, consideramos que la poblaciÃ³n
cambia por crecimiento y por movimiento

\[
\dv{}{t}\int_{\Omega} u(t,x)\,dx = 
\int_{\Omega} f(u)\,dx + \int_{\partial \Omega} -J(t,x) \cdot n(x)\,dS,
\]

aplicando el [[id:c87531f2-0004-425a-9b15-058eb2854731][Teorema de la divergencia]], nos queda

\[
\dv{}{t}u + \mathrm{div}(J) = f.
\]

***** Primera Ley de Fick y segunda Ley de Fick
Consideramos que la corriente en un punto fluye de las zonas mÃ¡s densas
a las menos densas, *Ley de Fick*,

\[
J(x,t) = -D \nabla_x u.
\]

Desde aquÃ­, en el caso $f = 0$, se obtiene la *ecuaciÃ³n del calor* o
*segunda Ley de Fick*,

\[
\dv{}{t}u = D \Delta u.
\]

****** Proof
Usamos crucialmente que la divergencia del gradiente es el laplaciano.

\[
\mathrm{div}(\nabla u) = \Delta u.
\]

***** Ley de conservaciÃ³n
**** 8.2. Modelos de dinÃ¡mica de poblaciones
***** Modelo de Malthus
Tasa positiva constante de crecimiento fijada una poblaciÃ³n inicial $u(0) = u_0$.

\[
u'(t) = \alpha u(t)
\]

La soluciÃ³n serÃ­a exponencial $u(t) = u_0e^{\alpha t}$. Dependiendo
del valor de $\alpha$, llegamos a *sobrepoblaciÃ³n* exponencial o a
*extinciÃ³n*.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       3053bb75-f914-4410-856b-b1c77728f2b5
:DRILL_LAST_INTERVAL: 15.4559
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:16]
:END:
EcuaciÃ³n del modelo de Malthus.

******* Answer

\[
u'(t) = \alpha u(t)
\]

***** Modelo logÃ­stico de Verlhust
Tasa cambiante segÃºn los recursos.  Tenemos una *capacidad mÃ¡xima de carga*
$u_{\infty}$ a partir de la cual vuelve a decrecer la poblaciÃ³n. SÃ³lo tenemos una
constante $\alpha$ que marca la *tasa intrÃ­nseca*.

\[
u'(t) = \alpha \left( 1 - \frac{u(t)}{u_{\infty}} \right)u(t)
\]

Tenemos una ecuaciÃ³n autÃ³noma con equilibrio en $0$ y en $u_{\infty}$, siendo el
segundo un atractor.

****** DerivaciÃ³n del modelo                                                                               :extra:
Suponemos que $\alpha(t) = k_1c(t)$, donde $c$ es la cantidad de
recursos y suponemos que la reproducciÃ³n consume recursos, 
$c'(t) = -k_2u'(t)$. Obtenemos $\alpha(t) = k_1(c_0+k_2u_0-k_2u(t))$,
que nos lleva a llamar a toda la constante $\alpha = k_1(c_0 + k_2u_0)$ y
llamar $u_{\infty} = k_2/(c_0 + k_2u_0)$ para obtener la fÃ³rmula.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-26 Thu>
:PROPERTIES:
:ID:       c5c6cf39-5ee7-4fe8-97cd-7d3c6d339190
:DRILL_LAST_INTERVAL: 26.724
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:52]
:END:
EcuaciÃ³n del modelo logÃ­stico de Verlhust.

******* Answer

\[
u'(t) = \alpha \left( 1 - \frac{u(t)}{u_{\infty}} \right)u(t)
\]

***** Efecto Alle fuerte
Tasa cambiante y con un mÃ­nimo inferior $u_{min}$. 

\[
u(t) = \alpha \left( 1 - \frac{u}{u_{\infty}} \right) \left( \frac{u}{u_{min}} - 1 \right).
\]

De nuevo la resoluciÃ³n nos da una ecuaciÃ³n autÃ³noma con equilibrios
estables en $u_{\infty}$ y en $0$, pero equilibrio inestable en $u_{min}$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-31 Tue>
:PROPERTIES:
:ID:       f6ebc901-c8ae-4ee8-b3fc-6df77c73ffdf
:DRILL_LAST_INTERVAL: 31.7039
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:51]
:END:
EcuaciÃ³n del modelo con efecto Alle fuerte.

******* Answer 

\[
u(t) = \alpha \left( 1 - \frac{u}{u_{\infty}} \right) \left( \frac{u}{u_{min}} - 1 \right).
\]

***** Efecto Alle dÃ©bil
Podemos debilitar el efecto Alle tomando una potencia y no dejando
ningÃºn punto de extinciÃ³n

\[
u(t) = \alpha \left( 1 - \frac{u}{u_{\infty}} \right) \left( \frac{u}{u_{min}} \right)^k u.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       fba88321-65e9-4bf7-b90e-608b10608f7e
:DRILL_LAST_INTERVAL: 13.021
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:16]
:END:
EcuaciÃ³n del modelo con efecto Alle *dÃ©bil*.

******* Answer
\[
u(t) = \alpha u\left( 1 - \frac{u}{u_{\infty}} \right) \left( \frac{u}{u_{min}} \right)^k
\]

**** 8.3. Transformada de Fourier
***** Clase de Schwartz
La *clase de Schwartz* es el espacio de las funciones complejas
infinitamente diferenciables que tienen, ella y sus derivadas,
decrecimiento mÃ¡s rÃ¡pido que cualquier polinomio en infinito.

\[
{\cal S}(\mathbb{R}) =
\left\{ f \in {\cal C}^{\infty}(\mathbb{R},\mathbb{C}) \mid
\sup_{x \in \mathbb{R}} \abs{x^kf^{(n)}(x)} < \infty,
\forall k,n \geq 0
\right\}
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-30 Mon>
:PROPERTIES:
:ID:       ee0ff838-94f3-4200-bfe8-8b104c8b3f04
:DRILL_LAST_INTERVAL: 33.213
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:17]
:END:
Clase de Schwartz.

******* Answer

\[
{\cal S}(\mathbb{R}) =
\left\{ f \in {\cal C}^{\infty}(\mathbb{R},\mathbb{C}) \mid
\sup_{x \in \mathbb{R}} \abs{x^kf^{(n)}(x)} < \infty,
\forall k,n \geq 0
\right\}
\]

Espacio de las funciones complejas infinitamente diferenciables que
tienen, ella y sus derivadas, decrecimiento mÃ¡s rÃ¡pido que cualquier
polinomio en infinito.

***** FunciÃ³n de Gauss
La *funciÃ³n de Gauss* estÃ¡ en ${\cal S}(\mathbb{R})$ y es un punto fijo
de la transformada de Fourier.

\[
G(x) = e^{-\pi x^2}
\]

****** Card: definiciÃ³n                                                                                    :drill:
SCHEDULED: <2018-08-13 Mon>
:PROPERTIES:
:ID:       b3bc2aba-9d48-4ece-bfd1-b326b55f2db0
:DRILL_LAST_INTERVAL: 47.3129
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:21]
:END:
CuÃ¡l es la funciÃ³n de Gauss, punto fijo de la transformada de Fourier.

******* DefiniciÃ³n

\[
G(x) = e^{-\pi x^2}
\]

****** Card: integral                                                                                      :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       89250ba0-0c07-4417-a4af-deb2c7a043e9
:DRILL_LAST_INTERVAL: 4.1245
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:24]
:END:
Calcula la integral

\[
\int^{+\infty}_{-\infty} e^{-\pi x^2}\,dx = \dots
\]

******* Integral

\[
\int^{+\infty}_{-\infty} e^{-\pi x^2}\,dx = 1
\]

***** Transformada de Fourier
Se define la *transformada de Fourier* de $f \in {\cal S}(\mathbb{R})$ como
la funciÃ³n $\hat{f} \in {\cal S}(\mathbb{R})$ siguiente

\[
\hat{f}(y) = {\cal F}[f](y) = \int_{-\infty}^{+\infty} f(x) e^{-2\pi i xy}\,dx.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-03 Fri>
:PROPERTIES:
:ID:       122f1a79-d63b-4d44-8943-47b0cc09d663
:DRILL_LAST_INTERVAL: 36.9634
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:20]
:END:
Calcula la transformada de Fourier de $f$.

******* Answer

\[
\hat{f}(y) = \int_{-\infty}^{+\infty} f(x) e^{-2\pi i xy}\,dx.
\]

***** Transformada inversa de Fourier
Se define la *transformada inversa de Fourier* de $g \in {\cal S}(\mathbb{R})$ como
la funciÃ³n $\check{g} \in {\cal S}(\mathbb{R})$ siguiente

\[
\check{g}(x) = {\cal F}^{-1}[g](x) = \int_{-\infty}^{+\infty}g(y) e^{2\pi i xy}\,dy
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-01 Wed>
:PROPERTIES:
:ID:       3a53e04d-fbe2-4208-aff0-fe746fea4d70
:DRILL_LAST_INTERVAL: 34.5136
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:17]
:END:
Calcula la transformada inversa de Fourier de $g$.

******* Answer

\[
\check{g}(x) = \int_{-\infty}^{+\infty}g(y) e^{2\pi i xy}\,dy
\]

***** TODO Distribuciones temperadas
***** ConvoluciÃ³n de funciones
Definimos la convoluciÃ³n de $f,g \in {\cal S}(\mathbb{R})$ como

\[
(f \ast g)(x) = (g \ast f)(x) = \int_{\mathbb{R}} f(x-y)g(y)\,dy.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-08 Wed>
:PROPERTIES:
:ID:       104c0f7b-d8c4-448a-8486-3af033bba368
:DRILL_LAST_INTERVAL: 42.4715
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:22]
:END:
Define la convoluciÃ³n $(f \ast g)$

******* DefiniciÃ³n

\[
(f \ast g)(x) = \int_{\mathbb{R}} f(x-y)g(y)\,dy.
\]

***** Teorema 31. Propiedades de Fourier y la convoluciÃ³n
Para $f,g \in {\cal S}(\mathbb{R})$ se tiene

 1. $F$ es una biyecciÃ³n sobre la clase de Swartz,

    \[
    f(x) = \int_{-\infty}^{\infty} e^{2\pi ixy} \left( \int_{-\infty}^{\infty}f(z)e^{-2\pi izy}\,dz \right)\,dy.
    \]

 2. La funciÃ³n de Gauss es un punto fijo de la transformada,

    \[
    F[G] = G.
    \]

 3. La transformada convierte derivadas en polinomios multiplicando

    \[
    F[f'](y) = 2\pi iy F[f](y).
    \]

 4. Convierte dilataciones en homotecias y dilataciones. Dada $h_af(x) = f(ax)$,
    se tiene
    
    * \[F[h_af](y) = \frac{1}{\abs{a}}F[f] \left( \frac{y}{a} \right)\]

    * \[F^{-1}[h_ag](x) = \frac{1}{\abs{a}}F^{-1}[g] \left( \frac{x}{a} \right)\]

 5. Convierte convoluciones en productos y productos en convoluciones,

    \[
    F[f \ast g] = F[f]F[g],\quad F[fg] = F[f]\ast F[g].
    \]

 6. Si $f \in L^2(\mathbb{R})$ y para $G_{\varepsilon}(x) = \frac{1}{\sqrt{\varepsilon}}G \left( \frac{x}{\sqrt{\varepsilon}} \right)$, se tiene $f \ast G_{\varepsilon} \in {\cal C}^{\infty}(\mathbb{R})$,
    y ademÃ¡s converge a $f$ en $L^2$ cuando $\varepsilon \to 0$. Cuando $f$ es continua, la
    convergencia es uniforme en compactos.

 7. Transformada de la Delta de Dirac

    \[
    F[\delta_0] = 1,\quad\mbox{ y }\quad
    G_{\varepsilon} \overset{\varepsilon \to 0}\longrightarrow \delta_0.
    \] 

****** Proof 1                                                                                             :extra:
****** TODO Proof 2
****** Proof 3
Integrando por partes.

****** Proof 4
IntegraciÃ³n por sustituciÃ³n.

****** Proof 5
Usando Fubini con las integrales y haciendo una cambio de variable
para que una no dependa del valor de $z$.

****** Proof 6                                                                                             :extra:

****** Proof 7
Interpretando la transformada de Fourier en el sentido dÃ©bil.

****** Card: transformada de derivada                                                                      :drill:
SCHEDULED: <2018-06-26 Tue>
:PROPERTIES:
:ID:       6e04de59-3992-46f7-b0a7-a9e9f54d4c95
:DRILL_LAST_INTERVAL: 20.121
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:13]
:END:
Calcula la transformada de una derivada.

\[F[f'](y) = \dots\]

******* Answer
La transformada convierte derivadas en polinomios multiplicando

\[
F[f'](y) = 2\pi iy F[f](y).
\]

**** 8.4. ResoluciÃ³n de la ecuaciÃ³n del calor
Resolvemos la ecuaciÃ³n del calor

\[\left.\begin{aligned}
v_t &= Dv_{xx} \\
v_t(0,x) &= f(x),
\end{aligned}\right\}\]

simplificamos primero tomando $u(t,x) = v(t,\sqrt{D} x)$ quedando

\[\left.\begin{aligned}
u_t &= u_{xx} \\
u_t(0,x) &= f(\sqrt{D}x).
\end{aligned}\right\}\]

***** SoluciÃ³n fundamental
La versiÃ³n fundamental de la ecuaciÃ³n, con Delta de Dirac

\[\left.\begin{aligned}
U_t &= U_{xx} \\
U_t(0,x) &= \delta_0(x).
\end{aligned}\right\}\]

Tiene soluciÃ³n

\[
U = \frac{1}{\sqrt{4\pi t}}e^{-\frac{x^2}{4t}}.
\]

****** ResoluciÃ³n
Tomamos Fourier en espacio

\[
\hat{u}(t,y) = \int_{-\infty}^{+\infty} e^{-2\pi i xy}u(t,x)\,dx
\]

y obtenemos una ecuaciÃ³n que tiene como soluciÃ³n a $h_{\sqrt{4\pi t}}G$; al
volver a aplicar la transformada inversa de Fourier obtenemos
la soluciÃ³n.

***** SoluciÃ³n general
La ecuaciÃ³n del calor

\[\left.\begin{aligned}
v_t &= Dv_{xx} \\
v_t(0,x) &= f(x),
\end{aligned}\right\}\]

tiene soluciÃ³n explÃ­cita

\[
v(t,x) = \frac{1}{\sqrt{4\pi Dt}} \int_{-\infty}^{+\infty} e^{-\frac{(x-y)^2}{4tD}} f(y)\,dy
\]

****** Proof
Tomando $u(t,x) = v(t,\sqrt{D} x)$ queda

\[\left.\begin{aligned}
u_t &= u_{xx} \\
u_t(0,x) &= f(\sqrt{D}x) = f_D(x).
\end{aligned}\right\}\]

pero como tenemos resuelta la versiÃ³n fundamental

\[\left.\begin{aligned}
U_t &= U_{xx} \\
U_t(0,x) &= \delta_0(x).
\end{aligned}\right\}\]

por

\[
U = \frac{1}{\sqrt{4\pi t}}e^{-\frac{x^2}{4t}}.
\]

tomamos convoluciones, aprovechando que la derivada entra y sale
de las convoluciones para resolver la $u$ como

\[
u(t,x) = \frac{1}{\sqrt{4\pi t}}\int_{\mathbb{R}} e^{-\frac{(x-y)^2}{4t}}f_D(y)\,dy
\]

y finalmente aplicamos un cambio de variables para tener la soluciÃ³n
explÃ­cita

\[
v(t,x) = \frac{1}{\sqrt{4\pi Dt}} \int_{-\infty}^{+\infty} e^{-\frac{(x-y)^2}{4tD}} f(y)\,dy.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       b2aab25a-7757-4e23-91f4-eab49a0dc98c
:DRILL_LAST_INTERVAL: 14.3904
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:15]
:END:
SoluciÃ³n explÃ­cita de la ecuaciÃ³n del calor.

\[\left.\begin{aligned}
v_t &= Dv_{xx} \\
v_t(0,x) &= f(x),
\end{aligned}\right\}\]

******* Answer

\[
v(t,x) = \frac{1}{\sqrt{4\pi Dt}} \int_{-\infty}^{+\infty} e^{-\frac{(x-y)^2}{4tD}} f(y)\,dy
\]

***** Comentarios sobre unicidad
En general no tendrÃ­amos unicidad salvo que impusiÃ©ramos una condiciÃ³n
sobre decaimiento en infinito. Tenemos de hecho una familia de
soluciones

\[
u(x,t) = \sum_{k=0}^{\infty} \frac{x^{2k}}{(2k)!} \dv[k]{}{t} e^{-\frac{1}{t^2}}.
\]

***** Propiedades de las soluciones del calor

 1. *ConservaciÃ³n del signo y principio del mÃ¡ximo*. Si $f \geq 0$ y no es nula,
    entonces $v > 0$. En general, si $f_1,f_2$ tienen soluciones $v_1,v_2$, no son
    idÃ©nticas y $f_1 \geq f_2$, entonces $v_1 > v_2$.

 2. *ConservaciÃ³n de la masa*. La masa es constante en todos los
    instantes de tiempo.

    \[
    \int v(t,x)\,dx = \int f(x)\,dx.
    \]

 3. *DifusiÃ³n final*. Existe $c_f$ tal que para cualquier $t > 0$,
    
    \[
    \abs{v(t,x)} \leq \frac{c}{\sqrt{t}}.
    \]

 4. *Soporte infinito*. Aunque el soporte de $f\neq 0$ sea finito, el de $v$
    es instantÃ¡neamente infinito, la difusiÃ³n es instantÃ¡nea a todos
    los puntos.

 5. *DisipaciÃ³n de la energÃ­a*. La energÃ­a se va disipando, porque se
    cumple

    \[
    \dv{}{t} \left( \frac{1}{2}\int_{\mathbb{R}} (v(t,x))^2\,dx \right) =
    -D \int_{\mathbb{R}} \abs{v_x(t,x)}^2\,dx.
    \]

**** 8.5. Ecuaciones de reacciÃ³n-difusiÃ³n

\[
\dv{}{t} \left( \int_{\Omega} u(t,x)\,dx \right) =
\int_{\Omega} f(u)\,dx + \int_{\partial\Omega} (- J \cdot n)\,dx
\]

***** FKPP
Es una ecuaciÃ³n reacciÃ³n-difusiÃ³n en dimensiÃ³n $n = 1$. Se obtiene
tomando *crecimiento logÃ­stico* $f(u) = \alpha ( 1 - \frac{u}{u_{\infty}}) u$,

\[
u_t - D u_{xx} = \alpha u \left( 1 - \frac{u}{u_{\infty}} \right).
\]

Adimensionalizando,

\[
v_t = v_{yy} + (1-v)v.
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       f5e1e842-dfc2-48f6-af9f-c1b6fa6f7a57
:DRILL_LAST_INTERVAL: 14.7985
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-06 Wed 12:06]
:END:
Forma adimensionalizada de una FKPP.

******* Answer

\[
v_t = v_{yy} + (1-v)v.
\]

***** Biestable
Se obtiene introduciendo el efecto Alle,

\[
u_t - Du_{xx} = \alpha u \left( 1 - \frac{u}{u_{\infty}} \right) \left( \frac{u}{u_{min}} - 1 \right)
\]

adimensionalizando y llamando $\beta = u_{min}/u_{\infty}$, obtenemos

\[
v_{\tau} = v_{yy} + v(1-v)(v-\beta).
\]

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-06-11 Mon>
:PROPERTIES:
:ID:       e40e461b-ca97-43f6-a141-d4ade2fc4077
:DRILL_LAST_INTERVAL: 7.3791
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:15]
:END:
Forma adimensionalizada de una biestable.

******* Answer

\[
v_{\tau} = v_{yy} + v(1-v)(v-\beta).
\]

**** 8.6. Ondas viajeras
***** Ondas viajeras
Las *ondas viajeras* son soluciones de reacciÃ³n-difusiÃ³n en el caso
unidimensional de la forma $u(\tau,y) = \phi(y - c\tau)$. CumplirÃ¡n

\[
\phi'' + c\phi' + f(\phi) = 0
\]

con condiciones $\phi(-\infty) = 1$, $\phi(+\infty)=0$. Y multiplicando podemos
comprobar que

\[\mathrm{sgn}(c) = \mathrm{sgn}\left(\int_0^1f\right).\]

Luego,

 * en la FKPP, las ondas tienen $c$ positiva y avanzan;
 * en la biestable, las ondas pueden retroceder.

***** Ondas viajeras de la biestable
Las ondas viajeras para $u(1-u)(u-\alpha)$ se buscan con una
constante $A$ que cumpla $\phi' = A\phi(1-\phi)$. Derivando y aplicando
en la ecuaciÃ³n de ondas viajeras, sabiendo que la soluciÃ³n queremos
que sea decreciente, llegamos a $A = -1/\sqrt{2}$ y a $c = \sqrt{2}(1/2-\beta)$.
Finalmente, puede resolverse con variables separadas para llegar a

\[
\phi(y) = \frac{1}{1 + ke^{(y-y_0)/\sqrt{2}}}.
\]

***** Ondas viajeras de la FKPP
*** Ejercicios
**** Examen 25 mayo 2016 (LÃ³pez) [1/3]
***** TODO Ejercicio 1
***** CHECK [#A] Ejercicio 2
La derivada clÃ¡sica es derivada dÃ©bil cuando todo es ${\cal C}^1$. Hay
que comprobar los $L^p$ para $1 < p <\infty$ pero se hace fÃ¡cil porque
se puede acotar todo dentro de $\Omega$.

***** TODO [#A] Ejercicio 3
**** Examen 24 junio 2016 [3/3]
***** DONE Ejercicio 1

 1. Derivada dÃ©bil de una funciÃ³n con derivada clÃ¡sica.  Acotamos a la
    $f$ y trabajamos con eso.

 2. PoincarÃ© sobre ella y sobre su derivada.

 3. Linealidad y continuidad por Cauchy-Schwartz.

 4. Lax-Milgram por lo anterior.

 5. Hay que realizar todo el Euler-Lagrange para obtener las condiciones
    de contorno.

***** DONE Ejercicio 2
Nos fijamos que tenemos un Problema Sturm-Liouville implÃ­cito y podemos,
o buscar los autovalores, o darnos cuenta de que 0 lo es y debe ser el
primer autovalor.

***** DONE Ejercicio 3
Aplicamos resoluciÃ³n de la ecuaciÃ³n de ondas.

***** Ejercicio 4 (Parte II)

**** Examen 15 septiembre 2016 (LÃ³pez) [0/4]
***** TODO Ejercicio 1
***** TODO Ejercicio 2
***** TODO Ejercicio 3
***** TODO Ejercicio 4
**** Examen 3 mayo 2017 [3/5]
***** TODO Ejercicio 1
Muy interesante porque tiene todo lo que se estudia, pero es Fourier y
Liouville.

***** DONE Ejercicio 2
***** DONE Ejercicio 3
***** TODO Ejercicio 4
***** DONE Ejercicio 5
**** Examen 31 mayo 2017 [6/6]
***** DONE Ejercicio 1.1 (Skellam)
***** DONE Ejercicio 1.2 (Skellam)
***** DONE Ejercicio 1.3 (Skellam)
***** DONE Ejercicio 2.1 (Robertson)
***** DONE Ejercicio 2.2 (Robertson)
***** DONE Ejercicio 2.3 (Robertson)
**** Examen 19 junio 2017 [2/2]
***** DONE 2. Lax-Milgram
***** DONE 3. Sturm-Liouville
**** Examen 8 septiembre 2017 [6/6]
***** DONE Ejercicio 1
***** DONE Ejercicio 2

 * El punto 2.3 es especialmente importante porque recuerda que no vale
   con $L_{loc}$ para que la derivada dÃ©bil y clÃ¡sica sean iguales. Necesitamos
   derivabilidad y continuidad respecto una variable.

***** DONE Ejercicio 3
***** DONE Ejercicio 4.a
***** DONE Ejercicio 4.b
***** DONE Ejercicio 4.c
**** Ejemplos de los apuntes [1/1]
***** DONE Problema de Dido
**** Resolver ondas
#+begin_statement
Resolver la ecuaciÃ³n de ondas en $[0,\infty) \times [0,L]$  con condiciones
\[\left\{\begin{array}{c}
u(t,0) = u(t,L) = 0 \\
u(0,x) = u_0(x),\\ 
u_t(0,x) = v_0(x).
\end{array}\right\}\]
en el caso $c = 1$, $L = \pi$, $u_0(x) = \sin(175x)$ y $v_0=\sin(3x)$.
#+end_statement

Notamos que la serie de Fourier de $u_0$ tiene coeficiente $b_{175}=1$ pero
$b_m = 0$ en cualquier $m \neq 175$. Mientras que la serie de $v_0$ tiene $a_3 = 1$
y $a_m = 0$ en otro caso.

La soluciÃ³n es por tanto,

\[\begin{aligned}
u(t,x) &= \sum_{n \geq 1} \sin(nx)
\left(
\frac{a_n}{n} \sin(nt) + b_n \cos(nt)
\right) \\&=
\sin(175x)\cos(175t) + \frac{1}{3}\sin(3x)\sin(3t).
\end{aligned}\]

*** Entregas
# Escribir nombre y apellidos, curso, asignatura, aÃ±o, firma.

**** Ejercicio 1
#+begin_statement
Encontrar una funciÃ³n $\phi \in {\cal C}^{\infty}_{0}[x_0,x_1]$ tal que $\phi(x) > 0$ Ãºnicamente en
$(x_2-\varepsilon,x_2+\varepsilon) \subset [x_0,x_1]$ y $\phi(x) = 0$ en otro caso, para un $x_2$ dado.
#+end_statement

Sin pÃ©rdida de generalidad, tratamos sÃ³lo el caso $(-1,1)$,
recuperando el resto de casos restringiendo la funciÃ³n y usando
homotecias y traslaciones, que son funciones suaves.  Buscaremos
una funciÃ³n tal que todas sus derivadas se aproximen a
cero en los extremos para concatenarla con la funciÃ³n constantemente
cero.  Tomamos por tanto,
\[
f(x) = \left\{
\begin{array}{cl}
\exp({\frac{-1}{(1-x)(1+x)}}) & \mbox{ si } -1 < x < 1, \\
0 & \mbox{ en otro caso.} 
\end{array}\right.
\]

Comprobaremos que es ${\cal C}^{\infty}$ trabajando en $-1 < x < 1$.
Si llamamos $y = 1/(1-x^2)$, la funciÃ³n es de la forma $P(x)y^ne^{-y}$
con $n \in \mathbb{N}_0$ y $P$ polinomio. Sabiendo que $y' = 2xy^2$, la derivada de una
funciÃ³n asÃ­ vuelve a ser de la misma forma para otro polinomio $Q$.
\[
\dv{}{x} \left( P(x)\frac{y^n}{e^y} \right) = 
P'(x) \frac{y^n}{e^y} + 
P(x) \left( 2xy^2n \frac{y^{n-1}}{e^y} + 2xy^2\frac{y^n}{e^y} \right) =
\frac{y^{n+2}}{e^y}Q(x)
\]

Finalmente, por regla de L'HÃ´pital, cualquier funciÃ³n de esta forma
tiene, para $z = -1^+ \text{ Ã³ }1^-$,
\[\lim_{x \to z}
\left( P(x) y^n e^{-y} \right) =
P(z) \lim_{y \to +\infty}
\left(\frac{y^n}{e^y} \right) = 0.
\]

AsÃ­, nuestra $f$ es continua y derivable por coincidir en $-1$ y $1$ las
derivadas laterales, y cada una de sus derivadas vuelve a serlo por
el mismo razonamiento.

**** Ejercicio 2
#+attr_latex: :options [Teorema 4]
#+begin_statement
Dado un funcional ${\cal F} \colon D \to \mathbb{R}$ donde
\[
D = \left\{ y \in {\cal C}^1[x_0,x_1] \mid y(x_0) = y_0, y(x_1) = y_1 \right\},
\]
para $x_0 < x_1, y_0 < y_1 \in \mathbb{R}$, de la forma
\[
{\cal F}(y) = \int_{x_0}^{x_1} F(x,y(x),y'(x)) \; dx
\]
para una $F : [x_0,x_1] \times \Omega \to \mathbb{R}$ con $\Omega$ dominio de $\mathbb{R}^2$, continua
y derivable respecto $x$ e $y$; tenemos que cualquier $\overline{y} \in {\cal D} \cap {\cal C}^2[x_0,x_1]$
que minimice el funcional cumple la EcuaciÃ³n de Euler-Lagrange.
\[
\pdv{F}{y} \left( x,\overline{y}(x), \overline{y}'(x) \right) -
\dv{}{x}\left( \pdv{F}{p} (x, \overline{y}(x), \overline{y}'(x)) \right) = 0.
\]

En estas condiciones, si ademÃ¡s $\pdv{F}{y} = 0$, entonces tenemos
que $F_p(x,\overline{y}'(x)) = \mathrm{cte.}$
#+end_statement
#+begin_proof
Por la EcuaciÃ³n de Euler-Lagrange se tiene
\[-\dv{}{x}\left( \pdv{F}{p} (x, \overline{y}(x), \overline{y}'(x)) \right) = 0,
\]
y por ser derivable con derivada nula es constante.
#+end_proof

#+attr_latex: :options [Teorema 5]
#+begin_statement
En las mismas condiciones anteriores, si se tiene $\pdv{F}{x} = 0$, entonces
se cumple
\[
F(\overline{y}(x),\overline{y}'(x)) - y'(x) F_p(\overline{y}(x),\overline{y}'(x)) = \mathrm{cte.}
\]
#+end_statement
#+begin_proof
Derivamos usando linealidad y regla de la cadena, siendo la Ãºltima
igualdad la ecuaciÃ³n de Euler-Lagrange.
\[\begin{aligned}
\dv{}{x} &\Big(F\big(\oy,\oy'\big) -
\oy' \pdv{F}{p} \big(\oy,\oy'\big) \Big) \\&=
\oy' \cdot \pdv{F}{y} (\oy, \oy') +
\oy'' \cdot \pdv{F}{p} (\oy, \oy')
-\oy'' \cdot \pdv{F}{p} (\oy, \oy') - 
\oy' \cdot \dv{}{x} \pdv{F}{p} (\oy, \oy') \\&=
\oy' \left( \pdv{F}{y} (\oy, \oy') - 
\dv{}{x} \pdv{F}{p} (\oy, \oy') \right) \\&= 0.
\end{aligned}\]
De nuevo, por tener derivada nula, debe ser constante.
#+end_proof

**** Ejercicio 3: sÃ³lo en clase
#+begin_statement
Encontrar funciones $y \colon [x_0,x_1]\to \mathbb{R}$ que minimicen la superficie de
revoluciÃ³n de la grÃ¡fica fijado $y(x_1) = y_1 > 0$ and $y(x_0) = y_0$.
#+end_statement

El Ã¡rea bajo la grÃ¡fica se mide como

\[
A(y) = \int_{x_0}^{x_1} 2\pi y(x) \sqrt{1 + y'(x)^2} \;dx
\]

Por Euler-Lagrange tenemos $F - y' F_p = \mathrm{cte}$. Es decir,

\[
\frac{2 \pi y(x)}{\sqrt{1 + y'(x)^2}} = \mathrm{cte}.
\]

Asi que resolvemos la ecuaciÃ³n diferencial $y = C \sqrt{1 + y'^2}$.
Es una ecuaciÃ³n de Lagrange que se resuelve llamando $p(x) = y'(x)$,
y derivando respecto de $x$

\[
p = C \left( \frac{pp'}{\sqrt{1 + p^2}} \right)
\]

Llegando a la ecuaciÃ³n diferencial siguiente para una constante $D$
excepto en el caso $C=0$, que darÃ­a $y' = 0$ como soluciÃ³n; o el caso
$p = 0$, (???) que llevarÃ­a a lo mismo.

\[
p' = D\sqrt{1 + p^2}
\]

Resolvemos por variables separadas

\[
\int \frac{D}{\sqrt{1+p^2}} \;dp = D\log|\sqrt{1 + p^2} + p| = x + E
\]

Despejamos, quitando el caso $D = 0$,

**** Ejercicio 4
:PROPERTIES:
:ID:       de1af79c-3a0b-410e-8cca-d26c92021ec8
:END:
#+begin_statement
Hemos probado que cada problema de contorno con condiciones de
Dirichlet no homogÃ©neas es equivalente a otro con condiciones de
Dirichlet homogÃ©neas.  Enunciar y demostrar un resultado anÃ¡logo para
condiciones tipo Neumann no homogÃ©neas.
#+end_statement

Dado un problema con condiciones de contorno tipo Newmann
\[\left\{\begin{array}{l}
(Py')' + Qy = R, \\
y'(x_0) = y'_0, \\
y'(x_1) = y'_1,
\end{array}\]
construiremos un problema equivalente tomando un cambio de variable
de la forma $z(x) = y(x) - (Ax^2 + Bx)$. Buscamos que las condiciones
sean homogÃ©neas,
\[\left\{\begin{array}{l}
z'(x_0) = y'_0 - (2Ax_0 + B) = 0, \\
z'(x_1) = y'_1 - (2Ax_1 + B) = 0, \\
\end{array}\]
por lo que obtenemos $A = (y'_1 - y'_0)/(2(x_1 - x_0))$ y
$B = (y'_0x_1-x_0y'_1)/(x_1-x_0)$. Si ademÃ¡s buscamos $(Pz')' + Qz = {\widetilde R}$.
\[\begin{aligned}
{\widetilde R} &= (Pz')' + Qz \\
               &= (Py')' - (P(2Ax + B))' + Qy - Q(Ax^2 + Bx) \\
               &= R - (P(2Ax + B))' - Q(Ax^2 + Bx) \\
               &= R - P'(2Ax + B) - 2AP - Q(Ax^2 + Bx).
\end{aligned}\]
AsÃ­, hemos llegado a que el problema original es equivalente a otro
de la forma siguiente, con la $\widetilde R$ dada anteriormente.
\[\left\{\begin{array}{l}
(Pz')' + Qz = {\widetilde R}, \\
z'(x_0) = 0, \\
z'(x_1) = 0.
\end{array}\]

**** Ejercicio 5
#+begin_statement
Demostrar que la ecuaciÃ³n general de la viga, $Mu^{iv}(x) + Nu''(x) = f(x)$ 
necesariamente la verifica un mÃ­nimo del funcional siguiente.
\[
{\cal F}(u) = \int_0^L  \left(
\frac{M}{2}(u''(x))^2 -
\frac{N}{2}(u'(x))^2 -
f(x)u(x) 
\right)\;dx
\]
#+end_statement

Empezamos llamando
\[
F(x,y,p,q) = \frac{M}{2}q^2 - \frac{N}{2}p^2 - f(x)y.
\]
Si $\overline{u}$ fuera un mÃ¡ximo, consideramos $g(s) = {\cal F}(\overline{u} + s\phi)$ tomando
$|s| < \varepsilon$ suficientemente pequeÃ±o para que se cumpla $\overline{u} + s\phi \in \Omega$
y $\phi \in D_0 = \left\{ \phi \in {\cal C}^2[0,L] \mid \phi(0)=\phi(L), \phi'(0)=\phi'(L) \right\}$. La condiciÃ³n
de minimalidad nos da $g'(0) = 0$. Usando regla de derivaciÃ³n bajo una
integral y regla de la cadena en varias variables.
\[\begin{aligned}
g'(s) &= \dv{}{s} \left( \int_0^L 
F(x;\overline{u}(x) + s\phi(x), \overline{u}'(x) + s\phi'(x), \overline{u}''(x) + s\phi''(x))
\;dx\right) \\&=
\int_0^L  \dv{}{s}
F(x;\overline{u}(x) + s\phi(x), \overline{u}'(x) + s\phi'(x), \overline{u}''(x) + s\phi''(x))
\;dx \\&=
\left( \int_0^L F_y(...)\phi(x) \;dx \right) +
\left( \int_0^L F_p(...)\phi'(x) \;dx \right) +
\left( \int_0^L F_q(...)\phi''(x) \;dx \right)
\end{aligned}\]
Usando integraciÃ³n por partes y las condiciones sobre $\phi$ para
anular los tÃ©rminos.
\[\begin{aligned}
g'(s) &= 
 \int_0^L F_y(...)\phi(x)\;dx -
 \int_0^L \dv{}{x}F_p(...)\phi(x)\;dx +
 \int_0^L \dv[2]{}{x}F_q(...)\phi(x)\;dx \\&=
\int_0^L \left( F_y(...) - \dv{}{x}F_p(...) + \dv[2]{}{x}F_{q}(...) \right)\phi(x)\;dx
\end{aligned}\]
Podemos evaluar en $s = 0$ y aplicar la construcciÃ³n del Teorema 1
para encontrar una $\phi \in D_0$ anulÃ¡ndose en los extremos y con derivada
nula en los extremos tal que si el primer factor fuera distinto de
cero en algÃºn punto, la integral deberÃ­a ser distinta de cero.
Tenemos entonces
\[
F_y(x,\overline{u},\overline{u}',\overline{u}'') - 
\dv{}{x}F_p(x,\overline{u},\overline{u}',\overline{u}'') + 
\dv[2]{}{x}F_{q}(x,\overline{u},\overline{u}',\overline{u}'') = 0.
\]
Calculando obtenemos $F_y(...) = -f(x)$, $F_p(...) = -Nu'(x)$, $F_q(...) = Mu''(x)$,
por lo que la ecuaciÃ³n final es precisamente
\[
Mu^{iv}(x) + Nu''(x) - f(x) = 0.
\]

**** Ejercicio 6
:PROPERTIES:
:ID:       e15aa2f8-76b4-4aa6-82bb-c0b424bda884
:END:
#+begin_statement
Calcular los valores y vectores propios del problema de Sturm-Liouville
en $L^2(x_0,x_1)$: $y'' + \lambda y = 0$, con $y(x_0) = 0 = y(x_1)$.
#+end_statement

Tratamos por casos el problema segÃºn el signo de $\lambda$.
En el caso $\lambda < 0$, el polinomio caracterÃ­stico $\mu^2 + \lambda = 0$ tiene
soluciones $\mu = \pm \sqrt{-\lambda}$, que dan lugar a soluciones de la
ecuaciÃ³n de la forma

\[
y = Ae^{\sqrt{-\lambda} x} + Be^{-\sqrt{-\lambda}x};
\]

que al exigir $y(x_0) = y(x_1) = 0$ nos llevan a un sistema
\[\begin{pmatrix}
e^{\sqrt{-\lambda} x_0} & e^{-\sqrt{-\lambda} x_0} \\
e^{\sqrt{-\lambda} x_1} & e^{-\sqrt{-\lambda} x_1} 
\end{pmatrix}
\begin{pmatrix} A \\ B \end{pmatrix} =
\begin{pmatrix} 0 \\ 0 \end{pmatrix}\]

donde, al tener la matriz determinante $e^{\sqrt{-\lambda}(x_0-x_1)} - e^{\sqrt{-\lambda}(x_1-x_0)} = 0$,
sabemos que si $x_0 \neq x_1$ sÃ³lo tendrÃ¡ la soluciÃ³n trivial.
En el caso $\lambda = 0$ tendrÃ­amos soluciones de la forma $y = Ax + B$,
y al imponer las condiciones, llegarÃ­amos a la soluciÃ³n trivial.

En el caso $\lambda > 0$, tendrÃ­amos soluciones del polinomio $\mu = i\sqrt{\lambda}$ que
darÃ­an lugar a soluciones de la ecuaciÃ³n de la forma
\[
y = A \cos(\sqrt{\lambda}x) + B \sin(\sqrt{\lambda} x)
\]

que al exigir $y(x_0) = y(x_1) = 0$ nos llevan a un sistema
\[\begin{pmatrix}
\cos(\sqrt{\lambda}x_0) & \sin(\sqrt{\lambda}x_0) \\
\cos(\sqrt{\lambda}x_1) & \sin(\sqrt{\lambda}x_1) \\
\end{pmatrix}\begin{pmatrix} A \\ B \end{pmatrix} =
\begin{pmatrix} 0 \\ 0 \end{pmatrix}\]

donde, para que la matriz tenga determinante nulo y existan soluciones
no triviales, debe tenerse
\[
\cos(\sqrt{\lambda} x_0)\sin(\sqrt{\lambda} x_1) - 
\sin(\sqrt{\lambda} x_0)\cos(\sqrt{\lambda} x_1) = 0
\]

que equivale a $\tan(\sqrt{\lambda} x_1) = \tan(\sqrt{\lambda} x_0)$ o $\cos(\sqrt{\lambda}x_0) = \cos(\sqrt{\lambda}x_1) = 0$.
En ambos casos se debe tener $\sqrt{\lambda}(x_1-x_0) = n\pi$ para algÃºn $n \in \mathbb{N}$ y por tanto
$\lambda_n = \left( n\pi/(x_1-x_0) \right)^2$.

Resolviendo el sistema lineal para este valor de $\lambda$ llegamos a
$A = -C_n\sin(n\pi x_0/(x_1-x_0))$ y $B = C_n\cos(n\pi x_0/(x_1-x_0))$
para algÃºn $C_n$. Esto se traduce en soluciones
\[\begin{aligned}
y &= 
-C_n\sin(n\pi\frac{x_0}{x_1-x_0})\cos(n\pi\frac{x}{x_1-x_0}) +
C_n\cos(n\pi\frac{x_0}{x_1-x_0})\sin(n\pi\frac{x}{x_1-x_0}) \\&=
C_n\sin(n\pi\frac{x-x_0}{x_1-x_0}).
\end{aligned}\]

Finalmente, para obtener las funciones propias normalizadas,
integramos
\[
\int_{x_0}^{x_1} C_n^2\sin^2\left(n\pi\frac{x-x_0}{x_1-x_0}\right)\,dx =
C^2_n\frac{(x_1-x_0)}{n\pi}\int_{0}^{n\pi} \sin^2\left(s\right)\,ds =
\frac{1}{2}C^2_n(x_1-x_0) = 1,
\]

y tenemos $C_n = \sqrt{\frac{2}{x_1-x_0}}$. Es decir los valores propios son $\lambda_n = (n\pi/(x_1-x_0))^2$
y las funciones propias normalizadas son
\[
y_n = \sqrt{\frac{2}{x_1-x_0}} \sin(n\pi\frac{x-x_0}{x_1-x_0}).
\]

NÃ³tese que este resultado particulariza al obtenido en clase para $L^2(0,L)$ en el caso $x_0 = 0$
y $x_1 = L$; y que ademÃ¡s puede ser obtenido desde Ã©l aplicÃ¡ndolo en el caso
$L = x_1-x_0$ a la funciÃ³n $\widetilde y(x) = y(x+x_0)$, que cumple $\widetilde{y}(0) = 0 = \widetilde y(L)$.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       cb97f1a8-a3de-415d-9770-7b22e287d9f3
:DRILL_LAST_INTERVAL: 27.0722
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 22:02]
:END:
Valores propios de $y'' + \lambda y = 0$, con $y(x_0) = 0 = y(x_1)$.

/Sturm-Liouville en funciones de cuadrado integrable/.

****** Valores

\[\lambda_n = \left( \frac{n\pi}{x_1-x_0} \right)^2\]

**** Ejercicio 7
#+begin_statement
Calcula $B_n$ para que la funciÃ³n $y_n(x) = B_n\sin(n\pi \log(x))$ cumpla

\[
\int_1^e \frac{y^2_n(x)}{x} \,dx = 1
\]
#+end_statement

Aplicando un cambio de variables tenemos

\[
B^2_n\int_1^e \sin^2 \left( n\pi \log(x) \right) \frac{1}{x}\,dx = 
\frac{B^2_n}{n\pi}\int_0^{n\pi} \sin^2 \left(  s \right) \,ds = 
\frac{B^2_n}{2} = 1.
\]

Luego $B_n = \sqrt{2}$.

\medskip

/(NÃ³tese que este ejercicio lo realizamos en clase en un caso mÃ¡s general)/

**** Ejercicio 8
#+begin_statement
Dado $\alpha \in \mathbb{R}$, calcula la derivada dÃ©bil de $\abs{x}^{\alpha}$ en $\mathbb{R}^{N}\setminus \left\{ 0 \right\}$.
#+end_statement

Tomamos una funciÃ³n $\phi \in {\cal C}^{\infty}_0(\mathbb{R}^N)$ y calculamos, para $u(x) = \abs{x}^{\alpha}$ usando
Fubini
\[\begin{aligned}
\left\langle \pdv{u}{x_i}, \phi \right\rangle = 
-\int_{\mathbb{R}^N} u(x) \pdv{\phi}{x_i}(x)\,dx =
-\int_{\mathbb{R}^{N-1}}\int_{-\infty}^{\infty} 
\left(\sqrt{\abs{\widetilde x}^2 + x_i^2}\right)^{\alpha} 
\pdv{\phi}{x_i}(x) \,dx_i\,d{\widetilde x}
\end{aligned}\]
donde $\widetilde x \in \mathbb{R}^{N-1}$ es la proyecciÃ³n del vector $x$ sin la componente $x_i$,
calculamos
\[\begin{aligned}
=-\int_{\mathbb{R}^{N-1}}
\left(
\int_{-\infty}^{0} 
\left(\sqrt{\abs{\widetilde x}^2 + x_i^2}\right)^{\alpha} 
\pdv{\phi}{x_i}(x) \,dx_i +
\int_{0}^{\infty} 
\left(\sqrt{\abs{\widetilde x}^2 + x_i^2}\right)^{\alpha} 
\pdv{\phi}{x_i}(x) \,dx_i\right)\,d{\widetilde x}
\end{aligned}\]
por partes, y derivando donde sabemos que $|x| \neq 0$ porque $x_i \neq 0$,
\[\begin{aligned}
=& -\int_{\mathbb{R}^{N-1}} \Bigg(
\left[ \abs{x}^{\alpha}\phi(x) \right]^{x_i = \infty}_{x_i = 0}
-\int_{-\infty}^{0} 
\frac{x_i}{|x|}\alpha \abs{x}^{\alpha-1} 
\phi(x) \,dx_i \\&+
\left[ \abs{x}^{\alpha}\phi(x) \right]^{x_i = 0}_{x_i = -\infty} -
\int_{0}^{\infty}
\frac{x_i}{|x|} \alpha \abs{x}^{\alpha-1} 
\phi(x) \,dx_i\Bigg)\,d{\widetilde x}
\end{aligned}\]
donde usamos que $\phi \in {\cal C}^{\infty}_0$ se anula en infinito y ambos tÃ©rminos $|x|^{\alpha}\phi(x)_{|_{x_i=0}}$
se cancelan para tener finalmente
\[
= \int_{\mathbb{R}} \alpha x_i|x|^{\alpha-2}\phi(x)\,dx.
\]
Tenemos entonces el mismo razonamiento para cualquier componente y 
\[
\nabla \abs{x}^{\alpha} = \alpha x |x|^{\alpha-2}
\]
en sentido dÃ©bil.

***** Card                                                                                                  :drill:
SCHEDULED: <2018-06-28 Thu>
:PROPERTIES:
:ID:       7697631c-8f3b-4517-97fe-02cbf69ec0e4
:DRILL_LAST_INTERVAL: 35.3492
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-24 Thu 23:47]
:END:
CuÃ¡l es la derivada dÃ©bil de $|x|^{\alpha}$ para $x \in \mathbb{R}^n$.

****** Derivada

\[
\nabla |x|^{\alpha} = \alpha x|x|^{\alpha-2}
\]

**** Ejercicio 9
#+begin_statement
Prueba que

 1. $\widehat{f \ast g} = \widehat{f}\widehat{g}$,

 2. $\widecheck{f \ast g} =\widecheck{f}\widecheck{g}$,

 3. $\widehat{fg} = \widehat{f}\ast\widehat{g}$,

 4. $\widecheck{fg} = \widecheck{f}\ast\widecheck{g}$.
#+end_statement

1. Usando Fubini y un cambio de variable $x - z \mapsto x$.
  \[\begin{aligned}
  \widehat{f \ast g}(y) &=
  \int (f \ast g)(x) e^{-2\pi ixy}\,dx =
  \int \int f(x-z)g(z) e^{-2\pi ixy}\,dz\,dx \\&=
  \int g(z) \int f(x-z) e^{-2\pi ixy}\,dx\,dz =
  \int g(z) \int f(x) e^{-2\pi i(x+z)y}\,dx\,dz \\&=
  \int g(z)e^{-2\pi izy} \int f(x) e^{-2\pi ixy}\,dx\,dz =
  \widehat{f}(y)\widehat{g}(y).
  \end{aligned}\]

2. Repetimos el argumento, cambiando el signo de la exponencial.
  \[\begin{aligned}
  \widecheck{f \ast g}(y) &=
  \int (f \ast g)(x) e^{2\pi ixy}\,dx =
  \int \int f(x-z)g(z) e^{2\pi ixy}\,dz\,dx \\&=
  \int g(z) \int f(x-z) e^{2\pi ixy}\,dx\,dz =
  \int g(z) \int f(x) e^{2\pi i(x+z)y}\,dx\,dz \\&=
  \int g(z)e^{2\pi izy} \int f(x) e^{2\pi ixy}\,dx\,dz =
  \widecheck{f}(y)\widecheck{g}(y).
  \end{aligned}\]

3. Aplicamos (2) a $\hat{f}$ y $\hat{g}$ y usamos que la transformada de Fourier
   es una biyecciÃ³n con su inversa.

4. Aplicamos (1) a $\check{f}$ y $\check{g}$ y usamos que la transformada de Fourier
   es una biyecciÃ³n con su inversa.

** Curvas y superficies
*** Datos de la asignatura
CÃ©sar Rosales - crosales@ugr.es
Lunes, Martes y MiÃ©rcoles de 12 a 14.
http://www.ugr.es/~crosales/cys1718.html

*** Preliminares ([[~/pdf/Rosales_Preliminares%20a%20curvas%20y%20superficies.pdf][PDF]])
**** 0. Extra
***** BiyecciÃ³n continua de compacto a Hausdorff es homeomorfismo
Sea $f \colon A \to B$ biyecciÃ³n continua. Si $A$ es compacto y $B$ es Hausdorff,
$f$ es homeomorfismo.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-27 Wed>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       d1143d39-749d-4560-8789-7ec31fa25f47
:DRILL_LAST_INTERVAL: 32.4358
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:26]
:END:
BiyecciÃ³n continua de [compacto] a [Hausdorff] es [homeomorfismo].

****** Proof
Tomamos $g = f^{-1}$; sea $U \subset A$ un cerrado. Por ser cerrado de un compacto,
serÃ¡ compacto y $f(U)$ serÃ¡ compacto por continuidad. Por ser $B$ Hausdorff,
un compacto suyo serÃ¡ cerrado. AsÃ­ $g^{-1}(U) = f(U)$ es cerrado y tenemos
continuidad.

**** 1. GeometrÃ­a afÃ­n euclÃ­dea
***** Producto escalar
***** OrientaciÃ³n
Una base $B = \left\{ u_1,\dots,u_n \right\}$ es *positiva* si la matriz de cambio de 
base de $B$ a $B_u$ es positiva. Es decir,

\[\mathrm{det}(u_1 & \dots & u_n) > 0.
\]

****** Card: definiciÃ³n de base positiva                                                                   :drill:
SCHEDULED: <2018-08-23 Thu>
:PROPERTIES:
:ID:       e15a4412-930d-4f6a-aa20-d64196a6f4e2
:DRILL_LAST_INTERVAL: 89.1296
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.167
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:31]
:END:
Fijada la orientaciÃ³n de la base usual, Â¿cuÃ¡ndo es una base
$B = \left\{ u_1,\dots,u_n \right\}$ positiva?

******* DefiniciÃ³n
Cuando la matriz de cambio de base de $B$ a $B_u$ es positiva. Es
decir,

\[\mathrm{det}(u_1 & \dots & u_n) > 0.
\]

***** Bases ortonormales
Para $B= \left\{ u_1,\dots,u_n \right\}$ base ortonormal,

\[\left| v \right|^2 = \sum_{i=1}^n \left\langle v,u_i \right\rangle^2.
\]

En general,

\[
\left\langle u,v \right\rangle = \sum_{i=1}^n \left\langle u,u_i \right\rangle \left\langle v,u_i \right\rangle.
\]

***** Movimientos rÃ­gidos
:PROPERTIES:
:ID:       92d75a96-b872-4ea6-ae5a-b83d975e2b37
:END:
Todo movimiento rÃ­gido es de la forma

\[
\phi(p) = \vec{\phi}(p) + b,
\]

para $\vec{\phi}$ una isometrÃ­a lineal.

****** IsometrÃ­as lineales
Una aplicaciÃ³n $\vec\phi$ es isometrÃ­a lineal si $\left\langle \vec{\phi}(u),\vec{\phi}(v) \right\rangle = \left\langle u,v \right\rangle$, para
cualesquiera $u$ y $v$.

Esto equivale a $|\vec{\phi}(u)| = u$ para cualquier $u$.

****** Matriz asociada, movimientos directos o inversos
La matriz asociada a una isometrÃ­a lineal es ortogonal, $A^{-1} = A^t$,
luego su determinante es $|A| = \pm 1$. La isometrÃ­a es *directa* si
es positivo, e *inversa* si es negativo.

***** Movimientos rÃ­gidos de R2

|----------------------+----------------+--------------+----------|
| Tipo de movimiento   | IsometrÃ­a      | Puntos fijos | CarÃ¡cter |
|----------------------+----------------+--------------+----------|
| Identidad            | Identidad      | Plano        | directo  |
| TraslaciÃ³n           | Identidad      | ninguno      | directo  |
| Giro                 | Giro           | Punto        | directo  |
| ReflexiÃ³n            | SimetrÃ­a axial | Recta        | inverso  |
| ReflexiÃ³n deslizante | SimetrÃ­a axial | ninguno      | inverso  |
|----------------------+----------------+--------------+----------|

****** Card: tabla de movimientos en R2                                                                    :drill:
SCHEDULED: <2018-07-30 Mon>
:PROPERTIES:
:DRILL_CARD_TYPE: hide2cloze
:ID:       16a826aa-c93d-4e46-afb4-05682a832bb1
:DRILL_LAST_INTERVAL: 82.1164
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.75
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-09 Wed 13:36]
:END:
|----------------------+------------------+--------------+-----------|
| Tipo de movimiento   | IsometrÃ­a        | Puntos fijos | CarÃ¡cter  |
|----------------------+------------------+--------------+-----------|
| Identidad            | Identidad        | Todo R2      | directo   |
| TraslaciÃ³n           | Identidad        | [ninguno]    | directo   |
| Giro                 | Giro             | [Centro]     | directo   |
| ReflexiÃ³n            | [SimetrÃ­a axial] | [Eje]        | [inverso] |
| ReflexiÃ³n deslizante | SimetrÃ­a axial   | [ninguno]    | inverso   |
|----------------------+------------------+--------------+-----------|

****** Card: lista de movimientos                                                                          :drill:
SCHEDULED: <2018-08-10 Fri>
:PROPERTIES:
:ID:       836f3f1e-b3f5-4f41-80f1-58203b69513f
:DRILL_LAST_INTERVAL: 63.2101
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:36]
:END:
Lista los movimientos rÃ­gidos de $\mathbb{R}^2$.

******* Movimientos

 1. Identidad.
 2. TraslaciÃ³n.
 3. Giro o rotaciÃ³n.
 4. SimetrÃ­a. (ReflexiÃ³n)
 5. SimetrÃ­a deslizante. (ReflexiÃ³n deslizante)

***** Movimientos rÃ­gidos de R3
Movimientos rÃ­gidos de $\mathbb{R}^3$.

|-----------------------+--------------------+--------------+----------|
| Tipo de movimiento    | IsometrÃ­a          | Puntos fijos | CarÃ¡cter |
|-----------------------+--------------------+--------------+----------|
| Identidad             | identidad          | Espacio      | directo  |
| TraslaciÃ³n            | identidad          | ninguno      | directo  |
| RotaciÃ³n              | RotaciÃ³n           | Recta        | directo  |
| Movimiento helicoidal | RotaciÃ³n           | ninguno      | directo  |
| ReflexiÃ³n             | SimetrÃ­a especular | Plano        | inverso  |
| RelfexiÃ³n deslizante  | SimetrÃ­a especular | ninguno      | inverso  |
| RotaciÃ³n y reflexiÃ³n  | Giro y simetrÃ­a    | Punto        | inverso  |
|-----------------------+--------------------+--------------+----------|

****** Card                                                                                                :drill:
SCHEDULED: <2018-09-02 Sun>
:PROPERTIES:
:DRILL_CARD_TYPE: hide2cloze
:ID:       e3a04df3-1d89-49c7-a0e1-77559f0989f1
:DRILL_LAST_INTERVAL: 67.3146
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:18]
:END:

|-------------------------+----------------------+--------------+----------|
| Tipo de movimiento      | IsometrÃ­a            | Puntos fijos | CarÃ¡cter |
|-------------------------+----------------------+--------------+----------|
| Identidad               | identidad            | Espacio      | directo  |
| [TraslaciÃ³n]            | identidad            | ninguno      | directo  |
| RotaciÃ³n                | [RotaciÃ³n]           | Recta        | directo  |
| [Movimiento helicoidal] | RotaciÃ³n             | ninguno      | directo  |
| ReflexiÃ³n               | [SimetrÃ­a especular] | [Plano]      | inverso  |
| [ReflexiÃ³n deslizante]  | SimetrÃ­a especular   | [ninguno]    | inverso  |
| RotaciÃ³n y reflexiÃ³n    | [Giro y simetrÃ­a]    | Punto        | inverso  |
|-------------------------+----------------------+--------------+----------|

****** Card: lista de movimientos                                                                          :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       0b376393-635e-4b49-a699-418212970a3f
:DRILL_LAST_INTERVAL: 30.0148
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:48]
:END:
Lista los movimientos rÃ­gidos de $\mathbb{R}^3$.

******* Movimientos

 1. Identidad.
 2. TraslaciÃ³n.
 3. RotaciÃ³n.
 4. Movimiento helicoidal.
 5. ReflexiÃ³n respecto un plano.
 6. ReflexiÃ³n deslizante.
 7. RotaciÃ³n (del eje) y reflexiÃ³n (de su plano tangente).

**** 2. Producto vectorial en R3
***** Producto vectorial
Llamamos *producto vectorial* $u \times v$ al Ãºnico vector tal que

\[
\forall w:\quad \mathrm{det}(u,v,w) = \left\langle u\times v,w \right\rangle.
\]

***** Propiedades del producto vectorial
Tenemos

 1. $\times$ bilineal antisimÃ©trica,
 2. $\left\langle u \times v, u \right\rangle = \left\langle u \times v,v \right\rangle = 0$,
 3. $|u \times v|^2 = \mathrm{det}(u,v,u \times v) = |u|^2|v|^2 - \left\langle u,v \right\rangle^2$,
 4. $u \times v = 0$ si y sÃ³lo si son linealmente independientes,
 5. para $\left\{ u,v \right\}$ independientes, $\left\{ u,v,u \times v \right\}$ es base positiva,
 6. para $\left\{ u,v \right\}$ ortonormales, $\left\{ u,v,u \times v \right\}$ es base positiva ortonormal.

****** Proof 1
Desde las propiedades del determinante.

****** Proof 2
Por definiciÃ³n y por las propiedades del determinante.

****** Proof 3
Se comprueba calculando explÃ­citamente $|u \times v|^2$.

****** TODO Proof 4

****** TODO Proof 5

****** Card: norma del producto vectorial                                                                  :drill:
SCHEDULED: <2018-07-13 Fri>
:PROPERTIES:
:ID:       1fd12884-9b75-47ec-ac19-dcb6e51931da
:DRILL_LAST_INTERVAL: 65.64
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-08 Tue 14:02]
:END:
Da la fÃ³rmula de $\left| u \times v \right|^2$ 

 * en funciÃ³n de $|u|,|v|,\left\langle u,v \right\rangle$,

 * y en funciÃ³n de $\mathrm{det}(u,v,-)$.

******* FÃ³rmula

\[ |u \times v|^2 = 
\mathrm{det}(u,v,u \times v) = 
\left| u \right|^2|v|^2 - \left\langle u,v \right\rangle^2 
\]

**** 3. Distancia con signo asociada a un hiperplano
**** 4. Reglas de derivaciÃ³n para funciones de una variable
Para $\alpha , \beta \colon I \to \mathbb{R}^{n}$ curvas ${\cal C}^{\infty}$, dado un $\lambda \colon I \to \mathbb{R}$ diferenciable, se tiene

 1. $\left\langle \alpha(t),\beta(t) \right\rangle' = \left\langle \alpha'(t),\beta(t)  \right\rangle + \left\langle \alpha(t)\beta'(t) \right\rangle$,

 2. $(|\alpha(t)|^2)' = 2 \left\langle \alpha'(t),\alpha(t) \right\rangle$

 3. $|\alpha(t)|' = \pair{\alpha'(t), \frac{\alpha(t)}{|\alpha(t)|}}$ cuando $\alpha(t) \neq 0$ para cada $t \in I$

 4. $(\lambda(t)\alpha(t))' = \lambda'(t)\alpha(t) + \lambda(t)\alpha'(t)$

 5. $(\alpha(t) \times \beta(t))' = \alpha'(t) \times \beta(t) + \alpha(t) \times \beta'(t)$ cuando $n = 3$

**** 5. Diferenciabilidad de funciones de varias variables
***** Diferenciabilidad de una funciÃ³n real de varias variables
:PROPERTIES:
:ID:       d3a1f687-19f0-4129-90a1-04976ef88344
:END:
Una $f \colon O \subset \mathbb{R}^n \to \mathbb{R}$ es *infinitamente diferenciable* $f \in {\cal C}^{\infty}$ si
tiene derivadas parciales continuas de todos los Ã³rdenes. En tal
caso tenemos el *gradiente*,

\[
(\nabla f)(p) = \left( \pdv{f}{x_1}(p), \dots, \pdv{f}{x_n}(p) \right) = 
(f_{x_1}(p), \dots,f_{x_n}(p))
\]

que puede verse como una funciÃ³n $\mathbb{R}^n \to \mathbb{R}$.

***** Diferenciabilidad de una funciÃ³n de varias variables
Decimos $f \in {\cal C}^{\infty}(O,\mathbb{R}^m)$ *diferenciable* si cada $f_i \in {\cal C}^{\infty}(O,\mathbb{R})$ es
[[id:d3a1f687-19f0-4129-90a1-04976ef88344][diferenciable]].

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       7c7a55ed-a5e9-43fd-8bb1-d3830db44c33
:DRILL_LAST_INTERVAL: 21.7992
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.2
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:16]
:END:
Â¿CuÃ¡ndo es $f \in {\cal C}^{\infty}(O,\mathbb{R}^m)$, de varias variables, diferenciable?

******* Diferenciable
Cuando cada una de sus componentes lo es, $f_i \in {\cal C}^{\infty}(O,\mathbb{R})$.

***** Diferencial de una funciÃ³n de varias variables
La *diferencial* de $f \colon \mathbb{R}^n \to \mathbb{R}^m$ es la aplicaciÃ³n lineal $(df)_p\colon \mathbb{R}^n \to \mathbb{R}^m$
que mejor aproxima a $f$ cerca de $p$, es decir

\[
\lim_{h \to 0} \frac{\abs{f(p+h) - f(p) - (df)_p(h)}}{\abs{h}} = 0,
\]

esta viene dada por el Jacobiano, que generaliza al gradiente componente
a componente

\[
(\operatorname{Jac} f)_p = \left( \pdv{f}{x_1} (p), \dots \pdv{f}{x_n} (p) \right);
\]

dicho de otra forma, las filas del jacobiano son los gradientes $(\nabla f_i)(p)$.

**** 6. Difeomorfismos entre abiertos. Teorema de la funciÃ³n inversa
***** Difeomorfismo
:PROPERTIES:
:ID:       ef28dfb1-a3fc-4730-b21c-8dbd06e6988b
:END:
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ es *difeomorfismo* cuando es
diferenciable y biyectiva con inversa diferenciable $\phi^{-1} \colon O' \to O$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       5fc5ff28-bffc-441c-9161-f13889ed291a
:DRILL_LAST_INTERVAL: 39.3578
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-13 Sun 20:47]
:END:
DefiniciÃ³n de *difeomorfismo*.

******* DefiniciÃ³n
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ es *difeomorfismo* cuando es
diferenciable y biyectiva con inversa diferenciable $\phi^{-1} \colon O' \to O$.

***** Difeomorfismo local
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ es *difeomorfismo local* en
un punto $p \in O$ si existen entornos $p \in U \subset O$ y $\phi(p) \in U' \subset O'$ tales
que $\phi|_U \colon U \to U'$ es difeomorfismo.

****** Card                                                                                                :drill:
SCHEDULED: <2018-08-28 Tue>
:PROPERTIES:
:ID:       a226eca2-2050-4936-bf74-5ba70a400d57
:DRILL_LAST_INTERVAL: 62.1563
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:16]
:END:
CuÃ¡ndo es $\phi \colon O \to O'$ *difeomorfismo local* en $p \in O$.

******* Answer
Cada punto tiene un entorno en el que la aplicaciÃ³n es difeomorfismo.

******* DefiniciÃ³n
Cuando existen entornos $p \in U \subset O$ y $\phi(p) \in U' \subset O'$
tales que $\phi|_U \colon U \to U'$ es difeomorfismo.

***** Difeomorfismos locales y globales
Un difeomorfismo global es un difeomorfismo local en todo punto trivialmente,
pero no todo difeomorfismo local en todo punto es difeomorfismo global.

****** Contraejemplo
:PROPERTIES:
:ID:       1395d142-5681-4763-bd06-a7f307c486a5
:END:
No todo difeomorfismo local en todo punto es un difeomorfismo global.
PodrÃ­amos tener el jacobiano con determinante no nula en cada punto y
que la funciÃ³n no fuera siquiera inyectiva. Por ejemplo,

\[
f(x,y) = (e^x \sin(y), e^x \cos(y))
\]

[[https://math.stackexchange.com/questions/39142/a-local-diffeomorphism-of-euclidean-space-that-is-not-a-diffeomorphism][Ejemplo en SE.]]

****** Card: difeomorfismo local no global                                                                 :drill:
SCHEDULED: <2018-08-08 Wed>
:PROPERTIES:
:ID:       2125fe6e-2af6-4d4d-a641-7e1e9759f42e
:DRILL_LAST_INTERVAL: 92.3211
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.75
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-08 Tue 14:03]
:END:
Â¿CÃ³mo podemos construir un difeomorfismo local en cada punto que no
sea difeomorfismo global?

******* Construirlo
PodrÃ­amos tener una funciÃ³n con Jacobiano con determinante no nulo
en cada punto pero que no fuera siquiera inyectiva.

\[
f(x,y) = (e^x \sin(y), e^x \cos(y))
\]

Desde [[id:1395d142-5681-4763-bd06-a7f307c486a5][aquÃ­]]. Se tiene isomorfismo por $\det Df(x,y)=e^{2x}$.

***** Difeomorfismo local biyectivo es difeomorfismo
$\phi\colon O \to O'$ es difeomorfismo ssi es difeomorfismo local biyectivo.

****** Proof
En un sentido es trivial. En el otro, tendrÃ­amos una funciÃ³n biyectiva,
que serÃ­a diferenciable por serlo localmente. Su inversa es localmente
diferenciable tambiÃ©n en todo punto, por lo que serÃ­a diferenciable.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-23 Mon>
:PROPERTIES:
:ID:       0867560c-e572-4ef7-84e8-01ff5154571a
:DRILL_LAST_INTERVAL: 49.1467
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:54]
:END:
CaracterizaciÃ³n de difeomorfismo por difeomorfismo local.

******* CaracterizaciÃ³n
$\phi\colon O \to O'$ difeomorfismo ssi es difeomorfismo local en todo
punto *y biyectiva*.

***** Difeomorfismo local es abierta
Una aplicaciÃ³n entre abiertos $\phi\colon O \to O'$ que es difeomorfismo es abierta.

****** Proof
Dado un abierto $U \subset O$, lo enviarÃ¡ a $\phi(U)$. Todos los puntos ahÃ­ tendrÃ¡n
una preimagen y esta tendrÃ¡ un entorno, en particular un entorno contenido
en $U$, tal que $\phi$ sea difeomorfismo entre Ã©l y un entorno del punto. Este
entorno estarÃ¡ contenido en $O'$, lo que lo harÃ¡ abierto.

***** Difeomorfismo tiene diferencial isomorfismo
:PROPERTIES:
:ID:       6a2928b0-e787-4240-b76c-1856d14421bb
:END:
Si una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ es [[id:ef28dfb1-a3fc-4730-b21c-8dbd06e6988b][difeomorfismo]], se tiene
para cada $p \in O$ que $(d\phi)_p \colon \mathbb{R}^n\to \mathbb{R}^n$ es isomorfismo lineal y que
$(d\phi)^{-1}_p = (d\phi^{-1})_{\phi(p)}$.

****** Proof
Usando que $d \mathrm{Id}_p = \mathrm{Id}$ y que $\phi^{-1} \circ \phi = \mathrm{Id}$, tenemos por regla de la
cadena clÃ¡sica que

\[\mathrm{Id} =
d(\phi^{-1} \circ \phi)_p = (d\phi^{-1})_{\phi(p)} \circ (d\phi)_p
\]

y $(d\phi)_p$ es isomorfismo.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-12 Thu>
:PROPERTIES:
:ID:       92ff4784-7391-448d-8248-0f984f6e1b98
:DRILL_LAST_INTERVAL: 47.0707
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.4
:DRILL_EASE: 2.9
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:21]
:END:
Si $\phi$ es difeomorfismo, Â¿quÃ© sabemos de $(d\phi)_p$?

******* Sabemos que
es un isomorfismo.
***** Teorema de la funciÃ³n inversa
:PROPERTIES:
:ID:       16bd6b42-7b91-4fcf-8979-faabc0acd594
:END:
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ diferenciable tal que para $p \in O$
se tiene $(d\phi)_p \colon \mathbb{R}^n \to \mathbb{R}^n$ isomorfismo lineal, es difeomorfismo local en $p$.

****** TODO Proof

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       63d019ff-edf9-4e64-aed4-9a0823572061
:DRILL_LAST_INTERVAL: 43.471
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.08
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-08 Tue 14:00]
:END:
Enuncia el teorema de la funciÃ³n inversa para una aplicaciÃ³n entre
abiertos $\phi \colon O \to O'$ diferenciable.

/Hint/: hay que dar una condiciÃ³n sobre $(d\phi)_p$.

******* Enunciado
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ diferenciable tal que para $p \in O$
se tiene $(d\phi)_p \colon \mathbb{R}^n \to \mathbb{R}^n$ isomorfismo lineal, es difeomorfismo local en $p$.

***** Corolarios al teorema de la funciÃ³n inversa
Una aplicaciÃ³n entre abiertos $\phi \colon O \to O'$ diferenciable cumple

 1) que es difeomorfismo local si y sÃ³lo si $(d\phi)_p \colon \mathbb{R}^n \to \mathbb{R}^n$ es
    isomorfismo lineal para cada $p \in O$;

 2) que es difeomorfismo si y sÃ³lo si $\phi$ es biyectiva y $(d\phi)_p \colon \mathbb{R}^n \to \mathbb{R}^n$
    es isomorfismo lineal para cada $p \in O$.

****** Proof 1
Si es un isomorfismo lineal para cada $p \in O$, por el [[id:16bd6b42-7b91-4fcf-8979-faabc0acd594][teorema]] de la
funciÃ³n inversa es difeomorfismo local en cada punto.

Si es un difeomorfismo local, en cada entorno es un difeomorfismo y
entonces en ese entorno [[id:6a2928b0-e787-4240-b76c-1856d14421bb][la derivada es un isomorfismo]] lineal. Si un
isomorfismo lineal lo es en un entorno, lo es globalmente.

****** TODO Proof 2

*** Tema 1. Curvas ([[~/pdf/Rosales_Curvas.pdf][PDF]])
**** 1.1. Curvas regulares y longitud de arco
***** 1.1.1. Definiciones
****** Curva, traza y curva regular
:PROPERTIES:
:ID:       7fbe4138-469a-4d17-a2d8-a04434cf8489
:END:
Una *curva* es $\alpha\colon I \to \mathbb{R}^n$, con $\alpha \in {\cal C}^{\infty}$ para $I$ intervalo abierto.
Llamamos *traza* a su imagen. Es *curva regular* si su velocidad $\alpha'$
es no nula. Toda curva regular tiene rectas tangentes en todo punto,

\[
\alpha(t) + L(\alpha'(t)) =
\left\{ \alpha(t) + \lambda \alpha'(t) \mid
\lambda \in \mathbb{R} \right\}.
\]

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       68445172-0dfd-4e02-a7ee-8306eb327ce9
:DRILL_LAST_INTERVAL: 26.8725
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:50]
:END:
DefiniciÃ³n de curva regular.

******** DefiniciÃ³n
$\alpha \colon I \to \mathbb{R}^n$ con $\alpha \in {\cal C}^{\infty}$, $I$ intervalo abierto es curva.
Es regular si $|\alpha'(t)| \neq 0$ para cualquier $t \in I$.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       38f704fa-7ee0-4aac-89e8-93153ed48971
:DRILL_LAST_INTERVAL: 37.3865
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:49]
:END:
Sea $\alpha$ curva regular, da la ecuaciÃ³n de su recta tangente
en un instante $t$.

******** Respuesta

\[
\alpha(t) + L(\alpha'(t))
\]

****** Embebimiento
$\alpha\colon I \to \mathbb{R}^n$ es *embebimiento* si $\alpha \colon I \to \alpha(I)$ es homeomorfismo.

******* Card                                                                                              :drill:
SCHEDULED: <2018-10-10 Wed>
:PROPERTIES:
:ID:       47cb18d3-b024-48e7-9b40-9fd5efa6c87a
:DRILL_LAST_INTERVAL: 124.432
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.571
:DRILL_EASE: 3.1
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:36]
:END:
Â¿CuÃ¡ndo es $\alpha\colon I \to \mathbb{R}^n$ un embebimiento?

******** DefiniciÃ³n
Cuando $\alpha \colon I \to \alpha(I)$ es un homeomorfismo.

******* Contraejemplo                                                                                     :drill:
SCHEDULED: <2018-10-09 Tue>
:PROPERTIES:
:ID:       52adbafb-00f1-4b63-b08a-1e89cf23b94b
:DRILL_LAST_INTERVAL: 122.9907
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.286
:DRILL_EASE: 3.1
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:36]
:END:
Dar un ejemplo de curva inyectiva que no sea embebimiento.

******** Ejemplo
La curva siguiente, en un intervalo bien escogido

\[
\alpha(t) = \left( \frac{3t}{1+t^3}, \frac{3t^2}{1+t^3} \right)
\]

es inyectiva, pero al tender a infinito se acerca tanto como queramos
al $(0,0)$, por el que ya habÃ­a pasado antes, lo que hace que no podamos
tener un homeomorfismo.

****** Ejemplos de curvas regulares
Son regulares,

 * rectas $\alpha(t) = p + tv$.

 * circunferencias $\alpha(t) = p + r(\cos(t),\sin(t))$.

 * grafos diferenciables $\alpha(t) = (t,\varphi(t))$.

 * hÃ©lices circulares de eje vertical $\alpha(t) = (x_0 + r\cos(t), y_0 + r\sin(t), qt)$.

***** 1.1.2. ReparametrizaciÃ³n de curvas
****** ReparametrizaciÃ³n
Para $\alpha\colon I \to \mathbb{R}^n$ curva, $\phi\colon J \to I$ difeomorfismo (*cambio de parÃ¡metros*),
$\beta =\alpha \circ \phi$ es una *reparametrizaciÃ³n*.

 * Tiene la misma traza.
 * Conserva regularidad.

******* Card                                                                                              :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       ff9c749b-25a9-4000-8716-25e18dbf6b9b
:DRILL_LAST_INTERVAL: 24.6143
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:17]
:END:
Â¿QuÃ© es una reparametrizaciÃ³n de una curva $\alpha$?

******** Respuesta
Una curva $\alpha \circ \phi$ donde $\phi$ es un difeomorfismo de
intervalos abiertos.

***** 1.1.3. Longitud de un arco de curva
****** 1.11. Longitud
Definimos la longitud de una funciÃ³n continua $\alpha \colon [a,b] \to \mathbb{R}^n$ como

\[
L_a^b(\alpha) = \sup\left\{
\sum_{k=0}^m |\alpha(t_k+1) - \alpha(t_{k})| \;\middle|\;  
\left\{ k_0,\dots,k_{m+1} \right\} \mbox{ particiÃ³n de } [a,b] 
\right\}
\]

******* Las rectas tienen longitud mÃ­nima
Por definiciÃ³n, usando la particiÃ³n trivial $\left\{ a,b \right\}$.

******* La longitud de una curva continua puede ser infinita
La longitud de una curva /continua/ puede ser infinita, pero
si es una curva ${\cal C}^1$, [[id:6c8461d6-7792-4a55-b2eb-9e3e719d1f21][tiene longitud finita]]. Un ejemplo de curva
continua con longitud infinita es

\[
\alpha(t) = \left\{\begin{array}{ll}
(t,\cos(\pi/t)), & \mbox{ si } t \neq 0, \\
0, & \mbox{ si } t = 0.
\end{array}\right.
\]

****** 1.12. Longitud en compactos
:PROPERTIES:
:ID:       6c8461d6-7792-4a55-b2eb-9e3e719d1f21
:END:
Para $\alpha \colon [a,b] \subseteq I \to \mathbb{R}^n$ diferenciable ${\cal C}^1$, tenemos

\[
L_a^b(\alpha) = \int_a^b |\alpha'(t)|\,dt < \infty.
\]

AsÃ­, las curvas tienen longitud finita en intervalos compactos.

******* TODO Proof                                                                                        :extra:
******** AcotaciÃ³n
Por ser $\alpha'$ continua, estÃ¡ acotada en el compacto $[a,b]$ y la
integral entera es finita. Para cualquier pariciÃ³n se tiene,
usando Teorema fundamental del cÃ¡lculo, una cota uniforme

\[\begin{aligned}
L(\alpha,P) &= \sum_{k=0}^{m-1} |\alpha(t_{k+1}) - \alpha(t_k)| \\
&= \sum_{k=0}^{m-1} \abs{\int_{t_k}^{t_{k+1}} \alpha'(s)\,ds} \\
&\leq \sum_{k=0}^{m-1} \int_{t_k}^{t_{k+1}} \abs{\alpha'(s)}\,ds \\
&= \int_{a}^{b} \abs{\alpha'(s)}\,ds.
\end{aligned}\]

******** TODO Igualdad
Lo que nos queda por demostrar es que se tiene la igualdad,
viendo que para cualquier $\varepsilon > 0$, hay una particiÃ³n $P$ tal
que

\[
\int_a^b |\alpha'(t)|\,dt - L(\alpha,P) < \varepsilon.
\]

Para cualquier particiÃ³n $P = \left\{ t_0<t_1<\dots<t_m \right\}$, podemos
aplicar el Teorema del Valor Medio a las componentes $\alpha(t) = (x_1(t),\dots,x_n(t))$
para tener $c_{ik} \in [t_k,t_{k+1}]$ tales que

\[
x_i(t_{k+1}) - x_i(t_k) = x'_i(c_{ik})(t_{k+1}-t_k), \quad \forall i = 1,\dots,n.
\]

Definimos $f(s_1,\dots,s_n) = \sqrt{x_1'(s_1)^2 + \dots + x_n'(s_n)^2}$
******* Card: longitud de una curva                                                                       :drill:
SCHEDULED: <2018-08-18 Sat>
:PROPERTIES:
:ID:       3fb16a48-0357-42f8-98c2-c74d4562bc49
:DRILL_LAST_INTERVAL: 68.8101
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.715
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-10 Sun 13:57]
:END:
Para $\alpha\colon I \to \mathbb{R}^n$ diferenciable ${\cal C}^1$, calcula la longitud entre
$[a,b]$ como una integral.

******** Longitud

\[
L_a^b(\alpha) = \int_a^b |\alpha'(t)|\,dt < \infty.
\]

****** 1.14. PreservaciÃ³n de la longitud
Para $\alpha\colon I \to \mathbb{R}^n$ curva con $[a,b] \subseteq I$, tenemos

 1. que si $\phi\colon \mathbb{R}^n \to \mathbb{R}^n$ es un movimiento rÃ­gido, $L_a^b(\phi \circ \alpha) = L_a^b(\alpha)$;

 2. que si $\phi\colon J \to I$ es un difeomorfismo y $\beta = \alpha\circ\phi$, entonces
    $L_c^d(\beta) = L_a^b(\alpha)$, para $[c,d] = \phi^{-1}[a,b]$.

Es decir, la longitud se preserva por [[id:92d75a96-b872-4ea6-ae5a-b83d975e2b37][movimientos rÃ­gidos]] y
reparametrizaciones.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-01 Sun>
:PROPERTIES:
:ID:       102ec980-bf93-4cac-be57-c3b34b2bdd68
:DRILL_LAST_INTERVAL: 26.6744
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:51]
:END:
Â¿La composiciÃ³n de quÃ© morfismos preserva la longitud de una
curva?

******** Clase de morfismos

 * Movimientos rÃ­gidos, $L_a^b(\phi \circ \alpha) = L_a^b(\alpha)$.

 * Reparametrizaciones, $L_c^d(\beta) = L_a^b(\alpha)$.

******* TODO Proof
***** 1.1.4. Curvas parametrizadas por el arco
****** DefiniciÃ³n 1.16. Curva parametrizada por el arco
:PROPERTIES:
:ID:       cb6ea57b-b063-4959-9a36-44a2662b6fce
:END:
 $\alpha\colon I \to \mathbb{R}^n$ [[id:7fbe4138-469a-4d17-a2d8-a04434cf8489][curva]] es *parametrizada por el arco* (p.p.a) si $|\alpha'(t)| = 1$.
Las p.p.a. son regulares.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       976be4e8-a312-4644-bb1b-5b699363607c
:DRILL_LAST_INTERVAL: 33.853
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:54]
:END:
DefiniciÃ³n de curva p.p.a.

******** DefiniciÃ³n
$\alpha\colon I \to \mathbb{R}^n$ [[id:7fbe4138-469a-4d17-a2d8-a04434cf8489][curva]] es *parametrizada por el arco* (p.p.a) si $|\alpha'(t)| = 1$.

****** ProposiciÃ³n 1.17. Lema de reparametrizaciÃ³n
:PROPERTIES:
:ID:       1e330295-fe16-4262-a88b-af5a84c66fb8
:END:
$\alpha\colon I \to \mathbb{R}^n$ regular, existe $\phi\colon J \to I$ difeomorfismo 
creciente de intervalos abiertos tal que $\beta = \alpha\circ \phi$ es [[id:cb6ea57b-b063-4959-9a36-44a2662b6fce][p.p.a.]]

/La demostraciÃ³n da una construcciÃ³n explÃ­cita./ Definimos

\[
\psi(t) = L_a^t(\alpha) = \int_a^t |\alpha'(s)|\,ds.
\]

y tomamos $\phi =\psi^{-1}$.

******* Proof
Para un $a \in I$ fijo, definimos $\psi \colon I \to \mathbb{R}$ por

\[
\psi(t) = L_a^t(\alpha) = \int_a^t |\alpha'(s)|\,ds.
\]

Como $\alpha \in {\cal C}^{\infty}$, por Teorema fundamental del cÃ¡lculo $\psi'(t) = |\alpha'(t)| > 0$
por regularidad. Queda $\psi \in {\cal C}^{\infty}$ estrictamente creciente e inyectiva.
AsÃ­, $J = \psi(I)$ intervalo y $\psi \colon I \to J$ biyectiva. Por Teorema de la
funciÃ³n inversa, es difeomorfismo y llamamos $\phi =\psi^{-1}$. Ahora,

\[
\phi'(s) = \frac{1}{|\alpha'(\phi(s))|} > 0
\]

que implica

\[\abs{\beta'(s)} = 
\abs{(\alpha \circ \phi)'(s)} = 
\abs{\frac{\alpha'(\phi(s))}{ |\alpha'(\phi(s))|}} = 
1.\]

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-16 Mon>
:PROPERTIES:
:ID:       ff49f39c-0657-4411-86da-a586b9673753
:DRILL_LAST_INTERVAL: 50.8001
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:19]
:END:
Enuncia el lema de reparametrizaciÃ³n para $\alpha\colon I \to \mathbb{R}^n$ regular.

******** Lema
Existe $\phi\colon J \to I$

 * difeomorfismo,
 * creciente,
 * de intervalos abiertos,

tal que $\alpha \circ \phi$ es [[id:cb6ea57b-b063-4959-9a36-44a2662b6fce][p.p.a.]]

******* Card: cÃ¡lculo                                                                                     :drill:
SCHEDULED: <2018-08-11 Sat>
:PROPERTIES:
:ID:       5b34db3c-8a95-4c74-9965-b87e184b414a
:DRILL_LAST_INTERVAL: 76.896
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:19]
:END:
Dada una $\alpha\colon I \to \mathbb{R}^n$ buscar la reparametrizaciÃ³n $\phi\colon J \to I$ que
hace que $\beta = \alpha\circ \phi$ sea una curva p.p.a.

******** ReparametrizaciÃ³n
Fijamos algÃºn $t_0$ y calculamos la integral

\[
\psi(t) = L_{t_0}^t(\alpha) =\int_{t_0}^t |\alpha'(s)|\,ds;
\]

y la inversa $\phi =\psi^{-1}$ es la reparametrizaciÃ³n buscada.

**** 1.2. GeometrÃ­a de curvas regulares en el plano
***** 1.2.1. Diedro de Frenet y curvatura de curvas p.p.a.
****** 2.0. Diedro de Frenet
:PROPERTIES:
:ID:       b0a55e94-4a63-4257-a7b5-9641d848d4e5
:END:
Sea $\alpha$ p.p.a., el *diedro de Frenet* es $\left\{ T(s),N(s) \right\}$, base ortonormal
positiva con

 * el tangente $T(s) = \alpha'(s)$;
 * y el normal $N(s) = J(T(s))$, donde $J(x,y) = (-y,x)$.

****** 2.1. DefiniciÃ³n de curvatura
Sea $\alpha$ p.p.a., se define la curvatura $k \colon I \to \mathbb{R}$ como

\[
k(s) = \left\langle T'(s),N(s) \right\rangle = \left\langle \alpha''(s),J(\alpha'(s)) \right\rangle.
\]

NÃ³tese que $k \in {\cal C}^{\infty}$.

****** 2.0. Ecuaciones de Frenet
Sea $\alpha$ p.p.a., las coordenadas de $T'(s),N'(s)$ en el [[id:b0a55e94-4a63-4257-a7b5-9641d848d4e5][diedro de Frenet]] son

 * $T'(s) = k(s)N(s)$,
 * $N'(s) = -k(s)T(s)$.

******* Proof
Derivando $|T(s)|^2 = |N(s)|^2 = 1$ obtenemos $\pair{T(s),T'(s)} = \pair{N(s),N'(s)} = 0$,
simplemente usamos que $T,N$es una base ortonormal.

****** 2.2. ExpresiÃ³n alternativa de la curvatura
Sea $\alpha$ p.p.a.,

\[
k(s) = \mathrm{det}(\alpha'(s),\alpha''(s)).
\]

******* Proof
Como $\mathrm{det}(T(t),N(t)) = 1$, tenemos

\[
k(t) = \mathrm{det}(T(t),k(s)N(t)) = \mathrm{det}(T(t),T'(t)) = \mathrm{det}(\alpha'(t),\alpha''(t)).
\]

****** 2.4. Curvatura de rectas y circunferencias
La curvatura de una recta es nula, la curvatura de una circunferencia
de radio $R$ es constante $k(s) = 1/R$.

***** 1.2.2. Diedro de Frenet y curvatura de curvas regulares
****** 2.5. Definiciones para curvas regulares
Para $\alpha \colon I \to \mathbb{R}^2$ regular consideramos una reparametrizaciÃ³n
a p.p.a. $\beta = \alpha\circ\phi$ y definimos

 * $T_{\alpha}(t) = T_{\beta}(\phi^{-1}(t))$,
 * $N_{\alpha}(t) = N_{\beta}(\phi^{-1}(t))$,
 * $k_{\alpha}(t) = k_{\beta}(\phi^{-1}(t))$.

NÃ³tese que una reparametrizaciÃ³n asÃ­ [[id:1e330295-fe16-4262-a88b-af5a84c66fb8][existe]] siempre.

****** 2.5. Ecuaciones para curvas regulares en el plano
:PROPERTIES:
:ID:       fc237c54-3cf6-446e-9c62-1e256a3684ff
:END:
Para $\alpha \colon I \to \mathbb{R}^2$ curva regular se tiene el siguiente diedro de Frenet
y la siguiente curvatura

\[
T(t) = \frac{\alpha'(t)}{|\alpha'(t)|},\quad
N(t) = \frac{J(\alpha'(t))}{|\alpha'(t)|},\quad
k(t) = \frac{\mathrm{det}(\alpha'(t),\alpha''(t))}{|\alpha'(t)|^3}.
\]

y ademÃ¡s se cumplen las siguientes ecuaciones de Frenet

 * $T'(t) = |\alpha'(t)|k(t)N(t)$,

 * $N'(t) = -|\alpha'(t)|k(t)T(t)$,

y como consecuencia, la curvatura orientada se escribe como

\[
k(t) = \frac{\left\langle T'(t),N(t) \right\rangle}{|\alpha'(t)|}.
\]

Como corolario hemos demostrado ademÃ¡s que $T_{\alpha},N_{\alpha},k_{\alpha}$ no
dependen de la reparametrizaciÃ³n elegida.

******* Proof
Reparametrizamos la curva p.p.a. $\beta = \alpha\circ\phi$ con un difeomorfismo
creciente y usamos que

\[
\abs{\phi'(s)}\abs{\alpha'(\phi(s))} = 1
\]

para comprobar las igualdades.

******* Card: diedro                                                                                      :drill:
SCHEDULED: <2018-08-05 Sun>
:PROPERTIES:
:ID:       dc77cdaa-f13a-4306-960e-f63531e65620
:DRILL_LAST_INTERVAL: 70.544
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:35]
:END:
Dar el diedro de Frenet de una curva regular $\alpha(t) \colon I \to \mathbb{R}^2$.

******** Diedro

\[
T_{\alpha}(t) = \frac{\alpha'(t)}{|\alpha'(t)|},\quad
N_{\alpha}(t) = \frac{J(\alpha'(t))}{|\alpha'(t)|}
\]

******* Card: curvatura                                                                                   :drill:
SCHEDULED: <2018-08-20 Mon>
:PROPERTIES:
:ID:       054cec15-5c54-4d1f-84f7-56cb9df2ede5
:DRILL_LAST_INTERVAL: 71.3774
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 12
:DRILL_FAILURE_COUNT: 5
:DRILL_AVERAGE_QUALITY: 2.417
:DRILL_EASE: 2.38
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-10 Sun 13:57]
:END:
Dar dos fÃ³rmulas de la curvatura de una curva regular en
el plano, $\alpha(t)\colon I \to\mathbb{R}^2$.

******** Curvatura

\[ k_{\alpha}(t) 
= \frac{\mathrm{det}(\alpha'(t),\alpha''(t))}{|\alpha'(t)|^3}
= \frac{\left\langle T'(t),N(t) \right\rangle}{|\alpha'(t)|}
\]

******* Card: ecuaciones de Frenet                                                                        :drill:
SCHEDULED: <2018-10-01 Mon>
:PROPERTIES:
:ID:       cca9ded0-aa24-4138-bbb8-c057e5d1015b
:DRILL_LAST_INTERVAL: 94.4685
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 10
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 3.1
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:45]
:END:
Enunciar las ecuaciones de Frenet para una curva regular en el plano.

******** Ecuaciones de Frenet
Son las coordenadas de los derivados del tangente y del
normal en el diedro de Frenet.

\[\left\{\begin{array}{l}
T'_{\alpha}(t) = |\alpha'(t)| k_{\alpha}(t) N_{\alpha}(t) \\
N'_{\alpha}(t) = -|\alpha'(t)| k_{\alpha}(t) T_{\alpha}(t) 
\end{array} \right.\]

****** 2.8. Propiedades de la curvatura en el plano
La curvatura es

 1. *Local*, para $\alpha \colon I \to \mathbb{R}^2$ y $\beta \colon J \to \mathbb{R}^2$, si $\alpha = \beta$ en
    $(t_0-\varepsilon,t_0+\varepsilon)\subseteq I \cap J$, entonces $k_{\alpha}(t_0) = k_{\beta}(t_0)$.
    
 2. *Invariante a movimientos rÃ­gidos directos*, si $\alpha \colon I \to \mathbb{R}^2$
    regular, $\phi \circ \alpha$ regular con $k_{\phi \circ \alpha} = k_{\alpha}$ si $\phi$ es directo y
    $k_{\phi \circ \alpha} = -k_{\alpha}$ si $\phi$ es inverso.

 3. *Invariante a reparametrizaciones*, para $\alpha \colon I \to \mathbb{R}^2$
    regular y $\phi\colon J \to I$ difeomorfismo, $\alpha \circ \phi$ es regular 
    y $k_{\alpha\circ\phi} = k_{\alpha} \circ \phi$ si $\phi$ es creciente o $k_{\alpha \circ \phi} = - k_{\alpha} \circ \phi$ si
    $\phi$ es decreciente.

******* Proof 1
Tenemos una [[id:fc237c54-3cf6-446e-9c62-1e256a3684ff][fÃ³rmula de la curvatura]] que sÃ³lo depende de las
derivadas que son locales.

******* Proof 2
Tomamos $\phi = Ax + b$ para alguna $A \in {\cal O}(2)$, que tendrÃ¡ $|A| = \pm 1$
dependiendo de si es directa o inversa y calculamos.

******* Proof 3
Calculando la curvatura nos queda que depende del signo de $\phi'(t)^3$
y la fÃ³rmula buscada.

******* Card: curvatura en el plano bajo reparametrizaciones                                              :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       f42e0f4d-e16f-48fd-b03c-ad07c94bf795
:DRILL_LAST_INTERVAL: 31.6821
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:52]
:END:
Â¿CÃ³mo varÃ­a la curvatura en el plano bajo reparametrizaciones?
Calcular $k_{\alpha \circ \phi}$.

******** Answer

 1. Compone con reparametrizaciones crecientes, $k_{\alpha \circ \phi} = k_{\alpha} \circ \phi$.
 2. Opuesta de componer con decrecientes, $k_{\alpha \circ \phi} = - k_{\alpha} \circ \phi$.

******* Card: curvatura en el plano bajo movimientos rÃ­gidos                                              :drill:
SCHEDULED: <2018-06-30 Sat>
:PROPERTIES:
:ID:       906e3710-34fb-4a1b-9307-b0080cc5ab7b
:DRILL_LAST_INTERVAL: 26.3376
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:51]
:END:
Â¿CÃ³mo varÃ­a la curvatura en el plano bajo movimientos rÃ­gidos?
Calcular $k_{\phi \circ \alpha}$.

******** Answer

 1. Invariante a movimientos rÃ­gidos directos, $k_{\phi \circ \alpha} = k_{\alpha}$.
 2. Opuesta bajo movimientos rÃ­gidos inversos, $k_{\phi \circ \alpha} = -k_{\alpha}$.

****** 2.9. Curvas de curvatura constante
Si $\alpha \colon I \to \mathbb{R}^2$ tiene curvatura constante $k(t) = c$,

 1. si $c = 0$, $\alpha(I)$ estÃ¡ en una recta afÃ­n;
 2. si $c \neq 0$, $\alpha(I)$ estÃ¡ en una circunferencia de radio $1/|c|$.

******* Proof 1
Por Frenet, $T'(t) = 0$, luego $\alpha'(t) = |\alpha'(t)|v$, e integramos
para tener $\alpha(t) = p + f(t)v$ para algunos $p$ y $f$.

******* Proof 2
Derivando y usando Frenet en 

\[
f(t) = \alpha(t) + \frac{1}{c}N(t)
\]

tenemos $f$ constante y $\abs{\alpha(t) - v} = 1/c$.

**** 1.3. GeometrÃ­a de curvas regulares en el espacio
***** 1.3.1. Triedro de Frenet, curvatura y torsiÃ³n de curvas p.p.a.
****** 3.1. Curvatura de una curva p.p.a.
Sea $\alpha$ p.p.a., su *curvatura* es

\[
k(s) = |T'(s)| = |\alpha''(s)|.
\]

En este caso, y a diferencia de $\mathbb{R}^2$, la curvatura es
siempre no negativa. Es diferenciable en todos los puntos
en los que es positiva.

****** Vector normal a la curva p.p.a. en un punto
El *normal* a $\alpha \colon I \to \mathbb{R}^3$ p.p.a. en $s$ se define cuando $k(s) > 0$
como

\[
N(s) = \frac{T'(s)}{\abs{T'(s)}} = \frac{T'(s)}{k(s)} = \frac{\alpha''(s)}{|\alpha''(s)|}.
\]

****** Vector binormal a la curva p.p.a. en un punto
El *binormal* a $\alpha \colon I \to \mathbb{R}^3$ p.p.a. en $s$ se define cuando $k(s) > 0$
como

\[
B(s) = T(s) \times N(s).
\]

****** Triedro de Frenet de una curva p.p.a. en un punto
Llamamos *triedro de Frenet* a la base ortonormal positiva
dada por $\left\{ T(s),N(s),B(s) \right\}$.

****** Rectas afines y plano afÃ­n osculador

 * *Recta afÃ­n tangente* $\alpha(s) + L(T(s))$,
 * *Recta afÃ­n normal* $\alpha(s) + L(N(s))$,
 * *Recta afÃ­n binormal* $\alpha(s) + L(B(s))$,
 * *Plano afÃ­n osculador* $\alpha(s) + L(T(s),N(s))$.

NÃ³tese que $B(s)$ es vector normal unitario al plano osculador.
Tendremos que $|B'(s)| = \tau(s)$, medida de cÃ³mo cambia el plano
osculador.

****** TorsiÃ³n de una curva p.p.a.
Para $\alpha \colon I \to \mathbb{R}^3$ curva p.p.a. con $k > 0$, definimos

\[
\tau(s) = \left\langle B'(s), N(s) \right\rangle.
\]

******* Card: buena definiciÃ³n                                                                            :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       651ce58d-4284-4b21-a958-e42dac558cca
:DRILL_LAST_INTERVAL: 36.6461
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:54]
:END:
Â¿DÃ³nde estÃ¡n definidas la torsiÃ³n, el normal y el binormal
para una curva p.p.a. en el espacio?

******** Respuesta
En los puntos en los que la curvatura es no nula.

****** CÃ¡lculo de la torsiÃ³n
Tenemos

\[
\tau(s) = -\frac{\mathrm{det}(\alpha'(s),\alpha''(s),\alpha'''(s))}{k(s)^2}.
\]

******* Proof
Desde la definiciÃ³n usando $B = T \times N$ y propiedades del
determinante.

****** 3.4. Ejemplo: rectas
La normal, binormal y torsiÃ³n no estÃ¡n definidos en el caso de rectas.
La tangente la da la propia recta y la curvatura es nula.

****** 3.5. Ejemplo: las curvas planas tienen torsiÃ³n nula
Una curva (no recta) contenida en un plano afÃ­n tiene torsiÃ³n nula.

******* Proof
Derivando tenemos $T$ y $N$ en el plano y $B$ ortogonal a Ã©l. El plano
es constante y sÃ³lo tiene dos vectores normales unitarios $\pm n$; por
conexiÃ³n del intervalo, $B$ serÃ¡ uno de ellos.

****** Ejemplo: hÃ©lices
Las hÃ©lices circulares de eje vertical con parÃ¡metros $R > 0$ y $Q \neq 0$ tienen

\[
k(s) = \frac{R}{R^2 + Q^2},\quad \tau(s) = \frac{-Q}{R^2+Q^2}.
\]

***** 1.3.2. Triedro de Frenet, curvatura y torsiÃ³n de curvas regulares
****** Definiciones para curvas regulares en el espacio
Para $\alpha \colon I \to \mathbb{R}^3$ regular consideramos un difeomorfismo *creciente* $\phi\colon J \to I$
reparametrizaciÃ³n a $\beta = \alpha\circ \phi$ curva p.p.a. y definimos las
caracterÃ­sticas de la curva segÃºn ella.

 * $T_{\alpha}(t) = T_{\beta}(\phi^{-1}(t))$,
 * $N_{\alpha}(t) = N_{\beta}(\phi^{-1}(t))$,
 * $B_{\alpha}(t) = B_{\beta}(\phi^{-1}(t))$,
 * $k_{\alpha}(t) = k_{\beta}(\phi^{-1}(t))$,
 * $\tau_{\alpha}(t) = \tau_{\beta}(\phi^{-1}(t))$.

NÃ³tese que esa reparametrizaciÃ³n [[id:1e330295-fe16-4262-a88b-af5a84c66fb8][existe]] siempre.

****** 3.7. Ecuaciones para curvas regulares en el espacio
:PROPERTIES:
:ID:       6efcd992-f089-4c91-ac08-446846840c0b
:END:
Para $\alpha \colon I \to \mathbb{R}^3$ curva regular se tiene el siguiente triedro de
Frenet

\[
T_{\alpha} = \frac{\alpha'}{|\alpha'|},\quad
N_{\alpha} = B_{\alpha} \times T_{\alpha},\quad
B_{\alpha} = \frac{\alpha' \times \alpha''}{|\alpha' \times \alpha''|}
\]

y la siguiente cruvatura y torsiÃ³n

\[
k_{\alpha}(t) = \frac{|\alpha' \times \alpha''|}{|\alpha'|^3},\quad
\tau_{\alpha} = - \frac{\mathrm{det}(\alpha',\alpha'',\alpha''')}{|\alpha'\times \alpha''|^2}
\]

y ademÃ¡s se cumplen las siguientes ecuaciones de Frenet

 * $T'_{\alpha} = |\alpha'|k_{\alpha}N_{\alpha}$,
 * $N'_{\alpha} = -|\alpha'|k_{\alpha}T_{\alpha} - |\alpha'|\tau_{\alpha}B_{\alpha}$,
 * $B'_{\alpha} = |\alpha'|\tau_{\alpha}N_{\alpha}$.

******* TODO Proof
******* Card: triedro                                                                                     :drill:
SCHEDULED: <2018-06-26 Tue>
:PROPERTIES:
:ID:       dfe88e2d-9b42-44ed-ae62-2bac55c7c108
:DRILL_LAST_INTERVAL: 31.3733
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:24]
:END:
Triedro de Frenet $T,N,B$ de una curva regular en el espacio.

******** Triedro

\[
T_{\alpha} = \frac{\alpha'}{|\alpha'|},\quad
N_{\alpha} = B_{\alpha} \times T_{\alpha},\quad
B_{\alpha} = \frac{\alpha' \times \alpha''}{|\alpha' \times \alpha''|}
\]

******* Card: curvatura                                                                                 :nodrill:
SCHEDULED: <2018-08-16 Thu>
:PROPERTIES:
:ID:       920516ed-24b4-4506-ae4d-41d505077bae
:DRILL_LAST_INTERVAL: 48.0707
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 7
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.571
:DRILL_EASE: 2.18
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:43]
:END:
Curvatura de una curva regular en el *espacio*.

******** Curvatura

\[
k_{\alpha} = \frac{|\alpha' \times \alpha''|}{|\alpha'|^3}
\]

******* Card: torsiÃ³n                                                                                   :nodrill:
SCHEDULED: <2018-06-12 Tue>
:PROPERTIES:
:ID:       be660965-a5df-4345-b491-2e73601a8069
:DRILL_LAST_INTERVAL: 4.3149
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 9
:DRILL_FAILURE_COUNT: 5
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.42
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:35]
:END:
TorsiÃ³n de una curva regular en el espacio.

******** TorsiÃ³n

\[
\tau_{\alpha} = -\frac
{\mathrm{det}(\alpha',\alpha'',\alpha''')}
{|\alpha'\times \alpha''|^2}
\]

******* Card: ecuaciones de Frenet                                                                        :drill:
SCHEDULED: <2018-07-06 Fri>
:PROPERTIES:
:ID:       afe3152a-639e-4a7f-9e69-74f74f00c3ce
:DRILL_LAST_INTERVAL: 40.9431
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:33]
:END:
Ecuaciones de Frenet. Dar $T',N',B'$ de una curva regular en
el espacio.

******** Ecuaciones

 * $T' = |\alpha'|kN$,
 * $N' = -|\alpha'|kT - |\alpha'|\tau B$,
 * $B' = |\alpha'|\tau N$.

****** 3.9. Propiedades de la curvatura y torsiÃ³n en el espacio
La curvatura y la torsiÃ³n son

 1. *Locales*, para $\alpha \colon I \to \mathbb{R}^2$ y $\beta \colon J \to \mathbb{R}^2$, si $\alpha = \beta$ en
    $(t_0-\varepsilon,t_0+\varepsilon)\subseteq I \cap J$, entonces $k_{\alpha}(t_0) = k_{\beta}(t_0)$ y $\tau_{\alpha}(t_0) = \tau_{\beta}(t_0)$
    cuando estÃ¡ definida.

 2. *Invariante a movimientos rÃ­gidos directos*, si $\alpha \colon I \to \mathbb{R}^3$
    regular, $\phi \circ \alpha$ regular con $k_{\phi \circ \alpha} = k_{\alpha}$, y con $\tau_{\alpha} = \tau_{\beta}$
    si $\phi$ es directo y $\tau_{\phi \circ \alpha} = -\tau_{\alpha}$ si $\phi$ es inverso cuando estÃ¡ definido.

 3. *Invariante a reparametrizaciones*, para $\alpha \colon I \to \mathbb{R}^3$
    regular y $\phi\colon J \to I$ difeomorfismo, $\alpha \circ \phi$ regular y
    $k_{\beta} = k_{\alpha} \circ \phi$ y $\tau_{\beta} = \tau_{\alpha}\circ\phi$.

******* TODO Proof
****** 3.10. Curvas de curvatura o torsiÃ³n nulas
Si $\alpha \colon I \to \mathbb{R}^3$ es regular

 1. y $k_{\alpha}=0$, la curva estÃ¡ contenida en una recta afÃ­n;
 2. y si $k_{\alpha} > 0$ y $\tau_{\alpha} = 0$, entonces la curva estÃ¡ contenida en un 
    plano afÃ­n.
   
******* Proof 1
Por ser la [[id:6efcd992-f089-4c91-ac08-446846840c0b][curvatura]] nula tenemos $|\alpha'(t) \times \alpha''(t)|$ y entonces
$\alpha''(t) = \lambda(t)\alpha'(t)$ para escalares

\[
\lambda(t) = \frac{\pair{\alpha'(t),\alpha''(t)}}{|\alpha'(t)|^2}.
\]

Dada una ecuaciÃ³n diferencial de la forma $x'(t) = \lambda(t)x(t)$, las
soluciones son de la forma 

\[
x(t) = x_0 \exp \left( \int_{t_0}^t \lambda(s)\,ds \right);
\]

asÃ­ que si aplicamos esto componente a componente tenemos un
vector $v$ tal que

\[
\alpha'(t) = v \exp \left( \int_{t_0}^t \lambda(s)\,ds \right),
\]

que ademÃ¡s es no nulo porque $\alpha'$ es regular.  Integrando en cada
componente tenemos que $\alpha(t) = p + f(t)v \subseteq p + L(v)$ para algÃºn 
$p$ fijo y $f(t)$ escalar.

******* Proof 2
Tenemos $B'_{\alpha}(t) = 0$ por [[id:6efcd992-f089-4c91-ac08-446846840c0b][Frenet]], luego $B(t) = B_0$. Fijamos un punto
en la curva $p_0 = \alpha(t_0)$ y tomamos el plano $P = \left\{ p \in \mathbb{R}^3 \mid \left\langle p-p_0, B_0 \right\rangle \right\} = 0$.

Finalmente comprobamos que $f(t) = \pair{\alpha(t) - p_0,B_0}$ es constante
derivando

\[
f'(t) = \pair{\alpha'(t),B_0} = |\alpha'(t)| \pair{T(t),B(t)} = 0,
\]

y que es trivialmente nula en $f(t_0) = 0$.

**** 1.4. Teorema fundamental de curvas en el plano y en el espacio
***** 1.4.1. Teorema fundamental de curvas
****** 4.1. Lema: resoluciÃ³n de ecuaciones lineales
Si $A \colon I \to M_n(\mathbb{R})$ y $b \colon I \to \mathbb{R}^n$ diferenciables sobre intervalo abierto,
para $s_0 \in I$, $c_0 \in \mathbb{R}^n$ existe una Ãºnica $c \colon I \to \mathbb{R}^n$ tal que $c(s_0) = c_0$ y
tal que $c'(s) = A(s)c(s) + b(s)$.

****** 4.2. Teorema fundamental de curvas en el espacio
:PROPERTIES:
:ID:       a6da951d-2a14-4818-be6c-0b39a732b12d
:END:
Para $I \subseteq \mathbb{R}$ intervalo abierto y $k,\tau\in {\cal C}^{\infty}(I)$ con $k > 0$;

 1. existe $\alpha \colon I \to \mathbb{R}^3$ p.p.a. con $k_{\alpha} = k$ y $\tau_{\alpha} = \tau$;

 2. para cualquier otra $\beta \colon I \to \mathbb{R}^3$ p.p.a. con $k_{\beta} = k$ y $\tau_{\beta} = \tau$, existe un
    movimiento rÃ­gido directo $\phi \colon \mathbb{R}^3 \to \mathbb{R}^3$ con $\beta = \phi \circ \alpha$.

Es decir, dada una curvatura positiva y una torsiÃ³n, existe una curva
p.p.a. que las realiza, Ãºnica p.p.a. salvo movimientos rÃ­gidos
directos.

******* TODO Proof
******* Card                                                                                              :drill:
SCHEDULED: <2018-07-19 Thu>
:PROPERTIES:
:ID:       bb2b3785-52de-4496-b89e-f0d9862b0019
:DRILL_LAST_INTERVAL: 72.2864
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.25
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-08 Tue 14:02]
:END:
Enuncia con palabras el teorema fundamental de curvas.

******** Enunciado
Dada una curvatura positiva y una torsiÃ³n en un intervalo, existe una
curva p.p.a. que las realiza, Ãºnica p.p.a. salvo movimientos rÃ­gidos
directos.

****** 4.5. Curvas en el espacio de curvatura y torsiÃ³n constantes
Si $\alpha \colon I \to \mathbb{R}^3$ tiene curvatura positiva constante $k_0 > 0$ y torsiÃ³n
constante $\tau_0$, entonces 

 * si $\tau_0 = 0$, estÃ¡ en una circunferencia $\alpha(I) \subseteq C$,
 * si $\tau_0 \neq 0$, estÃ¡ en una hÃ©lice circular $\alpha(I) \subseteq H$.

******* Proof
Siendo regulares, podemos reparametrizarlas en p.p.a. con una
[[id:1e330295-fe16-4262-a88b-af5a84c66fb8][reparametrizaciÃ³n positiva]]; que mantendrÃ¡ curvatura y torsiÃ³n.
La circunferencia tiene curvatura positiva y torsiÃ³n nula, y
una curva queda [[id:a6da951d-2a14-4818-be6c-0b39a732b12d][determinada]] salvo movimientos rÃ­gidos, que
llevan circunferencias en circunferencias.

Las hÃ©lices tienen curvatura y torsiÃ³n constantes, por Teorema
fundamental, quedan determinadas por ellas salvo movimientos
rÃ­gidos.

****** 4.7. Teorema fundamental de curvas en el plano
Para $I \subseteq \mathbb{R}$ intervalo abierto y $k \in {\cal C}^{\infty}(I)$, 

 1. existe $\alpha \colon I \to \mathbb{R}^2$ p.p.a. con $k_{\alpha}=k$;

 2. para cualquier otra $\beta \colon I \to \mathbb{R}^2$ p.p.a. con $k_{\beta}=k$ y $\tau_{\beta}=\tau$, existe un
    movimiento rÃ­gido directo $\phi \colon\mathbb{R}^2\to\mathbb{R}^2$ con $\beta = \phi \circ \alpha$.

Es decir, dada una curvatura cualquiera, existe una curva p.p.a.
que la realiza, salvo movimientos rÃ­gidos directos.

******* TODO DemostraciÃ³n
******* DemostraciÃ³n explÃ­cita de existencia
Tomamos $\theta$ una primitiva de $k$ y

\[
\alpha = \left( \int_{s_0}^s \cos \theta(t)\,dt , \int_{s_0}^s \sin \theta(t)\,dt \right).
\]

Tenemos que $\alpha$ es claramente diferenciable y p.p.a. y su
curvatura es

\[
k_{\alpha}(s) = 
\pair{ T_{\alpha}'(s) , N_{\alpha}(s) } =
k(s)(\sin^2 \theta(s) + \cos^2 \theta(s)) =
k(s).
\]

*** Tema 2. Superficies ([[~/pdf/Rosales_Superficies.pdf][PDF]])
**** 2.1. DefiniciÃ³n de superficie y ejemplos
***** 2.1.0. DefiniciÃ³n de superficie
****** 1.1. Superficie
:PROPERTIES:
:ID:       061e98dc-b077-4e5d-a7fd-f114f6908ed5
:END:
Un $\varnothing \neq S \subseteq \mathbb{R}^3$ es *superficie* si para cada $p \in S$ tiene un entorno
abierto $V \subseteq S$ (*entorno coordenado*) y una *parametrizaciÃ³n* desde
$X \colon U \to S$ para un abierto $U \subset \mathbb{R}^2$ tal que

 1. $X \colon U \to \mathbb{R}^3$ es diferenciable,
 2. $(dX)_q\colon \mathbb{R}^2 \to \mathbb{R}^3$ es inyectiva para cada $q \in U$,
 3. $X(U) = V$ con $X \colon U \to X(U)$ homeomorfismo.

******* Card: superficie                                                                                  :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       ca91733a-9d55-4b63-997a-79aad4762897
:DRILL_LAST_INTERVAL: 22.2251
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:23]
:END:
DefiniciÃ³n de superficie $S$.

******** DefiniciÃ³n
Cada $p \in S$ tiene entorno abierto $V_p \subseteq S$ con una parametrizaciÃ³n
$X \colon U \to S$ desde $U \subset \mathbb{R}^2$.

******* Card: parametrizaciÃ³n                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       1b03debc-6865-4b76-af7a-ac8de199ff7a
:DRILL_LAST_INTERVAL: 13.7426
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:36]
:END:
QuÃ© significa $X \colon U \to S$ parametrizaciÃ³n.

******** Respuesta
Embebimineto diferenciable con diferencial inyectiva.

 1. $X \colon U \to \mathbb{R}^3$ es *diferenciable*,

 2. $(dX)_q\colon \mathbb{R}^2 \to \mathbb{R}^3$ es inyectiva para cada $q \in U$, *diferencial inyectiva*,

 3. $X(U) = V$ con $X \colon U \to V$ *embebimiento*.

****** 1.2. Notas sobre la definiciÃ³n de superficie
:PROPERTIES:
:ID:       39a8bc2e-b4b9-4131-9fe4-7c1ed8fa3d87
:END:

 1. Consideramos la topologÃ­a inducida de $\mathbb{R}^3$, las superficies son
    Hausdorff IIAN localmente euclÃ­deos.

 2. La diferenciabilidad $X \in {\cal C}^{\infty}(U)$ nos da
    $X(u,v) = (x(u,v),y(u,v),z(u,v))$ con derivadas parciales
    como componentes.

 3. Equivalen
   
    1. $(dX)_q$ inyectiva,

    2. $\left\{ X_u(q), X_v(q) \right\}$ linealmente independientes,

    3. $\abs{X_u(q) \times X_v(q)}^2 > 0$,

    4. $\mathrm{rango}(\mathrm{Jac}\ X(q)) = 2$.

 4. Llamamos *atlas* a $X_i$ parametrizaciones locales recubriendo $S$.

******* Card: comprobar inyectividad de la diferencial                                                    :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       136c1adf-f85e-45ac-bc4a-19150882eabc
:DRILL_LAST_INTERVAL: 36.7265
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:55]
:END:
Dos formas de comprobar $dX_{q}$ inyectiva para $X \colon U \to \mathbb{R}^3$.

******** Formas
Equivalen
   
    1. $(dX)_q$ inyectiva,

    2. $\left\{ X_u(q), X_v(q) \right\}$ linealmente independientes,

    3. $\abs{X_u(q) \times X_v(q)}^2 > 0$,

    4. $\mathrm{rango}(\mathrm{Jac}\ X(q)) = 2$.

****** 1.3. Abiertos de superficie, componentes conexas
:PROPERTIES:
:ID:       798b32ea-aeca-4f78-9265-53ce523e0630
:END:
Un abierto no vacÃ­o de una superficie es superficie.  En particular,
las componentes conexas de una superficie son superficies.

******* Proof
Para cada punto, tomamos $p \in V$, y restringimos la parametrizaciÃ³n
que nos dÃ© a $X^{-1}(V \cap S')$, que es abierto.

******* Card: componentes conexas                                                                         :drill:
SCHEDULED: <2018-06-25 Mon>
:PROPERTIES:
:ID:       a29b9077-f8cf-4178-9979-c709f6c1ff8f
:DRILL_LAST_INTERVAL: 29.9261
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:26]
:END:
Â¿Por quÃ© las componentes conexas de una superficie son
superficies?

******** Respuesta
Porque son abiertos de la superficie. Todo abierto de
una superficie es superficie.

****** 1.3. Superficie bajo difeomorfismos
:PROPERTIES:
:ID:       2e12aed9-1de2-4dcf-a729-8068eae76532
:END:
Para $S \subset O$ superficie, $\phi \colon O \to O'$ difeomorfismo entre abiertos,
$\phi(S)$ es superficie.

******* Proof
Tomaremos $\phi \circ X$ eligiendo en los entornos abiertos adecuados y
comprobaremos diferenciabilidad y homeomorfismo con regla de la
cadena.

****** Ej.3. Superficie ssi cada punto tiene entorno superficie
:PROPERTIES:
:ID:       b0e77770-60d2-4de6-9cd9-10f4d1f35cf1
:END:
Si $S = \bigcup_{i \in I} S_i$ uniÃ³n de abiertos de $S$; $S$ es superficie ssi
$S_i$ es siempre superficie.

******* Proof
Un abierto de superficie es abierto, y las parametrizaciones son
locales.

****** Ej.4. ConstrucciÃ³n desde parametrizaciones globales
:PROPERTIES:
:ID:       11ae111c-e696-4a21-9f13-c7345aa83461
:END:
Sea $X \in {\cal C}^{\infty}(U,\mathbb{R}^3)$ no vacÃ­o con $(dX)_q$ inyectiva y $U \cong X(U)$.
Entonces $X(U)$ superficie con esa parametrizaciÃ³n global.

******* Proof
Cada punto tendrÃ­a como entorno a $S$, con $X$ diferenciable
con diferencial inyectiva y homeomorfismo. SerÃ­a parametrizaciÃ³n
global directamente.

***** 2.1.1. Grafos diferenciables
****** 1.4. Grafos
:PROPERTIES:
:ID:       1f5dceca-fdfe-4278-b8f4-7d4ccec964f3
:END:
El *grafo* de $\varphi \in {\cal C}^{\infty}(U)$ abierto de $\mathbb{R}^2$ es superficie,

\[
G^z(\varphi) = \left\{ x,y,z \mid (x,y) \in U, z = \varphi(x,y) \right\}.
\]

AnÃ¡logamente sobre los grafos $G^x(\varphi)$ y $G^y(\varphi)$.

******* 1.5. Consecuencias
Los planos son grafos, el paraboloide elÃ­ptico, el
paraboloide hiperbÃ³lico y el cilindro parabÃ³lico son
grafos. Se sigue que todos son superficies.

******* Proof
Construimos una [[id:11ae111c-e696-4a21-9f13-c7345aa83461][parametrizaciÃ³n global]].
Tenemos $X(u,v) = (u,v,\varphi(u,v))$ diferenciable con
inversa $\pi(u,v,w) = (u,v)$ que la hace homeomorfismo
y con $X_u(u,v) = (1,0,\varphi_u(u,v))$, $X_v(u,v) = (0,1,\varphi_v(u,v))$
independientes.

******* Card: paraboloide hiperbÃ³lico                                                                     :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       702ca00b-5bde-424b-96c8-ac7f5a8687d5
:DRILL_LAST_INTERVAL: 27.0485
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:26]
:END:
Â¿Por quÃ© es el *paraboloide hiperbÃ³lico* una superficie?

$\left\{ x,y,z \mid z = x^2-y^2 \right\}$

******** Respuesta
Es un grafo sobre $\mathbb{R}^2$.

******* Card: por quÃ© grafo es superficie                                                                 :drill:
SCHEDULED: <2018-07-19 Thu>
:PROPERTIES:
:ID:       3dbc7c1c-592b-45cd-b0dd-52b4f4395ffe
:DRILL_LAST_INTERVAL: 38.9819
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.66
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-10 Sun 13:58]
:END:
Â¿Por quÃ© un grafo con $\varphi \in {\cal C}^{\infty}(U)$ es una superficie?

\[
G^z(\varphi) = \left\{ x,y,z \mid (x,y) \in U, z = \varphi(x,y) \right\}.
\]

******** Respuesta
Hay una parametrizaciÃ³n global.

$X(u,v) = (u,v,\varphi(u,v))$ diferenciable con
inversa $\pi(u,v,w) = (u,v)$ luego embebimiento.
Tiene $X_u(u,v) = (1,0,\varphi_u(u,v))$, $X_v(u,v) = (0,1,\varphi_v(u,v))$
independientes que dan diferencial inyectiva.

***** 2.1.2. Superficies en forma implÃ­cita
****** 1.7. Teorema de la funciÃ³n implÃ­cita
:PROPERTIES:
:ID:       d513ba34-c5ce-4662-8e88-f6ab6779a7c2
:END:
Si $f \in {\cal C}^{\infty}(O \subseteq \mathbb{R}^3, \mathbb{R})$ en abierto con $f(x_0,y_0,z_0) = a$ y $f_z(x_0,y_0,z_0) \neq 0$;
existen $(x_0,y_0) \in U \subseteq \mathbb{R}^2$ entorno abierto, $z_0 \in I \subseteq \mathbb{R}$ intervalo abierto
y $\varphi \in {\cal C}^{\infty}(U)$ con $\varphi(U) \subseteq I$, tales que

 1. $U \times I \subseteq O$,

 2. $\varphi(x_0,y_0) = z_0$,

 3. $f^{-1}(a) \cap U \times I = G^z(\varphi)$.

******* Proof                                                                                             :extra:
# Se asume.

******* Card: implÃ­cita                                                                                   :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       b95d941e-d94a-403d-bc35-cf7a8b1e0fbd
:DRILL_LAST_INTERVAL: 37.1359
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:49]
:END:
Consecuencia prÃ¡ctica del teorema de la funciÃ³n implÃ­cita
sobre $\nabla f$.

******** Respuesta
Si $\nabla f(p) \neq 0$, la ecuaciÃ³n $\left\{ f(x,y,z) = f(p) \right\}$ es *localmente*
un grafo diferenciable sobre un plano coordenado.

****** 1.8. Valores singulares y valores regulares
Sea $O \subseteq \mathbb{R}^3$ abierto y $f \in {\cal C}^{\infty}(O)$. Un $a \in \mathbb{R}$ es *valor singular* si
es la imagen de un punto crÃ­tico, $f(p) = a$ con $(\nabla f)(p) = 0$. En otro
caso, es un *valor regular*. $VS(f)$ es el conjunto de valores
singulares y $VR(f)$ el conjuto de valores regulares.

******* Card: singular                                                                                    :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       c52b65a4-ad12-41e4-83a7-779f0f73f8c3
:DRILL_LAST_INTERVAL: 26.1369
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:24]
:END:
Valor singular de $f \in {\cal C}^{\infty}(O,\mathbb{R})$.

******** DefiniciÃ³n
$a$ es valor singular si es imagen de punto crÃ­tico, $f(p) = a$
con $\nabla f(p) = 0$.

******* Card: regular                                                                                     :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       58a580dd-5aa1-491c-9a7a-e9f04172e3a0
:DRILL_LAST_INTERVAL: 24.8425
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:24]
:END:
Valor regular de $f \in {\cal C}^{\infty}(O,\mathbb{R})$.

******** DefiniciÃ³n
$a$ es valor regular si no es singular, es decir, si no es
imagen de un punto crÃ­tico. Para cada $\nabla f(p) = 0$ se tiene
que $f(p) \neq a$.

****** 1.9. Teorema de Sard                                                                                :extra:
$VS(f)$ tiene medida nula. $VR(f)$ es denso en $\mathbb{R}$.

******* Card                                                                                              :drill:
SCHEDULED: <2018-08-10 Fri>
:PROPERTIES:
:ID:       034c652e-0f0d-4c3f-8c7b-d358ec404110
:DRILL_LAST_INTERVAL: 43.1634
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.18
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:13]
:END:
Enuncia el Teorema de Sard.

******** Enunciado
El conjunto de los valores singulares $VS(f)$ tiene medida
nula. $VR(f)$ es denso en $\mathbb{R}$.

Un $a \in \mathbb{R}$ es *valor singular* si es la imagen de un punto
crÃ­tico, $f(p) = a$ con $(\nabla f)(p) = 0$. En otro caso, es un
*valor regular*.

****** 1.10. Superficies implÃ­citas
$f \in {\cal C}^{\infty}(O \subset \mathbb{R}^3)$ en abierto. Si $a \in VR(f)$, entonces $f^{-1}(a)$
es vacÃ­o o una superficie de $\mathbb{R}^3$.

AdemÃ¡s, admite un atlas formado por parametrizaciones locales
de grafos.

******* 1.11. RecÃ­proco falso: inversa de singular puede ser superficie
$f(x,y,z) = z^2$ tiene $0 \in VS(f)$ pero $f^{-1}(0)$ es un plano.

******* Proof
Si $p \in S \neq \varnothing$, como $\nabla f(p) \neq 0$ tomamos $f_z(p) \neq 0$ s.p.g. y por
[[id:d513ba34-c5ce-4662-8e88-f6ab6779a7c2][Teorema de FunciÃ³n ImplÃ­cita]] tenemos $p \in f^{-1}(a) \cap U \times I = G^z(\varphi)$,
que es grafo [[id:1f5dceca-fdfe-4278-b8f4-7d4ccec964f3][luego superficie]] y como [[id:b0e77770-60d2-4de6-9cd9-10f4d1f35cf1][cada punto tiene entorno
superficie]], es una superficie.

****** 1.12. Ejemplos
Las esferas son superficies implÃ­citas y pueden tener atlas con
proyecciones estereogrÃ¡ficas o con seis proyecciones. Los cilindros
son superficies con un atlas de cuatro proyecciones.

***** 2.1.3. Superficies de revoluciÃ³n
****** 1.13. Subconjunto de revoluciÃ³n
$A \subset \mathbb{R}^3$ es de *revoluciÃ³n* si $\mathrm{rot}_{\theta}(A) = A$ para $\theta \in \mathbb{R}$.
Equivalentemente, $\mathrm{rot}_{\theta}(A) \subseteq A$.

******* Card                                                                                              :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       40312210-08af-43e1-8a80-7f3db9927644
:DRILL_LAST_INTERVAL: 24.4752
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:21]
:END:
Conjunto de revoluciÃ³n $A \subset \mathbb{R}^3$.

******** DefiniciÃ³n
$A \subset \mathbb{R}^3$ es de *revoluciÃ³n* si $\mathrm{rot}_{\theta}(A) = A$ para $\theta \in \mathbb{R}$.
Equivalentemente, $\mathrm{rot}_{\theta}(A) \subseteq A$.

******* Proof
Usamos $\mathrm{rot}_{-\theta}(A) \subset A$.

****** 1.13. Superficie de revoluciÃ³n
Sea $\alpha \colon I \to \mathbb{R}^3$ regular y embebida en $\alpha(I) \subseteq P^+$, el semiplano
abierto $x > 0$ de $y = 0$. Definimos

\[
S_{\alpha} =
\left\{ \phi_{\theta}(\alpha(t))
\mid t \in I, \theta \in \mathbb{R}
\right\},
\]

y $X \colon I \times \mathbb{R} \to \mathbb{R}^3$ dada por

\[
X(t,\theta) = \varphi_{\theta}(\alpha(t)) = (x(t)\cos(\theta), x(t)\sin(\theta), z(t)),
\]

para $\alpha(t) = (x(t),0,z(t))$.

AquÃ­ $S_{\alpha}$ es *superficie de revoluciÃ³n generada* respecto del eje Z.
Para $U_1 = I \times (0,2\pi)$ y $U_2 = I \times (-\pi,\pi)$ tenemos
un atlas dado por $X_{|U_1}$ y $X_{|U_2}$.

******* Proof: es de revoluciÃ³n
Si $p \in S_{\alpha}$, entonces $p = \phi_{\theta}(\alpha(t))$, luego $\phi_{\theta}(\phi_{\theta'}(\alpha(t))) = \phi_{\theta + \theta'}(\alpha(t)) \in S_{\alpha}$.

******* Proof: atlas
Tenemos $X(t,\theta) = (x(t)\sin(\theta), x(t)\cos(\theta), z(t)) \in {\cal C}^{\infty}$, sabemos que tiene
diferencial inyectiva porque

\[\abs{X_t(t,\theta) \times X_{\theta}(t,\theta)} =
\abs{x(t)}\abs{\alpha'(t)}.
\]

La inyectividad se tiene porque $X(t_1,\theta_1) = X(t_2,\theta_2)$ darÃ­a $x(t_1)^2 = x(t_2)^{2}$
(siendo $x$ positivo) y $z(t_1) = z(t_2)$; tendremos $\theta_1,\theta_2$ con mismo seno y coseno,
asÃ­ que serÃ¡ inyectiva en intervalos de longitud $2\pi$.

Finalmente queda comprobar $X^{-1}_1,X^{-1}_2$ continuas, que se puede hacer
calculÃ¡ndolas explÃ­citamente con la arcotangente o usando cÃ¡lculo
[[id:cfc42651-eabd-4024-8fe6-6ce2b4284e3c][diferencial de superficies]].

****** 1.14. Paralelos y meridianos
En una superficie de revoluciÃ³n generada, un *paralelo* es la curva $X(t_0,\theta)$,
un *meridiano* es la curva $X(t,\theta_0)$.

****** TODO 1.15. RevoluciÃ³n sobre grafos de funciones

**** 2.2. Cambio de parÃ¡metros
***** 2.2.1. Lema tÃ©cnico para el cambio de parÃ¡metros
:PROPERTIES:
:ID:       e55cb9c9-d19d-4a7a-8c3b-910e4f0ca5bf
:END:
$S$ superficie, $X \colon U \to S$ parametrizaciÃ³n local y $q \in U$. Existen
$q \in U_q \subset U$, $X(q) \in V_q \subset S$ entornos abiertos, $\pi \colon \mathbb{R}^3 \to \mathbb{R}^2$ proyecciÃ³n
coordenada, $(\pi \circ X)(q) \in D_q \subset \mathbb{R}^2$ entorno abierto y $\varphi \in {\cal C}^{\infty}(D_q)$ tales
que

 1. $\pi \circ X \colon U_q \to D_q$ difeomorfismo

 2. $\pi \colon V_q \to D_q$ biyectiva con $V_q = G(\varphi)$ grafo.

\[\begin{tikzcd}
& V_q\dar{\pi} \\
U_q \urar{X} & D_q \uar[bend right=90,swap]{\mathrm{id} \times \varphi}
\end{tikzcd}\]

****** Proof
******* Primer punto
Por $X = (x,y,z)$ [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][parametrizaciÃ³n]], $dX_{(u,v)}$ tiene rango 2 y un menor

\[\begin{vmatrix}
x_u(q) & x_v(q) \\
y_u(q) & y_v(q)
\end{vmatrix} \neq 0\]

Ahora, la Jacobiana de $\pi \circ X$ es esta, y $d(\pi \circ X)$ es isomorfismo.
Por [[id:16bd6b42-7b91-4fcf-8979-faabc0acd594][Teorema de la funciÃ³n inversa]], es difeomorfismo local, luego
tenemos entornos donde $(\pi \circ X)_{|U_q}\colon U_q \to D_q$ es un difeomorfismo.

******* Segundo punto
$V_q = X(U_q)$ es abierto de $S$ porque $U_q \subset U$ es abierto, $X \colon U \to X(U)$
es homeomorfismo y $X(U)$ es un [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][entorno coordenado]] y por tanto abierto.
Llamamos $Y = X \circ (\pi \circ X)^{-1} = (\alpha,\beta,\varphi)$, diferenciables por $Y \in {\cal C}^{\infty}(D_q)$ por
regla de la cadena clÃ¡sica. Como $\pi \circ Y = \id$, debe tenerse $Y_{|D_q}(u,v) = (u,v,\varphi(u,v))$,
luego $V_q = G^z(\varphi)$.

******* Tercer punto
$\pi(V_q) = (\pi \circ X)(U_{q}) = D_q$ y es biyectiva porque tiene como
inversa a la $Y$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       ab744492-3f4c-464e-bb8d-1801809037c2
:DRILL_LAST_INTERVAL: 22.2662
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:34]
:END:
Describe el diagrama de la demostraciÃ³n del lema tÃ©cnico para el
cambio de parÃ¡metros.

******* Respuesta
Para $q \in U$ tenemos la construcciÃ³n $q \in U_q \subset U$ siguiente

\[\begin{tikzcd}
& V_q\dar{\pi} \\
U_q \urar{X} & D_q \uar[bend right=90,swap]{\mathrm{id} \times \varphi}
\end{tikzcd}\]

***** 2.2.2. Corolario: una superficie es localmente un grafo
:PROPERTIES:
:ID:       e4c73e71-b439-499e-b722-d67eee8ac318
:END:
Para $p \in S$ superficie, existe un $p \in V \subset S$ entorno que es
el grafo de una funciÃ³n diferenciable. Esto puede usarse para
probar que ciertos conjuntos no son superficies.

****** Proof
Para $p \in S$, aplicamos el lema tÃ©cnico al $q = X^{-1}(p)$ y tenemos
$X(U_q)$ grafo.

****** Ejemplo de uso: el cono no es superficie
El semicono circular no es una superficie porque no es localmente
el grafo de una funciÃ³n diferenciable en el punto $(0,0,0)$.

***** 2.2.4. Cambio de parÃ¡metros
:PROPERTIES:
:ID:       3b537594-9c48-4d88-b979-cb841d93f776
:END:
Sean $X \colon U \to S$ e $Y \colon U' \to S$ parametrizaciones locales
con $W = X(U) \cap Y(U') \neq \varnothing$. Entonces el *cambio de parÃ¡metros*
$h = X^{-1} \circ Y \colon Y^{-1}(W) \to X^{-1}(W)$ es un difeomorfismo.

****** Proof
Sea $q' \in Y^{-1}(W)$ y $q = h(q')$. Por el [[id:e55cb9c9-d19d-4a7a-8c3b-910e4f0ca5bf][lema de cambio de parÃ¡metros]]
tenemos $q \in U_q \subset X^{-1}(W)$, $X(q) \in V_q \subset W$ entornos y una proyecciÃ³n
$\pi \colon V_q \to D_q$ biyectiva con $\pi \circ X \colon U_q \to D$ difeomorfismo.

Ahora, en $B' = Y^{-1}(V_q)$ tenemos

\[
h_{|B'} = (X^{-1} \circ Y)_{|B'} = (\pi \circ X)^{-1} \circ \pi \circ Y
\]

composiciÃ³n de difeomorfismos. Como cada $q'$ tiene un entorno
$B'$ donde $h$ es diferenciable y la diferenciabilidad es local,
tenemos $h$ diferenciable. Sabemos $h$ homeomorfismo por [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][definiciÃ³n]]
de parametrizaciÃ³n y un argumento similar nos da $h^{-1}$ diferenciable,
luego $h$ es difeomorfismo por definiciÃ³n.

\[\begin{tikzcd}
& V_q \subset W \dar{\pi} & \\
U_q \urar{X} & D_q & B' \ular[swap]{Y}
\end{tikzcd}\]

**** 2.3. El plano tangente
***** 2.3.1. Plano invariante a cambios de parÃ¡metros
Sean $X\colon U \to S$, $Y \colon U' \to S$ parametrizaciones con $p \in X(U) \cap Y(U')$.
Si $q = X^{-1}(p)$ y $q' = Y^{-1}(p)$, entonces

\[
(dX)_q(\mathbb{R}^2) = (dY)_{q'}(\mathbb{R}^2).
\]

****** Proof
Sea $W = XU \cap YU'$, por [[id:3b537594-9c48-4d88-b979-cb841d93f776][cambio de parÃ¡metros]], $h = X^{-1} \circ Y \colon Y^{-1}(W) \to X^{-1}(W)$
es difeomorfismo, $Y_{|Y^{-1}(W)} = X \circ h$ , y por regla de la cadena clÃ¡sica,

\[
(dY)_{q'}(\mathbb{R}^2) = ((dX)_{q} \circ (dh)_{q'}) (\mathbb{R}^2) = (dX)_q (\mathbb{R}^2)
\]

porque $(dh)_{q'}$ es [[id:6a2928b0-e787-4240-b76c-1856d14421bb][isomorfismo por]] ser $h$ difeomorfismo.

***** 2.3.2. Espacio tangente
$v \in \mathbb{R}^n$ es *vector tangente* a $A$ en $p \in A \subset \mathbb{R}^n$ si existe $\alpha \colon (-\varepsilon,\varepsilon) \to A$
con $\alpha(0) = p$ y $\alpha'(0) = v$. El *espacio tangente* $T_pA$ es el conjunto de los
vectores tangentes a $A$ en $p$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-09-17 Mon>
:PROPERTIES:
:ID:       f157e9c9-1f94-45c9-bc9a-fa82e35cf8c7
:DRILL_LAST_INTERVAL: 80.6142
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:14]
:END:
DefiniciÃ³n de *espacio tangente* $T_p(A)$.

******* DefiniciÃ³n
El *espacio tangente* $T_pA$ es el conjunto de los $v \in \mathbb{R}^n$ tales que
existe $\alpha \colon (-\varepsilon,\varepsilon) \to A$ con $\alpha(0) = p$ y $\alpha'(0) = v$.

***** 2.3.3. Propiedades del espacio tangente

1. $0 \in T_pA$.

2. MonotonÃ­a, $A \subset B \implies T_pA \subset T_pB$.

3. $O\subset \mathbb{R}^n$ abierto implica $T_pO = \mathbb{R}^n$.

4. En general, $T_pA$ no es espacio vectorial. (Contraejemplo en la uniÃ³n 
   de dos rectas.

****** Card: contraejemplo de espacio tangente no vectorial                                                :drill:
SCHEDULED: <2018-10-16 Tue>
:PROPERTIES:
:ID:       3eac5216-6c28-4763-8cd7-856bdd12a1c6
:DRILL_LAST_INTERVAL: 110.1017
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.75
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:10]
:END:
Da un ejemplo donde $T_pA$ no es espacio vectorial.

******* Contraejemplo
La uniÃ³n de dos rectas es su propio espacio tangente pero
no es espacio vectorial.

***** 2.3.4. El espacio tangente es un plano tangente
:PROPERTIES:
:ID:       477f9b05-a00e-4c40-84b6-9dd312b90b86
:END:
Para $X \colon U \to S$ parametrizaciÃ³n con $X(q) = p$,

\[
(dX)_q(\mathbb{R}^2) = T_pS.
\]

En particular, $T_pS$ es plano vectorial con base $B_q = \left\{ X_u(q),X_v(q) \right\}$.

****** Proof
******* Primera inclusiÃ³n
Si $v \in (dX)_q(u)$, como $T_pU = \mathbb{R}^2$, $u \in T_pU$ gracias a una curva $\alpha$.
Definimos $X \circ \alpha$ y comprobamos derivando que hace tangente a $v$.

******* Segunda inclusiÃ³n
$V = X(U)$ es [[id:e4c73e71-b439-499e-b722-d67eee8ac318][locamente un grafo]] y hay un entorno $p \in V_p$ donde es grafo,
$\pi \colon V_p \to D_p$ proyecciÃ³n con inversa $Y \colon D_p \to V$. Si $v \in T_pS$ gracias a $\alpha$,
tenemos $\alpha(-\delta,\delta) \subset V_p$ para algÃºn $\delta$ y llamamos $\beta_{|(-\delta,\delta)} = X^{-1}\circ \alpha$. Es
diferenciable por tenerse

\[
\beta = X^{-1} \circ \alpha = (X^{-1} \circ Y) \circ (\pi \circ \alpha)
\]

diferenciable por ser composiciÃ³n de [[id:3b537594-9c48-4d88-b979-cb841d93f776][cambio de parÃ¡metros]] y funciones
diferenciables. Finalmente comprobamos que 

\[
v = \alpha'(0) = (X \circ \beta)'(0) = (dX)_q(\beta'(0)).
\]

******** Esquema
Para comprobar $\beta$ diferenciable,

\[\begin{tikzcd}
& \mathbb{R}\dar{\alpha}\ar[bend right,swap]{ddl}{\beta}\\
& V_p\dar[bend left]{\pi}\dlar[swap]{X^{-1}} \\
U & D_p\lar{h}\uar[bend left]{Y}
\end{tikzcd}\]

****** Card: espacio tangente cuando es parametrizaciÃ³n                                                    :drill:
SCHEDULED: <2018-09-18 Tue>
:PROPERTIES:
:ID:       bcd93e24-5a7c-46bc-a8d6-39719007e6b1
:DRILL_LAST_INTERVAL: 83.1766
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:23]
:END:
Dada $X \colon U \to S$ parametrizaciÃ³n, da el espacio tangente
a $X(q) = p$.

******* Espacio tangente

\[
T_pS = (dX)_q(\mathbb{R}^2)
\]

****** Card: base del plano tangente                                                                :drill:curvas:
SCHEDULED: <2018-09-17 Mon>
:PROPERTIES:
:ID:       426e5b5a-7e3f-453e-b172-2526547e6ca4
:DRILL_LAST_INTERVAL: 82.0468
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:24]
:END:
Â¿CuÃ¡l es la base del plano $T_pS$ en funciÃ³n de una
parametrizaciÃ³n $X$?

******* Base
$B_q = \left\{ X_u(q),X_v(q) \right\}$, donde $X(q) = p$.

***** 2.3.6. Plano afÃ­n tangente y recta afÃ­n normal
El *plano afÃ­n tangente* de $S$ en $p$ es $p + T_pS$. La *recta afÃ­n normal*
de $S$ en $p$ es $p + (T_pS)^{\bot}$.

***** 2.3.7. Propiedades del plano tangente

 1. $dX_q \colon \mathbb{R}^2 \to T_pS$ es un isomorfismo lineal.

 2. El plano afÃ­n es el que mejor aproxima a $S$ cerca de $p$,

    \[
    \lim_{h \to 0} \frac{\abs{X(q+h) - X(q) - dX_q(h)}}{\abs{h}} = 0.
    \]

***** 2.3.8. El plano tangente es local
Si $S' \subseteq S$ entonces $T_pS' \subseteq T_pS$.

****** Proof
Por definiciÃ³n $T_pS' \subseteq T_pS$, siendo ambos planos se tiene igualdad.

***** 2.3.9. Planos bajo difeomorfismos
$\phi \colon O \to O'$ difeomorfismo y $S \subseteq O$ superficie,

\[
T_{\phi(p)}\phi(S) = (d\phi)_p(T_pS)
\]

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       11cf5e7c-ff50-4c64-b6a0-a9b455bb3024
:DRILL_LAST_INTERVAL: 29.465
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-10 Sun 13:57]
:END:
Sea $\phi \colon O \to O'$ difeomorfismo, calcula en funciÃ³n de la 
diferencial

\[
T_{\phi(p)}\phi(S)
\]

******* Respuesta

\[
T_{\phi(p)}\phi(S) = (d\phi)_p(T_pS)
\]

****** Proof                                                                                               :extra:
Tenemos para $p$ con parametrizaciÃ³n $X$, que $\phi \circ X$ es
[[id:2e12aed9-1de2-4dcf-a729-8068eae76532][parametrizaciÃ³n de]] $\phi(S)$, luego

\[
d\phi_p(T_pS) = d(\phi \circ X)_q(\mathbb{R}^2) = T_{\phi(p)}\phi(S).
\]

****** TODO Proof: apuntes
***** 2.3.10 Ejemplo: plano tangente de un grafo
Para $S = G^z(\varphi)$ tenemos el plano

\[
p + T_pS = \left\{ 
(x,y,z) \mid 
z = \varphi(q) + \varphi_x(q)(x-x_0) + \varphi_y(q)(y-y_0) 
\right\}.
\]

****** Proof
Se comprueba usando $(dX)_q(\mathbb{R}^2) = T_pS$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       f3e06ea3-900a-4b48-9714-b58d50465f0f
:DRILL_LAST_INTERVAL: 25.4211
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:30]
:END:
EcuaciÃ³n del plano tangente a una superficie grafo $S = G^z(\varphi)$
en $p$ con $X^{-1}(p) = q = (x_0,y_0)$.

******* Plano

\[
p + T_pS = \left\{ 
(x,y,z) \mid 
z = \varphi(q) + \varphi_x(q)(x-x_0) + \varphi_y(q)(y-y_0) 
\right\}.
\]

***** 2.3.11. Ejemplo: plano tangente de una superficie implÃ­cita
Sea $S = f^{-1}(a) \neq \varnothing$ con $a \in \mathrm{VR}(f)$, $f \in {\cal C}^{\infty}(O)$. Entonces

\[
T_pS = (\nabla f(p))^{\bot}
\]

****** Proof
Si $v \in T_pS$ gracias a $\alpha$, $f(\alpha(t)) = a$ constante y

\[
\pair{\nabla f(a), v} = (f \circ \alpha)'(0) = 0.
\]

Luego $v \in (\nabla f)(p)^{\bot}$ y tenemos dos planos uno contenido en otro.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       db778307-cfe6-45e9-ae36-2becb553fe9a
:DRILL_LAST_INTERVAL: 11.6809
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:11]
:END:
Si $S = f^{-1}(a) \neq \varnothing$ es superficie implÃ­cita dada por un valor
regular, Â¿cuÃ¡l es el plano $T_p(S)$?

******* Plano

\[
T_p(S) = (\nabla f(p))^{\bot}
\]

***** Ej.10. Corte al eje normal
Todas las rectas afines normales de una superficie de revoluciÃ³n
creada por una curva $\alpha(t) = (x(t),0,z(t))$ con $z'(t) \neq 0$ cortan al
eje $Z$. 

****** Proof
Calculamos $p + (T_pS)^{\bot}$ usando las parametrizaciones usuales de
una superficie de revoluciÃ³n y comprobamos que en el caso dado
se tiene la intersecciÃ³n.

***** 2.3.14. Primera forma fundamental
La restricciÃ³n a $T_pS$ de $\pair{-,-}$ se llama *primera forma fundamental*.
Es una mÃ©trica definida positiva.

**** 2.4. Superficies y cÃ¡lculo diferencial
***** 2.4.1. Aplicaciones diferenciables
****** 2.4.1. Aplicaciones diferenciables
:PROPERTIES:
:ID:       230b03e1-ea5a-487e-9b27-f497edd8b400
:END:
Dados $S,S'$ superficies, $O \subset \mathbb{R}^n, O' \subset \mathbb{R}^m$ abiertos. Definimos
*diferenciabilidad* en los siguientes casos

 * $f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$ si $f \circ X \in {\cal C}^{\infty}(U,\mathbb{R}^m)$ para cualquier parametrizaciÃ³n.

 * $f \in {\cal C}^{\infty}(S,O')$ si $i \circ f \in {\cal C}^{\infty}(S, \mathbb{R}^m)$, donde $i$ es la inclusiÃ³n.

 * $f \in {\cal C}^{\infty}(S,S')$ si $i \circ f \in {\cal C}^{\infty}(S,\mathbb{R}^3)$, donde $i$ es la inclusiÃ³n.

 * $f \in {\cal C}^{\infty}(O,S')$ si $i \circ f \in {\cal C}^{\infty}(O,\mathbb{R}^3)$, donde $i$ es la inclusiÃ³n.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-17 Tue>
:PROPERTIES:
:ID:       d5eeb9fd-2b74-47d4-8a94-11d8d4216455
:DRILL_LAST_INTERVAL: 42.9311
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:54]
:END:
Definir diferenciabilidad en los cuatro casos

 * $f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$ 

 * $f \in {\cal C}^{\infty}(S,O')$ 

 * $f \in {\cal C}^{\infty}(S,S')$ 

 * $f \in {\cal C}^{\infty}(O,S')$ 

******** DefiniciÃ³n

 * $f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$ si $f \circ X \in {\cal C}^{\infty}(U,\mathbb{R}^m)$ para cualquier parametrizaciÃ³n.

 * $f \in {\cal C}^{\infty}(S,O')$ si $i \circ f \in {\cal C}^{\infty}(S, \mathbb{R}^m)$, donde $i$ es la inclusiÃ³n.

 * $f \in {\cal C}^{\infty}(S,S')$ si $i \circ f \in {\cal C}^{\infty}(S,\mathbb{R}^3)$, donde $i$ es la inclusiÃ³n.

 * $f \in {\cal C}^{\infty}(O,S')$ si $i \circ f \in {\cal C}^{\infty}(O,\mathbb{R}^3)$, donde $i$ es la inclusiÃ³n.

****** 2.4.2. Diferenciabilidad de parametrizaciones
:PROPERTIES:
:ID:       38f14c25-5a11-44aa-971c-240238dbd2a7
:END:
Para $X\colon U \to S$ parametrizaciÃ³n local, $X \in {\cal C}^{\infty}(U,XU)$ y $X^{-1} \in {\cal C}^{\infty}(XU,U)$.

******* Proof
$X \colon U \to V = XU$ es diferenciable por serlo $i \circ X$ por
definiciÃ³n de [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][parametrizaciÃ³n]].

$X^{-1}$ es diferenciable [[id:be30c69b-44e3-428c-a5a9-9f055fff8b24][si]] lo es $j \circ X^{-1} \colon V \to \mathbb{R}^2$, donde
$V$ es superficie [[id:798b32ea-aeca-4f78-9265-53ce523e0630][por ser abierto]] no vacÃ­o de $S$. Esta lo
[[id:230b03e1-ea5a-487e-9b27-f497edd8b400][es]] si cada parametrizaciÃ³n $Y \colon U' \to V$ hace $(j \circ X^{-1}) \circ Y$
diferenciable, pero $j \circ (X^{-1} \circ Y)$ es una composiciÃ³n de
inclusiÃ³n y de [[id:3b537594-9c48-4d88-b979-cb841d93f776][cambio de parÃ¡metros]].

******* Cards                                                                                             :drill:
SCHEDULED: <2018-09-22 Sat>
:PROPERTIES:
:ID:       4cf91a32-d1d8-43a8-929c-873dbe42351f
:DRILL_LAST_INTERVAL: 84.7624
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:51]
:END:
Diferenciabilidad (no clÃ¡sica) de las parametrizaciones locales y sus inversas.

******** Diferenciabilidad
Para $X$ parametrizaciÃ³n local, $X \in {\cal C}^{\infty}(U,S)$ y $X^{-1} \in {\cal C}^{\infty}(XU,U)$.
NÃ³tese que sabÃ­amos por definiciÃ³n $X \in {\cal C}^{\infty}(U,\mathbb{R}^3)$.

Esta diferenciabilidad lo es en el sentido no clÃ¡sico.

****** Ej. 11 y 12. InclusiÃ³n e identidad diferenciables
$i \colon S \to \mathbb{R}^3$ e identidad $\mathrm{Id}\colon S \to S$ son diferenciables. La inclusiÃ³n
$i \colon S' \to S$ para una subsuperficie es diferenciable.

******* Proof
La inclusiÃ³n es diferenciable por definiciÃ³n de [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][parametrizaciÃ³n]].
La identidad lo es por serlo la inclusiÃ³n. NÃ³tese que en el caso
de una subsuperficie, una parametrizaciÃ³n de $S'$ es diferenciable
tambiÃ©n despuÃ©s de la inclusiÃ³n, que no es siquiera relevante.

****** 2.4.3. CaracterizaciÃ³n de diferenciable por atlas
Equivalen

 * $f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$,

 * existe un [[id:39a8bc2e-b4b9-4131-9fe4-7c1ed8fa3d87][atlas]] tal que $f \circ X_i \colon U_i \to \mathbb{R}^m$ es siempre diferenciable.

******* Proof
******** Primera implicaciÃ³n
Existe un atlas por [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][definiciÃ³n de superficie]] y allÃ­ $f \circ X_i$ es
diferenciable por [[id:230b03e1-ea5a-487e-9b27-f497edd8b400][definiciÃ³n de diferenciabilidad]] de superficies.

******** Segunda implicaciÃ³n
Dada $Y \colon U' \to S$ parametrizaciÃ³n con $q' \in U'$, en $Y(q') = p$ debe existir
una parametrizaciÃ³n local del atlas $X \colon U \to S$, con $p \in U$ y $f \circ X$ diferenciable.
Tomamos $W = X(U) \cap Y(U')$ y tenemos

\[
(f \circ Y)_{|Y^{-1}(W)} = (f \circ X) \circ (X^{-1} \circ Y)
\]

diferenciable por regla de la cadena clÃ¡sica por ser composiciÃ³n de
difernciable y [[id:3b537594-9c48-4d88-b979-cb841d93f776][cambio de parÃ¡metros]].

******* Card                                                                                              :drill:
SCHEDULED: <2018-09-25 Tue>
:PROPERTIES:
:ID:       688ed1e0-d2a2-4639-a681-1ce01f696780
:DRILL_LAST_INTERVAL: 88.961
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.75
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:12]
:END:
Da una caracterizaciÃ³n de $f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$ usando atlas.

******** CaracterizaciÃ³n
Existe un atlas tal que $f \circ X_i \colon U_i \to \mathbb{R}^m$ es siempre
diferenciable.

****** 2.4.4. Propiedades de aplicaciones diferenciables
:PROPERTIES:
:ID:       c57ee25a-ece7-4c4a-92a5-253ff821001b
:END:

 1) Toda diferenciable es continua.

 2) $f+g$, $\varphi f$, $\lambda f$, $\pair{f,g}$, $\varphi/\psi$ son diferenciables con $\psi(p) \neq 0$.

 3) para $S \subset O$, se tiene $f \in {\cal C}^{\infty}(O,\mathbb{R}^m) \implies f_{|S} \in {\cal C}^{\infty}(S,\mathbb{R}^m)$

 4) para $S = \bigcup S_i$, se tiene $f \in {\cal C}^{\infty}(S_i,\mathbb{R}^m) \iff f \in {\cal C}^{\infty}(S,\mathbb{R}^m)$

 5) regla de la cadena $f \in {\cal C}^{\infty}(S,S'), g\in {\cal C}^{\infty}(S',S'') \implies f \circ g\in {\cal C}^{\infty}(S,S'')$.

******* Proof 1
Dado $p$ con parametrizaciÃ³n $X \colon U \to V$, consideramos

\[
f_{|V} = (f \circ X) \circ X^{-1},
\]

composiciÃ³n de diferenciable (clÃ¡sica) y homeomorfismo.
AsÃ­ $f$ es continua en $p$ arbitrario.

******* Proof 2
Se usan las propiedades de diferenciabilidad. Por ejemplo,

\[
(f + g) \circ X = f \circ X + g \circ X
\]

es suma de diferenciables (en sentido clÃ¡sico).

******* Proof 3
Tenemos $f_{|S} = f \circ (i \circ X)$ composiciÃ³n de diferenciables luego
diferenciable por regla de la cadena clÃ¡sica.

******* TODO Proof 4
******* TODO Proof 5
En el caso $S'$ superficie y $S=O$, $S''=O''$ abiertos 
[...]

\[
(i \circ g\circ f)_{|B} = (i \circ g \circ I_{V'}\circ f)_{|B} = (i\circ g \circ Y) \circ (\pi \circ f)_{|B}
\]
***** 2.4.2. Diferencial de una aplicaciÃ³n diferenciable
****** 2.4.6. Diferencial de una aplicaciÃ³n diferenciable
:PROPERTIES:
:ID:       51f883ea-541f-41ae-bfba-ebb99bc64622
:END:
La diferencial de $f \colon S \to S'$ en $p \in S$ es la aplicaciÃ³n
$(df)_p \colon T_pS \to T_{f(p)}S'$ dada por

\[
(df)_p(v) := (f \circ \alpha)'(0)
\]

donde $\alpha \colon (-\varepsilon,\varepsilon) \to S$ es una curva $\alpha(0) = p$ y $\alpha'(0) = v$.
Esto da una aplicaciÃ³n lineal independiente de $\alpha$.

******* Proof: dominios
Tenemos $f \circ \alpha \colon (-\varepsilon,\varepsilon) \to S'$ curva con $f(\alpha(0)) = f(p)$, luego
$(f \circ \alpha)'(0) \in T_{f(p)}S'$. NÃ³tese que una curva $\alpha$ asÃ­ existe para
cualquier $v \in T_pS$ por definiciÃ³n.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-23 Mon>
:PROPERTIES:
:ID:       e8520435-9cb6-4b93-8d2f-bf62c5ae7a19
:DRILL_LAST_INTERVAL: 44.786
:DRILL_REPEATS_SINCE_FAIL: 5
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.167
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:49]
:END:
Define la diferencial de $f \colon S \to S'$ en $p \in S$.

******** DefiniciÃ³n
La aplicaciÃ³n $(df)_p \colon T_pS \to T_{f(p)}S'$ dada por

\[
(df)_p(v) := (f \circ \alpha)'(0)
\]

donde $\alpha \colon (-\varepsilon,\varepsilon) \to S$ es una curva $\alpha(0) = p$ y $\alpha'(0) = v$.
Se puede demostrar que asÃ­ estÃ¡ bien definida y no depende de $\alpha$.

****** 2.4.8. La diferencial estÃ¡ bien definida
La [[id:51f883ea-541f-41ae-bfba-ebb99bc64622][diferencial]] no depende de la curva escogida.

******* Proof
Tomamos $X \colon U \to S$, $Y \colon U' \to S'$ parametrizaciones tales que
$f(X(U)) \subseteq Y(U')$ con $X(q) = p$. Dada una curva $\alpha$ definimos a$\beta = X^{-1} \circ \alpha$
diferenciable por regla de la cadena con

\[
v = \alpha'(0) = (X \circ \beta)'(0) = dX_q(\beta'(0))
\]

siendo $dX_q$ isomorfismo, $\beta'(0) = dX_q^{-1}(v)$. Finalmente,

\[
(f \circ \alpha)'(0) = 
(Y \circ (Y^{-1} \circ f \circ X) \circ (X^{-1}\circ \alpha))'(0) =
(dY_{q'} \circ d(Y^{-1}\circ f\circ X)_q \circ dX_q) (v),
\]

donde tenemos tres diferenciales no dependientes de $\alpha$.

****** 2.4.9. Diferencial de constante es nula
Si $f \colon S \to S'$ es constante, $df_p\colon T_pS \to T_{f(p)}S$ es nula.

******* Proof
Trivial calculando.

****** 2.4.10. Diferencial de inclusiÃ³n es inclusiÃ³n
Si $i \colon S \to \mathbb{R}^3$ es la inclusiÃ³n, $di_p \colon T_pS \to \mathbb{R}^3$ es la inclusiÃ³n.

******* Proof
Trivial calculando.

****** 2.4.11. Diferencial de identidad es identidad
La diferencial de la identidad en $S$ es la identidad en $T_pS$.

****** 2.4.12. Diferencial de inclusiÃ³n de superficies es identidad
Si $S' \subseteq S$ abierto, entonces la diferencial de $p \in S$ de la inclusiÃ³n
es la identidad en $T_pS$.

****** 2.4.13. Propiedades de la diferencial
La diferencial cumple

 1. Para $f,g \in {\cal C}^{\infty}(S,\mathbb{R}^m)$, $\varphi \in {\cal C}^{\infty}(S)$,

    1. $d(f+g)_p(v) = df_p(v) + dg_p(v)$,

    2. $d(\varphi f)_p(v) = (d\varphi_p(v)) f(p) + \varphi(p) (df_p(v))$,

    3. $d\pair{f,g}_p(v) = \pair{(df)_p(v),g(p)} + \pair{f(p),(dg)_p(v)}$,

    4. $d(f \times g)_p(v) = df_p(v) \times g(p) + f(p) \times dg_p(v)$

 2. Para $p \in S \subset O$ con $f \in {\cal C}^{\infty}(O,\mathbb{R}^m)$, se tiene $(df_{|S})_p = (df_p)_{|T_pS}$.

 3. Para $f \colon S \to S'$ con $V \subset S$, se tiene $(df_{|V})_p = df_p$.

 4. $d(g \circ f)_p = dg_{f(p)} \circ df_p$.

******* TODO Proof

******* Card: diferencial del producto vectorial                                                          :drill:
SCHEDULED: <2018-07-18 Wed>
:PROPERTIES:
:ID:       797e93ca-a758-4751-9ce4-a7d532280cc9
:DRILL_LAST_INTERVAL: 39.8571
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:37]
:END:
Calcular $d(f \times g)_p(v)$.

******** Resultado

\[
df_p(v) \times g(p) + f(p) \times dg_p(v)
\]

****** 2.4.14. Diferencial nula es aplicaciÃ³n constante
:PROPERTIES:
:ID:       280958a3-c918-492b-8f0e-697d515101d4
:END:
Si $f \colon S \to S'$ cumple $df_p = 0$ para cada $p \in S$, entonces $f$ es constante
en cada componente conexa.

******* Proof
Sea $f(p_0) = a$, tenemos $f^{-1}(a) \subset S$ cerrado relativo no vacÃ­o.

Para $p \in f^{-1}(a)$, tomamos parametrizaciones $X \colon U \to S$ e $Y \colon U' \to S'$
con $p = X(q)$ y $a = Y(q')$; podemos suponer $U$ conexo, $f(X(U)) \subset Y(U')$.
Se tiene $F = Y^{-1}\circ f \circ X$ diferenciable por regla de la cadena con

\[
dF_q = (dY^{-1})_{f(X(q))} \circ (df)_{X(q)} \circ (dX)_q = 0
\]

y por tanto $F$, aplicaciÃ³n entre abiertos, es constante. AsÃ­,
$f = Y \circ F \circ X^{-1}$ es constante en $X(U)$, entorno abierto de $p$.

Por tanto, $f^{-1}(a)$ es abierto y cerrado y debe ser todo $S$.

****** 2.4.15. Extremos locales
Para $f \colon S \to \mathbb{R}$ sobre superficie o abierto, consideramos

 * *mÃ­nimo local* a $p_0$ si hay un entorno abierto $p_0 \in V \subset S$
   con $f(p) \geq f(p_0)$; anÃ¡logamente *mÃ¡ximo local*;

 * *punto crÃ­tico* a $p_0$ si $f$ es diferenciable y $(df)_{p_0} = 0$.

****** TODO [#A] 2.4.17. Puntos crÃ­ticos en extremos locales
Si $f \in {\cal C}^{\infty}(S)$ alcanza en $p_0 \in S$ extremo local, entonces $\nabla f(p) = 0$,
siendo punto crÃ­tico.

******* TODO Proof
***** 2.4.3. Difeomorfismos. Teorema de la funciÃ³n inversa
****** 4.18. Difeomorfismo
:PROPERTIES:
:ID:       f2a430ff-2f4b-485f-802e-6333afbbb337
:END:
$f \colon S \to S'$ entre superficies es *difeomorfismo* si es
diferenciable, biyectiva y con inversa diferenciable.

****** Ej.13. Difeomorfismo es relaciÃ³n de equivalencia
La identidad, el inverso de difeomorfismo y la composiciÃ³n de
difeomorfismos son difeomorfismos. Ser difeomorfos es relaciÃ³n
de equivalencia.

******* Proof
Trivial desde la [[id:f2a430ff-2f4b-485f-802e-6333afbbb337][definiciÃ³n]].

****** Ej.14. Difeomorfismo restringido sobre superficie es difeomorfismo
Si $\phi \colon O \to O'$ es difeomorfismo y $S \subsetP$ es superficie, entonces
$\phi_{|S} \colon S \to \phi(S)$ es difeomorfismo.

******* Proof
Trivial considerando que $\phi^{-1} \colon \mathbb{R}^3 \to \mathbb{R}^3$ es tambiÃ©n diferenciable
y que la [[id:c57ee25a-ece7-4c4a-92a5-253ff821001b][restricciÃ³n a superficie de diferenciable es diferenciable]].

****** 4.19. Parametrizaciones son difeomorfismos
Las parametrizaciones de una superficie son difeomorfismos por ser
ellas y sus inversas [[id:38f14c25-5a11-44aa-971c-240238dbd2a7][diferenciables]]. AsÃ­ las superficies son
subconjuntos localmente difeomorfos a abiertos del plano.

****** 4.20. Difeomorfismo local
$f \colon S \to S'$ es difeomorfismo local en $p \in S$ si hay entornos abiertos
$p \in V \subset S$ y $f(p) \in V' \subset S'$ tales que $f_{|V}\colon V \to V'$ es difeomorfismo.
Es un *difeomorfismo local* si lo es en cada punto.

****** 4.21. Difeomorfismo local es abierta
:PROPERTIES:
:ID:       3c74d759-f8e6-4497-8eb3-5af5996a840d
:END:
Si $f \colon S \to S'$ es difeomorfismo local, es aplicaciÃ³n abierta.

******* Proof
Sea $V \subset S$ abierto. Para $q \in f(V)$, si $f(p) = q$ tenemos por ser
difeomorfismo local que $f_{|W}$ es difeomorfismo, donde $p \in W$ y $q \in W'$
entornos abiertos. Ahora $f(V \cap W)$ por difeomorfismo (y en particular
homeomorfismo) es un abierto de $W$ y por tanto de $S$ que estÃ¡ en 
$f(V)$ y contiene a $q$.

****** 4.22. Difeomorfismo es difeomorfismo local biyectivo
:PROPERTIES:
:ID:       1c6c3936-2bbb-4237-804e-072c1ea05d02
:END:
$f \colon S \to S'$ difeomorfismo ssi es difeomorfismo local biyectivo.

******* Proof
Si es difeomorfismo local biyectivo, sÃ³lo falta probar $f^{-1}$
diferenciable, pero eso [[id:c57ee25a-ece7-4c4a-92a5-253ff821001b][lo podemos hacer localmente]] en un
recubrimiento de abiertos de $S'$. Si $q \in S'$ con $f(p) = q$,
tomamos los entornos haciendo $f_{|V_p}\colon V_p \to V'_q$ difeomorfismo,
que nos da $f^{-1}$ diferenciable.

****** 4.23. Teorema de la funciÃ³n inversa
:PROPERTIES:
:ID:       8e0b57e0-deed-4aac-8e29-e5b43406c141
:END:
Para $f \colon S \to S'$ diferenciable, si $(df)_p \colon T_pS \to T_{f(p)}S'$ es
isomorfismo, entonces $f$ es difeomorfismo local en $p$.

******* TODO Proof
******* Card                                                                                              :drill:
SCHEDULED: <2018-06-30 Sat>
:PROPERTIES:
:ID:       e2083ec1-dfff-4974-a684-a2611c6f5c70
:DRILL_LAST_INTERVAL: 25.5324
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.32
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:55]
:END:
Enuncia el teorema de la funciÃ³n inversa entre superficies.

******** Enunciado
Para $f \colon S \to S'$ diferenciable, si $(df)_p \colon T_pS \to T_{f(p)}S'$ es
isomorfismo, entonces $f$ es difeomorfismo local en $p$.

******** Nota extra
NÃ³tese que si es difeomorfismo local, debe tener diferencial
isomorfismo.

****** 4.24. Corolarios al teorema de la funciÃ³n inversa
:PROPERTIES:
:ID:       2568912c-5677-40ac-873e-e49d08c26a5c
:END:
Sea $f \colon S \to S'$ diferenciable

 1. $f$ difeomorfismo local ssi $df_p$ siempre isomorfismo.
 2. $f$ difeomorfismo ssi $f$ biyectiva y $df_p$ siempre isomorfismo.

****** 4.25. Superficie contenida en superficie es abierto relativo
Si $S' \subset S$, entonces $S'$ es abierto de $S$.

******* Card                                                                                              :drill:
SCHEDULED: <2018-07-21 Sat>
:PROPERTIES:
:ID:       35427325-1210-4286-a7be-9d5ff9231027
:DRILL_LAST_INTERVAL: 46.9166
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:55]
:END:
Si tenemos $S' \subset S$, Â¿quÃ© informaciÃ³n topolÃ³gica nos da?

******** Respuesta
Sabemos $S'$ abierto relativo de $S$.

******* Proof
Tomamos $i \colon S' \to S$ inclusiÃ³n con $di_p \colon T_pS \to T_pS' = T_pS$ la
identidad. Por [[id:8e0b57e0-deed-4aac-8e29-e5b43406c141][Teorema de la funciÃ³n inversa]] es difeomorfismo
local en todo punto; y [[id:1c6c3936-2bbb-4237-804e-072c1ea05d02][siendo biyectiva es difeomorfismo]].
Pero un [[id:3c74d759-f8e6-4497-8eb3-5af5996a840d][difeomorfismo es aplicaciÃ³n abierta]], luego $i(S')$ es
abierto relativo de $S$.

****** 4.26. CaracterizaciÃ³n de parametrizaciÃ³n
:PROPERTIES:
:ID:       cfc42651-eabd-4024-8fe6-6ce2b4284e3c
:END:
Si $X \colon U \to \mathbb{R}^3$ con $X(U) \subset S$ es diferenciable, inyectiva, y con
diferencial inyectiva $(dX)_q \colon \mathbb{R}^2 \to \mathbb{R}^3$, es una parametrizaciÃ³n
local de $S$.

/Estamos ganando el ser homeomorfismo./

******* Proof
Tenemos $dX_q \colon \mathbb{R}^2 \to T_{X(q)}S$ inyectiva luego isomorfismo, asÃ­,
$X$ es difeomorfismo local y por tanto abierto. $X(U)$ es un
abierto de $S$ y $X \colon U \to X(U)$ es difeomorfismo local biyectivo
luego difeomorfismo, siendo en particular homeomorfismo.

**** 2.5. Superficies orientables
***** 2.5.1. Campo de vectores
Un *campo de vectores* es un $F \colon S \to \mathbb{R}^3$. Es

 * *diferenciable* si $F \in {\cal C}^{\infty}(S,\mathbb{R}^3)$,
 * *tangente* si $F(p) \in T_p(S)$,
 * *normal* si $F(p) \in (T_p(S))^{\bot}$,
 * *unitario* si $X(S) \subset \mathbb{S}^2$.

***** 2.5.2. OrientaciÃ³n
:PROPERTIES:
:ID:       18f9a9b7-8def-49c2-a185-a536b9bea052
:END:
Una *orientaciÃ³n* es un campo normal, unitario y diferenciable.
Una superficie es orientable si admite una orientaciÃ³n.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:ID:       564f702b-78ce-48bf-9a67-02e0a4b51a16
:DRILL_LAST_INTERVAL: 30.5675
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:51]
:END:
OrientaciÃ³n de una superficie.

******* DefiniciÃ³n
Un campo de vectores $F \colon S \to \mathbb{R}^3$ normal, unitario y
diferenciable.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-30 Sat>
:PROPERTIES:
:ID:       5c3f56c5-cdb3-4d7c-b09a-4ba12c642c2f
:DRILL_LAST_INTERVAL: 35.0266
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.8
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-05-26 Sat 22:19]
:END:
Superficie orientable.

******* DefiniciÃ³n
Admite un campo de vectores normal, unitario y diferenciable.

***** 2.5.4. Una superficie orientable tiene dos orientaciones
Si $N_1,N_2 \colon S \to \mathbb{R}^3$ son dos orientaciones de $S$ conexa, $N_1 = \pm N_2$.
Una superficie conexa y orientable tiene dos orientaciones.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-09 Mon>
:PROPERTIES:
:ID:       d172d5aa-810b-43b9-81d2-02a8a3071acb
:DRILL_LAST_INTERVAL: 35.4275
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-04 Mon 14:47]
:END:
Tenemos dos orientaciones de una superficie conexa, 

\[
N_1,N_2 \colon S \to \mathbb{R}^3,
\]

Â¿quÃ© sabemos?

******* Respuesta
Sabemos que son iguales o opuestas, $N_1 = \pm N_2$.

****** TODO Proof
Por conexiÃ³n.

***** TODO 2.5.5. Cinta de MÃ¶bius no es orientable                                                          :extra:
***** 2.5.6. Orientabilidad es local
Si $S$ es orientable, entonces $S' \subset S$ abierta es orientable con
la restricciÃ³n de la orientaciÃ³n.

****** Proof
NÃ³tese que la restricciÃ³n seguirÃ¡ siendo diferenciable (la
diferenciabilidad es local) y unitaria. AdemÃ¡s, $T_pS = T_pS'$,
luego si era normal, seguirÃ¡ siendo normal.

***** 2.5.7. Orientabilidad es invariante diferenciable
Calcular la orientaciÃ³n tras un difeomorfismo en general no es
sencillo.  SÃ³lo en el caso de movimiento rÃ­gido es fÃ¡cil.

****** TODO Proof
***** 2.5.8. Orientabilidad de entornos coordenados
Si $X \colon U \to S$ es parametrizaciÃ³n, $X(U)$ es orientable con
$N_X = n_X \circ X^{-1}$, donde

\[
n_X(q) = \frac{X_u(q) \times X_v(q)}{\abs{X_u(q) \times X_v(q)}}.
\]

****** Proof
Tenemos $n_X(q) \perp T_{X(q)}S$ y es unitario. Por regla de la cadena,
tenemos $N_X$ diferenciable.

****** Corolario: toda superficie es orientable localmente
****** Corolario: parametrizaciÃ³n global da orientabilidad
****** Corolario: los grafos son orientables
Para $X(u,v) = (u,v,\varphi(u,v))$, tenemos la orientaciÃ³n,

\[\begin{aligned}
N_X(u,v,w) &=
(n_X \circ X^{-1})(u,v,w) = n_X(u,v) \\&=
\frac{(-\varphi_u(u,v), -\varphi_v(u,v), 1)}{\sqrt{1 + |\nabla \varphi(u,v)|^2}}
\end{aligned}\]

***** 2.5.9. Orientabilidad de superficies implÃ­citas
Para $a \in VR(f)$, $f \in {\cal C}^{\infty}(O)$, si $S = f^{-1}(a) \neq \varnothing$, tenemos

\[
N(p) = \frac{\nabla f(p)}{\abs{\nabla f(p)}}
\]

orientaciÃ³n.

****** Proof
Es un campo unitario que sabemos que es normal por tenerse
$T_pS = (\nabla f(p))^{\bot}$. La aplicaciÃ³n es diferenciable porque
$f \in {\cal C}^{\infty}(O)$.

*** Tema 3. Curvatura de superficies ([[file:~/pdf/rosales_curvaturas.pdf][PDF]])
**** 3.1. AplicaciÃ³n de Gauss, endomorfismo de Weingarten y curvaturas
***** 1.1. AplicaciÃ³n de Gauss
La restricciÃ³n de una [[id:18f9a9b7-8def-49c2-a185-a536b9bea052][orientaciÃ³n]] a la esfera $N \colon S \to \mathbb{S}^2$
se llama *aplicaciÃ³n de Gauss*. Una superficie conexa y
orientable tiene exactamente dos aplicaciones de Gauss,
$N$ y $-N$.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       25fbcbe6-bd07-4c93-be6d-4676de47efe9
:DRILL_LAST_INTERVAL: 12.3841
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:45]
:END:
Una superficie [conexa y orientable] tiene exactamente [dos] aplicaciones
de Gauss.

***** 1.2. Endomorfismo de Weingarten
La aplicaciÃ³n $A_p = -dN_p$ es un endomorfismo de $T_pS$ llamado 
*endomorfismo de Weingarten*.

****** Proof
Tenemos $dN_p \colon T_pS \to T_{N(p)}\mathbb{S}^2$. Como en la esfera una
orientaciÃ³n viene determinada por direcciones que une el origen
con el punto, $T_{N(p)}\mathbb{S}^2 = N(p)^{\bot} = T_p\mathbb{S}^2$.

****** Card: es un endomorfismo                                                                            :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       85038fce-1ba9-40e5-bf9a-804cac2cf6b3
:DRILL_LAST_INTERVAL: 10.1339
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:40]
:END:
Â¿Por quÃ© el endomorfismo de Weingarten es un endomorfismo?

******* Answer
Tenemos $dN_p \colon T_pS \to T_{N(p)}\mathbb{S}^2$. Como en la esfera una
orientaciÃ³n viene determinada por direcciones que une el origen
con el punto, $T_{N(p)}\mathbb{S}^2 = N(p)^{\bot} = T_pS^2$.

El endomorfismo es $A_p = -dN_p$.

****** Card: definiciÃ³n                                                                                    :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       d7231d09-7b51-4da7-af79-37990a773acd
:DRILL_LAST_INTERVAL: 11.4736
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:42]
:END:
DefiniciÃ³n del endomorfismo de Weingarten.

******* Answer
$A_p = -dN_p$, donde $N$ es una orientaciÃ³n de la superficie.

***** 1.4. Segunda forma fundamental
La *segunda forma fundamental* es $\sigma_p = II_p \colon T_pS \times T_pS \to \mathbb{R}$ dada
por

\[
\sigma_p(u,v) = \pair{A_p(u),v} = -\pair{dN_p(u),v}.
\]

La forma $\sigma_p$ es una mÃ©trica sobre $T_pS$, esto es, una aplicaciÃ³n
bilineal simÃ©trica. En particular tenemos, para la base de una
parametrizaciÃ³n,

 * $\pair{N(p),X_{vu}(q)} = \sigma_p(X_u(q),X_v(q))$,
 * $\pair{N(p),X_{uv}(q)} = \sigma_p(X_v(q),X_u(q))$.

****** Proof: simetrÃ­a
Bilineal por definiciÃ³n. Comprobamos simetrÃ­a sobre la base dada por
una parametrizaciÃ³n $X$ y extendemos por bilinealidad.

Por definiciÃ³n de orientaciÃ³n, $\pair{N \circ X, X_v} = 0$; derivando
$\pair{(N \circ X)_u,X_v} + \pair{N \circ X,X_{vu}} = 0$; y evaluando en $q = X^{-1}(p)$,

 * $\pair{N(p),X_{vu}(q)} = \sigma_p(X_u(q),X_v(q))$
 * $\pair{N(p),X_{uv}(q)} = \sigma_p(X_v(q),X_u(q))$

por Schwarz-Clairaut, tenemos la simetrÃ­a.

****** Card: definiciÃ³n                                                                                    :drill:
SCHEDULED: <2018-07-04 Wed>
:PROPERTIES:
:ID:       f9d7f47e-ccbe-43f2-b654-db6b6e1ef7ea
:DRILL_LAST_INTERVAL: 26.4288
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:46]
:END:
Define la segunda forma fundamental $\sigma_p(u,v)$.

******* Answer
La aplicaciÃ³n $\sigma_p = II_p \colon T_pS \times T_pS \to \mathbb{R}$ definida por

\[
\sigma_p(u,v) = \pair{A_p(u),v} = -\pair{dN_p(u),v}.
\]

****** Card: fÃ³rmula                                                                                       :drill:
SCHEDULED: <2018-07-27 Fri>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       1b7f1bd4-af26-4ddb-809b-bf8b039cf9ef
:DRILL_LAST_INTERVAL: 27.8933
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:55]
:END:

[ $\sigma_p(X_v(q),X_u(q))$ ] = [ $\pair{N(p),X_{uv}(q)}$ ]

****** Card: simetrÃ­a                                                                                    :nodrill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:ID:       c52637a8-c5b5-4974-b9a3-3ffc1bd75806
:DRILL_LAST_INTERVAL: 8.5527
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:12]
:END:
Â¿Por quÃ© sabemos que $\sigma$, la segunda forma fundamental, es simÃ©trica?

******* Answer
Comprobamos la simetrÃ­a sobre la base dada por una parametrizaciÃ³n,

 * $\pair{N(p),X_{vu}(q)} = \sigma_p(X_u(q),X_v(q))$
 * $\pair{N(p),X_{uv}(q)} = \sigma_p(X_v(q),X_u(q))$

por Schwarz-Clairaut, tenemos la simetrÃ­a.

***** 1.6. El endomorfismo de Weingarten es autoadjunto
$A_p$ es autoadjunto en el espacio euclÃ­deo $(T_pS,I_p)$.

En particular existe una base ortonormal de vectores propios y es
diagonalizable con valores propios reales. Llamamos
*curvaturas principales* a sus valores propios

\[
A_p = \begin{pmatrix}
k_1(p) & 0 \\
0 & k_2(p)
\end{pmatrix}.
\]

****** Proof
Usamos que $\sigma_p$ y el producto escalar son simÃ©tricos.

****** Card: endomorfismo                                                                                  :drill:
SCHEDULED: <2018-08-02 Thu>
:PROPERTIES:
:ID:       15643ff5-968c-47a5-a5af-fcb5c8a4fdb6
:DRILL_LAST_INTERVAL: 33.548
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.333
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:56]
:END:
Por quÃ© sabemos que $\pair{A_p(a),b} = \pair{a,A_p(b)}$.

******* Answer
El endomorfismo de Weingarten es autoadjunto. Se deduce del hecho de que
$\sigma_p(u,v) = \pair{A_p(u),v}$ es simÃ©trica y el producto escalar lo es.
El que $\sigma$ sea simÃ©trica viene de Schwartz-Clairaut.

****** Card: curvaturas                                                                                    :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       e9ce52f0-87c8-4e32-8ccf-0c05547db768
:DRILL_LAST_INTERVAL: 9.4025
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:42]
:END:
CÃ³mo se definen las curvaturas de una superficie en un punto.

******* Answer
Son los valores propios del endomorfismo de Weingarten.
El endomorfismo de Weingarten es autoadjunto luego diagonalizable,
tomamos

\[
A_p = \begin{pmatrix}
k_1(p) & 0 \\
0 & k_2(p)
\end{pmatrix}.
\]

RecuÃ©rdese que $A_p = - dN_p$, donde $N$ es la orientaciÃ³n.

***** 1.7. Curvaturas de una superficie

 * Llamamos *curvaturas principales* a $k_1(p)$ y a $k_2(p)$, tomamos
   por convenciÃ³n $k_1(p) \leq k_2(p)$.

 * Llamamos *direcciones principales* a los vectores propios de $A_p$.

 * La *curvatura de Gauss* es el cuadrado de la media geomÃ©trica
   de las curvaturas, el determinante del endomorfismo de Weingarten,
   $K(p) = k_1(p)k_2(p) = \mathrm{det}(A_p)$.

 * La *curvatura media* es la media aritmÃ©tica de las curvaturas,
   la traza del endomorfismo de Weingarten
   $H(p) = (k_1(p) + k_2(p))/2 = \mathrm{traza}(A_p)/2$.

 * Una superficie es *llana* si $K(p)=0$ para cualquier $p \in S$.

 * Una superficie es *minimal* si $H(p)=0$ para cualquier $p \in S$.

****** Card: llana                                                                                         :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       f20774c6-2bd4-4608-8aea-c1728c400366
:DRILL_LAST_INTERVAL: 13.2789
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:37]
:END:
Superficie llana.

******* Definition
Tiene la curvatura de Gauss nula, $K(p)=0$ para cualquier $p \in S$.

****** Card: minimal                                                                                       :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       70697a18-d8cc-4043-8a24-d2b1215e60c9
:DRILL_LAST_INTERVAL: 13.9383
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:49]
:END:
Superficie minimal.

******* Definition
Tiene la curvatura media nula, $H(p)=0$ para cualquier $p \in S$.

****** Card: definiciÃ³n de curvatura de Gauss                                                              :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       654716d6-a356-465c-9062-d2bef5e0ba8f
:DRILL_LAST_INTERVAL: 14.9361
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:46]
:END:
Curvatura de Gauss.

******* Definition
Producto de las curvaturas principales. Dicho de otra forma, cuadrado
de la media geomÃ©trica de las curvaturas principales

$K(p) = k_1(p)k_2(p) = \mathrm{det}(A_p)$.

****** Card: definiciÃ³n de curvatura media                                                                 :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       931c9037-0868-458e-b028-5157a3ded78e
:DRILL_LAST_INTERVAL: 9.4062
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:40]
:END:
Curvatura media.

******* Definition
Media aritmÃ©tica de las curvaturas principales

$H(p) = (k_1(p) + k_2(p))/2 = \mathrm{traza}(A_p)/2$.

***** 1.8. Nota. Calcular las curvaturas principales desde Gauss y media
Siendo raÃ­ces del polinomio, podemos calcular las curvaturas
principales como

\[
k_1 = H - \sqrt{H^2 - K},
\qquad
k_2 = H + \sqrt{H^2 - K}.
\]

Sabemos por desigualdad de las medias (considerando los casos negativos)
que $K(p) \leq H(p)^2$.

****** Card: cÃ¡lculo                                                                                       :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       c2b14d22-21b7-4090-b6d6-b569b7e58461
:DRILL_LAST_INTERVAL: 10.8021
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:45]
:END:
Calcular curvaturas principales desde la curvatura de Gauss y
curvatura media.

******* Answer

\[
k_1 = H - \sqrt{H^2 - K},
\qquad
k_2 = H + \sqrt{H^2 - K}.
\]

****** Card: desigualdad                                                                                   :drill:
SCHEDULED: <2018-06-17 Sun>
:PROPERTIES:
:ID:       d90f6395-c496-421d-8dcd-98a8a152e390
:DRILL_LAST_INTERVAL: 9.3348
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:40]
:END:
Por quÃ© sabemos que $K(p) \leq H(p)^2$.

******* Answer 
Podemos aplicar la desigualdad de las medias a las curvaturas
principales, teniendo en cuenta los casos negativos.

***** 1.9. Punto umbilical
$p \in S$ es *umbilical* si $k_1(p) = k_2(p)$.

****** Card: punto umbilical                                                                               :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       566d963c-5455-43cc-be54-c1a747778a7b
:DRILL_LAST_INTERVAL: 10.8104
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:37]
:END:
Punto umbilical.

******* Answer
Punto donde las curvaturas principales coinciden, $p \in S$ es
*umbilical* si $k_1(p) = k_2(p)$.

***** Ej.1. Caracterizaciones de punto umbilical
:PROPERTIES:
:ID:       7dd64b5f-efdb-420c-b173-d388c47e8c44
:END:
Equivalen

 - $p$ punto umbilical,

 - $H(p)^2 = K(p)$, caso de igualdad en las medias,

 - $A_p = H(p) \mathrm{Id}$,

 - $\sigma_p = H(p) I_p$.

****** Proof
Ssi es umbilical, se da el caso de igualdad en la desigualdad de las
medias.  Como podemos diagonalizar con dos valores propios iguales
y la identidad conmuta, tenemos $A_p = PH(p) \mathrm{Id} P^{-1} = H(p) \mathrm{Id}$ y
la Ãºltima igualdad es equivalente. NÃ³tese que si se da alguna de
estas igualdades, la diagonalizaciÃ³n es trivial.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-28 Sat>
:PROPERTIES:
:ID:       f6b5acef-e53a-4ecc-ba46-d13e217e2a47
:DRILL_LAST_INTERVAL: 28.776
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:53]
:END:
Dar la caracterizaciÃ³n y definiciÃ³n de punto umbilical.

******* Answer

 * $A_p = H(p) \mathrm{Id}$, curvaturas principales iguales.
 * $H(p)^2 = K(p)$, caso de igualdad en las medias.

***** Ej.2. Elementos geomÃ©tricos de la orientaciÃ³n contraria
Se puede comprobar que si se toma la orientaciÃ³n $-N$, el endomorfismo
de Weingarten, la segunda forma fundamental, las curvaturas
principales y la curvatura media se invierten; pero la curvatura de
Gauss permanece igual.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       9d2c3dcb-d8f4-44cb-ada5-07d243bfece2
:DRILL_LAST_INTERVAL: 15.7637
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:46]
:END:
Â¿QuÃ© elemento geomÃ©trico permanece con el mismo signo si tomamos la
orientaciÃ³n opuesta en una superficie?

******* Answer
La curvatura de Gauss. La media y las curvaturas se invierten.

***** Ej.3. AplicaciÃ³n de Gauss en una superficie contenida
Si $S' \subset S$ es superficie, probar que los elementos geomÃ©tricos coinciden
en los puntos $p \in S'$.

****** Proof
Sabemos que ${\widetilde A}_p \colon T_pS' \to T_pS'$ puede ser vista como un endomorfismo
de $T_pS = T_pS'$.  Su valor es el mismo por el carÃ¡cter local de la
diferenciabilidad.

***** 1.10. Ej.4. ClasificaciÃ³n de puntos de una superficie
Un punto es 

 - *ElÃ­ptico* si $K(p) > 0$. Equivale a $\sigma_{p}$ definida positiva o
   negativa.

 - *HiperbÃ³lico* si $K(p)<0$. Equivale a $\sigma_{p}$ indefinida.

 - *ParabÃ³lico* si $K(p) = 0$ con $H(p) \neq 0$. Nos da $\sigma_{p}$ semidefinida positiva o
   negativa.

 - *Plano* si $K(p) = 0$ con $H(p)=0$. Nos da $\sigma_{p}$ nula.

NÃ³tese que la clasificaciÃ³n es invariante a cambio de orientaciÃ³n.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       e286e2f1-3fdc-4f2b-ba92-0d5da844d623
:DRILL_LAST_INTERVAL: 10.4829
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:43]
:END:
Clasifica los puntos de una superficie segÃºn sus curvaturas.

******* Answer

 - *ElÃ­ptico* si $K(p) > 0$.

 - *HiperbÃ³lico* si $K(p)<0$.

 - *ParabÃ³lico* si $K(p) = 0$ con $H(p) \neq 0$.

 - *Plano* si $K(p) = 0$ con $H(p)=0$.

***** Ej.5. Puntos de superficies llanas y minimales

 - Los puntos umbilicales son elÃ­pticos o planos.
 - Una superficie llana tiene solo puntos parabÃ³licos o planos.
 - Una superficie minimal tiene solo puntos hiperbÃ³licos o planos.

****** Card: tipo de puntos umbilicales                                                                    :drill:
SCHEDULED: <2018-07-30 Mon>
:PROPERTIES:
:ID:       ad298a68-119d-4ee0-9464-be4595ce0bbd
:DRILL_LAST_INTERVAL: 30.5786
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.667
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:56]
:END:
Â¿QuÃ© tipo de puntos puede ser un punto umbilical?

******* Answer
Puede ser sÃ³lo elÃ­ptico o plano.

****** Card: puntos de una superficie llana                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       3be0d1e8-0383-4254-9636-686092c4e169
:DRILL_LAST_INTERVAL: 11.8947
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:49]
:END:
Â¿QuÃ© tipo de puntos puede tener una superficie llana?

******* Answer
ParabÃ³licos y planos.

****** Card: puntos de una superficie minimal                                                              :drill:
SCHEDULED: <2018-08-08 Wed>
:PROPERTIES:
:ID:       26d13b01-746b-4d1a-a063-9b5fc8ae5059
:DRILL_LAST_INTERVAL: 40.2505
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.8
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:50]
:END:
Â¿QuÃ© tipo de puntos puede tener una superficie minimal?

******* Answer
HiperbÃ³licos o planos.

***** TODO Ejemplo 1.12. Planos afines
***** TODO Ejemplo 1.13. Esferas
***** TODO Ejemplo 1.14. Cilindros circulares
***** TODO Ejemplo 1.15. Punto hiperbÃ³lico en el paraboloide hiperbÃ³lico
***** TODO Ej.6. Paraboloide elÃ­ptico
***** 1.17. Movimientos rÃ­gidos y elementos geomÃ©tricos
Dada $S$ superficie con orientaciÃ³n $N\colon S \to \mathbb{S}^2$ y $\phi \colon \mathbb{R}^3\to \mathbb{R}^3$ movimiento
rÃ­gido, 

 * ${\tilde N} = \vec{\phi} \circ N \circ \phi^{-1}$ es orientaciÃ³n de $\phi(S)$, en particular,
   $A'_{\phi(p)} = \vec{\phi}\circ A_p\circ (\vec{\phi})^{-1}$ es el endomorfismo de Weingarten;

 * $\phi(e_1),\phi(e_2)$ son base de direcciones principales de $\phi(S)$.

 * ${\tilde \sigma_{\phi(p)}}(d\phi_pu,d\phi_pv) = \sigma_p(u,v)$,

 * las curvaturas se preservan.

***** 1.18. Curvatura de Gauss de una superficie cualquiera
***** TODO Ej.7. Problemas de la curvatura media de una superficie cualquiera
***** TODO Ej.8. Curvatura invariante a movimientos rÃ­gidos
**** 3.2. Expresiones locales y diferenciabilidad de las curvaturas
***** 2.0. Matriz de la primera forma fundamental respecto de parametrizaciÃ³n
Para una parametrizaciÃ³n local $X \colon U \to S$, consideramos la
base $B_q = \left\{ X_u(q),X_v(q) \right\}$. La primera forma fundamental tiene matriz

\[
M_q = \begin{pmatrix}
E & F \\
F & G
\end{pmatrix}(q).
\]

Los coeficientes de la primera forma fundamental son

\[E = \abs{X_u}^2,
\qquad
F = \pair{X_u,X_v},
\qquad
G = \abs{X_v}^2.\]

****** Card: coeficientes de la primera forma fundamental                                                :nodrill:
SCHEDULED: <2018-07-16 Mon>
:PROPERTIES:
:ID:       fbfd7142-995b-48a7-8f58-96a7c41e3f01
:DRILL_LAST_INTERVAL: 16.6908
:DRILL_REPEATS_SINCE_FAIL: 4
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 17:44]
:END:
Da los coeficientes de la primera forma fundamental respecto de la base 
$\left\{X_u(q),X_v(q) \right\}$ dada por una parametrizaciÃ³n.

\[
M_q = \begin{pmatrix}
E & F \\
F & G
\end{pmatrix}(q),\qquad
\]

******* Answer

\[E = \abs{X_u}^2,\qquad F = \pair{X_u,X_v},\qquad G = \abs{X_v}^2\]

***** 2.0. Matriz de la segunda forma fundamental repecto de parametrizaciÃ³n
Para una parametrizaciÃ³n local $X \colon U \to S$, consideramos la
base $B_q = \left\{ X_u(q),X_v(q) \right\}$. La segunda forma fundamental
respecto de la aplicaciÃ³n de Gauss $N_X$ tiene matriz

\[\Sigma_q = \begin{pmatrix}
e & f \\ f & g
\end{pmatrix}(q).\]

Los coeficientes de la segunda forma fundamental son

\[
e = \frac{\mathrm{det}(X_u,X_v,X_{uu})}{\sqrt{EG-F^2}},\qquad
f = \frac{\mathrm{det}(X_u,X_v,X_{uv})}{\sqrt{EG-F^2}},\qquad
g = \frac{\mathrm{det}(X_u,X_v,X_{vv})}{\sqrt{EG-F^2}}.
\]

NÃ³tese que son diferenciables.

****** TODO Proof: cÃ¡lculo
****** TODO Proof: diferenciabilidad
****** Card                                                                                                :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       80cc9bc1-11c8-44e7-9a7b-16bb7f5def8e
:DRILL_LAST_INTERVAL: 15.7721
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.46
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 23:35]
:END:
Coeficientes de la segunda forma fundamental respecto de la base 
$\left\{X_u(q),X_v(q) \right\}$ dada por una parametrizaciÃ³n.

******* Answer

\[
e = \frac{\mathrm{det}(X_u,X_v,X_{uu})}{\sqrt{EG-F^2}},\qquad
f = \frac{\mathrm{det}(X_u,X_v,X_{uv})}{\sqrt{EG-F^2}},\qquad
g = \frac{\mathrm{det}(X_u,X_v,X_{vv})}{\sqrt{EG-F^2}}.
\]

***** 2.0. Curvatura media y de Gauss en funciÃ³n de coeficientes
Para una parametrizaciÃ³n local $X \colon U \to S$, consideramos la
base $B_q = \left\{ X_u(q),X_v(q) \right\}$. Tenemos curvaturas respecto de la
aplicaciÃ³n de Gauss $N_X$ dadas por

\[
K = \frac{eg - f^2}{EG-F^2},
\qquad
H = \frac{1}{2}\frac{eG-2fF+gE}{EG-F^2}.
\]

Ambas $K \circ X$ y $H \circ X$ son diferenciables.

****** TODO Proof
****** Card: curvatura media y de Gauss en funciÃ³n de coeficientes                                       :nodrill:
SCHEDULED: <2018-07-12 Thu>
:PROPERTIES:
:ID:       26690650-3be6-456a-8531-26880aefaad5
:DRILL_LAST_INTERVAL: 13.7639
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 6
:DRILL_FAILURE_COUNT: 3
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:13]
:END:
Da la curvatura media y la curvatura de Gauss en funciÃ³n de los
coeficientes de la primera y segunda formas normales, para poder
calcularlas en un entorno coordenado.

******* Answer

\[
K = \frac{eg - f^2}{EG-F^2},
\qquad
H = \frac{1}{2}\frac{eG-2fF+gE}{EG-F^2}.
\]

***** 2.2. Ejemplo. Curvatura media y de Gauss en una superficie grafo
:PROPERTIES:
:ID:       b007b192-89c8-4ff5-a19d-989e9a401ecf
:END:
En general,

\[
K = \frac{\varphi_{xx}\varphi_{yy}-\varphi_{xy}^2}{\left(1 + \abs{\nabla\varphi}^2\right)^2},
= \frac{\abs{\nabla^2\varphi}}{\left(1 + \abs{\nabla\varphi}^2\right)^2},
\qquad
H = \frac{1}{2}
\frac{(1 + \varphi_y^2)\varphi_{xx} - 2\varphi_{xy}\varphi_x\varphi_y + (1 + \varphi_x^2)\varphi_{yy}}
{\left( 1 + \abs{\nabla \varphi}^2 \right)^{3/2}}.
\]

Pero ademÃ¡s, cuando tenemos un punto crÃ­tico con $\nabla\varphi(x,y)=(0,0)$,
obtenemos $K = \abs{\nabla^2\varphi}$ y $H = \Delta\varphi / 2$.

****** Card: curvatura media y de Gauss en una superficie grafo                                            :drill:
SCHEDULED: <2018-06-18 Mon>
:PROPERTIES:
:ID:       6c2ce13a-db7b-4978-826b-64ecdeb8510f
:DRILL_LAST_INTERVAL: 10.0697
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:59]
:END:
Calcular la curvatura media y la curvatura de Gauss de una superficie
grafo desde la orientaciÃ³n usual de los grafos. *En un punto cualquiera*

******* Answer

\[
K = \frac{\varphi_{xx}\varphi_{yy}-\varphi_{xy}^2}{\left(1 + \abs{\nabla\varphi}^2\right)^2},
= \frac{\abs{\nabla^2\varphi}}{\left(1 + \abs{\nabla\varphi}^2\right)^2},
\qquad
H = \frac{1}{2}
\frac{(1 + \varphi_y^2)\varphi_{xx} - 2\varphi_{xy}\varphi_x\varphi_y + (1 + \varphi_x^2)\varphi_{yy}}
{\left( 1 + \abs{\nabla \varphi}^2 \right)^{3/2}}.
\]

****** Card: curvaturas en el caso de punto crÃ­tico de un grafo                                            :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       3019e94e-7a0f-497b-ad3f-741c87fe42da
:DRILL_LAST_INTERVAL: 12.0594
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 23:33]
:END:
*En un punto crÃ­tico*. Calcular la curvatura media y la curvatura de
Gauss de una superficie grafo desde la orientaciÃ³n usual de los
grafos.

******* Answer

\[
K = \abs{\nabla^2\varphi},\qquad
H = \frac{1}{2}\Delta\varphi
\]

***** 2.3. Nota. Grafos llanos y minimales
Por los cÃ¡lculos [[id:b007b192-89c8-4ff5-a19d-989e9a401ecf][anteriores]], un grafo es llano ssi

\[
\abs{\nabla^2 \varphi} = \varphi_{xx}\varphi_{yy} - \varphi_{xy}^2 = 0.
\]

Y un grafo es minimal ssi

\[
    (1+\varphi_y^2)\varphi_{xx} - 
    2\varphi_{xy}\varphi_x\varphi_y + 
    (1+\varphi_x^2)\varphi_{yy} = 0.
\]

****** Card: condiciÃ³n de minimalidad                                                                      :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       871e7133-1d9d-410d-b533-44fff118cca2
:DRILL_LAST_INTERVAL: 10.6778
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.333
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:57]
:END:
Â¿CuÃ¡ndo es un grafo *minimal*? NÃ³tese que preguntamos el caso especÃ­fico
de grafos.

******* Answer
Si y sÃ³lo si

\[
    (1+\varphi_y^2)\varphi_{xx} - 
    2\varphi_{xy}\varphi_x\varphi_y + 
    (1+\varphi_x^2)\varphi_{yy} = 0.
\]

RecuÃ©rdese que la curvatura media de un grafo es

\[
H = \frac{1}{2}
\frac{(1 + \varphi_y^2)\varphi_{xx} - 2\varphi_{xy}\varphi_x\varphi_y + (1 + \varphi_x^2)\varphi_{yy}}
{\left( 1 + \abs{\nabla \varphi}^2 \right)^{3/2}}.
\]

****** Card: condiciÃ³n de llanitud                                                                         :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       2cda9781-d5aa-4ade-853a-75aaf00fae40
:DRILL_LAST_INTERVAL: 11.1319
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.667
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 23:32]
:END:
Â¿CuÃ¡ndo es una superficie *grafo* llana? Estudiar sÃ³lo el caso
particular de una superficie grafo.

******* Answer
Si y sÃ³lo si

\[
\abs{\nabla^2 \varphi} = \varphi_{xx}\varphi_{yy} - \varphi_{xy}^2 = 0.
\]

***** 2.4. Curvaturas diferenciables en superficies orientables
:PROPERTIES:
:ID:       a111157d-1652-4d3a-b661-f5252e9e6126
:END:
En $S$ orientable, se tiene $K,H \in {\cal C}^{\infty}(S)$, y ademÃ¡s $k_i \in {\cal C}^{\infty}(S -A)$,
donde $A = \left\{ k_1(p) = k_2(p) \right\}$.

****** TODO Proof
***** TODO 2.5.
***** TODO 2.6.
***** 2.7. Curvatura de Gauss diferenciable
Para cualquier $S$, tenemos $K \in {\cal C}^{\infty}(S)$.

**** 3.3. Curvaturas y geometrÃ­a local de superficies
***** 3.1. Lema del grafo
Para $S$ superficie con $p \in S$ existe $\phi \colon \mathbb{R}^3\to \mathbb{R}^3$ movimiento rÃ­gido tal
que $S' = \phi(S)$ cumple

 * $0 = \phi(p_0) \in S'$, envÃ­a el punto al origen,

 * $T_0S' = \phi(T_{p_0}S) = \left\{ (u,v,w) \mid w =0 \right\}$, tangente al plano horizontal,

 * Tenemos entorno $W'$ y disco $U'$ centrado en $0$ tales que hay un $\varphi \in {\cal C}^{\infty}(U')$
   cumpliendo $\varphi(0)=0$, $\nabla \varphi (0)=0$, y ademÃ¡s $W' = G^z(\varphi)$.

****** Proof
Tomamos el Ãºnico movimiento rÃ­gido $\phi$ que mueve una base ortonormal de $T_{p_0}S$
con un vector normal hacia la base usual de $\mathbb{R}^3$ en $0$. Aplicar un movimiento
rÃ­gido a una superficie nos da una superficie con $0 \in S'$. AdemÃ¡s,

\[
T_0S' = T_{\phi(p_0) \phi(S)} = d\phi_{p_0}(T_{p_0}S) = \vec{\phi}_{p_0}(T_{p_0}S) = \left\{ (u,v,w)\mid w=0 \right\}.
\]

Sea $\pi \colon S' \to \mathbb{R}^2$ proyecciÃ³n en el eje $z$, sabemos $d\pi_0 \colon T_0S' \to \mathbb{R}^2$
isomorfismo y por [[id:16bd6b42-7b91-4fcf-8979-faabc0acd594][Teorema de la funciÃ³n inversa]] es difeomorfismo local
en $0$ y tenemos entornos (podemos restringir a un disco) donde $\pi \colon W' \to U'$
es difeomorfismo.

Finalmente, $X = \pi^{-1} \colon U' \to W'$ es diferenciable y tomamos $X(x,y)=(x,y,\varphi(x,y))$
para alguna $\varphi \in C^{\infty}(U)$, lo que nos da $\varphi(0) = 0$. Como ademÃ¡s $X_x(0),X_y(0)$ forman
una base del plano $T_0S'$, tenemos $\varphi_x(0)=\varphi_y=0$.

***** 3.3. PosiciÃ³n local de una superficie respecto curvatura
Para $S$ superficie con $p_0 \in S$, si llamamos $P = p_0 + T_{p_0}S$ y tomamos $P^+,P^{-}$
como los semiplanos determinados por el normal.

 - Si $K(p_0) > 0$, hay un entorno tal que $V -\{p_0\} \subset P^+$ Ã³ $V -\{p_0\} \subset P^-$.

 - Si $K(p_0) < 0$, para todo entorno, $V \cap P^+ \neq \varnothing$ y $V \cap P^- \neq \varnothing$.

****** Proof
Usamos el lema del grafo para colocar la superficie y luego aplicamos
herramientas del anÃ¡lisis real en un entorno suficientemente
pequeÃ±o. Usamos que el determinante de la hessiana es la curvatura de
Gauss en el caso de un punto crÃ­tico.

***** 3.5. La intersecciÃ³n de superficie y plano afÃ­n es una curva
Para $S$ superficie con $p_0 \in S$, $n$ unitario ortonormal a $T_{p_0}S$, y $P$ plano
afÃ­n de $\mathbb{R}^3$ con $p_0 \in P$ y $n \in \vec{P}$; existen un entorno $W$ de $p_0$ y una curva
p.p.a. $\alpha \colon I \to S$ tales que $\alpha(0) = p_0$ y que $\alpha(I) = W \cap P$.

****** Proof
Aplicamos el lema del grafo.  En el entorno podemos despejar la curva
usando el grafo y la parametrizaciÃ³n de un plano para obtener una
curva regular.  El movimiento rÃ­gido que devuelve la superficie a la
posiciÃ³n original preserva curvas regulares. Reparametrizamos.

***** 3.6. La intersecciÃ³n de superficie y plano transverso es una curva
En general, si $p_0 \in S_1\cap S_2$ con $T_{p_0} S_1 \neq T_{p_0}S_2$, decimos
que son superficies transversas en $p_0$ y su intersecciÃ³n es localmente
una curva regular.

****** Proof                                                                                               :extra:
***** 3.7. SecciÃ³n normal
Dada $S$ con orientaciÃ³n $N \colon S \to \mathbb{S}^2$, con $p_0 \in S$ y con $v \in T_{p_0}S$ unitario;
sabemos que existe una curva p.p.a. $\alpha_v(I_v) = W_v \cap (p_0 + L(v,N(p_0))$ para
algÃºn entorno abierto $W_v$.  Se llama *secciÃ³n normal*.

Comprobamos ademÃ¡s $\alpha'(0) = \pm v$.

***** 3.8. InterpretaciÃ³n de la segunda forma fundamental
La curvatura de una secciÃ³n normal es $k_v(0) = \sigma_{p_0}(v,v)$.

****** TODO Proof
Usando que los normales coinciden, derivando y despejando.

***** 3.9. Signo de la segunda forma fundamental
Sabiendo $\sigma_{p_0}(v,v) = k_v(0)$,

 - si $\sigma_{p_0}(v,v) > 0$, entonces $\alpha_v$ se dobla hacia $N_v(0)$;

 - si $\sigma_{p_0}(v,v) < 0$, entonces $\alpha_v$ se dobla hacia $-N_v(0)$.

****** TODO Proof

***** TODO 3.10. Signo de la curvatura de Gauss
***** 3.10. FÃ³rmula de Euler
Escribimos un vector en coordenadas de la base de direcciones principales
como $v = xe_1 + ye_2$.  Si $\abs{v}=1$ entonces $x^2+y^2 = 1$ y tenemos

\[
\sigma_{p_0}(v,v) = k_1(p_0)\cos^2(\theta) + k_2(p_0)\sin^2(\theta).
\]

Como esta expresiÃ³n alcanza mÃ¡ximo y mÃ­nimo absolutos y $\cos(\theta)e_1 + \sin(\theta)e_2$
tneemos que las curvaturas de secciÃ³n normal estÃ¡n en $k_1(p_0) \leq k_v(p_0) \leq k_2(p_0)$.

***** Ejercicio 10. Secciones normales en planos, esferas y cilindros
***** Ejercicio 11. Indicatriz de Dupin
***** 3.11. Existencia de puntos elÃ­pticos
:PROPERTIES:
:ID:       3e527821-5a65-4d06-b1c1-2508ef410b71
:END:
Para $S$ compacta, existe $S' = \mathbb{S}^2(0,R)$ y un $p_0 \in S$ tales que

 - $S$ y $S'$ son tangentes en $p_0 \in S \cap S'$, teniÃ©ndose $T_{p_0}S = T_{p_0}S'$.

 - $K(p_0)\geq 1/R^2 = K'(p_0)$.

Por lo tanto $K(p_0)>0$ y $p_0$ es un punto elÃ­ptico.

****** TODO Proof

***** 3.13. No hay superficies compactas orientables minimales
No existen superficies compactas orientables y minimales.

****** Proof
Toda superficie compacta tiene un punto elÃ­ptico, pero las superficies
orientables minimales no pueden tener puntos elÃ­pticos.

**** 3.4. Curvaturas y geometrÃ­a global de superficies
***** 4.1. CaracterizaciÃ³n de superficies totalmente umbilicales
:PROPERTIES:
:ID:       90b211e5-17c4-4167-8873-5abfd20858f6
:END:
Sea $S$ superficie orientable, conexa y totalmente umbilical. Entonces,
es un abierto de un plano o de una esfera.

****** Proof
Por [[id:7dd64b5f-efdb-420c-b173-d388c47e8c44][caracterizaciÃ³n de punto umbilical]], en una superficie umbilical
tenemos $A_p(v) = H(p)v$. [[id:a111157d-1652-4d3a-b661-f5252e9e6126][Sabemos]] que $H \in {\cal C}^{\infty}$ en una superficie
orientable, y tenemos que $-(N\circ X)_u(z) = (H\circ X)(z) \cdot X_u(z)$
y que $-(N\circ X)_v(z) = (H\circ X)(z) \cdot X_v(z)$. Derivando ambos y
usando Schwarz-Clairaut, llegamos a

\[
0 = (H \circ X)_v(q) X_u(q) + (H\circ X)_u(q) X_{v}(q),
\]

que por independencia lineal de la base, nos dan
$dH_p(X_u(q)) = dH_p(X_v(q)) = 0$. La funciÃ³n $dH_p$, por ser nula
en una base, es nula. Por [[id:280958a3-c918-492b-8f0e-697d515101d4][conexiÃ³n y diferencial nula]], tenemos
$H(p) = H_0$ constante.

 * En el caso $H_0 = 0$, tendrÃ­amos $dN_p \equiv 0$ y de nuevo por conexiÃ³n,
   $N$ serÃ­a constante. Si todas las rectas afines normales son paralelas,
   la superficie debe ser el abierto de un plano (Ejercicio 10, Tema 2).

 * En el caso $H_0 \neq 0$, definimos $f(p) = p + N(p)/H_0$, que es diferenciable
   por construcciÃ³n con diferencial nula

   \[
   (df_p)(v) = v + \frac{dN_p(v)}{H_0} = v - \frac{H_0}{H_0}v = 0.
   \]

   Siendo constante, $\abs{p - p_0} = 1/H_0$, por lo que la superficie es abierto
   de una esfera.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-07 Sat>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       debf9527-ba14-4f4a-aae3-b7954ec6cb0b
:DRILL_LAST_INTERVAL: 8.9726
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.75
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-28 Thu 22:13]
:END:

Una superficie [orientable, conexa, y totalmente umbilical] es
[un abierto de un plano o de una esfera].

***** 4.3. CaracterizaciÃ³n de superficies totalmente umbilicales completas
Sea $S$ superficie *cerrada* orientable, conexa, y totalmente umbilical.
Entonces, es un plano afÃ­n o una esfera. Si es compacta, debe ser
una esfera.

****** Proof
Por el [[id:90b211e5-17c4-4167-8873-5abfd20858f6][teorema anterior]], debe ser un abierto de plano o de esfera,
pero por ser cerrada, debe ser cerrada relativa al plano o a la 
esfera, y entonces por conexiÃ³n del plano y de la esfera, debe
ser la superficie completa.

Si ademÃ¡s es compacta, sÃ³lo puede ser una esfera.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       fcb9e233-50c9-488b-99e0-4ceb25f73f40
:DRILL_CARD_TYPE: hide1cloze
:DRILL_LAST_INTERVAL: 12.0691
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 23:34]
:END:
Una superficie [cerrada, orientable, conexa, y totalmente umbilical] es
[un plano o una esfera].

***** 4.4. Orientabilidad de superficies con curvatura siempre positiva
:PROPERTIES:
:ID:       fa4736f0-e81a-4a6f-a2d2-18dde905ac47
:END:
Una $S$ con curvatura de Gauss siempre positiva es orientable.

****** TODO Proof

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       1ac1f904-1081-4bec-bee3-f7d5c234cc64
:DRILL_LAST_INTERVAL: 12.7555
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 14:57]
:END:
QuÃ© podemos decir de una superficie con curvatura de Gauss siempre
positiva.

******* Answer
Es orientable.  La curvatura y mÃ¡s concretamente el signo de las dos
curvaturas principales, nos dan un criterio para elegir una
orientaciÃ³n en cada entorno de forma que podemos crear una orientaciÃ³n
global.

***** 4.5. Teorema de Hilbert-Chern
:PROPERTIES:
:ID:       b4a8fa29-b6fe-4948-996d-39d57f574507
:END:
Si $S$ es orientable y existe $p_0 \in S$ donde ocurren /a la vez/,

 - $k_1$ tiene un mÃ­nimo local en $p_0$,
 - $k_2$ tiene un mÃ¡ximo local en $p_0$,
 - $K(p_0)>0$;

entonces $k_1(p_0)=k_2(p_0)$ y $p_0$ es umbilical.

****** Proof (Montiel-Ros)                                                                                 :extra:
***** 4.7. Teorema de Hilbert-Liebmann
Una superficie compacta y conexa con curvatura de Gauss constante es
una esfera.

****** Proof
Por compacidad, [[id:3e527821-5a65-4d06-b1c1-2508ef410b71][existen]] puntos elÃ­pticos y la curvatura es constante
positiva, [[id:fa4736f0-e81a-4a6f-a2d2-18dde905ac47][luego orientable]]. Sabemos que las curvaturas principales son
[[id:a111157d-1652-4d3a-b661-f5252e9e6126][continuas]]. Por Weierstrass, $k_1$ tendrÃ¡ un mÃ­nimo que llamamos $p_0$. Como
$k_1(p)k_2(p) > 0$, ambas curvaturas conservarÃ¡n el signo y tendrÃ¡n el
mismo. Entonces $k_2(p) = c/k_1(p) \leq c/k_1(p_0) = k_2(p_0)$ y tenemos un
mÃ¡ximo. Aplicamos [[id:b4a8fa29-b6fe-4948-996d-39d57f574507][Hilbert-Chern]] para tener $p_0$ umbilical. Pero entonces,
usando que ambos son extremos absolutos,

\[
k_1(p_0) \leq k_1(p) \leq k_2(p) \leq k_2(p_0) = k_1(p_0).
\]

Una superficie orientable, conexa, compacta y totalmente umbilical debe
ser una esfera.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-08 Sun>
:PROPERTIES:
:ID:       fa02015c-e452-47cf-8f46-f1b195f491ec
:DRILL_LAST_INTERVAL: 8.7464
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:52]
:END:
Enuncia el Teorema de Hilbert-Liebmann.

******* Answer
Una superficie compacta y conexa con curvatura de Gauss constante es
una esfera.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       15fa8856-164a-4f43-aa45-cf8999f2fc4d
:DRILL_LAST_INTERVAL: 12.163
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:54]
:END:
Una superficie [compacta y conexa] con curvatura de Gauss constante es
una esfera.

***** 4.9. Bonnet: Cerrada conexa y curvatura de Gauss constante da una esfera
Una superficie cerrada y conexa con curvatura de Gauss constante *positiva* es
una esfera.

****** Card                                                                                              :nodrill:
SCHEDULED: <2018-07-11 Wed>
:PROPERTIES:
:ID:       e99468bd-c836-403e-8a9a-0d44f1a1475d
:DRILL_LAST_INTERVAL: 11.5349
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:54]
:END:
Caso de Bonnet sobre Hilbert-Liebmann.

******* Answer
Una superficie cerrada y conexa con curvatura de Gauss constante
*positiva* es una esfera.

***** 4.10. Ovaloide
Un *ovaloide* es una superficie compacta, conexa y con curvatura de
Gauss positiva.

****** Card                                                                                                :drill:
SCHEDULED: <2018-06-19 Tue>
:PROPERTIES:
:ID:       74159b04-057a-45c8-887c-078abf6e0f75
:DRILL_LAST_INTERVAL: 11.2415
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 4
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.25
:DRILL_EASE: 2.7
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-08 Fri 23:33]
:END:
DefiniciÃ³n de ovaloide.

******* Answer
Superficie compacta, conexa y con curvatura de Gauss positiva.

***** 4.10. Teorema de Jellet-Liebmann
Un ovaloide con curvatura media constante es una esfera.

****** TODO Proof

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-05 Thu>
:PROPERTIES:
:DRILL_CARD_TYPE: hide1cloze
:ID:       548234b4-b011-42ee-977a-12c54726bfd2
:DRILL_LAST_INTERVAL: 8.2809
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 5
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 3.4
:DRILL_EASE: 2.56
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-27 Wed 13:21]
:END:
[Un ovaloide] con [curvatura *media* constante] es [una esfera].

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-10 Tue>
:PROPERTIES:
:ID:       09612ba7-c6bf-4013-b75e-e9f5d29ab715
:DRILL_LAST_INTERVAL: 10.5797
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:52]
:END:
Enunciar el teorema de Jellet-Liebmann

******* Answer
Un ovaloide con curvatura media constante es una esfera.

***** 4.11. Teorema de Alexandrov
Una superficie compacta y conexa con curvatura media constante es una
esfera.

****** TODO Proof

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-03 Tue>
:PROPERTIES:
:ID:       0d2139eb-35d3-43ff-8314-4389ae1a083f
:DRILL_LAST_INTERVAL: 4.1207
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.22
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 10:25]
:END:
Enunciar el Teorema de Alexandrov.

******* Answer
Una superficie compacta y conexa con curvatura media constante es una
esfera.

****** Card                                                                                                :drill:
SCHEDULED: <2018-07-12 Thu>
:PROPERTIES:
:ID:       faad79ab-e6e6-4ba0-9542-6bdf774dacf9
:DRILL_LAST_INTERVAL: 13.2191
:DRILL_REPEATS_SINCE_FAIL: 3
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-29 Fri 09:55]
:END:
Â¿En quÃ© se diferencian el Teorema de Jellet-Liebmann y el teorema de
Alexandrov?

******* Answer
Jellet-Liebmann pide, dentro de la definiciÃ³n de ovaloide, que la curvatura
de Gauss sea positiva en cada punto.

*Jellet-Liebmann:* Un ovaloide con curvatura media constante es una esfera.
*Alexandrov:* Una compacta y conexa con curvatura media constante es una esfera.

***** Ej.12. Curvatura media constante no nula
Una superficie conexa, cerrada de curvatura media constante no nula no
tiene por quÃ© ser una esfera. El cilindro es un contraejemplo.

*** Tema 4. GeodÃ©sicas
*** Notas sueltas
**** Catenaria y paraboloide hiperbÃ³lico
Â¿En quÃ© se diferencian la catenoide del paraboloide hiperbÃ³lico?

***** Answer
La catenoide es minimal no reglada, el paraboloide hiperbÃ³lico es
reglado no minimal.

*** Ejercicios
**** Ejercicios de clase
***** Ejercicio 20-Feb A
#+begin_statement
Probar que no existe un plano afÃ­n conteniendo a la traza de
una hÃ©lice circular de eje vertical.
#+end_statement

Si un plano contiene a dos puntos, contiene a la recta entre ellos,
luego en particular el plano contiene a todos los puntos de la
forma $(\sin(t), \cos(t), k)$.

En particular, contendrÃ­a a un cÃ­rculo $(\sin(t), \cos(t), 0)$, por
tanto a tres de sus puntos y deberÃ­a ser el plano que determinan
esos tres puntos, $L(x,y,0)$; pero tambiÃ©n a $(1,0,1)$, por ejemplo,
lo que es imposible.

***** Ejercicio 20-Feb B
#+begin_statement
Probar que las rectas afines a la hÃ©lice forman un Ã¡ngulo constante
con el eje z.
#+end_statement
**** Ejercicios apuntes 2
***** Ejercicio 1
Si fuera homeomorfa a abierto del plano, serÃ­a abierto, cerrado y
acotado del plano luego vacÃ­o.

***** Ejercicio 2
Se comprueban las propiedades de una [[id:061e98dc-b077-4e5d-a7fd-f114f6908ed5][parametrizaciÃ³n]].

***** TODO Ejercicio 8
***** TODO Ejercicio 9

**** RelaciÃ³n Tema 1 [12/24]
***** DONE Ejercicio 1
***** DONE Ejercicio 2
***** DONE Ejercicio 3
***** DONE Ejercicio 4
***** TODO Ejercicio 5
***** DONE Ejercicio 6
***** DONE Ejercicio 7
***** DONE Ejercicio 8
***** TODO Ejercicio 9
***** TODO Ejercicio 10
***** DONE Ejercicio 11
La circunferencia osculatriz estÃ¡ en la direcciÃ³n de la normal
con radio $1/k(s_0)$.

***** TODO Ejercicio 12
***** TODO Ejercicio 13
***** TODO Ejercicio 14
***** TODO Ejercicio 15
***** TODO Ejercicio 16
***** TODO Ejercicio 17
***** TODO Ejercicio 18
***** TODO Ejercicio 19
***** DONE Ejercicio 20
***** TODO Ejercicio 21
***** DONE Ejercicio 22
#+begin_statement
Sean $\alpha,\beta \colon I \to \mathbb{R}^3$ dos curvas p.p.a. tales que $k_{\beta}(s) = k_{\alpha}(s) > 0$ y $\tau_{\beta}(s) = -\tau_{\alpha}(s)$
para cada $s \in I$. Probar que existe un movimiento rÃ­gido inverso $\phi \colon \mathbb{R}^3 \to \mathbb{R}^3$ tal
que $\beta = \phi \circ \alpha$.
#+end_statement

Aplicamos simetrÃ­a respecto de un plano a una de ellas, comprobamos que
se mantiene la curvatura y que cambia la torsiÃ³n usando propiedades de
los determinantes. Aplicamos el Teorema fundamental.

***** DONE Ejercicio 23
#+begin_statement
Sea $\alpha \colon \mathbb{R} \to \mathbb{R}^2$ una curva plana p.p.a. Utilizar el teorema fundamental de
curvas en $\mathbb{R}^2$ para probar que, si la curvatura de $\alpha$ es una funciÃ³n par, entonces
la traza de $\alpha$ es simÃ©trica con respecto a la recta afÃ­n normal $R$ de $\alpha$ en $s=0$.
#+end_statement

Tomamos la reparametrizaciÃ³n $\beta(t) = \alpha(-t)$, que nos da una curva plana $\beta \colon \mathbb{R}\to\mathbb{R}^2$ 
que ademÃ¡s es p.p.a., por tenerse $|\beta'(t)| = |-\alpha'(t)| = |\alpha'(t)| = 1$. Sabiendo que la
curvatura cambia de signo con reparametrizaciones decrecientes y que es una
funciÃ³n par, $k_{\beta}(t) = -k_{\alpha}(-t) = -k_{\alpha}(t)$.

Por otro lado, eligiendo un movimiento rÃ­gido inverso cualquiera, $\psi \colon \mathbb{R}^2 \to \mathbb{R}^2$, sabemos 
que $k_{\psi \circ \alpha}(t) = -k_{\alpha}(t) = k_{\beta}(t)$ y podemos aplicar el teorema fundamental de curvas
en el plano para obtener un movimiento rÃ­gido directo $\psi'$ tal que
$\psi' \circ \psi \circ \alpha = \beta$. En otras palabras, existe un movimiento rÃ­gido inverso $\phi$ tal
que $\phi\circ \alpha = \beta$, porque la composiciÃ³n de un movimiento rÃ­gido inverso con un
movimiento rÃ­gido directo cualquiera es un movimiento rÃ­gido inverso.

Ahora, si llamamos $\alpha(0) = p_0$, tenemos que $\phi(p_0) = p_0$, y al tener un punto
fijo sÃ³lo puede ser una simetrÃ­a. AdemÃ¡s, aplicando la regla de la cadena
en varias variables y siendo $\vec{\phi}$ la parte lineal de $\phi$, tenemos
\[\begin{aligned}
T_{\alpha}(0) &= \alpha'(0) = -\beta'(0) = -(\phi \circ \alpha)'(0) = - \mathrm{Jac}_{\phi}(\alpha(0)) (\alpha'(0))
\\&= -\vec{\phi}(\alpha'(0)) = -\vec{\phi}(T_{\alpha}(0))
\end{aligned}\]

por lo que el eje de simetrÃ­a es perpendicular a $T_{\alpha}(0)$, que es no nulo por
regularidad, y es por tanto paralelo a $N_{\alpha}(0)$. El movimiento $\phi$ es una
simetrÃ­a de eje paralelo a $N_{\alpha}(0)$ pasando por $p_0$.

Finalmente, si $q \in \alpha(\mathbb{R})$ entonces existe $s \in \mathbb{R}$ tal que $\alpha(s) = q$ y
tenemos que $\phi(q) = \phi(\alpha(s)) = \alpha(-s) \in \alpha(\mathbb{R})$.

***** DONE Ejercicio 24
**** RelaciÃ³n Tema 2 ([[~/pdf/Rosales_Relaci%C3%B3n%20de%20ejercicios%202.pdf][PDF]]) [6/20]
***** TODO Ejercicio 1
***** DONE Ejercicio 2
***** DONE Ejercicio 3
***** TODO Ejercicio 4
***** DONE Ejercicio 5
***** DONE Ejercicio 6
***** DONE Ejercicio 7
***** TODO Ejercicio 8
***** TODO Ejercicio 9
***** TODO Ejercicio 10
***** DONE Ejercicio 11
***** TODO Ejercicio 12
***** TODO Ejercicio 13
***** TODO Ejercicio 14
***** TODO Ejercicio 15
***** TODO Ejercicio 16
***** TODO Ejercicio 17
***** TODO Ejercicio 18
***** TODO Ejercicio 19
***** TODO Ejercicio 20
**** RelaciÃ³n Tema 3 ([[file:~/pdf/rosales_relaci%C3%B3n_de_ejercicios_3.pdf][PDF]])
***** Ejercicio 1                                                                                           :drill:
:PROPERTIES:
:ID:       4ba4f439-8f53-4266-8342-8072e3feb0e4
:END:
Â¿QuÃ© debe cumplir la curvatura de Gauss $K$ para que la aplicaciÃ³n de
Gauss $N$ sea difeomorfismo local?

****** Answer
Ser distinta de cero en todo punto, $K(p) \neq 0$.

***** Ejercicio 7                                                                                           :drill:
:PROPERTIES:
:ID:       65c8bdaa-f028-4e0c-ac37-6f31cd1a277d
:END:
Â¿CÃ³mo afecta a las curvaturas principales la homotecia de una superficie?

******* Answer
La homotecia $\phi(p) = \lambda p$ cambia las curvaturas a ${\tilde k}_i(p) = k_i(p)/\lambda$.

***** Ejercicio 11
Â¿QuÃ© sabemos de la aplicaciÃ³n de Gauss $N \colon S \to \mathbb{S}^2$ de un ovaloide?

****** Answer
Es sobreyectiva.  Se puede ver que es difeomorfismo local luego
abierta y cerrada por ser continua de compacto a Hausdorff. La
conexiÃ³n da la sobreyectividad.

***** Ejercicio 22
#+begin_statement
Supongamos que las curvaturas principales de un ovaloide $S$
satisfacen $k_2 = f(k_1)$, donde $f \colon \mathbb{R}\to \mathbb{R}$ es una funciÃ³n decreciente.
Probar que $S$ es una esfera. Deducir que

 - un ovaloide con la propiedad de que $H/K$ es constante es
   necesariamente una esfera;

 - un ovaloide con una de sus curvaturas principales constante
   es una esfera.
#+end_statement

Un ovaloide es una superficie compacta, conexa y con curvatura de
Gauss positiva en cada punto. Sabemos que una superficie con curvatura
de Gauss positiva en cada punto es orientable, por lo que los
ovaloides son orientables. Consideramos una aplicaciÃ³n de Gauss
$N\colon S \to \mathbb{S}^2$ fijada.

Por otro lado, las curvaturas principales en una superficie orientable
son continuas. Por Teorema de Weierstrass, la funciÃ³n continua $k_1$ tendrÃ¡
un mÃ­nimo absoluto en el compacto $S$, en un punto que llamamos $p_0$. Usamos que $f$ es
decreciente para comprobar que $k_2$ tiene un mÃ¡ximo absoluto en $p_0$, ya que
para cualquier $p \in S$ tenemos que $k_1(p) \geq k_1(p_0)$ y por tanto,
\[
k_2(p) = f(k_1(p)) \leq f(k_1(p_0)) = k_2(p_0).
\]
NÃ³tese que un razonamiento similar se podrÃ­a realizar si tuviÃ©ramos
la condiciÃ³n $k_1 = f(k_2)$ para $f$ decreciente, en lugar de la condiciÃ³n del
enunciado. PodrÃ­amos tomar $p_0$ como el mÃ¡ximo de $k_2$ en el 
compacto $S$ y comprobar que $k_1$ tiene un mÃ­nimo absoluto en Ã©l ya que
para cualquier $p \in S$ tendrÃ­amos que $k_2(p) \leq k_2(p_0)$ y por tanto,
\[
k_{1}(p) = f(k_2(p)) \geq f(k_2(p_0)) = k_1(p_0).
\]

En cualquiera de estos dos casos podemos aplicar el Teorema de Hilbert-Chern,
sabiendo que $k_1$ tiene un mÃ­nimo global (y, en particular, local) en $p_0$; que $k_2$ tiene un mÃ¡ximo
global (y, en particular, local) en $p_0$; y que por definiciÃ³n de ovaloide,
$K(p_0) > 0$. Tenemos que $p_0$ es asÃ­ un punto umbilical. Veamos que esto
implica que la superficie debe ser totalmente umbilical, ya que, para
cualquier $p \in S$, se tiene
\[
k_1(p_0) \leq k_1(p) \leq k_2(p) \leq k_2(p_0) = k_1(p_0).
\]
Por Ãºltimo, aplicamos que toda superficie orientable, conexa, compacta
y totalmente umbilical es una esfera.

 - Si $H(p)/K(p) = c$, tenemos que $k_1(p) + (1 - 2ck_1(p))k_2(p) = 0$, y como
   ninguna de las curvaturas principales puede ser nula en un ovaloide
   porque tiene curvatura de Gauss positiva, sabemos en particular que
   $1 - 2ck_1(p) \neq 0$ para cualquier $p \in S$. AsÃ­, si definimos $f(x) = x / (1 - 2cx)$, que es una
   funciÃ³n definida en $\mathbb{R} - \left\{ 1/2c \right\}$, diferenciable por construcciÃ³n, y
   decreciente por tenerse $f'(x) = -1/(1-2cx)^2 < 0$, estamos en las
   condiciones del enunciado, con $k_2 = f(k_1)$.

 - En caso de que $k_2$ fuera constantemente $c$, tendrÃ­amos $k_2 = f(k_1)$ para
   la funciÃ³n $f(x) = c$, que es decreciente, y volverÃ­amos a estar en
   las condiciones del enunciado.  En caso de que $k_1$ fuera constantemente $c$,
   podemos aplicar las condiciones del caso $k_1 = f(k_2)$ que 
   hemos estudiado antes, tomando de nuevo $f(x) = c$.

**** Repaso final
 - [X] 1
 - [X] 2
 - [X] 3
 - [X] 4
 - [X] 5
 - [X] 6
 - [X] 7
 - [X] 8
 - [X] 9
 - [X] 10
 - [X] 11
 - [X] 12
 - [X] 13
 - [X] 14
 - [X] 15
 - [ ] 16 (ver pÃ¡gina)
 - [X] 17
 - [X] 18
 - [X] 19
 - [X] 20
 - [X] 21
 - [X] 22
 - [X] 23
 - [X] 24

** Modelos avanzados de computaciÃ³n
SerafÃ­n Morales
smc@decsai.ugr.es
decsai.ugr.es

*** PresentaciÃ³n
L â NL â P â NP â PSPACE

*** Tema 1: MÃ¡quinas de Turing. Funciones y lenguajes calculables
**** DefiniciÃ³n, lenguajes calculables
***** DefiniciÃ³n de mÃ¡quina de Turing                                                                       :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       39340ed9-dafe-4bdc-8d65-a3d02950185a
:DRILL_LAST_INTERVAL: 3.7999
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:41]
:END:
Define mÃ¡quina de Turing.

****** Answer
SÃ©ptupla,

 * Q conjunto finito de estados,
 * A alfabeto de entrada,
 * B alfabeto de sÃ­mbolos de la cinta, A â B,
 * Î´ funciÃ³n de transiciÃ³n, Î´ : Q Ã B â Q Ã B Ã {L,R},
 * qâ estado inicial,
 * # sÃ­mbolo blanco,
 * F conjunto de estados finales.
 
***** ConfiguraciÃ³n de una mÃ¡quina de Turing                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       b6602529-dd6d-4ba1-a691-9aa72482fb60
:DRILL_LAST_INTERVAL: 3.6643
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:59]
:END:
ConfiguraciÃ³n de una mÃ¡quina de Turing.

****** Answer
Tripleta de estado y representaciÃ³n finita de las cintas a ambos
lados, (q,wâ,wâ). NÃ³tese que wâ contiene el cabezal.

***** Lenguaje aceptado por una mÃ¡quina de Turing                                                           :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       0f58d3ce-5c70-46c8-b6ba-a0ead31d57ec
:DRILL_LAST_INTERVAL: 3.1983
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:40]
:END:
Lenguaje aceptado por una mÃ¡quina de Turing.

****** Answer
Si M es una mÃ¡quina de Turing, L(M) son las palabras tales que desde
su configuraciÃ³n inicial se puede llegar mediante pasos de cÃ¡lculo a
una configuraciÃ³n en un estado final.

***** Lenguaje recursivamente enumerable                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       9f3567c6-561c-4ed2-ab41-7d27ae8c26bd
:DRILL_LAST_INTERVAL: 4.0499
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:12]
:END:
Lenguaje recursivamente enumerable.

****** Answer
Existe una mÃ¡quina de Turing que lo tiene como lenguaje aceptado.

***** Parada en mÃ¡quinas de Turing                                                                          :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       04db91b1-d990-464a-81c0-22c9025f0c3d
:DRILL_LAST_INTERVAL: 4.2726
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:03]
:END:
Â¿CuÃ¡ndo para una mÃ¡quina de Turing?Â¿CÃ³mo afecta a la clase de lenguajes
aceptados?

****** Answer
Una mÃ¡quina para si no hay transiciones definidas. PodrÃ­amos decir que
una mÃ¡quina de Turing acepta un lenguaje si para en esas entradas; la
clase de lenguajes recursivamente enumerables permanecerÃ­a invariante.

***** Lenguaje recursivo                                                                                    :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       dd51cb5b-aea5-4a75-944b-59cdba1e5dea
:DRILL_LAST_INTERVAL: 4.3031
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:40]
:END:
Lenguaje recursivo.

****** Answer
Un lenguaje es recursivo si lo acepta una mÃ¡quina de Turing que
siempre para.

**** ProgramaciÃ³n en mÃ¡quinas de Turing
***** Variantes                                                                                             :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       81ba498a-c551-4979-9e47-69a7c0400edc
:DRILL_LAST_INTERVAL: 4.0934
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:44]
:END:
Â¿QuÃ© extensiones aplicamos a la MT bÃ¡sica?

****** Answer

 * MTs que se quedan en la misma posiciÃ³n tras un paso.
 * MTs con mÃºltiples cintas.
 * MTs no deterministas.
 * MTs con cintas semiilimitadas.

***** ExtensiÃ³n: quedarse en la misma posiciÃ³n                                                              :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       603ab8e1-c603-4890-86f3-bffb5160e175
:DRILL_LAST_INTERVAL: 4.7012
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:05]
:END:
ExtensiÃ³n de mÃ¡quina de Turing: Â¿por quÃ© podemos quedarnos en la misma
posiciÃ³n en una mÃ¡quina de Turing?

****** Answer
PodrÃ­amos tener un estado asociado a cualquier estado de la mÃ¡quina
original que simplemente se moviera a la derecha y luego pasara al
nuevo estado. Las transiciones se harÃ­an en dos pasos.

***** ExtensiÃ³n: mÃºltiples pistas                                                                           :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       266225bf-f436-45d6-85d6-60ec7719bfa1
:DRILL_LAST_INTERVAL: 4.0145
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:44]
:END:
Â¿En quÃ© consiste extender con mÃºltiples pistas una MT?

****** Answer
Consideramos un alfabeto de cinta producto para simular que escribimos
sobre varias pistas paralelas.

***** ExtensiÃ³n: mÃºltiples cintas                                                                           :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       3bf69a7c-0ddd-4f74-b4aa-03ffa5d20ed3
:DRILL_LAST_INTERVAL: 5.2668
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:57]
:END:
Â¿En quÃ© consiste la extensiÃ³n en mÃºltiples cintas?

****** Answer
Consideramos k cintas ilimitadas independientes. La configuraciÃ³n de
la mÃ¡quina debe almacenar una configuraciÃ³n en cada una de ellas. Los
movimientos deben especificarse en cada una de las cintas.

***** Lenguaje aceptado por mÃºltiples cintas                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       f43ba876-37eb-4ea2-baa5-8d2277addf4e
:DRILL_LAST_INTERVAL: 4.3873
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:55]
:END:
Lenguaje aceptado por una mÃ¡quina de mÃºltiples cintas.

****** Answer
Conjunto de palabras tales que empezando en una configuraciÃ³n en la
que estÃ¡ la palabra en la primera cinta y el resto vacÃ­as, termina en
aceptaciÃ³n.

***** FunciÃ³n calculada por mÃºltiples cintas                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       654e80ad-dd9e-4311-9401-aa581dfc4d2e
:DRILL_LAST_INTERVAL: 3.9358
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:33]
:END:
FunciÃ³n calculada por una mÃ¡quina de mÃºltiples cintas.

****** Answer
Para una funciÃ³n f : D â B*.  Comienza con la entrada u â D en la
primera cinta y debe acabar con la f(u) en la Ãºltima cinta
aceptando. Si u â D, no debe parar.

***** Equivalencia con varias cintas                                                                        :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       0d9c9913-add3-402d-a92e-e6f712f64144
:DRILL_LAST_INTERVAL: 5.1175
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:23]
:END:
Todo lenguaje aceptado por una mÃ¡quina con varias cintas es
recursivamente enumerable, Â¿por quÃ©?

****** Answer
Para una mÃ¡quina con k cintas, podemos tomar una mÃ¡quina con 2k
pistas, usando las impares para seÃ±alar con un sÃ­mbolo el cabezal
de cada una de las mÃ¡quinas de varias cintas.

Luego necesitarÃ­amos tambiÃ©n poder almacenar el estado concreto de
todo antes de ejecutar un paso. Para eso podemos /memorizar/ sÃ­mbolos.

***** Complejidad en tiempo de simular mÃºltiples cintas                                                     :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       6ac42f2a-21b1-4248-af2a-b3815ad0c6ee
:DRILL_LAST_INTERVAL: 4.2694
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:34]
:END:
Tiempo empleado por una MT en simular n pasos de una MT con k cintas.

****** Answer
O(nÂ²)

**** MÃ¡quinas no deterministas
***** Diferencia                                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       54820fae-a7fc-4d43-bef5-fe081fd93eb7
:DRILL_LAST_INTERVAL: 3.953
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:52]
:END:
Diferencia entre mÃ¡quinas de Turing deterimnistas y no deterministas.

****** Answer
La funciÃ³n de transiciÃ³n es en mÃ¡quinas no deterministas capaz de
devolver un conjunto finito de tripletas Q Ã B Ã {L,R}.

***** Equivalencia no deterministas                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c94c66f6-9a92-4509-88be-297158c2d003
:DRILL_LAST_INTERVAL: 4.6741
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:40]
:END:
Un lenguaje aceptado por una no determinista es recursivamente
enumerable, Â¿por quÃ©?

****** Answer
Podemos usar dos cintas para hacer bÃºsqueda en anchura con en el
espacio de estados. En la primera guardamos las configuraciones
(q,u,v) y en la segunda simulamos un paso de cada una de ellas.

**** CodificaciÃ³n de cadenas
***** Codificar un alfabeto en naturales                                                                    :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       5481ba4a-6ae4-4651-a9ef-956b407c640c
:DRILL_LAST_INTERVAL: 4.754
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:19]
:END:
Da la aplicaciÃ³n $Z \colon A^{\ast} \to \mathbb{N}$ que codifica
palabras de $A$ como naturales.

****** Answer

\[
Z(a_{i_k}\dots a_{i_1}) = \sum_{j=1}^k i_jn^{j-1}
\]

***** Alfabetos intercambiables                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       adbb76a6-9410-4032-981b-56040dbd9461
:DRILL_LAST_INTERVAL: 4.0993
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:14]
:END:
Â¿Por quÃ© los distintos alfabetos son intercambiables?

****** Answer
Tenemos biyecciones de cualquier alfabeto hacia y desde los naturales.

**** Propiedades de lenguajes recursivos
***** Lenguaje de diagonalizaciÃ³n                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       3ba75eca-17b0-41ac-ad97-95d24c8a298e
:DRILL_LAST_INTERVAL: 3.948
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:52]
:END:
Define el lenguaje de diagonalizaciÃ³n. Â¿QuÃ© clase de lenguaje es?

****** Answer
w â Ld si y sÃ³lo si la MT cuya codificaciÃ³n es w no acepta w.  No es
recursivamente enumerable, la existencia de una MT que lo aceptara
lleva a contradicciÃ³n.

***** Lenguaje y complementario recursivamente enumerables                                                  :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       0eb33063-64eb-48e2-a0c3-46183a934dfb
:DRILL_LAST_INTERVAL: 3.933
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:36]
:END:
Si $L$ y su complementario $\overline{L}$ son recursivamente enumerables, entonces $L$ es
recursivo. Â¿Por quÃ©?

****** Answer
Podemos construir una mÃ¡quina producto de la que acepta L y la que
acepta el complementario y ejecutarla en paralelo, cuando termine en
alguno de los dos sabremos si aceptar o no. NÃ³tese que siempre termina
porque una palabra cualquiera estÃ¡ en L o en su complementario.

***** Lenguaje universal                                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       8a7e2c5e-b1d2-458f-b17e-263d57d63d7f
:DRILL_LAST_INTERVAL: 3.6692
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:07]
:END:
Define el lenguaje universal. Â¿QuÃ© clase de lenguaje es?

****** Answer
El lenguaje universal consta de las parejas (M,w) donde M es una
mÃ¡quina que acepta w. Es recursivamente enumerable (simulaciÃ³n), pero
no recursivo (por reducciÃ³n al absurdo con el lenguaje de
diagonalizaciÃ³n).

**** Teorema de Rice
***** Propiedad trivial                                                                                     :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       917822bd-6044-48eb-9263-2a1e758dd741
:DRILL_LAST_INTERVAL: 4.3438
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:15]
:END:
Propiedad trivial.

****** Answer
Una propiedad de los lenguajes r.e. es trivial si todos los r.e. la
verifican o si no la verifica ninguno.

***** Teorema de Rice                                                                                       :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       67e74b42-e272-402d-a582-ee7ba4a048c4
:DRILL_LAST_INTERVAL: 3.8979
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:05]
:END:
Enuncia el Teorema de Rice.

****** Answer
Toda propiedad no trivial sobre los lenguajes r.e. es indecidible.
(No trivial quiere decir que existe un r.e. que no la cumple y un r.e.
que la cumple)

***** DemostraciÃ³n de Rice                                                                                  :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       79eb42ef-24f0-483b-a753-61629f1a5fbd
:DRILL_LAST_INTERVAL: 3.8738
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:10]
:END:
Demuestra el Teorema de Rice.

****** Answer
Podemos suponer s.p.g. que el vacÃ­o no cumple la propiedad y $L$, con
una $M_L$ que lo reconoce, sÃ­.  Reducimos el lenguaje universal a el
reconocer esta propiedad. Si tenemos (M,w), creamos una mÃ¡quina que
empieza simulando M sobre w; si acepta, toma su entrada y simula $M_L$
sobre ella. Comprobamos si el lenguaje de esta mÃ¡quina creada cumple
la propiedad.

Si la cumple, entonces es porque es $L$ y (M,w) ha aceptado; en
cualquier otro caso serÃ­a el vacÃ­o y no cumplirÃ­a la propiedad.

**** Problema de las correspondencias de Post
***** Problema de las correspondencias                                                                      :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       5c9b3a6b-ba4a-450f-8e67-593ec234132f
:DRILL_LAST_INTERVAL: 3.7513
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:16]
:END:
Enuncia el problema de las correspondencias de Post.

****** Answer
Dadas dos listas wâ,...,wâ y uâ,...,uâ de palabras sobre A, determinar
si existe una secuencia de Ã­ndices tal que

\[
w_{i_1}\dots w_{i_m} = u_{i_1}\dots u_{i_m}.
\]

Podemos pensar cada pareja como un bloque de construcciÃ³n, debemos
conseguir la misma palabra en las dos filas.

|--------|
| ( wáµ¢ ) |
|--------|
| ( uáµ¢ ) |
|--------|

***** Problema de las correspondencias (versiÃ³n modificada)                                                 :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       a524dfd5-8401-4e1d-9709-572ac9927d67
:DRILL_LAST_INTERVAL: 5.6026
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:13]
:END:
En quÃ© se diferencia la versiÃ³n modificada del problema de las
correspondencias de Post.

****** Answer
Se fija un bloque por el que se debe comenzar y las palabras son todas
no vacÃ­as.

***** ReducciÃ³n: Lenguaje universal â Post modificado (2 sÃ­mbolos)                                          :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       6c282ced-298b-400f-ad11-9e3192a99ffd
:DRILL_LAST_INTERVAL: 4.38
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:14]
:END:
El problema de las correspondencias de Post modificado con al menos
dos sÃ­mbolos es indecidible, Â¿cÃ³mo reducimos el lenguaje universal a
Ã©l?

****** Answer
Dada (M,w), tomamos un alfabeto con los estados, un sÃ­mbolo adicional
â y sÃ­mbolos de la cinta y empezamos por el bloque siguiente.

|---------------|
| (     â     ) |
|---------------|
| (âqâaâ...aââ) |
|---------------|

Cada transiciÃ³n a derecha da un bloque que mete un estado arriba a
cambio de pasarlo abajo.

Cada transiciÃ³n a izquierda permite meter ~cqa~ arriba para cualquier
~c~.

Permitimos meter cualquier bloque doble para ~a~ y ~â~.

|---|
| a |
|---|
| a |
|---|

Permitimos que cualquier estado final ~qáµ¤~ meta sÃ­mbolos cualesquiera
arriba.

|-----| 
| aqáµ¤ |
|-----|
| qáµ¤  |
|-----|

|-----|
| qáµ¤a |
|-----|
| qáµ¤  |
|-----|

***** ReducciÃ³n: PCP(modificado) â PCP                                                                      :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       d361417f-abdf-430e-a6e6-914f3753f37b
:DRILL_LAST_INTERVAL: 5.5806
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:15]
:END:
Â¿CÃ³mo se reduce el problema de Post modificado al problema de Post
original?

****** Answer
Incluimos dos sÃ­mbolos nuevos y los usamos para forzar empezar por un
bloque dado y acabar corrigiendo esto al final.

âuâ  | ... |âuáµ¢| ... |ââ 
âvââ | ... |váµ¢â| ... |â  

**** Problemas sobre gramÃ¡ticas
***** Saber si una gramÃ¡tica independiente del contexto es ambigua                                          :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       8e9a1f00-5025-4949-ad9b-87f75da9253f
:DRILL_LAST_INTERVAL: 4.0358
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:35]
:END:
Â¿QuÃ© clase de problema es saber si una gramÃ¡tica independiente del
contexto es ambigua?

****** Answer
Semidecidible. Puede reducirse desde el problema de las
correspondencias de Post para verse que no es decidible; pero ademÃ¡s
puede elegirse de forma no determinista una palabra y construir sus
Ã¡rboles de derivaciÃ³n para ver si es ambigua.

*** Tema 2: Otros modelos de cÃ¡lculo. Tesis de Church-Turing
**** Lenguaje Post Turing
***** Instrucciones de un lenguaje Post Turing                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       c27bbf93-29b1-4cc7-8ba7-f26c73099c12
:DRILL_LAST_INTERVAL: 3.9073
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:35]
:END:
Da las instrucciones de un lenguaje Post Turing. Aparte de ellas
existirÃ¡n las etiquetas.

****** Answer
PRINT a, imprime el sÃ­mbolo a.
IF a GOTO l, si el sÃ­mbolo en cinta es a va a la etiqueta l.
RIGHT, mueve cabezal a izquierda.
LEFT, mueve cabezal a derecha.
HALT, termina y acepta.

***** Lenguaje aceptado por un programa Post Turing                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       e17ee57b-9e26-4f42-ad10-2679ea7ec7c2
:DRILL_LAST_INTERVAL: 3.8823
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:44]
:END:
Lenguaje aceptado por un programa Post Turing.

****** Answer
Cadenas del alfabeto de entrada que comenzando en la configuraciÃ³n
inicial llegan a la instrucciÃ³n HALT.

***** FunciÃ³n calculada                                                                                     :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       116932ec-cde9-408e-88bd-bc9496cf805a
:DRILL_LAST_INTERVAL: 4.0439
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:33]
:END:
FunciÃ³n calculada por un programa Post Turing.

****** Answer
f : D â A* â B* es parcialmente calculable por un Post Turing si para
todo u â A*, el programa llega a HALT con f(u) en la cinta si u â D y
no termina si u â D.

**** Programas con variables
***** Instrucciones de programas con variables                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       bd3dbf5c-b67e-40c3-8f8c-edd14fb2576e
:DRILL_LAST_INTERVAL: 4.1953
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:55]
:END:
Da las instrucciones de un programa con variables. Aparte de ellas
existirÃ¡ una variable X de entrada, una variable Y de salida y 
un conjunto finito de variables de trabajo.

****** Answer
A â aA, aÃ±ade el sÃ­mbolo a al principio de A.
A â A-, elimina el Ãºltimo sÃ­mbolo de A si no es vacÃ­a.
IF A ENDS a GOTO L, si A termina en a, va a la etiqueta L.
HALT.

***** Macros                                                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       ebd0c850-a43e-4d3f-aee8-8728caec011b
:DRILL_LAST_INTERVAL: 3.9391
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:33]
:END:
Di al menos tres macros que definimos sobre un lenguaje con variables.
Las instrucciones bÃ¡sicas son

  A â aA
  A â A-
  IF A ENDS a GOTO L
  HALT

****** Answer

  IF V â  Îµ GOTO L
  V â Îµ
  GOTO L
  IF V ENDS aáµ¢ GOTO Láµ¢
  U â V

***** Entradas mÃºltiples                                                                                    :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       a5ef48ff-4337-4597-8ff2-6c45fb731537
:DRILL_LAST_INTERVAL: 5.3464
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:02]
:END:
CÃ³mo tratar con entradas mÃºltiples en programas Post Turing y en
programas con variables.

****** Answer
Dos opciones

 1. Se puede hacer depender de un parÃ¡metro y separar los argumentos
    por un sÃ­mbolo delimitador.
 2. En variables, se pueden asumir varias variables de entrada.

**** Tesis de Church-Turing
***** Tesis de Church-Turing                                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       3273f000-d5c6-40e5-9aee-1a14e2f5c946
:DRILL_LAST_INTERVAL: 3.6825
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:57]
:END:
Enuncia la Tesis de Church-Turing.

****** Answer
Toda funciÃ³n efectivamente calculable puede ser calculada por una
MÃ¡quina de Turing.

*** Tema 3: Clases de complejidad
**** Problemas de ejemplo
***** Flujo mÃ¡ximo                                                                                          :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       74b107e3-61c6-4f26-a1ee-ac17b759ea4a
:DRILL_LAST_INTERVAL: 4.2788
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:27]
:END:
Describir el problema de flujo mÃ¡ximo. Â¿En quÃ© clase de complejidad
estÃ¡?

****** Answer
Dado un grafo dirigido con capacidad de flujo en cada arista, un
origen y un destino, calcular el flujo mÃ¡ximo posible entre origen y
destino.

Usando *Ford-Fulkenson* con elecciÃ³n del camino mÃ­nimo sabemos que
estÃ¡ en P.

***** Problema de colorear un grafo                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       899c69c7-d671-4abf-b00d-51a44e1ebb53
:DRILL_LAST_INTERVAL: 3.87
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:35]
:END:
Describir el problema de colorear un grafo (decisiÃ³n). Â¿En quÃ© clase
de complejidad estÃ¡?

****** Answer
Dado un grafo y un entero K, encontrar una K-coloraciÃ³n.

Es NP, no se sabe que estÃ© en P.

***** Problema de las parejas                                                                               :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       917ceb77-cd0d-405a-982f-ca2ef35118de
:DRILL_LAST_INTERVAL: 3.3086
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:21]
:END:
Describir el problema de las parejas. Clase de complejidad.

****** Answer
Damos dos conjuntos del mismo tamaÃ±o y una lista de posibles parejas,
decidir si existe un subconjunto de esa lista de parejas que deje a
cada elemento emparejado Ãºnicamente.

EstÃ¡ en P. Se reduce al problema del flujo mÃ¡ximo.

***** ReducciÃ³n: parejas â flujo mÃ¡ximo                                                                     :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       fdc49819-dcbc-4d94-901b-b39736c0c4f1
:DRILL_LAST_INTERVAL: 3.4501
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:19]
:END:
Â¿CÃ³mo se reduce el problema de las parejas al flujo mÃ¡ximo?

****** Answer
Se coloca el source conectado a la primera fila, y el target a la
segunda. Si hay m parejas, pedimos flujo m.

**** Definiciones de complejidad
***** Complejidad                                                                                           :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       26de031f-c920-4379-aceb-3e40f7b56d9b
:DRILL_LAST_INTERVAL: 4.4256
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:14]
:END:
Â¿CuÃ¡ndo es una mÃ¡quina de Turing de complejidad $f(n)$?
(Puede ser en tiempo o en espacio)

****** Answer
Una mÃ¡quina de Turing es *de complejidad* $f(n)$ si para cualquier
entrada de longitud $n$, acepta o rechaza en menos de $f(n)$ unidades.

 * Unidades de complejidad en tiempo: pasos de cÃ¡lculo.
 * Unidades de complejidad en espacio: casillas en la cinta.

***** Complejidad de un lenguaje                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       da792daa-e620-470b-a4af-517035c0adb1
:DRILL_LAST_INTERVAL: 4.9669
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:21]
:END:
Â¿CuÃ¡ndo es un lenguaje de complejidad $f(n)$?

****** Answer
Un lenguaje es de complejidad $f(n)$ si existe una mÃ¡quina de Turing
de complejidad $f(n)$ que lo acepta.

 * Unidades de complejidad en tiempo: pasos de cÃ¡lculo.
 * Unidades de complejidad en espacio: casillas en la cinta.

***** Teorema: lenguaje aceptado por mÃ¡quina de varias cintas                                               :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       c219368f-6cd3-437a-a71e-33c07aa9c221
:DRILL_LAST_INTERVAL: 3.7394
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:43]
:END:
Si L es aceptado por una MÃ¡quina de k cintas, existe una MÃ¡quina
con k+1 cintas aceptando el mismo lenguaje en tiempo...

****** Answer

\[
\frac{1}{2^m}t(n) + n
\]

para cualquier $m$.

***** Complejidad en problemas de grafos                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       f81aa3e0-f142-42df-b558-e31016eea522
:DRILL_LAST_INTERVAL: 5.0987
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:46]
:END:
Por quÃ© da igual la representaciÃ³n para estudiar la complejidad
polinÃ³mica de un problema en grafos.

****** Answer
La longitud de entrada $n$ y los vÃ©rtices $v$ cumplen que $v \leq n \leq v^3$.

***** Reglas para medir complejidad en espacio                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       b3627e60-6650-41a9-b883-1eb7d4b8d31e
:DRILL_LAST_INTERVAL: 3.7348
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:57]
:END:
Da las reglas para medir el nÃºmero de unidades que se tienen en cuenta
en una mÃ¡quina de Turing.

****** Answer

 * Se cuentan las casillas sobre las que se pasa o escribe.
 * Si nunca se escribe sobre entrada, no se cuenta.
 * Si nunca se vuelve atrÃ¡s en salida, no se cuenta.

***** BÃºsqueda de caminos en grafos                                                                         :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       d1e1b612-c28a-402c-8e81-c0d664d4cb2f
:DRILL_LAST_INTERVAL: 3.9712
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 1.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:20]
:END:
Buscamos un camino entre dos nodos de un grafo. Â¿QuÃ© complejidad
en espacio tiene?

****** Answer
*Teorema de Savitch*, su complejidad es del orden ${\cal O}(\log^2(n))$,
sobre nÃºmero de nodos. Se resuelve

CAMINO(x,y,i), existencia de camino de longitud â¤2â± entre x,y.

NÃ³tese que era trivial saber que los algoritmos de bÃºsqueda en
grafos nos dan tiempo polinÃ³mico y espacio lineal, este teorema
lo mejora.

***** TODO BÃºsqueda de caminos en grafos: espacio no determinista
***** Complejidad no determinista                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       02b8cbdb-6370-4944-8b4e-373ea5c98984
:DRILL_LAST_INTERVAL: 4.2671
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:48]
:END:
Â¿CuÃ¡ndo es una mÃ¡quina de Turing *no determinista* de complejidad
$f(n)$?

****** Answer
Una mÃ¡quina de Turing es *de complejidad* $f(n)$ si para cualquier
entrada de longitud $n$, todas las posibles opciones de cÃ¡lculo 
aceptan o rechazan en menos de $f(n)$ pasos.

**** Clases de complejidad bÃ¡sicas
***** Clases de complejidad bÃ¡sicas                                                                         :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       071896e8-410d-47ee-8e98-a82d929ee204
:DRILL_LAST_INTERVAL: 4.219
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:21]
:END:
Define

 * $\mathrm{TIEMPO}(f)$
 * $\mathrm{ESPACIO}(f)$
 * $\mathrm{NTIEMPO}(f)$
 * $\mathrm{NESPACIO}(f)$
 
****** Answer

 * $\mathrm{TIEMPO}(f)$, aceptados por una determinista en tiempo ${\cal O}(f(n))$.
 * $\mathrm{ESPACIO}(f)$, aceptados por una determinista en espacio ${\cal O}(f(n))$.
 * $\mathrm{NTIEMPO}(f)$, aceptados por una no determinista en tiempo ${\cal O}(f(n))$.
 * $\mathrm{NESPACIO}(f)$, aceptados por una no determinista en espacio ${\cal O}(f(n))$.

***** Clase L                                                                                               :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       2a26cd67-4005-46bf-aa0c-b3dce484dde8
:DRILL_LAST_INTERVAL: 4.3954
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:11]
:END:
Define $\mathrm{L}$.

****** Answer
Clase de espacio logarÃ­tmico determinista.

\[
\mathrm{L} = \mathrm{ESPACIO}(\log(n))
\]

***** Clase NL                                                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       588e49c9-347d-4e55-9e31-c1c6a628c1c4
:DRILL_LAST_INTERVAL: 4.0935
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:07]
:END:
Define NL.

****** Answer
Clase de espacio logarÃ­tmico no determinista.

NL = NESPACIO(log(n))

***** Clase P                                                                                               :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       c3fd1fcb-f4f1-49ba-b38d-cd0a4786a88d
:DRILL_LAST_INTERVAL: 4.3611
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:39]
:END:
Define $\mathrm{P}$.

****** Answer
Clase de tiempo polinÃ³mico.

\[
\mathrm{P} = \bigcup_{j > 0} \mathrm{TIEMPO}(n^j)
\]

***** Clase NP                                                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       beae6842-f842-43a1-a01a-0cb9ede8bf41
:DRILL_LAST_INTERVAL: 3.9523
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:28]
:END:
Define $\mathrm{NP}$.

****** Answer
Clase de tiempo polinÃ³mico no determinista.

\[
\mathrm{NP} = \bigcup_{j > 0} \mathrm{NTIEMPO}(n^j)
\]

***** Clase PESPACIO                                                                                        :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       a22787ec-6163-41e3-97ec-3a03e316d1cf
:DRILL_LAST_INTERVAL: 4.0559
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:10]
:END:
Define $\mathrm{PSPACE}$.

****** Answer
Clase de espacio polinÃ³mico determinista.

\[
\mathrm{PSPACE} = \bigcup_{j > 0} \mathrm{ESPACIO}(n^j)
\]

***** Clase NPESPACIO                                                                                       :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       74ebe4ff-2e1a-46c1-be5f-ec7ddaa9eff9
:DRILL_LAST_INTERVAL: 3.5239
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:12]
:END:
Define $\mathrm{NPESPACIO}$.

****** Answer
Clase de espacio polinÃ³mico determinista.

\[
\mathrm{NPESPACIO} = \bigcup_{j > 0} \mathrm{NESPACIO}(n^j)
\]

***** Clase EXP                                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       317523a6-2dac-483b-b16f-8a9f00396924
:DRILL_LAST_INTERVAL: 3.9152
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:14]
:END:
Define $\mathrm{EXP}$.

****** Answer

\[
\mathrm{EXP} = \bigcup_{j > 0} \mathrm{TIEMPO}(2^{n^j})
\]

***** Funciones de complejidad propias                                                                      :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       f4682aba-1cc4-4b66-ad9d-e3d8765765f8
:DRILL_LAST_INTERVAL: 5.1068
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:16]
:END:
DefiniciÃ³n de funciÃ³n propia en complejidad.

****** Answer
Son crecientes $f(n+1) \geq f(n)$ y existe una mÃ¡quina de Turing
calculadora que para una entrada de longitud $n$ imprime $f(n)$
ceros en tiempo ${\cal O}(n+f(n))$.

**** Simulaciones y tesis fuerte de Church-Turing
***** Tesis de Church-Turing fuerte                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       18cc3418-8c2c-4d0e-b9b4-cb1f32b3b637
:DRILL_LAST_INTERVAL: 3.7238
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:42]
:END:
Enuncia la Tesis de Church-Turing fuerte.

****** Answer
Todo procedimiento fÃ­sicamente realizable se puede simular por una
mÃ¡quina de Turing con sobrecarga polinÃ³mica en el nÃºmero de pasos.
Si una da $f(n)$, la otra puede dar hasta $f^k(n)$ pasos.

/No se verificarÃ­a para ordenadores cuÃ¡nticos./

***** TODO MÃ¡quina RAM
***** SimulaciÃ³n de mÃ¡quinas no-deterministas                                                               :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       b06381c8-22ed-4735-90c0-309381d13597
:DRILL_LAST_INTERVAL: 3.8525
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:34]
:END:
Supongamos L decidido por una mÃ¡quina de Turing no determinista en
tiempo f(n) â¥ n, entonces es decidido por una mÃ¡quina de Turing
determinista con tres cintas en tiempo...

****** Answer

${\cal O}(d^{f(n)})$ para alguna constante $d$.

**** Complementarios de clases
***** Complementario de clase                                                                               :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c4d55d6a-b795-4435-a0b6-00f4d34d2cf9
:DRILL_LAST_INTERVAL: 3.7009
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:07]
:END:
Define la clase $Co\mathrm{C}$ dada una clase $\mathrm{C}$.

****** Answer
Es la clase de los lenguajes complementarios de la clase $\mathrm{C}$.
Si $L \in \mathrm{C}$, entonces $\overline{L} \in Co\mathrm{C}$.

***** Complementario de clase determinista                                                                  :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       e3895850-63a9-4fbd-b214-0dfdc56fd0fb
:DRILL_LAST_INTERVAL: 3.9886
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:41]
:END:
Por quÃ© el complementario de una clase determinista coincide con esa
misma clase.

****** Answer
Desde la definiciÃ³n de complejidad de un lenguaje. Tenemos que debe
aceptar o rechazar cada instancia.
***** RelaciÃ³n calculable polinÃ³micamente                                                                   :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       4f749115-400a-4726-b16f-c315b77f8fd2
:DRILL_LAST_INTERVAL: 3.3987
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:15]
:END:
Â¿CuÃ¡ndo es una relaciÃ³n $R$ calculable polinÃ³micamente?

****** Answer
Acepta el lenguaje de pares de la relaciÃ³n en tiempo polinÃ³mico.

***** CaracterizaciÃ³n por verificador/certificado de NP                                                     :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       3cb61b03-74e8-4dc5-babe-3e101c576c6b
:DRILL_LAST_INTERVAL: 4.7226
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:59]
:END:
Dar la caracterizaciÃ³n verificador/certificado de NP.

****** Answer
Un lenguaje L estÃ¡ en NP ssi existe R relaciÃ³n calculable en tiempo
polinÃ³mico y p polinomio tal que

L = { x â A* â£ â y â A* : |y| â¤ p(|x|), R(x,y) }

El algoritmo calculando R es un /verificador/, el y se llama /certificado/.

***** CaracterizaciÃ³n por verificador/certificado de CoNP                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       dae9b434-98dd-4b76-94e5-f471a6b9b8b6
:DRILL_LAST_INTERVAL: 4.3939
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:57]
:END:
Dar la caracterizaciÃ³n verificador/certificado de CoNP.

****** Answer
Un lenguaje L estÃ¡ en CoNP ssi existe R relaciÃ³n calculable en tiempo
polinÃ³mico y p polinomio tal que

L = {x â A* â£ â y â A* : |y| â¤ p(|x|), R(x,y) }
***** Ejemplo: saber si un nÃºmero es compuesto                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       30ece1a6-5a03-4e5f-8e97-713bbf5f199a
:DRILL_LAST_INTERVAL: 4.1677
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:26]
:END:
Saber si un nÃºmero es compuesto es trivialmente Â¿NP o CoNP?

****** Answer
Es trivialmente NP, consiste en encontrar un nÃºmero que lo divida
propiamente.

**** Relaciones entre clases de complejidad
***** Relaciones bÃ¡sicas                                                                                    :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       a34d95a0-6d7c-4de8-a320-fbdc47dc14cd
:DRILL_LAST_INTERVAL: 4.8043
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:13]
:END:
Relaciones bÃ¡sicas son

ESPACIO(f(n)) â NESPACIO(f(n))
TIEMPO(f(n))  â NTIEMPO(f(n))
NTIEMPO(f(n)) â ESPACIO(f(n))

Â¿por quÃ© la Ãºltima? En particular, Â¿por quÃ© NP â PSPACE?

****** Answer
Podemos simular una mÃ¡quina no determinista en el espacio que gasta en
la mayor de las ejecuciones, que necesariamente debe ser menos que el
tiempo. Para eso, llevamos en una cinta aparte un contador con las
opciones no deterministas que toma.

***** MÃ©todo de la alcanzabilidad: resultado                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       157d1aee-90a4-4422-aefa-43d21afdce06
:DRILL_LAST_INTERVAL: 4.1324
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:20]
:END:
Â¿QuÃ© resultado nos da el mÃ©todo de la alcanzabilidad?

****** Answer
Obtenemos que

\[
\mathrm{NESPACIO}(f(n)) \subseteq \mathrm{TIEMPO}(k^{\log(n)+f(n)}).
\]

***** MÃ©todo de la alcanzabilidad: mÃ©todo                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c5ccda2e-c2f9-4d4e-8d69-16daf7cd213a
:DRILL_LAST_INTERVAL: 4.3614
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:33]
:END:
Â¿CuÃ¡l es el mÃ©todo de la alcanzabilidad? Sirve para obtener lo
siguiente,

\[
\mathrm{NESPACIO}(f(n)) \subseteq \mathrm{TIEMPO}(k^{\log(n)+f(n)}).
\]

****** Answer
Representar una mÃ¡quina de Turing como un grafo y conectar con los
arcos configuraciones tales que se pueda pasar de una a otra en un
paso de cÃ¡lculo. Como el nÃºmero mÃ¡ximo de configuraciones, sabiendo
que estamos acotados por espacio $f(n)$ es

\[ |Q||A|^{(2k-2)f(n)}(n+1)
\],

tenemos que serÃ¡ del orden de $k^{\log(n)+f(n)}$.

***** Teorema de la jerarquÃ­a                                                                               :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       21eae27e-aaf5-4117-b515-5e6cde76b521
:DRILL_LAST_INTERVAL: 4.5654
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 1.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:06]
:END:
Teorema de la jerarquÃ­a (en tiempo).

****** Answer
Si $f(n) \geq n$ es una funciÃ³n propia,

 - $\mathrm{TIEMPO}(f(n)) \subset \mathrm{TIEMPO}(f^2(n))$,
 - $\mathrm{TIEMPO}(f(n)) \neq \mathrm{TIEMPO}(f^2(n))$.

***** P â  EXP                                                                                               :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       bff038a1-de63-4107-8f45-97f7d77677c8
:DRILL_LAST_INTERVAL: 5.127
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:07]
:END:
Â¿Por quÃ© sabemos P â  EXP?

****** Answer
Usamos el [[id:21eae27e-aaf5-4117-b515-5e6cde76b521][teorema de la jerarquÃ­a]].

\[
P \subseteq \mathrm{TIEMPO}(2^n) \subsetneq \mathrm{TIEMPO}((2^n)^2) \subseteq \mathrm{EXP}.
\]

***** Teorema de la jerarquÃ­a en espacio                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       ab8fdd1d-65be-4a7d-8c0d-080d950adeac
:DRILL_LAST_INTERVAL: 3.7528
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:18]
:END:
Teorema de la jerarquÃ­a en espacio.

****** Answer
Si $f(n)$ es una funciÃ³n propia,

ESPACIO(f(n)) â ESPACIO(f(n)log(f(n))).
ESPACIO(f(n)) â  ESPACIO(f(n)log(f(n))).

***** InclusiÃ³n de clases                                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c60beb7d-5797-4b47-bf9f-069410c07677
:DRILL_LAST_INTERVAL: 4.1444
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:53]
:END:
Cadena de inclusiÃ³n de clases de complejidad bÃ¡sicas. Â¿CuÃ¡l es estricta?

****** Answer

L â NL â P â NP â PESPACIO

Sabemos que NL â PESPACIO.

***** Teorema del espacio no determinista                                                                   :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       33fb0049-5e89-4778-876e-6cbc8ddbeb66
:DRILL_LAST_INTERVAL: 5.2255
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:20]
:END:
Teorema del espacio no determinista.

****** Answer
Si $f(n) \geq \log(n)$ es propia,

NESPACIO(f(n)) â ESPACIO(fÂ²(n)).

***** PESPACIO = NPESPACIO                                                                                  :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       a95eda0f-37a2-47ef-a7bd-ea1ef934b16e
:DRILL_LAST_INTERVAL: 4.0487
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:29]
:END:
Â¿Por quÃ© sabemos PESPACIO = NPESPACIO?

****** Answer
Por el teorema del espacio no determinista

NESPACIO(f(n)) â ESPACIO(fÂ²(n)),

y entonces en el caso polinÃ³mico tenemos la igualdad buscada.

***** Teorema del co-espacio no determinista                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       5812c170-d707-40fd-8ec5-004e2fabb042
:DRILL_LAST_INTERVAL: 4.0615
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:08]
:END:
Teorema del co-espacio no determinista.

****** Answer
Si $f(n) \geq \log(n)$ es propia,

NESPACIO(f(n)) = coNESPACIO(f(n)).

***** coNL = NL                                                                                             :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       3b05c32d-f5b9-47ef-9340-5cc9770e4ded
:DRILL_LAST_INTERVAL: 4.9641
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:15]
:END:
Sabemos coNL=NL, Â¿por quÃ©?

****** Answer
Porque si $f(n) \geq \log(n)$ es propia,

NESPACIO(f(n)) â coNESPACIO(f(n)).

***** TODO Sistemas interactivos de demostraciÃ³n                                                            :drill:
:PROPERTIES:
:ID:       fe7623a5-dc87-4be6-bab2-71b097e99582
:END:

**** Ejemplo: Primality is in NP â© coNP
[[http://www.cmi.ac.in/~ramprasad/lecturenotes/comp_numb_theory/lecture17.pdf]]
**** Sistemas interactivos de demostraciÃ³n

*** Tema 4: NP-Completitud
**** ReducciÃ³n
***** Problema de decisiÃ³n                                                                                  :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       d898e65c-20db-46b0-99b4-fa1ba0d10873
:DRILL_LAST_INTERVAL: 3.5844
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:36]
:END:
Problema de decisiÃ³n Î .

****** Answer
Un *problema de decisiÃ³n* $\Pi$ consta de $D_{\Pi}$ conjunto de ejemplos
del problema, y de $Y_{\Pi} \subseteq D_{\Pi}$, conjunto de ejemplos positivos contenido
en el conjunto de instancias del problema.

***** ReducciÃ³n en tÃ©rminos de lenguajes                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       88393d02-8a50-4376-a089-4e1ed9147c6c
:DRILL_LAST_INTERVAL: 3.926
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:01]
:END:
ReducciÃ³n Lâ â Lâ.

****** Answer
$L_1 \propto L_2$ si existe una mÃ¡quina de Turing en espacio logarÃ­tmico
calculando una funciÃ³n $f \colon A^{\ast} \to B^{\ast}$ tal que

\[
x \in L_1 \iff f(x) \in L_2.
\]

***** ReducciÃ³n en tÃ©rminos de problemas                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       0235eb36-5292-4a12-9923-99b827ffdb22
:DRILL_LAST_INTERVAL: 3.9947
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:18]
:END:
ReducciÃ³n de problemas Î â â Î â.

****** Answer
$\Pi_1 \propto \Pi_2$ si existe una mÃ¡quina de Turing determinista en espacio
logarÃ­tmico calculando $f \colon D_{\Pi_1} \to D_{\Pi_2}$ tal que

\[
x \in Y_{\Pi_1} \iff f(x) \in Y_{\Pi_2}.
\]
***** CH - Problema del circuito hamiltoniano                                                               :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       5f1f5700-0d1d-4f60-a19a-715a55d6e65e
:DRILL_LAST_INTERVAL: 4.2004
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:34]
:END:
Enunciar el problema del circuito hamiltoniano (CH).

****** Answer
Un ciclo hamiltoniano es una sucesiÃ³n de aristas visitando todos los
vÃ©rtices del grafo una sola vez. Determinar si existe un ciclo
hamiltoniano.

Es NP-completo.

***** TSP - Problema del viajante de comercio                                                               :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       0aeb6294-e9a9-49dc-98e0-1e36c415c6bc
:DRILL_LAST_INTERVAL: 4.4751
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:23]
:END:
Enuncia el problema del viajante de comercio.

****** Answer
Dado un conjunto C de ciudades, una distancia d : C Ã C â â, y una
cota B â â. Determinar una permutaciÃ³n de las ciudades que al
recorrerla dÃ© distancia menor a B.
***** ReducciÃ³n: CH â TSP                                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       6cc47af4-0d30-48da-a226-81c76325d1b3
:DRILL_LAST_INTERVAL: 4.3484
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:28]
:END:
Reducir: circuito hamiltoniano a viajante de comercio. Hamilton â TSP.

****** Answer
Para m vÃ©rtices damos una instancia del TSP con distancia 1 en
vÃ©rtices conectados y 2 en no conectados. Tomamos la cota acorde.

 * Es en espacio logarÃ­tmico.
 * Hay soluciÃ³n en uno si y sÃ³lo si hay soluciÃ³n en el otro.

***** ComposiciÃ³n de reducciones                                                                            :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       e40d577a-4fd4-4e2c-ae6b-120c4c6b0e8a
:DRILL_LAST_INTERVAL: 4.0099
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:09]
:END:
Si Lâ â Lâ, y ademÃ¡s Lâ â Lâ, Â¿quÃ© sabemos?

****** Proof
Las reducciones se componen, Lâ â Lâ. La demostraciÃ³n no es
trivial.

***** ComposiciÃ³n de reducciones: tÃ©cnica                                                                   :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       759e98fd-2b1a-4b87-8366-a94ed5ce0a2f
:DRILL_LAST_INTERVAL: 2.9668
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:18]
:END:
Hablamos de complejidad. Si $L_1 \propto L_2$ y $L_2 \propto L_3$,
entonces $L_1 \propto L_3$. Â¿Por quÃ©?

****** Proof
La composiciÃ³n no es trivial. Si la primera mÃ¡quina calcula
una salida y la usa la segunda, el espacio intermedio no serÃ¡
logarÃ­tmico.

Lo que hacemos es que cada vez que $M_2$ necesita una casilla
de la entrada, se simula $M_1$ hasta esa casilla y se envÃ­a a
$M_2$.

**** Complitud. SAT
***** Lenguaje completo para una clase                                                                      :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       c3722f05-acf3-4275-892e-9639c88f81d4
:DRILL_LAST_INTERVAL: 4.2547
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:20]
:END:
Lenguaje completo para una clase.

****** Answer
L es *completo* para C si L â C, y ademÃ¡s, â L' â C:  L' â L.

***** Equivalencia                                                                                          :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       08d5b61c-e6df-4346-8447-01ee124784c5
:DRILL_LAST_INTERVAL: 3.1134
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:58]
:END:
Lenguajes equivalentes.

****** Answer
Si $L_1 \propto L_2$, y $L_2 \propto L_1$, se dicen *equivalentes*.
***** SAT                                                                                                   :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       f572c6cc-aa6d-49ed-847c-dda840b80501
:DRILL_LAST_INTERVAL: 3.9144
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:43]
:END:
Enuncia el problema SAT.

****** Answer
Se da un conjunto $U = \left\{ p_1,\dots,p_m \right\}$ de sÃ­mbolos proposicionales y una
colecciÃ³n $C$ de clÃ¡usulas (disyunciÃ³n y negaciÃ³n) sobre estos sÃ­mbolos.
Decidir si son consistentes.

***** Teorema de Cook-Levine                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       589381c5-6ad4-4edc-ae38-86b26cb581a6
:DRILL_LAST_INTERVAL: 3.9148
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:14]
:END:
Enuncia el Teorema de Cook-Levine.

****** Proof
SAT es NP-completo, donde SAT es el problema de la consistencia de
clÃ¡usulas en lÃ³gica proposicional.

***** Teorema de Cook-Levine                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       e1140d15-2396-4d29-bf38-2ede346a8d4a
:DRILL_LAST_INTERVAL: 3.4195
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:31]
:END:
Idea de la demostraciÃ³n del Teorema de Cook-Levine, que dice que SAT
es NP-completo.

****** Answer
Podemos expresar todo el funcionamiento de una mÃ¡quina de Turing no
determinista tomando clÃ¡usulas que representen:

 * Q[i,k], estados en cada momento;
 * H[i,j], posiciÃ³n del cabezal en cada momento;
 * S[i,j,v], sÃ­mbolos en la cinta en cada posiciÃ³n en cada momento;
 * O[i,n], opciones de no determinismo tomadas en cada momento.

Necesitamos calcular el polinomio que acota el nÃºmero de pasos que
darÃ¡ $p(n)$ para tomar el nÃºmero de instantes necesario. El espacio
es logarÃ­tmico porque dependemos como mucho de Ã­ndices en $\log(p(n))$.

**** Variantes de SAT
***** 3-SAT                                                                                                 :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       9acf9fd6-6725-4523-af07-a72a1079c7ad
:DRILL_LAST_INTERVAL: 3.6077
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:23]
:END:
Enuncia el problema 3-SAT.

****** Answer
Se da un conjunto U = {pâ,...,pâ} de sÃ­mbolos proposicionales y una
colecciÃ³n C de clÃ¡usulas de a lo sumo 3 literales. Decidir si son
consistentes.

Puede restringirse a exactamente 3 literales.

Es NP-completo en ambos casos.

***** ReducciÃ³n: SAT â 3-SAT                                                                                :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       dd3951a1-e8f9-415a-98ca-d4b9e7b673b6
:DRILL_LAST_INTERVAL: 4.5225
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:27]
:END:
Idea de la reducciÃ³n SAT â 3-SAT.

****** Answer
P â¨ Q se satisface ssi se satisface (P â¨ x), (Q â¨ Â¬ x). Esto nos permite
ir partiendo 

lâ â¨ lâ â¨ ... â¨ lâ

en

(lâ â¨ lâ â¨ xâ), (Â¬ xâ â¨ lâ â¨ xâ), (Â¬ xâ â¨ lâ â¨ xâ), ...

Esa reducciÃ³n se hace en espacio logarÃ­tmico, Â¡no se debe hacer
recursivamente sino iterativamente!

***** ReducciÃ³n: 3-SAT â 3-SATexacto                                                                        :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       848632d1-19de-4ab4-9fc8-3be24fa810ac
:DRILL_LAST_INTERVAL: 5.0463
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:27]
:END:
Idea de la reducciÃ³n 3-SAT â 3-SATexacto.

****** Answer
AÃ±adimos variables nuevas a las clÃ¡usulas que falten y luego forzamos
a las variables a ser falsas, aÃ±adiendo todas las combinaciones
posibles de esas tres excepto la que las hace verdaderas a las tres a
la vez (xâ â¨ xâ â¨ xâ).

***** 2-SAT                                                                                                 :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       0bd56a7d-078f-4269-93cf-9885c9fb84c6
:DRILL_LAST_INTERVAL: 4.054
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:38]
:END:
Enuncia el problema 2-SAT. Â¿QuÃ© clase de complejidad tiene?

****** Answer
Se da un conjunto de sÃ­mbolos y una colecciÃ³n de clÃ¡usulas. Decidir si
son consistentes.

/2-SAT estÃ¡ en NL./

***** 2-SAT es NL                                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       9de963d3-f352-4651-9a9e-c5ed240f8ee7
:DRILL_LAST_INTERVAL: 4.3627
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:24]
:END:
Â¿Por quÃ© 2-SAT estÃ¡ en NL?

****** Answer
2-SAT permite escribir un grafo de implicaciones que es no consistente
ssi podemos encontrar un camino entre (x) y (Â¬x).
NÃ³tese que NL = coNL.

***** ClÃ¡usulas de Horn                                                                                     :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       7136c0b8-a13d-45a8-96dd-bb9d5569db47
:DRILL_LAST_INTERVAL: 4.1733
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:14]
:END:
Â¿QuÃ© es una clÃ¡usula de Horn?

****** Answer
Aquella en la que como mÃ¡ximo existe un literal positivo.

***** Horn-SAT                                                                                              :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       7ce4b2a1-56c6-4678-b218-c2c5935229b8
:DRILL_LAST_INTERVAL: 4.6474
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:13]
:END:
El resolver SAT para clÃ¡usulas de Horn es P-completo, Â¿cuÃ¡l es un
algoritmo polinÃ³mico?

****** Answer
Partimos las clÃ¡usulas en $C_1$ (todos los literales negativos) y
$C_2$ (algÃºn literal positivo). Tomamos un conjunto $H$ de variables
que sabemos que son necesariamente verdad e incluimos los literales
positivos aislados. Mientras $H$ cambie le incluimos todas las
implicaciones $(z_{1} \wedge  \dots \wedge z_n) \to y$ que se deduzcan de las clÃ¡usulas
de $C_2$. Podemos terminar haciendo $H$ verdad y todo lo demÃ¡s mentira.

La consistencia equivale a que despuÃ©s de este proceso, todas las
clÃ¡usulas en $C_1$ tengan algÃºn literal fuera de $H$.

***** MAX2SAT                                                                                               :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       3cd3c25e-41c0-4b1c-a708-29a325e856c9
:DRILL_LAST_INTERVAL: 4.4805
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:30]
:END:
Enuncia MAX2SAT. Â¿Clase de complejidad?

****** Answer
Dado un conjunto de clÃ¡usulas de dos literales y un K â¥ 0,
Â¿pueden satisfacerse al menos K clÃ¡usulas?

Es NP-completo.

***** ReducciÃ³n: 3-SAT â MAX2SAT                                                                            :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       8ae2f16d-95c1-490c-a476-9f1fab03bdaf
:DRILL_LAST_INTERVAL: 4.7355
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:26]
:END:
Idea de la reducciÃ³n 3-SAT â MAX2SAT.

****** Answer
Cada (x â¨ y â¨ z) puede transformarse en

x,y,z,w, Â¬xâ¨Â¬y, Â¬yâ¨Â¬z, Â¬zâ¨Â¬x, xâ¨Â¬w, yâ¨Â¬w, zâ¨Â¬w

y se toma k=7m, se puede comprobar que si xâ¨yâ¨z es verdadero, se
pueden llegar a las 7 clÃ¡usulas (no mÃ¡s), y que en otro caso no se
puede.

***** 3-SATrestricciÃ³n                                                                                      :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       661674d6-af50-4ae0-8a17-26a8fe4bfe1d
:DRILL_LAST_INTERVAL: 3.3528
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:27]
:END:
Enunciar 3-SATrestricciÃ³n.

****** Answer
3-SAT haciendo que cada literal nunca aparezca mÃ¡s de dos
veces. Pueden existir clÃ¡usulas de longitud menor que 3.

***** ReducciÃ³n: 3-SAT â 3-SATrestricciÃ³n                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       f73cf18b-94cf-4e08-9bbc-0329882bea9f
:DRILL_LAST_INTERVAL: 3.9824
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:23]
:END:
CÃ³mo se reduce 3-SAT a 3-SATrestricciÃ³n. Es decir, a 3-SAT haciendo
que cada literal nunca aparezca mÃ¡s de dos veces y cada variable mÃ¡s
de tres veces.

****** Answer
Si x aparece varias veces se sustituye por xââxââ...âxâ. Esto se
consigue sabiendo que xââxâ es una clÃ¡usula de dos literales.

***** NAESAT                                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       fca08a72-6a23-4e6f-94cc-910f12b1dabf
:DRILL_LAST_INTERVAL: 2.9569
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:14]
:END:
Enunciar NAESAT. Â¿Clase de complejidad?

****** Answer
Dadas clÃ¡usulas de longitud 3, dar una asignaciÃ³n de valores tal que
para cada clÃ¡usula alguno, pero no todos los literales sean ciertos.

NAESAT es NP-completo.

***** ReducciÃ³n: 3-SAT â NAESAT                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       30584957-b7a1-4107-9452-664387a264e0
:DRILL_LAST_INTERVAL: 4.0305
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:16]
:END:
Idea de la reducciÃ³n 3-SAT a NAESAT.

****** Answer
Tomamos una nueva variable z y para cada pâ¨qâ¨r aÃ±adimos una nueva
variable s y las clÃ¡usulas

sâ¨râ¨z, pâ¨qâ¨Â¬s, Â¬pâ¨sâ¨z, Â¬qâ¨sâ¨z.

Si hay soluciÃ³n a este NAESAT, tomamos la soluciÃ³n que deja z falso y
es una soluciÃ³n del original.  Si hay una soluciÃ³n al original,
tomamos z falso y s de acuerdo a que pâ¨q sea verdad y tenemos soluciÃ³n
al NAESAT.

**** Problemas de grafos y conjuntos
***** GI - Isomorfismo de grafos                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       8f2070c6-db8c-445b-ad90-1cdd14a2ba97
:DRILL_LAST_INTERVAL: 3.655
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:09]
:END:
Enunciar el problema de isomorfismo de grafos. Â¿Clase de complejidad?

****** Answer
Dados dos grafos Gâ y Gâ, determinar si existe una biyecciÃ³n
f : Vâ â Vâ tal que (u,v) â Eâ si y sÃ³lo si (f(u),f(v)) â Eâ.

Se supone que ni estÃ¡ en P ni es NP-completo. Se define la
clase GI de isomorfismo de grafos.

***** ACTRI                                                                                                 :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       e5fa3703-ddff-4b96-9551-693adec849f8
:DRILL_LAST_INTERVAL: 4.0583
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:42]
:END:
Enunciar ACTRI. Â¿Clase de complejidad?

****** Answer
Tres conjuntos disjuntos W,X,Y de cardinalidad q, y un subconjunto de
compatibilidades M â W Ã X Ã Y. Â¿Existe un M' â M con q elementos tal
que cubra todos los elementos y no se solape?

ACTRI es NP-completo.

***** ReducciÃ³n: 3-SAT â ACTRI                                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       769832e5-5417-47d6-ae55-1123788c71a5
:DRILL_LAST_INTERVAL: 3.2333
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:23]
:END:
Idea de la reducciÃ³n 3-SAT â ACTRI.

****** Answer
Dado un 3-SAT con variables uâ,...,uâ, y clÃ¡usulas câ,...,câ, para 1â¤jâ¤m,

 * metemos uáµ¢[j], Â¬uáµ¢[j] en W,
 * aÃ±adimos aáµ¢[j] a X,
 * aÃ±adimos báµ¢[j] a Y.

Hacemos un ciclo concatenado de tripletas usando aáµ¢,báµ¢ para obligarnos a elegir
uáµ¢ o Â¬uáµ¢ en todas las clÃ¡usulas. Luego

 * aÃ±adimos sâ[j] a X,
 * aÃ±adimos sâ[j] a Y,
 * aÃ±adimos (uáµ¢[j],sâ[j],sâ[j]) si estÃ¡ en la clÃ¡usula o (Â¬uáµ¢[j],sâ[j],sâ[j])
   si estÃ¡ su negaciÃ³n.

Esto nos fuerza a elegir que al menos se cumple uno de esos tres en la
clÃ¡usula. Finalmente, para 1â¤kâ¤m(n-1),

 * aÃ±adimos gâ[k] a X,
 * aÃ±adimos gâ[k] a Y,

permitiendo todas las compatibilidades, es decir todas las (uáµ¢[j],gâ[k],gâ[k])
y todas las (Â¬uáµ¢[j],gâ[k],gâ[k]). Esto sirve para dar salida a las que son
verdad pero no se necesitan para satisfacer clÃ¡usulas.

Todo esto se hace en espacio logarÃ­tmico y se comprueba equivalencia.

***** 3-SET                                                                                                 :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       1c82d410-9b87-45fe-aa50-266900f6055b
:DRILL_LAST_INTERVAL: 4.2866
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:43]
:END:
Enunciar 3-SET. Â¿Clase de complejidad?

****** Answer
Sea |X| = 3q y un conjunto dado de tripletas (subconjuntos de tres
elementos). Encontrar tripletas entre las dadas que cubran el conjunto
y sean disjuntas.

3-SET es NP-completo.

***** ReducciÃ³n: ACTRI â 3-SET                                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       7e1c57ac-a5db-4026-a7db-17eab06bcc4d
:DRILL_LAST_INTERVAL: 4.0426
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:27]
:END:
Idea de la reducciÃ³n ACTRI â 3-SET.

****** Answer
Simplemente dados W,Y,Z los unimos X = W âª Y âª Z y tomamos las mismas
tripletas.

***** Clique - Problema del clique mÃ¡ximo                                                                   :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       b6d07972-dbf6-4b1c-abcd-d15fb0233ab3
:DRILL_LAST_INTERVAL: 3.7855
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:46]
:END:
Enuncia el problema del clique mÃ¡ximo.

****** Answer
Un clique es un subconjunto maximal totalmente conectado. Dado un
grafo y un nÃºmero natural J â¤ |V|, determinar si existe un clique
de tamaÃ±o mayor o igual que J.

***** CV - Cubrimiento por vÃ©rtices                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       bb5dfaa3-b828-4aab-89f9-4a40c7b590c5
:DRILL_LAST_INTERVAL: 4.3388
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:48]
:END:
Enunciar el problema de cubrimiento por vÃ©rtices.

****** Answer
Un conjunto de vÃ©rtices Vâ /cubre el grafo/ si toda arista tiene un
extremo en ellos.  Determinar si existe un recubrimiento de tamaÃ±o
menor o igual que un K dado.

Es NP-completo.

***** CI - Conjunto independiente                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       be313e92-e7f3-4698-a194-b7f15b624519
:DRILL_LAST_INTERVAL: 5.0789
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 22:02]
:END:
Enunciar el problema del conjunto independiente.

****** Answer
Un subconjunto de vÃ©rtices de un grafo Váµ¢ â V es *independiente*
si no hay ninguna arista entre vÃ©rtices de Váµ¢. âu,v â Váµ¢: (u,v) â E.
Dado un grafo y un natural, determinar si existe un conjunto
independiente de tamaÃ±o mayor o igual que un J dado.

***** Equivalencia de tres problemas de grafos                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       29d879cc-e99b-42ff-b053-9bc32e85accc
:DRILL_LAST_INTERVAL: 3.9901
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:53]
:END:
QuÃ© tres problemas de grafos son equivalentes.

****** Answer

 1. Clique de tamaÃ±o mayor o igual que n-K en el complemento de G.
 2. Cubrimiento de vÃ©rtices de tamaÃ±o menor o igual que K.
 3. Conjunto independiente de tamaÃ±o mayor o igual que n-K.

***** ReducciÃ³n: 3-SAT â CV                                                                                 :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       ea83d9ea-cc25-41ef-a15f-ad4a4ef341da
:DRILL_LAST_INTERVAL: 4.1065
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:22]
:END:
Idea de la reducciÃ³n 3-SAT â CV (cubrimiento por vÃ©rtices).

****** Answer
Representamos las uâ,...,uâ uniendo cada una con su opuesta, tendremos
que elegir una de las dos. Cada clÃ¡usula viene dada por un triÃ¡ngulo,
conectado a sus tres literales; tendremos que tomar al menos 2 vÃ©rtices
del triÃ¡ngulo y el tercero nos lo podemos ahorrar si tiene el valor de
verdad correcto.

Si y sÃ³lo si hay satisfacibilidad podremos llegar a cubrir con n+2m.
***** ReducciÃ³n: CV â CH                                                                                    :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       812c2028-236a-487d-b804-4406d5dfa715
:DRILL_LAST_INTERVAL: 4.9712
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:25]
:END:
Idea de la reducciÃ³n CV â CH. (cubrimiento por vÃ©rtices a circuito hamiltoniano)

****** Answer
Hay que construir un gadget por cada arista en el CV y luego poner
puntos sueltos que sirven para enlazarlos. El gadget permite cruzar por los
dos sitios (si estÃ¡ cubierta dos veces la arista) o sÃ³lo por uno (si sÃ³lo una
vez estÃ¡ cubierta).

***** Problema de la particiÃ³n                                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       19a81ec7-0136-4176-85bc-06f6b8ec967a
:DRILL_LAST_INTERVAL: 3.2625
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:49]
:END:
Enunciar el problema de la particiÃ³n.

****** Answer
Dado un conjunto y una funciÃ³n tamaÃ±o s : A â â, determinar si existe
un A' â A tal que

\[
\sum_{a\in A'}s(a) = \sum_{a \in A\setminus A'} s(a).
\]

***** ReducciÃ³n: ACTRI â ParticiÃ³n                                                                          :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       1cb9605a-5607-4325-8635-af83ffefbb50
:DRILL_LAST_INTERVAL: 5.0866
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 20:26]
:END:
Idea de la reducciÃ³n ACTRI â ParticiÃ³n.

****** Answer
Para cada tripleta máµ¢, ponemos un elemento aáµ¢ que en su peso codifica
en binario varias zonas independientes en cuanto acarreo. Cada zona
tiene un 1 al final si la tripleta cubre el elemento asociado. AsÃ­,
tendrÃ¡ tres 1s en las tres posiciones que cubre. Sea B el nÃºmero que
tiene un uno en cada zona, un recubrimiento suma B.

AdemÃ¡s de estos, incluimos en el conjunto A un bâ de peso Î£s(aáµ¢) y un
bâ de peso 2B. Una particiÃ³n tendrÃ­a que encontrar elementos que
sumaran B + Î£s(aáµ¢), y alguna de ellas deberÃ­a contener a bâ, asÃ­
encontrando elementos de los aáµ¢ que sumen B.
**** TÃ©cnicas de reducciÃ³n
***** Problema de la mochila
***** TODO ReducciÃ³n: ParticiÃ³n â Mochila
# Usando tÃ©cnica de la restricciÃ³n.

***** TODO ReducciÃ³n: ParticiÃ³n â AsignaciÃ³n en multiprocesador
# Usando tÃ©cnica de la restricciÃ³n.
***** ParticiÃ³n de triÃ¡ngulos (PARTRI)                                                                      :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       02189a51-0b3f-48ba-a666-a524531370a5
:DRILL_LAST_INTERVAL: 4.5201
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:48]
:END:
Enunciar el problema de particiÃ³n de triÃ¡ngulos.

****** Answer
Damos un grafo con |V| = 3q vÃ©rtices. Â¿Existe una particiÃ³n de V en q
conjuntos distintos siendo cada uno de ellos un triÃ¡ngulo?

***** ReducciÃ³n: 3-SET â PARTRI                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       56ecd170-d2e1-498e-a224-c0641c169fe2
:DRILL_LAST_INTERVAL: 3.8535
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:14]
:END:
Idea de la reducciÃ³n 3-SET â PARTRI.

****** Answer
Tomamos un grafo que tiene los elementos de la particiÃ³n como vÃ©rtices
y ademÃ¡s para cada tripleta de 3-SET creamos una triple torre de
triÃ¡ngulos de 9 vÃ©rtices que permite elegir alternativamente los tres
o no elegir ninguno al cubrir por triÃ¡ngulos.

***** Conjunto mÃ­nimo de tests (CMT)                                                                        :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       2031cafb-4518-4cb0-8036-0318f1fb75a6
:DRILL_LAST_INTERVAL: 4.6855
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:19]
:END:
Enunciar el problema del conjunto mÃ­nimo de tests.

****** Answer
Damos un conjunto A de posibles diagnÃ³sticos, un conjunto C de subconjuntos de A
de /posibles tests/ y J natural nÃºmero de tests admisibles.  Â¿Existe una subfamilia
C' â C con |C'| â¤ J tal que para cada par de elementos de aáµ¢,aâ±¼ â A haya un test
en C' cuya pertenencia los distinga?

***** ReducciÃ³n: ACTRI â CMT                                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       3e972a53-0f7e-4136-834c-c77841d8d965
:DRILL_LAST_INTERVAL: 4.1262
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 23:19]
:END:
Idea de la reducciÃ³n ACTRI â CMT.

****** Answer
Dado un ACTRI con M â W Ã X Ã Y, de tamaÃ±o |W|=|X|=|Y|=q,
tomamos A = W âª X âª Y âª {wâ,xâ,yâ} y tomamos tests

C = {{w,x,y} â£ (w,x,y)âM} âª {W âª {wâ}} âª {X âª {xâ}}.

NÃ³tese que tenemos dos tests que separan las x's, las w's y las
y's. AdemÃ¡s, para poder separar cada una de ellas, necesitaremos
que cada una estÃ© en algÃºn {w,x,y}, asegurÃ¡ndonos asÃ­ que la
particiÃ³n cubre.

Pedimos que haya J=q+2 tests, para asegurar que no duplicamos en el
cubrimiento, si lo hiciÃ©ramos, no llegarÃ­amos a cubrir todo.

**** TODO Listado de Problemas NP-Completos
**** Reducibilidad Turing
***** Reducibilidad Turing                                                                                  :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       8483b9ae-0d98-40c6-ac63-5d27f7770c31
:DRILL_LAST_INTERVAL: 3.9325
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:54]
:END:
Â¿CuÃ¡ndo se *reduce Turing* un problema Î  a un problema Î '? Escribimos
Î  ââ Î '.

****** Answer
Cuando Î  puede resolverse en tiempo polinÃ³mico con un orÃ¡culo que
resuelve Î ' en un paso de cÃ¡lculo.

**** NP-hard, CoNP, certificados
***** Problemas NP-difÃ­ciles                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       42a4c9ed-5012-4e3c-899c-b7d27f9b49d9
:DRILL_LAST_INTERVAL: 4.135
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.5
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:54]
:END:
DefiniciÃ³n de problema NP-difÃ­cil.

****** Answer
Î  es NP-difÃ­cil si existe un NP-completo Î ' que se puede reducir-Turing
a Î .  Si un problema NP-difÃ­cil se resuelve en tiempo polinÃ³mico, entonces
P = NP.

***** RelaciÃ³n entre NP-completo y CoNP completo                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       705575c0-040c-470e-bfb6-371ecbb31fa9
:DRILL_LAST_INTERVAL: 4.0067
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:07]
:END:
Â¿CuÃ¡l es la relaciÃ³n entre NP-completo y CoNP completo?

****** Answer
$\mathrm{L}$ es NP-completo si y sÃ³lo si su complemento $\overline{\mathrm{L}}$ es CoNP completo.

***** RelaciÃ³n entre P, NP y CoNP                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       aa72321c-0d12-4bdc-a5f2-1aa0f1f39243
:DRILL_LAST_INTERVAL: 3.8653
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:21]
:END:
Â¿CuÃ¡l es la relaciÃ³n entre P, NP y CoNP?Â¿QuÃ© pasarÃ­a si P = NP?

****** Answer
Si P = NP entonces CoNP = NP.
***** Teorema de Pratt                                                                                      :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       3331c3f1-4479-434b-a719-c2260eb50cdd
:DRILL_LAST_INTERVAL: 3.3456
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:43]
:END:
Enuncia el Teorema de Pratt.

****** Answer
El determinar si un nÃºmero es primo estÃ¡ en NP.
**** Problemas de funciones
***** Problema de funciÃ³n                                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       1c0a5d62-7064-40fd-bd85-b2c84281d683
:DRILL_LAST_INTERVAL: 3.9279
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:52]
:END:
Â¿QuÃ© es el problema de funciÃ³n asociado a una relaciÃ³n R?

****** Answer
Dado un ejemplo x â A*, calcular un y tal que R(x,y) si existe o
devolver Îµ si no.
***** Clase FNP                                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       d2f147a7-f1b9-4322-8d8b-652984616ba0
:DRILL_LAST_INTERVAL: 3.538
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:36]
:END:
Clase FNP.

****** Answer
Un problema de funciÃ³n estÃ¡ en FNP ssi estÃ¡ asociado a una relaciÃ³n R
decidible en tiempo polinÃ³mico y tal que si R(x,y), entonces |y| â¤ p(|x|)
para un polinomio p.

***** Clase FP                                                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       dc97dd2f-f485-4c47-a460-4626f3b0770a
:DRILL_LAST_INTERVAL: 3.9305
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:07]
:END:
Clase FP.

****** Answer
(Function Polynomial-Time) Clase de problemas FNP tales que existe una
mÃ¡quina de Turing determinista que las calcula en tiempo polinÃ³mico.

***** Clase FNPT                                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       8b31d502-dce5-4028-8b4d-4c60b46896ea
:DRILL_LAST_INTERVAL: 3.8023
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:33]
:END:
Clase FNPT.

****** Answer
Clase de los problemas FNP totales; tales que para todo x sabemos que
existe un y con |y| â¤ p(|x|) tal que R(x,y) = 1.

/Ejemplo: encontrar una descomposiciÃ³n en nÃºmeros primos./
/Ejemplo: red feliz./

***** FNP-completos                                                                                         :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       84429db9-3517-424b-a157-b5caa12c5600
:DRILL_LAST_INTERVAL: 4.1801
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:36]
:END:
Problema FNP-completo. Dar un ejemplo.

****** Answer
EstÃ¡ en FNP y cualquier otro problema de esta clase de puede reducir a
Ã©l. 

/Un ejemplo es FSAT, dado un conjunto de clÃ¡usulas, encontrar una/
/asignaciÃ³n que haga a todas las clÃ¡usulas verdaderas./

***** P = NP ssi FP = FNP                                                                                   :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       2fa7c942-b809-4f1a-b191-e5a0457e71e4
:DRILL_LAST_INTERVAL: 3.9281
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:05]
:END:
Â¿Por quÃ© sabemos que P = NP ssi FP = FNP?

****** Answer
Si FP = FNP, trivialmente P = NP.  En el otro sentido, si P = NP,
entonces podrÃ­amos resolver SAT tiempo polinÃ³mico, y sabiendo
resolver SAT, resolver FSAT se hace en tiempo polinÃ³mico probando
asignaciones. FSAT es FNP-completo.

***** Reducciones en FNP                                                                                    :drill:
:PROPERTIES:
:ID:       41f5d701-676b-4137-b4f3-af8b3a943ddb
:END:
Concepto de reducciÃ³n en FNP.

****** Answer
Un Î  se reduce a Î ' ssi existe R y S calculables en espacio logarÃ­tmico tal que
si x es ejemplo de Î , entonces R(x) es ejemplo de Î '; y si z es soluciÃ³n correcta
de R(x), entonces S(z) es soluciÃ³n correcta de x.

***** TODO Resolver el viajante de comercio en versiÃ³n de funciones 
***** La red feliz                                                                                          :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       af69a46d-7f3d-4227-81f4-ddbf8cb7a720
:DRILL_LAST_INTERVAL: 4.0718
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:26]
:END:
Enunciar el problema de la red feliz.

****** Answer
Dado un grafo $(V,E)$ pesado en las aristas por $w$, un estado es una
aplicaciÃ³n $s \colon V \to \left\{ -1,1 \right\}$. El nodo $i$ es feliz bajo un estado si

\[
s(i) \sum_{(i,j) \in E} s(j)w(i,j) \geq 0.
\]

Buscamos un estado en el que todos los nodos sean felices.

***** Clase de la red feliz                                                                                 :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       d5d1c882-b2a4-40fa-b451-3ca47a2a8215
:DRILL_LAST_INTERVAL: 4.1898
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 09:58]
:END:
Â¿En quÃ© clase estÃ¡ la red feliz?

****** Answer
EstÃ¡ en FNPT.

***** La red feliz es total                                                                                 :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       c880e9ff-9e9d-45db-859b-55862751ebf7
:DRILL_LAST_INTERVAL: 4.4882
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:08]
:END:
Â¿Por quÃ© la red feliz es total?

****** Answer
Tomamos

\[
\Phi(s) = \sum_{(i,j) \in E} s(i)s(j)w(i,j).
\]

Si hubiera algÃºn nodo infeliz $i$ entonces


\[
s(i) \sum_{(i,j) \in E} s(j)w(i,j) = -k < 0,
\]

y lo cambiarÃ­amos de signo para que la suma pasara a ser $\Phi(s) + 2k$. 
Este valor no puede crecer indefinidamente (estÃ¡ acotado tomando
valores absolutos), luego el proceso termina en una red feliz.

EstÃ¡ en FNPT

**** TÃ©cnicas de reducciÃ³n de problemas
***** Reemplazamiento local
***** Reemplazamiento local con refuerzo

*** Tema 5: Complejidad de problemas de optimizaciÃ³n aproximados
**** AproximaciÃ³n y optimizaciÃ³n
***** Algoritmo Îµ-aproximado                                                                                :drill:
SCHEDULED: <2018-06-20 Wed>
:PROPERTIES:
:ID:       29b0079a-cd28-4c67-b95e-076f33f852da
:DRILL_LAST_INTERVAL: 3.3958
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:13]
:END:
Â¿CuÃ¡ndo es Îµ-aproximado un algoritmo?

****** Answer
M es un algoritmo Îµ-aproximado si

\[
\frac{\abs{c(M(x)) - opt(x)}}{\max\{ opt(x), c(M(x)) \}} \leq \varepsilon
\]

***** RazÃ³n de eficacia                                                                                     :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       4b81e602-a98c-4bf5-aa8d-5dc2d642dc16
:DRILL_LAST_INTERVAL: 4.4428
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:06]
:END:
RazÃ³n de eficacia de un problema de maximizaciÃ³n.

****** Answer
Un problema de maximizaciÃ³n tiene razÃ³n Î´ si 

\[
\delta \geq \frac{\mathrm{opt}(x)}{C(M(x))}
\]

y un problema de minimizaciÃ³n tiene razÃ³n Î´ si

\[
\delta \geq \frac{C(M(x))}{\mathrm{opt}(x)}
\]

***** Umbral de aproximaciÃ³n                                                                                :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       b1c8df40-fb7a-4f19-a2d6-73ab04c953c3
:DRILL_LAST_INTERVAL: 4.1359
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:47]
:END:
Â¿CuÃ¡l es el umbral de aproximaciÃ³n de un problema?

****** Answer
El Ã­nfimo de la razÃ³n de eficacia mediante algoritmos polinÃ³micos.

***** Esquema de aproximaciÃ³n polinÃ³mico                                                                    :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       3a0cc88b-e2a9-41bb-af2d-520da3cbfd8a
:DRILL_LAST_INTERVAL: 4.0526
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:10]
:END:
Â¿QuÃ© es un esquema de aproximaciÃ³n polinÃ³mico?

****** Answer
Familia de algoritmos parametrizada en Î´ > 1 tal que para cada ejemplo
x del problema Î  puede devolver una aproximaciÃ³n de grado Î´ del Ã³ptimo
en tiempo polinÃ³mico en funciÃ³n de |x|.

***** Esquema de aproximaciÃ³n polinÃ³mico total                                                              :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       64ea37fa-86ca-4cf7-9b01-9138fcc45731
:DRILL_LAST_INTERVAL: 4.3041
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 17:22]
:END:
Â¿CuÃ¡ndo es un esquema de aproximaciÃ³n polinÃ³mico *total*?

****** Answer
La dependencia de Î´ de la familia de algoritmos que dan el esquema de
aproximaciÃ³n se puede expresar como un polinomio en 1/(Î´-1).

**** Problemas de aproximaciÃ³n
***** 2-aproximado cubrimiento por vÃ©rtices                                                                 :drill:
:PROPERTIES:
:ID:       99c3b361-93b5-4522-a30c-5b241a5226f7
:END:
Dar un algoritmo 2-aproximado para cubrimiento por vÃ©rtices.

****** Answer
Mientras haya aristas tomar una arista cualquiera y aÃ±adir sus dos
extremos, se borra entonces la arista, los dos nodos, y todas las
aristas conectadas a ellos.

Es 2-aproximado porque todas las aristas tienen que tener al menos un
extremo en $C$, y esta familia de aristas no comparte ninguno.

***** Problema: Corte mÃ¡ximo                                                                                :drill:
:PROPERTIES:
:ID:       e758bcf7-cca4-432b-af89-f34b01f72daf
:END:
Enuncia el problema del corte mÃ¡ximo. Como problema de decisiÃ³n Â¿en
quÃ© clase estÃ¡?Â¿Como problema de optimizaciÃ³n, dÃ³nde estÃ¡?

****** Answer
Dado un grafo no dirigido, partir sus vÃ©rtices en dos conjuntos de
forma que el nÃºmero de aristas de un conjunto al otro sea mÃ¡ximo.

Es NP-completo como problema de decisiÃ³n. En optimizaciÃ³n estÃ¡
en APX.

***** 2-aproximado corte mÃ¡ximo                                                                             :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       b7760225-0274-4e9b-8e4c-49f007498741
:DRILL_LAST_INTERVAL: 4.586
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:07]
:END:
Corte mÃ¡ximo es 2-aproximado, dar el algoritmo.

****** Answer
Se toma un conjunto arbitrario de vÃ©rtices. Mientras el conjunto
cambie, se comprueba para cada vÃ©rtice si estarÃ­a mejor al otro lado
del conjunto (si tiene mÃ¡s arcos hacia la otra parte del grafo) y se
cambia si sÃ­. Esto es polinÃ³mico |E|.

Que es 2-aproximado se comprueba asumiendo que llegamos a la propiedad
buscada y usando allÃ­ desigualdades, partiendo en cuatro conjuntos de
vÃ©rtices que nos den el heurÃ­stico y el Ã³ptimo.

***** Problema: GMAXSAT                                                                                     :drill:
:PROPERTIES:
:ID:       a77d047f-76f4-4d73-a6bb-b6ca962a1cf3
:END:
Enunciar el problema de mÃ¡xima satisfacciÃ³n GMAXSAT. Â¿En quÃ© clase estÃ¡?

****** Answer
Para $m$ fÃ³rmulas booleanas con $n$ variables, encontrar una
asignaciÃ³n de valores que maximice el nÃºmero de fÃ³rmulas.

/EstÃ¡ en APX./

***** Algoritmo aproximado para GMAXSAT                                                                     :drill:
:PROPERTIES:
:ID:       ae8c9088-ce4b-46f7-a660-dae9ff2b99ba
:END:
Da la idea del algoritmo aproximado para GMAXSAT.

****** Answer
Calculamos la probabilidad de que una asignaciÃ³n de valores sea cierta
para cada clÃ¡usula $\phi_i$ como

\[
P(\phi_i) = \frac{t_i}{2^k}
\]

y elegimos a cada paso la asignaciÃ³n que maximice probabilidades. Esto
nunca decrece el valor esperado de fÃ³rmulas que se satisfacen, que es con
el que llegamos al final,

\[
P(\Phi) = \sum_{i=1}^m P(\phi_i).
\]

El Ã³ptimo lo podemos acotar por el nÃºmero de clÃ¡usulas que empiezan por
probabilidad no nula. Esto nos acaba dando

\[
\delta = \frac{1}{\min \left\{ P(\phi_i) \mid P(\phi_i) > 0 \right\}}.
\]

Para fÃ³rmulas con $k$ variables, llegamos a $\delta = 2^{k}$.

***** No aproximaciÃ³n del viajante de comercio                                                              :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       a66321cb-da30-4ec2-92c4-f3770c44c9cb
:DRILL_LAST_INTERVAL: 3.8702
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 09:57]
:END:
Â¿QuÃ© sabemos de las aproximaciones del viajante de comercio?

****** Answer
Si $P \neq NP$, entonces el viajante de comercio no tiene algoritmos
polinÃ³micos con factor $\delta$ para ningÃºn valor de $\delta$. Su umbral de
aproximaciÃ³n es $\infty$.

***** No aproximaciÃ³n del viajante de comercio: idea                                                        :drill:
:PROPERTIES:
:ID:       42f29a3e-98c4-4e90-8f6e-bf7a6a797b9c
:END:
Por quÃ© sabemos que el viajante de comercio tiene umbral de
aproximaciÃ³n $\infty$.

****** Answer
Si tuviera alguna aproximaciÃ³n polinÃ³mica de factor $\delta$, dado un
grafo, podrÃ­amos obtener su circuito hamiltoniano; para eso asignamos
un $1$ a las aristas del grafo y $\delta|V|$ a las que no estÃ¡n en el
grafo.

Aplicamos la aproximaciÃ³n, y excepto que encontrara un circuito de
coste $|V|$, tendrÃ­a que devolver uno con mÃ¡s coste que $\delta|V|$. Entonces
sabrÃ­amos que no hay ninguno de coste $|V|$ (por aproximaciÃ³n) y no
habrÃ­a circuito hamiltoniano.

***** Problema de la mochila (de funciÃ³n)                                                                   :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       7cebf32a-f727-4c4c-b48a-fb3d1047909c
:DRILL_LAST_INTERVAL: 3.8845
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:24]
:END:
Enuncia el problema de la mochila. Â¿QuÃ© tipo de algoritmo se le puede dar?

****** Answer
Dados pesos wâ,...,wâ y valores vâ,...,vâ; ademÃ¡s de un peso lÃ­mite W,
encontrar el subconjunto de objetos S â {1,...,n} tal que la suma de
pesos es menor del lÃ­mite y el valor es mÃ¡ximo.

Se le puede dar un algoritmo pseudopolinÃ³mico ${\cal O}(n^2V)$ donde
$V$ es el mÃ¡ximo de los valores.

***** Clase del problema de la mochila                                                                      :drill:
:PROPERTIES:
:ID:       c8495ee2-98da-4096-bac1-de3ef94f3c6c
:END:
En quÃ© clase estÃ¡ el problema de la mochila en su versiÃ³n de
aproximaciÃ³n.

****** Answer
EstÃ¡ en FPTAS. Tiene un esquema de aproximaciÃ³n polinÃ³mico total,
de orden

\[
{\cal O}\left( \frac{n^3}{\delta-1} \right)
\]

***** Algoritmos al problema de la mochila                                                                  :drill:
:PROPERTIES:
:ID:       ad9d357a-25cc-42fc-af1c-ccd2785fad8f
:END:
Â¿QuÃ© dos algoritmos podemos dar para el problema de la mochila en su
versiÃ³n de aproximaciÃ³n?Â¿QuÃ© caracterÃ­stica tiene cada uno?

****** Answer

 * Uno que es pseudopolinÃ³mico en ${\cal O}(nV)$.

 * Un esquema de aproximaciÃ³n polinÃ³mico total de complejidad

   \[
   {\cal O}\left(\frac{n^3}{\delta - 1}\right)
   \]

***** Teorema del esquema del conjunto independiente                                                        :drill:
:PROPERTIES:
:ID:       7b215d83-1c2c-4898-acb4-9d5b36a0def7
:END:
Si el mÃ¡ximo conjunto independiente estÃ¡ en APX, entonces estarÃ¡ en
PTAS, Â¿por quÃ©?

****** Answer
Dado un $\delta$ podemos considerar el algoritmo que toma el $G^2$, que tiene
un independiente de tamaÃ±o $k^2$ ssi el $G$ lo tiene de tamaÃ±o $k$. Calculando,
podemos deducir un factor $\sqrt{\delta}$ asÃ­.

**** Sobre las clases NPO y PO
***** Clase NPO                                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c9f72540-a4c4-48f4-b84d-fb0bbd163b56
:DRILL_LAST_INTERVAL: 4.1524
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:16]
:END:
Define la clase NPO. Da un ejemplo de problema.

****** Answer
(NP Optimization)
Problemas cumpliendo:

 1. Las entradas correctas se *reconocen* en tiempo polinÃ³mico.
 2. Existe q polinomio tal que para cada caso x y cada soluciÃ³n
    factible y, se cumple |y| â¤ q(|x|). AdemÃ¡s, para cada |y| â¤ q(|x|)
    podemos decidir en tiempo polinÃ³mico si es una soluciÃ³n factible.
 3. La funciÃ³n de *costo* se calcula en tiempo polinÃ³mico.

/Ejemplo: viajante de comercio en optimizaciÃ³n./

***** Clase PO                                                                                              :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       b3b88cdc-e96b-462a-8596-38cccde44cc5
:DRILL_LAST_INTERVAL: 3.7223
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:18]
:END:
Define la clase PO. Da un ejemplo de problema.

****** Answer
EstÃ¡ en NPO y existe un algoritmo polinÃ³mico tal que para cada x
calcula la soluciÃ³n Ã³ptima y su coste.

/Ejemplo: distancia mÃ­nima en grafos/

***** RelaciÃ³n NP vs NPO                                                                                    :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       bd7ca7ae-53c7-4950-9dca-fa20548b3459
:DRILL_LAST_INTERVAL: 3.9631
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:27]
:END:
RelaciÃ³n entre clases NP y NPO.

****** Answer
Cualquier problema de optimizaciÃ³n de NPO tiene su correspondiente
problema de decisiÃ³n en NP.

***** RelaciÃ³n optimizaciÃ³n, decisiÃ³n                                                                       :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       87031856-8a25-4b49-8055-553d095a0f5b
:DRILL_LAST_INTERVAL: 4.3514
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 21:42]
:END:
Â¿QuÃ© teorema relaciona problemas de optimizaciÃ³n y de decisiÃ³n?

****** Answer
Si P â  NP entonces PO â  NPO.

**** La clase APX
***** Clase APX                                                                                             :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       bb345650-df81-4b42-aea4-530895973224
:DRILL_LAST_INTERVAL: 3.2314
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 00:58]
:END:
Clase APX.

****** Answer
(Approximable)
La clase APX es de problemas que admiten un algoritmo Î´-aproximado
polinÃ³mico para algÃºn Î´ < â.

***** Ejemplos en APX                                                                                       :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       382598c2-5593-4dc9-82f5-f036aabbd483
:DRILL_LAST_INTERVAL: 3.979
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.333
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:05]
:END:
Da tres ejemplos de problema en APX, la clase de problemas que admiten
un algoritmo Î´-aproximado polinÃ³mico para algÃºn Î´ < â.

****** Answer
Cubrimiento mÃ­nimo por vÃ©rtices, problema de la mochila, corte mÃ¡ximo,
mÃ¡ximo nÃºmero de clÃ¡usulas satisfechas en SAT,...

***** Ejemplos que no estÃ¡n en APX                                                                          :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       76c9465f-cdb0-488f-a4d9-e6efa693a635
:DRILL_LAST_INTERVAL: 3.4901
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:13]
:END:
Da tres ejemplos de problema *que no estÃ©* en APX, la clase de
problemas que admiten un algoritmo Î´-aproximado polinÃ³mico para algÃºn
Î´ < â.

****** Answer
Viajante de comercio, mÃ¡ximo clique (este es especialmente interesante porque
podrÃ­a parecer lo contrario, pero hay que jugar con la diferencia entre Î´ para
optimizaciÃ³n y minimizaciÃ³n), mÃ¡ximo conjunto independiente.

***** RelaciÃ³n aproximaciÃ³n, decisiÃ³n                                                                       :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       a989a2db-bd9b-4756-8e09-56b2ec9928e9
:DRILL_LAST_INTERVAL: 4.34
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:38]
:END:
Teorema que relaciona decisiÃ³n y esquemas de aproximaciÃ³n polinÃ³micos.

****** Answer
Si P â  NP, entonces PO â  NPO.

**** La clase PTAS y FPTAS
***** Clase PTAS                                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       17f9aed8-071f-45ec-9c40-de2797bea332
:DRILL_LAST_INTERVAL: 3.6117
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:13]
:END:
DefiniciÃ³n de PTAS. Ejemplos de problema en PTAS.

****** Answer
(Polynomial-Time Approximation Scheme)
Problemas con esquema de aproximaciÃ³n polinÃ³mico.

/Ejemplo: el problema de la mochila con copias ilimitadas./
/Ejemplo: cubrimiento mÃ­nimo de grafos planares./
/Ejemplo: viajante de comercio Â¡en el plano euclÃ­deo!/

***** Lista de inclusiones                                                                                  :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       28527796-437b-4d0d-b2b3-fc57abb5843c
:DRILL_LAST_INTERVAL: 3.7476
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:27]
:END:
Da la lista de inclusiones entre FPTAS, APX, PO, NPO y PTAS.

****** Answer

PO â FPTAS â PTAS â APX â NPO

***** APX fuera de PTAS                                                                                     :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       7c7d6c48-c36f-4c53-8627-40fd62c9419c
:DRILL_LAST_INTERVAL: 3.2371
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:16]
:END:
Dar un problema en APX que no estÃ© en PTAS (asumimos P â  NP).

****** Answer
El corte mÃ¡ximo. El cubrimiento mÃ­nimo en general.

***** Cubrimiento mÃ­nimo en grafos planares                                                                 :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       a96326b9-518b-4d9b-a026-6381746b35e5
:DRILL_LAST_INTERVAL: 4.019
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:01]
:END:
Â¿En quÃ© clase de complejidad de aproximaciÃ³n estÃ¡ el problema del
cubrimiento mÃ­nimo?Â¿QuÃ© cambia en el caso de grafos planares?

****** Answer
El cubrimiento mÃ­nimo estÃ¡ en APX. SÃ³lo en el caso de grafos planares,
estÃ¡ en PTAS.

***** Clase FPTAS                                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       01c8ada7-8c4c-4e9b-860f-04e910167d0e
:DRILL_LAST_INTERVAL: 4.3085
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:09]
:END:
DefiniciÃ³n de la clase FPTAS.

****** Answer
(Fully Polynomial-Time Approximation Scheme)
Son problemas con un esquema de aproximaciÃ³n polinÃ³mico total. Para
cada ejemplo x de Î  y cada Î´ > 1, el algoritmo devuelve una soluciÃ³n
que es polinÃ³mica en |x| y en 1/(Î´-1).

***** Problema acotado polinÃ³micamente                                                                      :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       1a7e1140-3c9d-4c88-9a94-30d933b8318b
:DRILL_LAST_INTERVAL: 3.7841
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 01:16]
:END:
Problema acotado polinÃ³micamente. Ejemplo de problema que estÃ© y de
problema que seguramente no estÃ©.

****** Answer
(NPOPB - NPO Polinomially Bounded)
Un problema de optimizaciÃ³n estÃ¡ acotado polinÃ³micamente si existe un
polinomio p tal que para todo ejemplo x y toda soluciÃ³n factible y,
se tiene coste acotado polinÃ³micamente, C(x,y) â¤ p(|x|).

/Ejemplo: el cubrimiento mÃ­nimo por vÃ©rtices es NPOPB./
/Contraejemplo: el viajante de comercio NO estÃ¡ en NPOPB./

***** FPTAS y PTAS                                                                                          :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       ff0ec66c-1172-425b-917b-b2ea93b67e91
:DRILL_LAST_INTERVAL: 4.2708
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 3.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 22:03]
:END:
Teorema que relaciona FPTAS y PTAS.

****** Answer
No existe un problema NP-difÃ­cil acotado polinÃ³micamente (NPOPB) con
un esquema de aproximaciÃ³n polinÃ³mico total.

En particular, si P â  NP, entonces PTAS â  FPTAS.

**** PseudopolinÃ³micos
***** PseudopolinÃ³micos                                                                                     :drill:
:PROPERTIES:
:ID:       218323d9-16ce-4a45-a5bb-36795c0a4533
:END:
Â¿QuÃ© es un algoritmo pseudopolinÃ³mico?

****** Answer
Aquel que para cada ejemplo del problema, da la respuesta correcta un
tiempo que depende polinÃ³micamente de |x| y del mayor entero en la
especificaciÃ³n de x, que llamamos $\mathrm{max}(x)$.

***** Teorema de pseudopolinÃ³micos                                                                          :drill:
SCHEDULED: <2018-06-23 Sat>
:PROPERTIES:
:ID:       c82af765-2f19-4b10-a17d-b04a24fd0fc1
:DRILL_LAST_INTERVAL: 4.4148
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.5
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:25]
:END:
Enuncia el teorema de pseudopolinÃ³micos.

****** Answer
Si un problema estÃ¡ en FPTAS y tiene un pseudopolinomio verificando

\[
\mathrm{opt}(x) \leq p(|x|, \mathrm{max}(x))
\]

entonces el problema es pseudopolinÃ³mico.

**** L-reducciones y AP-reducciones
***** L-reducciÃ³n                                                                                           :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       ed54bad5-dfd0-46ef-9a13-55bff76f5531
:DRILL_LAST_INTERVAL: 4.9122
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-17 Sun 22:02]
:END:
Definir L-reducciÃ³n.

****** Answer
El problema de optimizaciÃ³n Î â se L-reduce a Î â, y se escribe Î â â Î â,
si y sÃ³lo si existen R y S espacio logarÃ­tmicas con constantes Î± y Î²
tales que
 
 * si x es ejemplo de Î â, entonces R(x) es ejemplo de Î â con

   opt(R(x)) â¤ Î± opt(x);

 * si s es soluciÃ³n factible de R(x), entonces S(s,x) es soluciÃ³n
   factible de x con

   â£opt(x) - C(S(s,x))â£ â¤ Î² â£opt(R(x)) - C(s)â£.

***** TODO AP-reducciÃ³n                                                                                     :drill:
:PROPERTIES:
:ID:       2843d6e8-8735-4aa2-b58d-f32ca8bf37bb
:END:
**** Problemas completos                                                                                     :extra:
*** Tema 6: Complejidad en espacio. JerarquÃ­a polinÃ³mica
**** Clase DP                                                                                                :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       b0ad156d-465c-4780-8da4-385cec85072b
:DRILL_LAST_INTERVAL: 4.0767
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:27]
:END:
DefiniciÃ³n de la clase DP.

***** Answer
(Difference Polynomial Time) Un lenguaje estÃ¡ en DP si es L = Lâ â© Lâ
con Lâ en NP y Lâ en CoNP.

**** Problemas DP-completos                                                                                  :drill:
SCHEDULED: <2018-06-21 Thu>
:PROPERTIES:
:ID:       037aad50-1103-4695-a80f-83d52e6441df
:DRILL_LAST_INTERVAL: 3.2474
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 2
:DRILL_FAILURE_COUNT: 1
:DRILL_AVERAGE_QUALITY: 2.0
:DRILL_EASE: 2.36
:DRILL_LAST_QUALITY: 3
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:06]
:END:
Dar dos problemas DP-completos.

***** Answer
*Problema del viajante de comercio exacto:* dado un mapa y un
valor X, determinar si la soluciÃ³n es de coste exactamente X.

*Problema SAT-UNSAT:* Dados dos conjuntos de clÃ¡usulas, determinar si
el primero es consistente y el segundo inconsistente.

*Consistencia crÃ­tica:* Dado un conjunto de clÃ¡usulas, determinar si
es inconsistente y si se puede hacer consistente quitÃ¡ndole una
clÃ¡usula.

*Consistencia Ãºnica:* Dada una fÃ³rmula booleana, determinar si existe
una /Ãºnica/ asignaciÃ³n de valores de verdad que la satisfaga.

*Circuito hamiltoniano crÃ­tico:*

*3 colores crÃ­tico:*

**** OrÃ¡culos y clases relativas
***** OrÃ¡culo                                                                                               :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       c4a03a13-81ac-47dc-a90f-a4420d02cada
:DRILL_LAST_INTERVAL: 3.9123
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:06]
:END:
Â¿QuÃ© es un orÃ¡culo?

****** Answer
Un programa que resuelve un problema determinado y al que se puede
llamar en la resoluciÃ³n de otro problema, contando la llamada como
un sÃ³lo paso.

***** Clase Pá´ºá´¾                                                                                             :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       d67cb758-6c5c-4548-a631-14855bfbe6de
:DRILL_LAST_INTERVAL: 3.721
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 11:06]
:END:
DefiniciÃ³n de clase Pá´ºá´¾.

****** Answer
Problemas que se resuelven en tiempo polinÃ³mico pudiendo llamar a un
orÃ¡culo que resuelve un problema NP.

***** Clase FPá´ºá´¾                                                                                            :drill:
SCHEDULED: <2018-06-22 Fri>
:PROPERTIES:
:ID:       7ff805d6-11c9-4b47-9f24-f29f718d6e52
:DRILL_LAST_INTERVAL: 3.9816
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 4.0
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-18 Mon 12:26]
:END:
DefiniciÃ³n de clase FPá´ºá´¾.

****** Answer
Problemas *de funciones* que se resuelven en tiempo polinÃ³mico
pudiendo llamar a un orÃ¡culo que resuelve un problema NP.

***** Problemas FPá´ºá´¾-completos                                                                              :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       24e5ca94-bea4-4a87-aada-926bf3c10708
:DRILL_LAST_INTERVAL: 4.5273
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 3
:DRILL_FAILURE_COUNT: 2
:DRILL_AVERAGE_QUALITY: 2.667
:DRILL_EASE: 2.5
:DRILL_LAST_QUALITY: 4
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:27]
:END:
Di cuatro problemas FPá´ºá´¾-completos.

****** Answer

 * *MAXSAT con peso:* dado un conjunto de clÃ¡usulas con pesos, dar una
   asignaciÃ³n de verdad que maximice la suma de pesos de las que
   satisface.

 * *Salida mÃ¡xima de MT no-determinista:* dada una mÃ¡quina de Turing
   no determinista M y una entrada 1â¿ de forma que termina en tiempo O(n)
   y escribe un entero en binario, determinar el mayor entero que puede
   escribir.

 * *Problema de optimizaciÃ³n en el viajante de comercio:* dado un
   problema de viajante de comercio, determinar circuito de coste
   Ã³ptimo.

 * *Problema de coste en el viajante de comercio:* dado un problema
   del viajante de comercio, determinar el mÃ­nimo coste de un
   circuito.
 
**** JerarquÃ­a polinÃ³mica
***** DefiniciÃ³n                                                                                            :drill:
:PROPERTIES:
:ID:       d432d64c-38b5-4374-b307-196d800c56e8
:END:
CÃ³mo se define la jerarquÃ­a polinÃ³mica.

****** Answer
Îâ = P
Î£â = P
Î â = P

Îáµ¢ââ = P^Î£áµ¢
Î£áµ¢ââ = NP^Î£áµ¢
Î áµ¢ââ = CoNP^Î£áµ¢

***** Clase PH                                                                                              :drill:
SCHEDULED: <2018-06-24 Sun>
:PROPERTIES:
:ID:       8d24850f-7611-4af2-b5e0-429d0a5e2bbb
:DRILL_LAST_INTERVAL: 4.7385
:DRILL_REPEATS_SINCE_FAIL: 2
:DRILL_TOTAL_REPEATS: 1
:DRILL_FAILURE_COUNT: 0
:DRILL_AVERAGE_QUALITY: 5.0
:DRILL_EASE: 2.6
:DRILL_LAST_QUALITY: 5
:DRILL_LAST_REVIEWED: [2018-06-19 Tue 10:08]
:END:
Definir la clase PH. Â¿DÃ³nde estÃ¡ contenida?

****** Answer

\[
PH = \bigcup_{i \geq 0} \Sigma_i
\]

si hubiera un problema PH-completo, la jerarquÃ­a polinÃ³mica colapsarÃ­a
en un nivel.

PH estÃ¡ en PSPACE.

*** Tema 7: Complejidad en modelos de computaciÃ³n paralela
*** Relaciones
**** RelaciÃ³n 1

***** Ejercicio 1
****** Apartado a
#+BEGIN_SRC turing
; Acepta las palabras con el mismo nÃºmero de ceros que de unos.
; Usa un alfabeto 0, 1, %.

; En el estado inicial, lee el primer 0 o el primer 1.
; Si la entrada estÃ¡ vacÃ­a, termina directamente.
0 0 _ r b1
0 1 _ r b0
0 % _ r 0
0 _ _ * accept

; En el estado b0 busca un 0.
; En el estado b1 busca un 1.
b0 0 % * f
b0 _ _ r reject
b0 * * r b0

b1 1 % * f
b1 _ _ r reject
b1 * * r b1

; En el estado f vuelve hasta el principio de la cadena.
f _ _ r 0
f * * l f 

; Estados de aceptaciÃ³n o rechazo.
accept * : r accept2
accept2 * ) * halt-accept

reject _ : r reject2
reject * _ l reject
reject2 * ( * halt-reject
#+END_SRC
****** Apartado b
#+BEGIN_SRC turing
; Acepta las palabras de la forma A^nB^nC^n.
; NÃ³tese que acepta la cadena vacÃ­a.

; Busca la primera A
0 % _ r 0
0 A _ r 1
0 _ _ * accept
0 * * * reject

; Busca la primera B
1 A * r 1
1 % * r 1
1 B % r 2
1 _ * * reject

; Busca la primera C
2 B * r 2
2 % * r 2
2 C % * f
2 _ * * reject

; En el estado f vuelve hasta el principio de la cadena.
f _ _ r 0
f * * l f 

; Estados de aceptaciÃ³n o rechazo.
accept * : r accept2
accept2 * ) * halt-accept

reject _ : r reject2
reject * _ l reject
reject2 * ( * halt-reject
#+END_SRC
****** Apartado c
#+BEGIN_SRC turing
; Acepta palÃ­ndromos

; Busca una letra
0 0 _ r a0
0 1 _ r a1
0 _ _ * accept
0 * * * reject

a0 _ _ l b0
a0 * * r a0

a1 _ _ l b1
a1 * * r a1

b0 0 _ l f
b0 _ * * accept
b0 * * * reject

b1 1 _ l f
b1 _ * * accept
b1 * * r reject

; En el estado f vuelve hasta el principio de la cadena.
f _ _ r 0
f * * l f 

; Estados de aceptaciÃ³n o rechazo.
accept * : r accept2
accept2 * ) * halt-accept

reject _ : r reject2
reject * _ l reject
reject2 * ( * halt-reject
#+END_SRC
****** Apartado d
#+BEGIN_SRC turing
; Acepta las palabras con el mismo nÃºmero de ceros que de unos.
; Usa un alfabeto 0, 1, %.

; En el estado inicial, lee el primer 0 o el primer 1.
; Si la entrada estÃ¡ vacÃ­a, termina directamente.
0 0 _ r bc0
0 1 _ r bc1
0 % _ r 0
0 c _ r aif
0 _ _ * accept

bc0 c * r b0
bc0 * * r bc0
bc1 c * r b1
bc1 * * r bc1

; En el estado b0 busca un 0.
; En el estado b1 busca un 1.
b0 0 % * f
b0 % * r b0
b0 * * r reject

b1 1 % * f
b1 % * r b1
b1 * * r reject

; En este Ãºltimo estado se comprueba que no quede nada detrÃ¡s de la c
aif % _ r aif
aif _ * r accept
aif * * * reject

; En el estado f vuelve hasta el principio de la cadena.
f _ _ r 0
f * * l f 

; Estados de aceptaciÃ³n o rechazo.
accept * : r accept2
accept2 * ) * halt-accept

reject _ : r reject2
reject * _ l reject
reject2 * ( * halt-reject
#+END_SRC
***** Ejercicio 2
***** Ejercicio 3
#+BEGIN_SRC text
; Mueve una cadena de 0s y 1s a la derecha

;; Estado inicial
0 _ * l leer
0 * * r 0

;; Lee una posiciÃ³n
leer 0 * r escribir0
leer 1 * r escribir1
leer _ * r final

;; Escribe un 0 o un 1 segÃºn lo que hubiera leÃ­do
escribir0 * 0 l mover
escribir1 * 1 l mover

;; Estado auxiliar para moverse dos posiciones a la izquierda
mover * * l leer

;; Estado final
final * _ * halt-accept
#+END_SRC

***** Ejercicio 4
#+begin_statement
Escribir una subrutina que comience en una posicioÌn con un cero y se
mueva a la derecha de todos los ceros hasta que alcance un uno o un
blanco. Se suponen que en la cinta solo hay caracteres del conjunto
{0, 1, #}. Si se comienza con un caraÌcter que es distinto de cero, la
subrutina de la MT debe de pararse. Utilizar dicha rutina para
escribir una MT que acepte todas las cadenas de ceros y unos, que no
tengan dos unos consecutivos.
#+end_statement

Siendo =q= el estado que llama al estado inicial =i= de la subrutina.

#+BEGIN_SRC text
-- Estado i inicial, que comprueba que empieza en 0.
d(i;0) = (j,0,R)
d(i,1) = (reject)
d(i,#) = (reject)

-- Mueve a la derecha mientras haya ceros.
d(j,0) = (j,0,R)
d(j;1) = (q,1,S)
d(j;#) = (q,#,S)
#+END_SRC

La MT que acepta cadenas sin dos unos consecutivos es la siguiente.

#+BEGIN_SRC text
d(u;0) = (u,0,R)
d(u;#) = (u,#,R) -- (accept)
d(u;1) = (i,1,R)
#+END_SRC

***** Ejercicio 5
#+attr_latex: :options [Ejercicio 5]
#+begin_statement
DiseÃ±ar una MT con dos cintas que dada una sucesioÌn de ceros de
longitud n en la primera cinta, calcula en la segunda cinta n en
binario.
#+end_statement

Una mÃ¡quina $T = \left( \left\{ i,s,r, \mathrm{accept} \right\} , \left\{ 0,1,\_ \right\} , d, i,\_, \left\{ \mathrm{accept} \right\} \right)$.
Tiene un estado inicial =i= que lee los ceros de la primera cinta,
luego pasa a una subrutina de sumar 1 en el estado =s=, llevÃ¡ndose
uno en cada suma. Finalmente el estado =r= vuelve a colocar el
cabezal de la segunda cinta al principio.

#+begin_src text
d(i;_,*) = (accept)
d(i;0,*) = (s;0,*,R,S)

d(s;*,0) = (r;*,1,S,S)
d(s;*,_) = (r;*,1,S,S)
d(s;*,1) = (s;*,0,S,R)

d(r;*,_) = (i;*,_,S,R)
d(r;*,0) = (r;*,0,S,L)
d(r;*,1) = (r;*,1,S,L)
#+end_src

***** Ejercicio 6
#+attr_latex: :options [Ejercicio 6]
#+begin_statement
Escribir una MT con muÌltiples cintas que sume dos nuÌmeros en
binario. Se supone que aparecen en la cinta de entrada separados por
un sÄ±Ìmbolo especial c.
#+end_statement

#+BEGIN_SRC text
-- Estado inicial, busca la C.
d(m;0,*) = (m;0,*,R,S)
d(m;1,*) = (m;1,*,R,S)
d(m;C,*) = (c;C,*,R,S)

-- Copia en la cinta de abajo.
d(c;0,*) = (c;0,0,R,R)
d(c;1,*) = (c;1,1,R,R)
d(c;_,*) = (r1;_,*,S,S)

-- Regresa en ambas cintas.
d(r1;0,*) = (r1;_,*,L,L)
d(r1;1,*) = (r1;_,*,L,L)
d(r1;C,*) = (r2;_,*,L,S)

-- Regresa sÃ³lo en la superior.
d(r2;0,*) = (r2;0,*,L,S)
d(r2;1,*) = (r2;1,*,L,S)
d(r2;_,*) = (s;_,*,R,R)

-- Suma sin llevadas
d(s;0,0) = (s;*,0,R,R)
d(s;1,0) = (s;*,1,R,R)
d(s;0,1) = (s;*,1,R,R)
d(s;_,0) = (s;*,0,R,R)
d(s;0,_) = (s;*,0,R,R)
d(s;1,_) = (s;*,1,R,R)
d(s;_,1) = (s;*,1,R,R)
d(s;1,1) = (sl;*,0,R,R)
d(s;_,_) = (accept)

-- Suma con llevadas
d(sl;0,0) = (s,*,1,R,R)
d(sl;1,0) = (sl,*,0,R,R)
d(sl;0,1) = (sl,*,0,R,R)
d(sl;_,0) = (s,*,1,R,R)
d(sl;0,_) = (s,*,1,R,R)
d(sl;1,_) = (sl,*,0,R,R)
d(sl;_,1) = (sl,*,0,R,R)
d(sl;1,1) = (sl,*,1,R,R)
d(sl;_,_) = (s,*,1,R,R)
#+END_SRC

***** Ejercicio 7
#+begin_statement
Describir una MT que dada una palabra ucw donde u y w son dos palabras
sobre el alfabeto {0, 1} y c es un sÄ±Ìmbolo adicional, calcule u
repetido tantas veces como indique la palabra w interpretaÌndola como
un entero escrito en binario.
#+end_statement

Usamos dos cintas. Movemos hasta el sÃ­mbolo c, luego decrementamos
uno y copiamos en la cinta de abajo; repetimos hasta llegar a 0 en 
el nÃºmero.

#+BEGIN_SRC text
-- Estado inicial, busca la C en la cinta superior
d(i;0,*) = (i;0,*,R,S)
d(i;1,*) = (i;1,*,R,S)
d(i;C,*) = (m;C,*,R,S)

-- Estado en el que busca el primer uno para decrementarlo.
-- Si no lo encuentra, ha terminado.
d(m;0,*) = (m;0,*,R,S)
d(m;1,*) = (c;0,*,L,S)
d(m;_,*) = (accept)

-- Estado en el que resta escribiendo los unos anteriores.
d(e;0,*) = (e;1,*,L,S)
d(e;C,*) = (r;e,*,L,S)

-- Estado en el que regresa al principio de la cinta superior.
d(r;0,*) = (r;0,*,L,S)
d(r;1,*) = (r;1,*,L,S)
d(r;_,*) = (c;_,*,R,S)

-- Estado para copiar la palabra. Termina volviendo al estado
-- en el que decrementa un 1.
d(c;0,*) = (c;0,0,R,R)
d(c;1,*) = (c;1,1,R,R)
d(c;C,*) = (m;C,*,R,S)
#+END_SRC

***** Ejercicio 9
#+begin_statement
Describir MTND que acepten los siguientes lenguajes

 a) Conjunto de palabras que contienen una subcadena de longitud 100
    que se repite aunque no necesariamente de forma consecutiva.

 b) El conjunto de las cadenas $w_1 \circ w_2 \circ \dots \circ w_{n}$ donde $w_i \in \left\{ 0,1 \right\}^{\ast}$ y 
    para algÃºn $j$, $w_j$ coincide con la representaciÃ³n en binario de $j$.
 
 c) El conjunto de las cadenas $w_1 \circ w_2 \circ \dots \circ w_{n}$ donde $w_i \in \left\{ 0,1 \right\}^{\ast}$ y 
    para al menos dos valores de $j$, $w_j$ coincide con la representaciÃ³n
    en binario de $j$.

 d) Palabras que contienen a un palÃ­ndromo de longitud mayor o igual
    a 5 como subcadena.
#+end_statement

a) Al principio, a cada paso, la mÃ¡quina puede elegir avanzar una
   casilla o marcarla en una segunda multipista y pasar a un estado en
   el que

   - avanza 100 casillas usando 100 estados dedicados a esto; luego
     puede elegir a cada paso entre avanzar o entrar a un estado
     en el que marca la casilla y entonces simplemente tiene que
     comprobar que haya 100 caracteres iguales empezando desde ambas
     marcas.
     
   Comprobar que hay 100 caracteres iguales empezando de ambas marcas
   puede hacerse memorizando (en el estado) a cada paso un nÃºmero del
   0 al 100 y el caracter despuÃ©s de la primera marca. Se mueve la
   primera marca a la derecha, busca la segunda marca y comprueba que
   sea el mismo; acaba moviendo la segunda marca a la derecha y
   volviendo a buscar el primer caracter.

b) Usamos una mÃ¡quina con dos cintas y empezamos escribiendo un 1 en
   la segunda. En cada paso, la mÃ¡quina puede elegir entre 

   - empezar una subrutina que incremente el valor binario de la
     segunda cinta en 1 y luego mueva la primera cinta hasta el
     prÃ³ximo sÃ­mbolo $\circ$, o

   - empezar una subrutina que comprueba que el nÃºmero de la segunda
     cinta sea igual que el de la primera hasta el prÃ³ximo sÃ­mbolo
     $\circ$; aceptando si lo es.
 
c) Usamos la misma mÃ¡quina del apartado anterior, pero en este caso,
   en lugar de aceptar, vuelve el cabezal de ambas cintas al principio
   de la comprobaciÃ³n, ejecuta la rutina que incrementa y pasa al siguiente
   bloque y pasa a un estado inicial en una rÃ©plica exacta de todos los
   estados de la mÃ¡quina.

d) A cada paso, la mÃ¡quina puede elegir entre moverse a la derecha o
   pasar a un estado en el que
   
     - deja una marca (asumiendo multipista), pasa primero por cinco
       estados numerados en los que sÃ³lo puede moverse a la derecha,
       para llegar a un estado en el que puede elegir si marcar una
       casilla y comprobar palÃ­ndromo entre las dos marcas o avanzar
       una casilla.

   Comprobar un palÃ­ndromo entre dos marcas es algo que podemos hacer
   yendo a la primera marca y empezando el caso de palÃ­ndromos del
   ejercicio 1 como subrutina.

***** Ejercicio 10
Acepta todas las cadenas que no empiecen por 1.

***** Ejercicio 11
#+begin_statement
Un autÃ³mata con pila de fuerza $k$ es un autÃ³mata con $k$ pilas (generalizando
la definiciÃ³n de autÃ³mata con pila). Demostrar que la clase de lenguajes
aceptados por los autÃ³matas con pila de fuerza $k$ para $k \geq 2$ coincide con
la clase de lenguajes aceptados por las MT.
#+end_statement

Si podemos usar mÃ¡s de una pila, dada una MT, podemos hacer que el
autÃ³mata lea la entrada a la primera pila, la pase a la segunda pila
y, a partir de ahÃ­, actÃºe como la MT usando ambas pilas como cinta
para la MT. Pasar a la izquierda o derecha se corresponde con mover
el sÃ­mbolo a una pila u otra.

Un autÃ³mata con pila de fuerza $k$ puede reproducirse con una mÃ¡quina
de Turing con $k$ cintas donde los movimientos de la pila pueden
reproducirse escribiendo o borrando en el tope de esas cintas
y avanzando en la primera, la de entrada.

***** Ejercicio 12
# EJERCICIO HECHO EN CLASE

#+begin_statement
Una MT de escritura simple es una MT que, a lo mÃ¡s, puede escribir en
cada casilla una vez. Demostrar que la clase de lenguajes aceptados por
las MT de escritura simple coincide con la clase de lenguajes aceptados
por las MT.
#+end_statement

Demostraremos que coincide con la clase de lenguajes aceptados por las
MT semilimitadas. Dada una MT semilimitada, tomamos su palabra de entrada
$\rhd a_1a_2\dots a_n$ y despuÃ©s de ella, la escribimos con dos espacios entre cada letra,
$\rhd \#\downarrow a_1\#\#a_2\#\#\dots a_n$.


Ahora, ejecutamos la misma mÃ¡quina en cinta ilimitada pero
moviÃ©ndonos dos pasos a izquierda o derecha cada vez que nos queramos mover
y haciendo lo siguiente cada vez que queramos escribir:

 * memorizamos el sÃ­mbolo que queremos escribir
   nuestro sÃ­mbolo, $\rhd \#\downarrow a_1\#\#a_2\#\#\dots A_j\# a_i \dots \#\# a_n$ y copiamos de nuevo
   la palabra separada por espacios simplemente cambiando el $a_i$ por
   el $a_j$ y marcando a su lado que allÃ­ estÃ¡ el cursor. 

    * $\rhd \#\#a_1\#\#a_2\#\#\dots A_j\#a_i \dots \#\# a_n$
    * $\rhd \#\#a_1\#\#a_2\#\#\dots \#\downarrow a_j \dots \#\# a_n$
 
   Podemos usar los espacios en blanco para ir marcando por dÃ³nde
   vamos en la copia sin tener que escribir dos veces en ninguna
   casilla.  Nos quedamos finalmente en el punto que hayamos cambiado,
   que podemos escribir el Ãºltimo.

NÃ³tese que cada transiciÃ³n con escritura y movimiento puede partirse en
una con escritura y otra con movimiento.  O podemos memorizar el movimiento
y escribir el cursor $\downarrow$ de forma adecuada.

Falta tener en cuenta que para la copia necesitamos saber por dÃ³nde Ã­bamos.

***** Ejercicio 13
#+begin_statement
Sea $L$ el lenguaje que contiene una sola palabra: $0$ si no hay vida fuera
de la tierra y $1$ si hay vida fuera de la tierra. Â¿Es $L$ calculable por una
MT?
#+end_statement

Asumiendo tercio excluso; en ambos casos el lenguaje es computable.

***** Ejercicio 14
#+begin_statement
DiseÃ±ar una MT que dada una palabra $u$ calcule una palabra formada por todos
los sÃ­mbolos que ocupan las posiciones pares de $u$.
#+end_statement

Podemos usar dos cintas.

#+BEGIN_SRC text
d(a;0,*) = (b;0,0,L,L)
d(a;1,*) = (b;1,1,L,L)
d(b;*,*') = (a;*,*',L,S)
#+END_SRC

Similar al ejercicio 15.

***** Ejercicio 15
#+begin_statement
DiseÃ±ar una MT con varias cintas que dada una palabra $u$ calcule una palabra
formada por todos los sÃ­mbolos que ocupan las posiciones pares de $u$ seguidos
por todos los sÃ­mbolos que ocupan las posiciones impares de $u$. Por ejemplo para
la entrada 0101 calcularÃ­a 1100.
#+end_statement

Podemos usar cuatro cintas. La mÃ¡quina tiene dos estados, y a cada paso avanza
en la primera cinta y cambia entre ellos. El estado decide a cuÃ¡l de las cintas
copiarÃ¡. HarÃ¡ esto hasta encontrar un blanco.

Finalmente copiarÃ¡ los contenidos de cada una de las cintas a la cinta final.

**** RelaciÃ³n 2
***** Ejercicio 1
#+begin_statement
Sobre el alfabeto $A = \left\{ a,b,c \right\}$, calcular las palabras $C(143)$ y $C(100)$.
#+end_statement

Tenemos

 * $C(143) = aacbb = 2+3\cdot 2+ 3^2\cdot 3 + 3^3 \cdot 1 + 3^4\cdot 1$,
 * $C(100) = c\;a\;c\;a = 1 + 3 + 3^2 + 3^3 \cdot 3$.

***** Ejercicio 2
#+begin_statement
Sobre el alfabeto $A = \left\{ a,b,c \right\}$, calcular las palabras $Z(aabc)$ y $Z(bac)$.
#+end_statement

Tenemos

 * $Z(aabc) = 3 + 2 \cdot 3 + 1 \cdot 3^2 + 1 \cdot 3^3 = 45$,
 * $Z(bac) = 3 + 1 \cdot 3 + 2 \cdot 3^2= 24$.

***** Ejercicio 3
#+begin_statement
Discutir la posibilidad de asignar un nÃºmero natural a cada MT con independencia
del alfabeto de entrada.
#+end_statement

***** Ejercicio 4
#+begin_statement
Construir una MT que dada una entrada $w$, la convierte en una salida $w111w$.
#+end_statement

Usando multicinta, se copia una vez, se escribe 111 y se copia otra vez.

***** Ejercicio 5
#+begin_statement
Demostrar que el problema de la parada: determinar el conjunto de parejas
$(M,w)$ tales que la MT $M$ para cuando tiene a $w$ como entrada es r.e. pero
no recursivo.
#+end_statement

Para comprobar que es r.e. simplemente usamos que podemos simular una
mÃ¡quina de Turing dada en una entrada dada. Si la simulaciÃ³n para la
mÃ¡quina de Turing original pararÃ­a.

No puede ser recursivo porque en otro caso resolverÃ­amos el problema
de la parada.

# Usando reducciÃ³n de problemas y entrando en bucle si fuÃ©ramos a
# rechazar.

***** Ejercicio 6
# dejando mÃ¡quinas en paralelo

#+begin_statement
Describir de manera informal MTs con varias cintas que enumeren
(produzcan como salida una lista que contenga todas sus palabras)
los siguientes lenguajes (se supone que los nÃºmeros se escriben
en binario):

 1. El conjunto de los cuadrados perfectos.
 2. El conjunto de todos los naturales primos.
 3. El conjunto de todos los nÃºmeros naturales $n$ tales que la
    MT cuya descripciÃ³n es la palabra $w_n$ acepta la palabra $w_n$
    como entrada ($w_n$ es la palabra sobre $\{0,1\}$ cuyo nÃºmero asociado
    es $n$.
#+end_statement

*Apartado 1.* La mÃ¡quina escribirÃ¡ en la segunda cinta un 1 y en la
tercera un 0. Para conseguir cada cuadrado, sumarÃ¡ la segunda cinta con la
tercera dejando el resultado en la tercera e incrementarÃ¡ dos veces el
contenido de la segunda cinta. Finalmente, escribirÃ¡ el contenido de la tercera
en la primera. NÃ³tese que usamos crucialmente que
\[
1 + 3 + \dots + (2n - 1) = n^2.
\]

*Apartado 2.* Conocemos una mÃ¡quina que comprueba si un entero dado
es primo comprobando que no sea divisible por ningÃºn nÃºmero mayor que
1 y menor que Ã©l. La comprobaciÃ³n de divisibilidad $a \mid b$ podrÃ­amos
hacerla escribiendo $0$ en una cinta y a cada paso sumando $a$ de nuevo
a esa cinta y comprobando si el contenido de esa cinta es igual que $b$
(en cuyo caso serÃ­a divisible) o mayor que $b$ (en cuyo caso no lo serÃ­a).

Podemos ir comprobando si cada nÃºmero es primo en otras cintas y, en
caso de que lo sea, escribirlo en la primera.

*Apartado 3.* Podemos simular una mÃ¡quina de Turing arbitraria con
entrada arbitraria con la mÃ¡quina de Turing universal. Lo que
haremos serÃ¡ simular todas las mÃ¡quinas de Turing $w_{n}$ sobre entrada
$w_n$ en paralelo. Para poder simular varias en paralelo cambiaremos de
mÃ¡quina entre ellas guardando el estado de las mÃ¡quinas en una
cinta para pasar a simular otra mÃ¡quina.

En paralelo, ejecutaremos el primer paso de $w_1$ sobre $w_{1}$. DespuÃ©s, 
ejecutaremos el primer paso de $w_2$, el segundo paso de $w_1$ y guardamos
el estado en la cinta. DespuÃ©s ejecutamos el primer paso de $w_3$, el
segundo de $w_2$ y el tercero de $w_1$. AsÃ­ sucesivamente, nos aseguramos
que el n-Ã©simo paso de cualquier mÃ¡quina se ejecutarÃ¡ eventualmente
para cualquier $n$.

Si el n-Ã©simo paso se ejecuta eventualmente, cualquier mÃ¡quina que
pare lo harÃ¡ eventualmente en la simulaciÃ³n; y podremos escribir su
nÃºmero natural asociado en la cinta. NÃ³tese que la salida asÃ­ obtenida
no proporciona necesariamente los nÃºmeros del lenguaje en orden (habrÃ¡
mÃ¡quinas que tarden menos en terminar en la simulaciÃ³n en paralelo que
otras con un nÃºmero asociado menor).

***** Ejercicio 7
#+begin_statement
Sean $L_1,\dots,L_k$ con $k \geq 2$ un conjunto de lenguajes sobre el alfabeto
$A$ tales que:

  1. Para cada $i \neq j$, tenemos que $L_i \cap L_j = \varnothing$.
  2. $\bigcup_{i=1}^k L_i = A^{\ast}$.
  3. $\forall i \in \left\{ 1,\dots,k \right\}$, el lenguaje $L_i$ es r.e.

Demostrar que $\forall i \in \left\{ 1,\dots,k \right\}$, el lenguaje $L_i$ es recursivo.
#+end_statement

Sabemos que $L_i$ es r.e., veremos que $\overline{L_i}$ es tambiÃ©n r.e.; probando
asÃ­ que $L_i$ es recursivo. Usando las dos primeras condiciones, tenemos
que
\[
\overline{L_i} = \bigcup_{j = 1, j \neq i}^{k} L_j.
\]
Como $\overline{L_i}$ es una uniÃ³n finita de lenguajes r.e., serÃ¡ r.e.; para
comprobar esto, podrÃ­amos usar una mÃ¡quina que simule las mÃ¡quinas
que reconocen a cada uno de los lenguajes $L_j$ en paralelo, aceptando
en cuanto una de ellas acepte. Si una palabra pertenece a la uniÃ³n,
pertenecerÃ¡ forzosamente a uno de los lenguajes y una de las mÃ¡quinas
de la simulaciÃ³n pararÃ¡ aceptÃ¡ndola.

***** Ejercicio 8
# usando reducciÃ³n entre L y L' y luego razonando al complementario.
#+begin_statement
Sea $L$ r.e., pero no recursivo. ConsidÃ©rese el lenguaje
\[
L' = \left\{ 0w \mid w \in L \right\} \cup \left\{ 1w \mid w \not\in L \right\}.
\]
Â¿Puede asegurarse que $L'$ o su complementario son recursivos, r.e. o no r.e.?
#+end_statement

El complementario de $L'$ es
\[
\overline{L'} = 
\left\{ 0w \mid w \not\in L \right\} 
\cup 
\left\{ 1w \mid w \in L \right\}
\cup \left\{ \varepsilon \right\}.
\]

Tenemos que $\overline{L}$ puede reducirse al lenguaje $L'$ con una mÃ¡quina de
Turing que toma una palabra $w$ y la desplaza a la derecha para
escribir $1w$. $L'$ no podrÃ­a ser por tanto r.e., porque entonces
lo serÃ­a tambiÃ©n $\overline{L}$ y $L$ serÃ­a recursivo.

AdemÃ¡s, $\overline{L}$ puede reducirse al lenguaje $\overline{L'}$ con una mÃ¡quina de
Turing que toma una palabra $w$ y la desplaza a la derecha para
escribir $0w$. $\overline{L'}$ no podrÃ­a ser por tanto r.e., porque entonces
lo serÃ­a tambiÃ©n $\overline{L}$ y $L$ serÃ­a recursivo.

En particular, $L'$ y $\overline{L'}$ no pueden ser recursivos al no ser
r.e.

***** TODO Ejercicio 9                                                                                  :important:
#+begin_statement
Estudiar si las clases de lenguajes recursivos y r.e. son cerradas para
las siguientes operaciones.

 1. UniÃ³n.
 2. IntersecciÃ³n
 3. ConcatenaciÃ³n
 4. Clausura.
 5. Homomorfismo.
 6. Homomorfismo inverso.
#+end_statement

1. La uniÃ³n de r.e. es r.e.
   La uniÃ³n de recursivos es recursiva.
2. La intersecciÃ³n de r.e. es r.e.
   La intersecciÃ³n de recursivos es recursivo.

***** Ejercicio 10
Rice.

***** Ejercicio 12
#+begin_statement
Demostrar que es indecidible (no recursivo) saber si una MT
termina escribiendo un $1$ cuando comienza con una cinta completamente
en blanco.
#+end_statement

Vamos a reducir el problema $L_U$ a esta propiedad. Dado $(M,w)$,
construimos $M'$ como una mÃ¡quina que empieza escribiendo la
entrada $w$ en la cinta, ejecuta $M$ sobre ella, y cada vez que
llega en $M$ a un estado de aceptaciÃ³n, escribe un $1$ y para.
Cada vez que llega a un estado de rechazo, escribe un $0$ y para.

Entonces, si $M'$ escribe un $1$ y para, es porque $(M,w) \in L_U$; 
si $M'$ no escribe un $1$ y para, es porque $(M,w) \notin L_U$.

***** Ejercicio 13
#+begin_statement
Determinar si los siguientes lenguajes son recursivos, r.e. o
no r.e.:

  1. Determinar si el lenguaje de una MT contiene, al menos, dos
     palabras distintas.

  2. Determinar si el lenguaje de una MT es finito o infinito.

  3. Determinar si el lenguaje de una MT es independiente del
     contexto.
#+end_statement

# EstÃ¡ mal la reducciÃ³n y estÃ¡ corregida en el folio. Â¡Tener cuidado
# con reducciones!

1. Es semidecidible, porque podemos probar pares
   de palabras de forma no determinista y parar cuando acepte
   ambas y sean distintas.

   Por Teorema de Rice, existe una mÃ¡quina de Turing que acepta
   un lenguaje con al menos dos palabras distintas (la que acepta
   todos); y una que acepta un lenguaje sin dos palabras distintas
   (la que rechaza siempre); asÃ­ que esta es una propiedad no trivial
   de los lenguajes r.e. y es indecidible.

2. Reduciremos el complemento del lenguaje universal al lenguaje
   de las MT que reconocen un lenguaje finito.

   Dado un $(M,w)$, construimos $M'$ que para cada entrada $n$
   leÃ­da como un nÃºmero natural, ejecuta $n$ pasos de $M$ sobre
   $w$ y acepta si no ha parado o ha rechazado y rechaza si $M$ para.
   Si $M'$ reconoce un lenguaje finito, entonces existe algÃºn $n_0$
   para el que rechaza, y $(M,w)$ para en $n_0$ pasos. Si $M'$
   reconoce un lenguaje no finito, entonces no puede darse el
   caso de que $M$ pare sobre entrada $w$.

   AsÃ­, el lenguaje de las MT que reconocen un lenguaje finito no
   es r.e.

   Ahora reduciremos el complemento del lenguaje universal al
   lenguaje de las MT que reconocen un lenguaje infinito.

   Dado un $(M,w)$, construimos una $M'$ que para cada entrada $n$
   la lee como un nÃºmero natural y ejecuta $n$ pasos de $M$ sobre
   $w$, aceptando si ha aceptado ya. Si $M'$ reconoce un lenguaje
   infinito, es porque eventualmente $M$ para. Si $M'$ no reconoce
   un lenguaje infinito, es porque $M$ no para nunca.

   AsÃ­, el lenguaje de las MT que reconocen un lenguaje infinito
   no es r.e.

3. Reduciremos el complemento del lenguaje universal al lenguaje
   de las MT que reconocen un lenguaje independiente del contexto.
   Dado $(M,w)$ construimos $M'$ que para cada entrada $a^nb^nc^n$, 
   ejecuta $n$ pasos de $M$ sobre $w$ y acepta si no ha parado o ha
   rechazado. Rechaza si ha parado o si el input no es de la
   forma de la entrada.

   Si $M'$ reconoce un lenguaje libre de contexto, es porque
   reconoce un lenguaje finito y por tanto la mÃ¡quina para en
   algÃºn punto. Si $M'$ reconoce un lenguaje que no es libre de
   contexto, debe ser infinito, luego debe ser porque $M$ no
   para sobre $w$ o lo rechaza.

   Existe una mÃ¡quina de Turing que acepta un lenguaje libre de
   contexto y otra que acepta un lenguaje no libre de contexto.
   Por ejemplo, existen mÃ¡quinas de Turing que aceptan $\left\{ a^nb^nc^n \right\}$,
   que no es libre de contexto. Por Teorema de Rice, saber si
   una mÃ¡quina de Turing acepta un lenguaje libre de contexto es
   indecidible. No es decidible.

***** Ejercicio 14
#+begin_statement
Sea $L$ el lenguaje formado por pares de cÃ³digos de MT mÃ¡s un entero
$(M_1,M_2,k)$ tales que $L(M_1) \cap L(M_2)$ contiene, al menos, $k$ palabras.
Demostrar que $L$ es r.e. pero no recursivo.
#+end_statement

Probaremos que el lenguaje es recursivamente enumerable.
Dada una entrada $(M_1,M_2,k)$, de forma no determinista probaremos
tuplas de la forma $(w_1,\dots,w_k)$; cada una de ellas pudiendo estar
codificada por un solo natural, $n = 2^{w_1}\dots p_k^{w_k}$. Aceptaremos si todas
las $(w_1,\dots,w_k)$ sean aceptadas por $M_1$ y por $M_2$. AsÃ­, el lenguaje
es recursivamente enumerable.

Para probar que no es recursivo, vamos a reducir el problema de
determinar si una mÃ¡quina acepta alguna palabra (que no es recursivo)
a este problema. Lo que hacemos es tomar $M_2$ como una mÃ¡quina que
simplemente acepta siempre y $k=1$. Comprobamos si $L(M_1) \cap L(M_2) = L(M_1)$
contiene alguna palabra. Por tanto, $L$ tampoco es recursivo.

****** Nota
ExplÃ­citamente, lo que queremos es una biyecciÃ³n computable
$\mathbb{N} \to \mathbb{N}^2$.
***** Ejercicio 20
****** Apartado b
Decidible, sÃ³lo consultando las cinco primeras letras.
Simulamos sÃ³lo los 6 primeros pasos.
***** TODO Ejercicio 21 (Interesante para practicar reducciÃ³n)
**** RelaciÃ³n 3
***** Ejercicio 7
#+begin_statement
Construir un programa con variables que dada una cadena $u \in \left\{ 0,1 \right\}^{\ast}$
calcule la cadena $w$ formada por los sÃ­mbolos que ocupan las posiciones
impares de $u$ y en el mismo orden que aparecen en $u$.
#+end_statement

#+BEGIN_SRC text
[A]   IF X ENDS 0 GOTO Iâ
      IF X ENDS 1 GOTO Iâ
      HALT
[Iâ]  X â X-
      Y â 0Y
      GOTO B
[Iâ]  X â X-
      Y â 1Y
      GOTO B
[B]   IF X â  Îµ GOTO P
      HALT
[P]   X â X-
      GOTO A
#+END_SRC

***** Ejercicio 8
#+begin_statement
Construir un programa con variables sobre $\left\{ a,b \right\}$ que dadas dos cadenas
$u_1,u_2 \in \left\{ a,b \right\}^{\ast}$ calcule la cadena $u$ cuyo nÃºmero verifica $Z(u) = Z(u_1) + Z(u_2)$
(es decir, hacer la suma de nÃºmeros representados por cadenas de
caracteres sobre $\left\{ a,b \right\}$).
#+end_statement

Suponemos que tenemos dos variables de entrada X1 y X2 y que la salida
se produce en la variable Y.

#+BEGIN_SRC text
$macro RESTAUNO{X}
  [B]   IF X ENDS a GOTO Aa
        IF X ENDS b GOTO Ab
        HALT
  [Aa]  X â X-
        IF X â  Îµ GOTO Ra
        HALT
  [Ra]  Y â bY
        GOTO B
  [Ab]  X â X-
        Y â aY
        GOTO C
  [C]   IF X ENDS a GOTO Da
        IF X ENDS b GOTO Db
        HALT
  [Da]  X â X-
        Y â aY
        GOTO C
  [Db]  X â X-
        Y â bY
$endmacro
$macro SUMAUNO{X}
  [B]   IF X ENDS a GOTO Aa
        IF X ENDS b GOTO Ab
        Y â aY
        HALT
  [Aa]  X â X-
        Y â bY
        GOTO C
  [Ab]  X â X-
        Y â aY
        GOTO B
  [C]   IF X ENDS a GOTO Da
        IF X ENDS b GOTO Db
        HALT
  [Da]  X â X-
        Y â aY
        GOTO C
  [Db]  X â X-
        Y â bY
$endmacro

[A]   IF X1 â  Îµ GOTO B1
      IF X2 â  Îµ GOTO B2
      HALT
[B1]  RESTAUNO{X1}
      SUMAUNO{Y}
      GOTO A
[B2]  RESTAUNO{X2}
      SUMAUNO{Y}
      GOTO A
#+END_SRC

***** Ejercicio 9
#+begin_statement
Considerar un lenguaje Post Turing para programas con varias cintas. Hay un
nÃºmero finito de cintas y en cada momento una de ellas estÃ¡ activa, inicialmente
la primera. Hay dos instrucciones UP y DOWN que se mueven a la cinta superior
e inferior respectivamente. Demostrar que todo cÃ¡lculo realizado por un programa
Post Turing con varias cintas puede realizarse con un programa Post Turing con
una sola cinta.
#+end_statement

Supongamos un programa $P$ para $n$ cintas y un alfabeto $A$. Consideramos
el lenguaje Post Turing con alfabeto $A^n$ y construimos un programa
que serÃ¡ equivalente.

Empezamos terminando $P$ con la instrucciÃ³n =HALT=, lo que no cambia
su significado (si un programa Post Turing lee una instrucciÃ³n vacÃ­a
para de todas formas). Por otro lado, ademÃ¡s de las etiquetas que ya
existieran, aÃ±adimos una etiqueta nueva en cada instrucciÃ³n posterior
a una ocurrencia de =UP= o =DOWN=; esto, de nuevo, no altera el
comportamiento del programa.

Ahora concatenamos $n$ veces el mismo programa $P$ usando un subÃ­ndice
$i = 1,\dots,n$ para identificar cada una de las $n$ instancias de
cada etiqueta. Cada =UP= en la instancia $i$ del programa se
sustituirÃ¡ por un =GOTO= a la etiqueta despuÃ©s del =UP= correspondiente
en la instancia $i+1$ del programa. Cada =DOWN= en la instancia $i$
del programa se sustituirÃ¡ por un =GOTO= a la etiqueta despuÃ©s del =DOWN=
correspondiente en la instancia $i-1$ del programa. Podemos convenir
que si se ejecuta =DOWN= en la cinta inferior o =UP= en la cinta superior,
simplemente se salta a la siguiente instrucciÃ³n. De esta forma,
la instancia del programa en la que estamos en cada momento marcarÃ¡
la cinta en la que estamos trabajando.

Las instrucciones =PRINT a= en la instancia $i$ serÃ¡n sustituidas por la
siguiente macro, que lo que hace es considerar todas las letras posibles
de $A^n$ y cambiar el elemento en el Ã­ndice $i$ en ellas por una =a=.

#+BEGIN_SRC text
                 IF (....,x,...) GOTO L(....,x,...)
                 IF (....,x,...) GOTO L(....,x,...)
                 ...
                 IF (....,y,...) GOTO L(....,y,...)
                 IF (....,y,...) GOTO L(....,y,...)
                 ...
[L(....,x,...)]  PRINT (....,a,...)
[L(....,x,...)]  PRINT (....,a,...)
...              ...
[L(....,y,...)]  PRINT (....,a,...)
[L(....,y,...)]  PRINT (....,a,...)
...              ...
#+END_SRC

Las instrucciones =IF a GOTO A= en la instancia $i$ serÃ¡n sustituidas por
la siguiente macro, donde diferenciamos en el caso de que la tupla tenga en
el Ã­ndice $i$ una =a=. Llamamos =A(i)= a la etiqueta =A= en la instancia $i$.

#+BEGIN_SRC text
# Para cada tupla que contenga una a en la posiciÃ³n i
      IF (....,a,...) GOTO A(i)
      IF (....,a,...) GOTO A(i)
      ...
#+END_SRC

De esta forma, estamos siempre leyendo y escribiendo sobre una de las
componentes de las tuplas que forman el alfabeto. SÃ³lo las instrucciones
=UP= y =DOWN= cambian la instancia del programa en la que estamos trabajando
y, con ella, la componente del $A^n$ en la que estamos trabajando. El programa
es por tanto equivalente al programa con cinta mÃºltiple.

**** RelaciÃ³n 5
***** Ejercicio 1
#+begin_statement
a) Un grafo dirigido se dice que es acÃ­clico si no tiene ciclos.
   Demostrar que todo grafo dirigido acÃ­clico tiene una fuente
   (un nodo al que no llegan arcos).

b) Demostrar que un grafo dirigido con n nodos es acÃ­clico si
   y sÃ³lo si se pueden numerar los nodos del 1 al n de manera 
   que siempre los arcos van desde nÃºmeros mÃ¡s pequeÃ±os a
   nÃºmeros mÃ¡s grandes.

c) Describir un algoritmo polinÃ³mico para determinar cuÃ¡ndo
   un grafo es acÃ­clico.
#+end_statement

Asumimendo un grafo finito no vacÃ­o.

a) Tomamos un vÃ©rtice del grafo $v_0 \in G$; si no fuera una fuente,
   existirÃ­a otro $v_1$ con un arco hacia Ã©l $v_1 \to v_0$. Iterando este
   proceso llegamos bien a una fuente o a una cadena infinita
   \[
   \dots \to v_n \to v_{n-1} \to \dots \to v_1 \to v_0;
   \]
   donde si tuviÃ©ramos dos vÃ©rtices iguales $v_i = v_j$ para $i \neq j$,
   tendrÃ­amos un ciclo, por lo que deberÃ­an ser todos distintos,
   contradiciendo la finitud del grafo. Debe ser por tanto una
   cadena finita con una fuente al principio.

b) Usaremos inducciÃ³n. Sabemos que se puede hacer para un grafo de
   un solo elemento, asignÃ¡ndole un $0$. Dado un grafo de $n$ elementos
   acÃ­clico, sabemos que tiene una fuente, a la que podemos asignar
   un $0$, numerar el resto del grafo usando la hipÃ³tesis de inducciÃ³n
   y aÃ±adiendo uno a cada uno de los nÃºmeros obtenidos. Sabemos que
   la numeraciÃ³n asÃ­ conseguida hace que todos los arcos entre nodos
   en el resto del grafo vayan de un nÃºmero mÃ¡s pequeÃ±o a uno mÃ¡s
   grande por hipÃ³tesis de inducciÃ³n y porque el aÃ±adir uno es una
   funciÃ³n monÃ³tona creciente. Sabemos que todos los arcos entre
   la fuente y un nodo del resto del grafo, que deben empezar en la
   fuente por definiciÃ³n, van de un nodo numerado con el $0$ a un nodo
   numerado como el sucesor de un natural, que forzosamente debe ser
   mayor que $0$.

c) Asumimos que el grafo estÃ¡ representado por una matriz de adyacencia,
   nÃ³tese que podrÃ­a transformarse desde otras representaciones usuales
   por un algoritmo polinÃ³mico, por lo que no afectarÃ¡ a la demostraciÃ³n.
   Demostraremos que es polinÃ³mico en el nÃºmero de vÃ©rtices, nÃ³tese que
   el tamaÃ±o de la entrada estÃ¡ acotado polinÃ³micamente por el nÃºmero de
   vÃ©rtices, por lo que no afectarÃ¡ a la demostraciÃ³n.

   Para cada vÃ©rtice recorremos la columna de aristas que llegan hacia
   Ã©l para consultar si llega alguna arista a Ã©l. Si es una fuente, no
   puede pertenecer a ningÃºn ciclo; y asÃ­, al retirarla del grafo, no
   estarÃ­amos perdiendo ningÃºn ciclo. Podemos mantener una lista de
   vÃ©rtices retirados y marcar sus entradas en la matriz a cero.

   Si el grafo no tiene fuentes, tendrÃ¡ por tanto un ciclo. Si por el
   contrario acabamos retirando todos los vÃ©rtices del grafo, llegaremos
   a un grafo vacÃ­o, que trivialmente no tiene ciclos.

   Acceder a una posiciÃ³n aleatoria de la matriz representada en la
   cinta se podrÃ­a hacer en ${\cal O}(v^4)$, calculando a quÃ© posiciÃ³n queremos
   acceder dada fila y columna $(f,c)$ como $fv + c$ en ${\cal O}(v^2)$ supuesta la
   multiplicaciÃ³n en ese tiempo y acceder a ella en ${\cal O}(v^2)$ para recorrer
   todas las casillas; leer la fila y columna
   asociada a un vÃ©rtice se podrÃ­a hacer
   entonces en ${\cal O}(v^5)$. Hacer esto para todos los vÃ©rtices se harÃ­a en
   ${\cal O}(v^6)$ y como lo haremos a lo sumo $v$ veces (retirando un vÃ©rtice cada
   vez), tenemos una cota en tiempo polinÃ³mica ${\cal O}(v^7)$.

***** Ejercicio 2
#+begin_statement
a) Demostrar que un grafo es /bipartito/ (se puede dividir
   en dos partes, que no son necesariamente iguales, de manera
   que todas las aristas van de una parte a otra) si y sÃ³lo si
   todos sus ciclos son de longitud par.

b) Describir un algoritmo polinÃ³mico para comprobar si un 
   grafo es bipartito.
#+end_statement

Asumimos un grafo no dirigido.

a) Si tiene un ciclo de longitud impar, serÃ­a imposible
   dividir los vÃ©rtices del ciclo en dos partes de forma
   que ninguno estuviera conectado con vÃ©rtices de la otra
   porque si fijamos la parte en la que estÃ¡ un vÃ©rtice,
   el resto de partes quedarÃ­an determinadas por la paridad
   de pasos desde ese vÃ©rtice y en particular, el Ãºltimo
   vÃ©rtice empezando desde Ã©l, estarÃ­a a un nÃºmero par de
   pasos en el ciclo pero conectado al inicial.

   Si no es bipartito, alguna de sus componentes conexas
   debe no ser bipartita (si todas lo fueran, podrÃ­amos
   partir cada una en dos partes y por conexiÃ³n sabrÃ­amos que
   no hay aristas de una parte a otra). Tomamos una
   componente conexa no bipartita y fijamos un vÃ©rtice $v$. Para
   cualquier otro vÃ©rtice, existirÃ¡ por conexiÃ³n un camino de
   aristas (en cualquier direcciÃ³n) entre ellos, y la paridad
   de este camino determinarÃ¡ su parte. Como el grafo no es
   bipartito, deben existir $w_1,w_2$ conectados a los que hemos
   asignado la misma paridad, luego el ciclo
   \[
   v \dots w_1w_2 \dots v
   \]
   es de longitud impar.

b) Asumimos de nuevo representaciÃ³n por matriz de adyacencia y
   demostraremos que es polinÃ³mico en $v$. Para ello, mantendremos una
   lista con el "color" que hemos asignado a cada vÃ©rtice y tambiÃ©n
   una lista que indica uno de los siguientes tres estados para cada
   vÃ©rtice

    * /no estudiado/, si todavÃ­a no hemos estudiado sus vecinos,
    * /a estudiar/, si hemos estudiado ya uno de sus vecinos, le
      hemos asignado un color y deberÃ­amos seguir estudiando sus
      vecinos, o
    * /estudiado/, si ya lo hemos estudiado.

   A cada iteraciÃ³n, podemos tener vÃ©rtices /a estudiar/ con un
   color ya asignado, en cuyo caso deberÃ­amos empezar por ellos,
   asignando colores a sus vecinos y aÃ±adiÃ©ndolos a la lista de
   vÃ©rtices a estudiar;
   o no tener vÃ©rtices /a estudiar/, en cuyo caso ningÃºn vÃ©rtice
   estÃ¡ determinando el color de ningÃºn vÃ©rtice nuevo y podemos
   tomar cualquiera para continuar.

   Si existe una bicoloraciÃ³n, este algoritmo debe encontrarla porque
   siempre que asume un color nuevo lo hace en un vÃ©rtice que no
   estaba conectado con ninguno anterior y por tanto estÃ¡ en una
   componente conexa nueva, donde, si existe una bicoloraciÃ³n,
   existirÃ¡n dos que quedan fijadas al fijar el color de cualquier
   vÃ©rtice. Si no existe, se llegarÃ¡ en algÃºn momento a encontrar
   dos vÃ©rtices vecinos con el mismo color; ya que si no fuera asÃ­,
   acabarÃ­amos habiendo asignado un vÃ©rtice a cada color y habiendo
   estudiado que todos sus vecinos tienen un color distinto.

   Acceder a la matriz se hace en tiempo polinÃ³mico ${\cal O}(v^4)$, como hemos visto
   en el ejercicio anterior. Buscar el prÃ³ximo vÃ©rtice a estudiar o
   comprobar que no hay ninguno para estudiar se puede hacer en tiempo
   lineal. Recolectar todos los vecinos del vÃ©rtice en la matriz (que
   serÃ¡ simÃ©trica) y marcarlos, se puede hacer en tiempo ${\cal O}(v^2v^{4})$.
   Este proceso tendremos que repetirlo en el peor de los casos para
   cada vÃ©rtice, teniendo una cota ${\cal O}(v^7)$.

***** Ejercicio 3
#+begin_statement
Demostrar que P es cerrada para la uniÃ³n y la intersecciÃ³n.
#+end_statement

Si tenemos dos algoritmos que reconocen una palabra del lenguaje en
tiempo polinÃ³mico, el tiempo de ejecuciÃ³n de ambos secuencialmente
estÃ¡ acotado tambiÃ©n por una suma de polinomios, que vuelve a ser
un polinomio. Ejecutando ambos secuencialmente y aceptando si ambos
aceptan (para la intersecciÃ³n) o si al menos uno de ellos acepta
(para la uniÃ³n), tenemos lo pedido.
***** TODO Ejercicio 4
#+begin_statement
Demostrar que $NP \neq ESPACIO(n)$.
#+end_statement

Hay una reducciÃ³n polinÃ³mica $\pi_1 \propto \pi_2$ teniendo
que $\pi_1 \notin ESPACIO(n)$ y $\pi_2 \in ESPACIO(n)$.

***** Ejercicio 5
#+begin_statement
Determinar cuÃ¡les de las siguientes clases de funciones son cerradas
para la composiciÃ³n polinÃ³mica por la derecha y cuÃ¡les lo son por la
izquierda.

  1. $\left\{ n^k \mid k > 0 \right\}$
  2. $\left\{ nk \mid k > 0 \right\}$
  3. $\left\{ k^n \mid k > 0 \right\}$
  4. $\left\{ 2^{n^{k}} \mid k > 0 \right\}$
  5. $\left\{ \log^k(n) \mid k > 0 \right\}$
#+end_statement

1. Lo es por la izquierda porque $p(n^k)$ con $p$ de grado $q$ vuelve a ser
   un polinomio de grado $kq$. Lo es por la derecha porque $p(n)^k$ tambiÃ©n
   es un polinomio de grado $kq$.

2. Con $p(x) = x^2$ se tiene $p(nk) = n^2k^2$, que no estÃ¡ en la clase. De
   la misma forma se tiene $p(n)k= n^2k^2$, que tampoco lo estÃ¡. No es
   cerrada ni a izquierda ni a derecha.

3. Usando que $(k^n)^{q} = (k^q)^n$, podemos escribir la composiciÃ³n con cualquier
   polinomio por la derecha como suma de funciones de la clase. Es cerrada
   a derecha.

   Sabemos sin embargo que $k^{n^2}$ no estÃ¡ en la misma clase de complejidad que
   ningÃºn $l^n$ porque $\lim_{n \to \infty} log(k^{n^2}/l^n) = \infty$. No es cerrada a izquierda.

4. Para $p$ polinomio de grado $q$ tenemos que $2^{p(n)^k} = {\cal O}(2^{n^{qk}})$, luego la clase
   es cerrada a derecha. AdemÃ¡s $p(2^{n^k}) = {\cal O}(2^q2^{n^k}) = {\cal O}(2^{n^{k}})$, luego es cerrada
   a izquierda.

5. Tenemos $(\log(n^q))^k = q^k\log(n)^k$, luego es cerrada a derecha. AdemÃ¡s,
   $((\log(n))^{k})^{q} = (\log(n))^{kq}$, luego es cerrada a izqueirda.

6. Para $q > 1$ tenemos $\lim_{n \to \infty} \log(n)^q / \log(n) = \infty$, luego no es cerrada
   a izquierda. Por otro lado, $\log(n^q) = q\log(n)$, luego es cerrada a
   derecha.
**** PrÃ¡ctica 4
***** Ejercicio 1.g
#+begin_statement
Demostrar que el siguiente problema es NP-completo.

*Cubrimiento exacto por conjuntos de 4 elementos*. Dado un conjunto finito $X$
con $|X| = 4q$, siendo $q$ entero y una familia $C$ de subconjuntos de 4 elementos
de $X$, Â¿existe una subfamilia $C' \subseteq C$ tal que todo elemento de $X$ pertenece a
uno y sÃ³lo uno de los subconjuntos de $C$?
#+end_statement

Podemos resolverlo de forma no determinista eligiendo una subfamilia
$C'$ de forma no determinista y comprobando si es particiÃ³n, si cumple
las propiedades del enunciado. Esto puede hacerse en tiempo polinÃ³mico
sobre el tamaÃ±o de la entrada porque elegir la subfamilia es
polinÃ³mico, comprobar si un elemento pertenece a un subconjunto es
polinÃ³mico, y tendremos que comprobar para cada elemento si pertenece
a sÃ³lo un conjunto; que se hace en tiempo polinÃ³mico sobre el mÃ¡ximo
entre el nÃºmero de conjuntos y elementos. Es por tanto NP.

Reduciremos ACTRI a este problema. Dados $W,X,Y$ de tamaÃ±o $q$ y un
subconjunto $M$ de compatibilidades, creamos $U = W_1 \cup W_2 \cup X \cup Y$,
una uniÃ³n disjunta de conjuntos donde $W_1$, $W_{2}$ son copias de $W$, y
tomamos $C$ como $\left\{(w,w,x,y) \mid (w,x,y) \in M \right\}$, es decir,
extendemos cada tupla del conjunto de compatibilidades copiando
su primera componente. Este es un proceso que puede hacerse en
espacio logarÃ­tmico simplemente duplicando en la salida parte
de la entrada.

Ahora, $|U| = 4q$ y $C$ es una familia de subconjuntos de 4 elementos de $X$.
Vemos que si existe una subfamilia que es particiÃ³n, entonces, esa misma
subfamilia sin considerar el elemento repetido (nÃ³tese que por construcciÃ³n
todos los elementos de $C$ tienen un elemento repetido) es una particiÃ³n
soluciÃ³n de ACTRI. Si tuviÃ©ramos soluciÃ³n al ACTRI inicial, nÃ³tese que la
misma particiÃ³n, duplicando el Ãºltimo elemento de cada compatibilidad
vuelve a ser soluciÃ³n de este problema, donde el ser disjuntos se mantiene
por ser disjuntos los iniciales (no pueden coincidir en la componente duplicada
porque no coinciden en la componente $W$; y cubren todos los elementos porque
la soluciÃ³n original cubrÃ­a todos los de $W$). AsÃ­, hay soluciÃ³n a este problema
si y sÃ³lo si la hay al ACTRI inicial.

***** Ejercicio 1.h
#+begin_statement
Demostrar que el siguiente problema es NP-completo.

*Conjunto dominante*. Dado un grafo $G=(V,E)$ y un entero positivo $K \leq |V|$,
Â¿existe un subconjunto $V' \subseteq V$ tal que $|V'| \leq K$ y tal que todo vÃ©rtice
$v \in V\setminus V'$ estÃ¡ conectado con al menos un vÃ©rtice de $V$?
#+end_statement

SerÃ¡ NP porque podemos elegir de forma no determinista un subconjunto
de vÃ©rtices y para cada vÃ©rtice, comprobar en tiempo polinÃ³mico si
estÃ¡ unido a alguno de los vÃ©rtices del subconjunto.

Para compbrobar que es NP-completo vamos a reducir ACTRI a Ã©l. Dado un
conjunto de compatibilidades $M$ y un $q$, lo que hacemos es tomar un
grafo que contenga un vÃ©rtice por cada uno de los elementos de $M$
(vÃ©rtices *de compatibilidad*), y por cada uno de los elementos de
$W,X,Y$ (vÃ©rtices *de elemento*). Cada vÃ©rtice de compatibilidad
estarÃ¡ unido a todos los demÃ¡s y ademÃ¡s a los tres vÃ©rtices de
elemento que representen a los elementos que cubra. Este proceso
puede hacerse en espacio logarÃ­tmico porque puede irse escribiendo
el grafo conforme leemos las compatibilidades.

Si hubiera una soluciÃ³n a ACTRI, el tomar los $q$ vÃ©rtices
representando las compatibilidades elegidas es claramente una soluciÃ³n
al problema porque cubrirÃ­an todos los vÃ©rtices de elemento por
definiciÃ³n y ademÃ¡s cubrirÃ­an todos los vÃ©rtices de compatibilidad
porque los hemos unido entre sÃ­.

Si hubiera una soluciÃ³n del conjunto dominante de tamaÃ±o $q$,
entonces, como cada vÃ©rtice de compatibilidad cubre como mÃ¡ximo a tres
vÃ©rtices de elemento y un vÃ©rtice de elemento solo cubre a uno, todos
los vÃ©rtices del conjunto dominante deberÃ­an ser de compatibilidad
(para poder cubrir los $3q$ vÃ©rtices de elemento).  AdemÃ¡s, los
vÃ©rtices de compatibilidad deberÃ­an cubrir a elementos distintos
porque si repitieran, cubrirÃ­an menos de $3q$ vÃ©rtices en total.
AsÃ­, el conjunto dominante debe estar dÃ¡ndonos una elecciÃ³n de
compatibilidades disjuntas que resuelve el problema del ACTRI.

***** Ejercicio 1.k
#+begin_statement
Demostrar que el siguiente problema es NP-completo.

*ParticiÃ³n de conjuntos*. Dada una familia $C$ de subconjuntos de un conjunto
finito $S$ Â¿existe una particiÃ³n de $S$ en dos partes $S_1$ y $S_2$ tales que no hay un
elemento $A \in C$ que estÃ© contenido en $S_1$ o estÃ© contenido en $S_2$ (o equivalentemente
todo $A \in C$ debe de tener intersecciÃ³n no vacÃ­a con $S_1$ y con $S_2$?
#+end_statement

SerÃ¡ un problema NP porque podemos elegir la particiÃ³n en dos subconjuntos
de forma no determinista (marcando cada elemento de $S$ con un bit, por ejemplo),
y luego comprobar en tiempo polinÃ³mico que cada elemento $A \in C$ tiene intersecciÃ³n
no vacÃ­a con $S_1$ y con $S_2$ (tiene un elemento marcado $0$ y otro marcado $1$).

Para demostrar que es NP-completo, reduciremos NAESAT a este problema. Dado
un conjunto de clÃ¡usulas de longitud $3$ sobre un alfabeto, tomamos $S$ como
el conjunto de todos los literales del alfabeto y sus negaciones,
\[
S = \left\{ a,b,c,\dots,\neg a,\neg b,\neg c, \dots \right\},
\]
y tomamos la familia $C$ dada por las parejas de cada literal con su negaciÃ³n,
$\left\{ a,\neg a \right\}$, y por los tres literales que forman cada clÃ¡usula: por ejemplo,
$a \vee b \vee \neg c$ nos darÃ­a el conjunto $\left\{ a,b,\neg c \right\}$. Una soluciÃ³n a este problema
nos darÃ­a dos conjuntos $S_1$ y $S_2$ representando los dos valores de verdad,
y asegurÃ¡ndonos que cada elemento tiene un valor distinto del de su negaciÃ³n,
y que cada clÃ¡usula tiene algÃºn literal cierto y otro falso. De nuevo, la
reescritura de la entrada puede hacerse en espacio logarÃ­tmico, ya que sÃ³lo
tendrÃ¡ que determinar el conjunto y reescribir el formato de las clÃ¡usulas.

***** Ejercicio 2
#+begin_statement
Una MÃ¡quina de Turing no-determinÃ­stica fuerte es una mÃ¡quina que tiene tres
posibles respuestas 'SÃ­', 'No' y 'Duda'. Una de estas mÃ¡quinas decide $L$ si y
solo si, para todo $x \in L$, todos los cÃ¡lculos posibles terminan en 'SÃ­' o 'Duda'
y al menos uno en 'SÃ­' y para todo $x \not\in L$, todos los cÃ¡lculos posibles terminan en 
'No' o 'Duda' y al menos uno en 'No'. Demostrar que $L$ es decidido por una MÃ¡quina
de Turing no-determinÃ­stica fuerte polinÃ³mica si y solo si $L$ estÃ¡ en $\mathrm{NP}\cap \mathrm{coNP}$.
#+end_statement

Supongamos $L$ aceptado por una mÃ¡quina de Turing no-determinÃ­stica
fuerte.  El lenguaje estarÃ­a en $\mathrm{NP}$, porque podrÃ­amos construir una mÃ¡quina
no-determinÃ­stica que funcionara igual que la no-determinÃ­stica fuerte, pero
devolviendo 'No' en los casos de 'Duda'; esta mÃ¡quina aceptarÃ­a $L$, ya que
para todo $x \in L$ existe algÃºn cÃ¡lculo devolviendo 'SÃ­' y para todo $x \notin L$
no existe ninguno.  El lenguaje estarÃ­a en $\mathrm{coNP}$, porque podrÃ­amos construir
una mÃ¡quna no-determinÃ­stica que funcionara igual que la no-determinÃ­stica
fuerte, pero devolviendo 'SÃ­' en los casos de 'No' y 'No' en el resto de
casos; esta mÃ¡quina aceptarÃ­a el complemento de $L$, ya que para todo
$x \notin L$ existe algÃºn cÃ¡lculo devolviendo 'SÃ­' y para todo $x \in L$ no
existirÃ­a ninguno.

Supongamos ahora $L$ en $\mathrm{NP} \cap \mathrm{coNP}$, existirÃ­an mÃ¡quinas no-determinÃ­sticas
aceptando $L$ y aceptando su complemento. Construimos una mÃ¡quina no-determinÃ­stica
fuerte que usarÃ­a ambas secuencialmente sobre la entrada tomando elecciones de
forma no-determinista en ambas. Si la primera aceptara, devolverÃ­amos 'SÃ­';
si la segunda aceptara, devolverÃ­amos 'No'; y si ninguna lo hiciera, devolverÃ­amos
'Duda'. NÃ³tese que si $x \in L$, habrÃ¡ algÃºn caso en el que la primera aceptarÃ¡, y
la segunda no lo harÃ¡ nunca; por el contrario, si $x \notin L$, habrÃ¡ algÃºn caso en el que
la segunda aceptarÃ¡, y la primera no lo harÃ¡ nunca.

**** RelaciÃ³n 6
***** Ejercicio 1

 b. Reducir 3-SET a este problema.
 c. Detectar hamiltonianos se reduce directamente.
 d. Detectar hamiltonianos se reduce poniendo un ciclo en el primer argumento.
 e. Reducimos el problema de la particiÃ³n a Ã©l.
**** PrÃ¡ctica 5
***** Ejercicio 2
#+begin_statement
Demostrar que el umbral de aproximaciÃ³n del problema del mÃ­nimo de
colores es mayor o igual que 4/3 (recordar que saber si un grafo se
puede colorear con 3 o 4 colores es NP-completo).
#+end_statement

Suponiendo que $P \neq NP$, veremos que si existiera un algoritmo
polinÃ³mico con una razÃ³n de eficacia $\delta < 4/3$, entonces
podrÃ­amos resolver el problema de determinar si un grafo se puede
colorear con 3 colores en tiempo polinÃ³mico y habremos llegado a
contradicciÃ³n.

En efecto, si tenemos un algoritmo polinÃ³mico para el problema del
mÃ­nimo de colores con $\delta < 4/3$, dado un grafo, podremos
aplicarlo sobre Ã©l; si obtenemos un resultado menor que $4$, eso
querrÃ­a decir que debe poder colorearse con $3$ colores, y si
obtenemos un resultado igual o mayor que $4$, eso querrÃ­a decir que

\[
\mathrm{opt}(x) \geq \frac{C(m(x))}{\delta} > 3\frac{C(m(x))}{4} \geq 3,
\]

luego el Ã³ptimo deberÃ­a ser mayor estricto que $3$ y no podrÃ­a
colorearse con $3$ colores.  TendrÃ­amos asÃ­ un algoritmo polinÃ³mico
capaz de resolver un problema NP-completo.

***** Ejercicio 3
#+begin_statement
Un problema de la clase NPO es simple si para cada entero positivo $k$
fijo, el problema de decidir si para un ejemplo $x$ el Ã³ptimo es menor
o igual que $k$ estÃ¡ en P. Demostrar que el problema del clique mÃ¡ximo es
simple, pero no el del nÃºmero mÃ­nimo de colores (excepto $\mathrm{NP}= \mathrm{P}$).
#+end_statement

Fijado $k$, podemos crear un algoritmo polinÃ³mico que toma todas las
posibles elecciones de $k+1$ vÃ©rtices del grafo y comprueba si forman
un clique. Hay ${v \choose k+1} = {\cal O}(v^{k+1})$ posibles elecciones y hay que comprobar
en cada una de ellas ${k+1 \choose 2}$ aristas, dÃ¡ndonos un algoritmo polinÃ³mico.
Ahora, si este algoritmo encuentra un clique, sabemos que el Ã³ptimo no
es menor o igual que $k$, ya que en particular tenemos un ejemplo con $k+1$.
Si este algoritmo no encuentra un clique, no puede existir ningÃºn clique
de tamaÃ±o $k+1$ o mayor, porque esto implicarÃ­a la existencia de un
clique de tamaÃ±o $k+1$; asÃ­, el Ã³ptimo debe ser menor o igual que $k$.

En problema del nÃºmero mÃ­nimo de colores no puede ser simple porque ya
sabemos que saber si un grafo se puede colorear con 3 colores (o
menos, que implicarÃ­a que se puede con 3) es un problema NP-completo.

***** Ejercicio 4
#+begin_statement
Demostrar que si un problema tiene un esquema de aproximaciÃ³n polinÃ³mico,
entonces es simple.
#+end_statement

Supongamos que es un problema de minimizaciÃ³n. Fijado un $k$, tomamos
$1 < \delta < (k+1)/k$ y usamos el esquema de aproximaciÃ³n polinÃ³mico
para obtener un algoritmo $m$ que nos da una aproximaciÃ³n de grado $\delta$
del Ã³ptimo de cada ejemplo. Ahora, como

\[
\mathrm{opt}(x) \geq \frac{C(m(x))}{\delta} > \frac{k}{k+1}C(m(x)),
\]

sabemos que si $C(m(x)) \geq k+1$, entonces $\mathrm{opt}(x) > k$; y que si
$C(m(x)) < k+1$, entonces $C(m(x)) \leq k$ y en particular $\mathrm{opt}(x) \leq k$.
Hemos conseguido un algoritmo que determina si el Ã³ptimo es menor o
igual que $k$ en tiempo polinÃ³mico.

 \\

Supongamos ahora que fuera un problema de maximizaciÃ³n. Fijado un $k$,
tomamos $1 < \delta < (k+1)/k$ y usamos el esquema de aproximaciÃ³n polinÃ³mico
para obtener un algoritmo $m$ que nos da una aproximaciÃ³n de grado
$\delta$ del Ã³ptimo de cada ejemplo. Ahora, como

\[
\mathrm{opt}(x) \leq \delta C(m(x)) < \frac{k+1}{k}C(m(x)),
\]

sabemos que si $C(m(x)) > k$, entonces, en particular,
$\mathrm{opt}(x) > k$; y que si $C(m(x)) \leq k$, entonces $\mathrm{opt}(x) < k+1$ y por
tanto $\mathrm{opt}(x) \leq k$. Hemos conseguido un algoritmo que determina si el
Ã³ptimo es menor o igual que $k$ en tiempo polinÃ³mico.

**** PrÃ¡ctica 5 (pistas)
***** Ejercicio 2
Si existiera la aproximaciÃ³n, se resuelve un NP en tiempo polinÃ³mico.

Asumimos P â  NP.

***** Ejercicio 3
El clique es simple porque saber si existe un clique de tamaÃ±o 3 es
polinÃ³mico.

Para cada k fijo, encontrar un clique de tamaÃ±o k es polinÃ³mico.

***** Ejercicio 4
Podemos usar el esquema de aproximaciÃ³n calculando el problema

** MetaheurÃ­sticas
*** Detalles de la asignatura y examen
Transparencias en la pÃ¡gina de la asignatura
http://sci2s.ugr.es/graduateCourses/Metaheuristicas

 * PrÃ¡ctica 1: bÃºsqueda local (2 pts).
 * PrÃ¡ctica 2: bÃºsqueda en poblaciones (2.5 pts).
 * PrÃ¡ctica 3: bÃºsqueda basada en trayectorias (3.5 pts).
 * PrÃ¡ctica opcional: sustituye examen de teorÃ­a.
   * Estudiar metaheurÃ­sticas de reciente creaciÃ³n.
   * Seleccionar un problema a resolver y analizarlo con metaheurÃ­sticas.
   * Documento explicando la tÃ©cnica.
   * ExposiciÃ³n en clase.

MÃ­nima puntuaciÃ³n de 1 en ambas partes (teorÃ­a y prÃ¡cticas) de la
asignatura.

**** Examen

 - Problema (2pt)
 - Cuestiones (3pt)
   - Dar una esquema de una parte del curso. (1pt)

*** Tema 1. IntroducciÃ³n a las metaheurÃ­sticas
Conjunto de tÃ©cnicas para resolver problemas de optimizaciÃ³n.  Una
*heurÃ­stica* es un algoritmo aproximado que depende del problema
concreto; una *metaheurÃ­stica* es un algoritmo aproximado general.

**** 1.1. ResoluciÃ³n aproximada, esquemas de representaciÃ³n
Un problema se debe resolver con metaheurÃ­sticas cuando:

 * Tiene gran complejidad computacional.
 * No se requiere soluciÃ³n exacta.
 * No hay algoritmos eficientes exactos o aproximados especÃ­ficos.

/Ejemplo: viajante de comercio./

Para diseÃ±ar una metaheurÃ­stica se necesita:

 * Una *funciÃ³n objetivo* a minimizar.
 * Un *esquema de representaciÃ³n* de soluciones. Debe facilitar el uso
   de operadores.
 * Un *conjunto de restricciones* sobre el espacio de bÃºsqueda.

Podemos elegir entre varios esquemas de representaciÃ³n.

 - *CodificaciÃ³n binaria*: Representa pertenencia a un conjunto de una
   lista de elementos o satisfacibilidades booleanas.

 - *RepresentaciÃ³n de orden*: Representa la soluciÃ³n como una
   permutaciÃ³n del conjunto $\left\{ 1,\dots,n \right\}$.

 - *Vector de valores discretos*: Representa una asignaciÃ³n como
   un vector de valores discretos.
 - *Vector de valores reales*: Representa una soluciÃ³n en optimizaciÃ³n
   continua global.

**** 1.2. Algoritmos aproximados, elementos de bÃºsqueda
Los *algoritmos aproximados* dan soluciones cercanas a la Ã³ptima
cuando esta no se necesita o serÃ­a ineficiente.  Elementos de un
algoritmo de bÃºsqueda son:

 * *SoluciÃ³n*, representaciÃ³n de valor de salida posible.

 * *Entorno*, soluciones cercanas una vez fijada una estructura
   de espacio mÃ©trico en el espacio de soluciones.

 * *Movimiento*, transformaciÃ³n de soluciÃ³n a una soluciÃ³n en
   algÃºn entorno cercano.

 * *EvaluaciÃ³n*, factibilidad de la soluciÃ³n, dada por la funciÃ³n
   objetivo.

**** 1.3. MetaheurÃ­sticas, clasificaciÃ³n
Las *metaheurÃ­sticas* son una familia de algoritmos aproximados de
propÃ³sito general.

 * Son de propÃ³sito general.
 * FÃ¡cilmente implementables.
 * FÃ¡cilmente paralelizables.

 * No son exactos, son aproximados.
 * No son deterministas.
 * No siempre tienen base teÃ³rica.

Se clasifican en tres grandes grupos.

 1. *MÃ©todos constructivos*. Como GRASP o colonias de hormigas.

 2. *Basadas en trayectorias*. BÃºsqueda local, Enfriamiento simulado,
    BÃºsqueda tabÃº, BÃºsqueda local iterativa.

 3. *Basadas en poblaciones*. GenÃ©ticos, MemÃ©ticos, EvoluciÃ³n diferencial.

**** 1.4. ParalelizaciÃ³n
Tiene tres objetivos

 1. Preservar la calidad y reducir el tiempo de ejecuciÃ³n.
 2. Incrementar la calidad sin aumentar el tiempo de cÃ¡lculo.
 3. Mejorar calidad por el efecto sinÃ©rgico de espaciar la bÃºsqueda.
 
**** 1.5. Aplicaciones
OptimizaciÃ³n, diseÃ±o de circuitos, planificaciÃ³n, trayectorias, control
de procesos quÃ­micos, flotas logÃ­sticas.

*** Tema 2. Modelos de bÃºsqueda. Entornos, trayectorias y poblaciones
**** 2.1. BÃºsqueda local
Decimos que una bÃºsqueda es *local* cuando usamos estructuras de 
entorno derivadas de un espacio mÃ©trico. Una *bÃºsqueda local* es
aquella que dada una soluciÃ³n actual, selecciona iterativamente
una de su entorno para seguir la bÃºsqueda.  Tiene elementos:

 * ElecciÃ³n de soluciÃ³n inicial.
 * Operador de vecino, determina estructura de entornos.
 * Proceso de aceptaciÃ³n de soluciones.

La bÃºsqueda basada en poblaciones regenera la soluciÃ³n a cada
paso.

**** 2.2. BÃºsqueda aleatoria
La *bÃºsqueda aleatoria pura* es muy ineficiente.  Puede considerarse
una bÃºsqueda aleatoria por recorrido al azar, escogiendo el primer
vecino que se encuentre.

**** 2.3. MÃ©todos de bÃºsqueda local bÃ¡sicos

 * *Ascent Hill Climbing*, bÃºsqueda local del mejor; genera todos los
   vecinos y escoge el mejor. Si no hay mejor que lo actual, termina.

 * *Simple Hill Climbing*, bÃºsqueda local del primer mejor; va
   generando vecinos hasta que encuentra alguno mejor que el actual,
   cambia en ese punto.

Es iteresante evitar Ã³ptimos locales, una idea puede ser permitir
empeoramientos, modificar la estructura de entornos o usar bÃºsquedas
multiarranque.

*** Tema 3. MetaheurÃ­sticas basadas en poblaciones I
**** 3.1. ComputaciÃ³n evolutiva
Nos basamos en poblaciones que se reproducen, se seleccionan y mutan.
Una *poblaciÃ³n* selecciona unos *padres*, que se combinan y mutan para
generar *descendientes* que reemplazan a otros en la poblaciÃ³n.

Los paradigmas bÃ¡sicos son:

 1. Algoritmos genÃ©ticos, a nivel de cromosomas.
 2. Estrategias de evoluciÃ³n, a nivel de individuos.
 3. ProgramaciÃ³n evolutiva, a nivel de especies.
 4. ProgramaciÃ³n genÃ©tica, evoluciÃ³n de expresiones sintÃ¡ticas.

Las ventajas son:

 1. Funcionan aceptablemente en amplia variedad de problemas.
 2. Paralelismo intrÃ­nseco.
 3. Superioridad en problemas muy complejos.
 4. FÃ¡ciles de desarrollar e hibridar.

**** 3.2. Algoritmos genÃ©ticos
Los *algoritmos genÃ©ticos* se inspiran en la evoluciÃ³n natural y la
evoluciÃ³n genÃ©tica. Requieren de reproducciÃ³n, selecciÃ³n, cruce y
mutaciÃ³n.
 
***** DiseÃ±o del genÃ©tico
Se construyen:

 1. Decidiendo representaciÃ³n, poblaciÃ³n inicial, cÃ³mo se expresa el
    genotipo en un fenotipo.
 2. Decidiendo operadores mutaciÃ³n, cruce, selecciÃ³n y reemplazamiento.
    La representaciÃ³n debe ser buena para los operadores.
 3. Decidiendo condiciÃ³n de parada.

***** Modelos de evoluciÃ³n
Hay dos *modelos de evoluciÃ³n*.

 * *Modelo generacional*, donde la nueva poblaciÃ³n reemplaza a la
   anterior completamente.

 * *Modelo estacionario*, donde en cada iteraciÃ³n se toman dos padres
   y se aplican operadores genÃ©ticos para reemplazar a algÃºn elemento
   de la poblaciÃ³n anterior.  Es un modelo elitista con una
   convergencia rÃ¡pida.

***** Estrategia de selecciÃ³n
La *estrategia de selecciÃ³n* puede ser.

 * *Torneo*, el mejor entre varios aleatorios.
 * *Orden lineal*, probabilidad ordenada segÃºn fitness.
 * *SelecciÃ³n aleatoria*.
 * *Emparejamiento variado inverso*. Busca un padre lejano al otro,
   introduce diversidad.
 * *Ruleta*. Proporcional al fitness.

***** Operador de cruce
Pueden ser varios, debe tomar caracterÃ­sticas de cada padre (si no, es
mutaciÃ³n); debe producir cromosomas vÃ¡lidos (!) en la representaciÃ³n.

 - Cruce binario.
 - Cruce aritmÃ©tico.
 - Cruce BLX.
 - Cruce de orden OX.
 
***** Operador de mutaciÃ³n
Debe producir soluciones vÃ¡lidas, intentar alcanzar todo el espacio de
bÃºsqueda y producir un cambio controlado no demasiado grande.

 - MutaciÃ³n binaria.
 - MutaciÃ³n real.
 - MutaciÃ³n de orden (intercambio).

***** Estrategia de reemplazamiento
Para modelos estacionarios.

 - Reemplazar al peor, genera mucha presiÃ³n.
 - Torneo restringido, reemplazando al mÃ¡s parecido.
 - Peor entre semejantes, reemplaza al peor entre los parecidos.
 - Crowding determinÃ­stico, reemplaza al padre mÃ¡s parecido.

*Elitismo* es no reemplazar al mejor de la poblaciÃ³n. 

**** 3.3. ExploraciÃ³n y explotaciÃ³n
**** 3.4. Conclusiones
*** Tema 3. MetaheurÃ­sticas basadas en poblaciones II
**** 3.1. OptimizaciÃ³n continua
Nos centramos en optimizar una funciÃ³n de variables reales.  Los algoritmos
mÃ¡s populares son EvoluciÃ³n diferencial, PSO y MemÃ©ticos.

 - Los genÃ©ticos toman ideas de la estructura de espacio vectorial de
   las soluciones (Cruce lineal). Hay operadores de cruce

   - discretos,
   - basados en agregaciÃ³n,
   - basados en vecindario.

   Los primeros usaban operadores componente a componente que no
   capturan interacciones no lineales.

 - CMA-ES. Soluciona un problema de autovalores.

**** 3.2. EvoluciÃ³n diferencial
La *EvoluciÃ³n diferencial* usa un operador de cruce a posteriori de la
mutaciÃ³n y enfatiza Ã©sta.  A partir de cada vector se genera un vector
mutado con un vector diferencia aleatorio escalado.

 - RecombinaciÃ³n discreta.  Cruza el objetivo respecto al mutado con
   ratio predefinido de cruce. Hay otras variantes de recombinaciÃ³n,
   aritmÃ©ticas, exponenciales y binomiales.

**** 3.3. Estrategias de evoluciÃ³n
**** 3.4. PSO (Particle Swarm Optimization)
De las parvadas. Sistema multiagente de soluciones comunicÃ¡ndose
fitness, posiciÃ³n, mejor soluciÃ³n hasta el momento y velocidad; siguen
a la mejor partÃ­cula. Hay dos componentes para ajustar velocidad.

 - *Cognitivo*, peso de la mejor encontrada por la partÃ­cula.
 - *Social*, peso del mejor individuo en la poblaciÃ³n. Puede ser
   *local*, si es el mejor del entorno, o *global* en otro caso.

Modelos.

 - Completo, si sigue ambos.
 - SÃ³lo cognitivo.
 - SÃ³lo social. 
 - Social exclusivo. La partÃ­cula no se sigue a sÃ­ misma aunque sea la
   mejor.

ParÃ¡metros a ajustar.

 - TamaÃ±o de la nube.
 - Velocidad mÃ¡xima.
 - Ratio de aprendizaje.
 - PSO Global o Local: sigue al mejor local o al mejor global.
 - TopologÃ­a de la nube: entornos geogrÃ¡ficos (cercanas en el espacio)
   o entornos sociales (fijadas de antemano). La topologÃ­a social de
   anillo es comÃºn.
 - Introducir inercia.

**** 3.5. OptimizaciÃ³n continua II
La evoluciÃ³n diferencial da buenos resultados en CEC.

***** SHADE
*JADE* es evoluciÃ³n diferencial que tiene parÃ¡metros CR y F
especÃ­ficos para cada individuo, elegidos segÃºn distribuciÃ³n normal y
Cauchy. Se van actualizando en una memoria histÃ³rica. Se usa una media
ponderada para actualizarla. Se necesita un parÃ¡metro para fijar la
greediness de current-to-best.

En *SHADE* el parÃ¡metro $p$ estÃ¡ asociado en cada individuo. *L-SHADE*
utiliza reducciÃ³n lineal de la poblaciÃ³n. Muy robustos.

**** 3.6. Modelos bioinspirados
Numerosas propuestas bioinspiradas de optimizaciÃ³n real. Es necesario
un anÃ¡lisis crÃ­tico.

*** Tema 4. Algoritmos memÃ©ticos
**** 4.1. Hibridar, lÃ­mites de los genÃ©ticos
Los evolutivos son buenos exploradores pero malos explotadores, las
bÃºsquedas locales al revÃ©s.  El comportamiento de cualquiera estÃ¡
limitado por el NoFreeLunch Theorem.  Necesitamos usar conocimiento
especÃ­fico del problema para mejorar. Los memÃ©ticos balancean
evolutivos y bÃºsquedas para ser mÃ¡s robustos.

**** 4.2. DiseÃ±o de memÃ©ticos
EvoluciÃ³n de poblaciones, usa conocimiento en tÃ©rminos de bÃºsquedas
locales. Un *algoritmo memÃ©tico* estÃ¡ dado por una poblaciÃ³n de agentes
alternando periodos de /mejora/ (bÃºsqueda local), /cooperaciÃ³n/ (recombinaciÃ³n)
y /competiciÃ³n/ (selecciÃ³n).

 - Â¿CuÃ¡ndo aplicar bÃºsqueda?

   - En inicializaciÃ³n.
   - Cada cierto nÃºmero de generaciones.
   - Al final del ciclo reproductivo.

 - Â¿Sobre quÃ© individuos aplicar bÃºsqueda?

   - A toda la poblaciÃ³n.
   - A los mejores.
   - Representantes tras una agrupaciÃ³n.
   - ProbabilÃ­sticamente

 - Â¿CÃ³mo se usa el optimizado?

   - Lamarkiano. Reemplaza a su versiÃ³n no optimizada.
   - Baldwiniano. Se toma el fitness del optimizado pero genotipo no
     optimizado.

 - Â¿CÃ³mo se regula?

   - Anchura. Frecuencia de aplicaciÃ³n de la bÃºsqueda.
   - Profundidad. Intensidad de la bÃºsqueda.

**** 4.3. Conclusiones
Explotan el conocimiento disponible, pero no son puristas, teniendo
muchos parÃ¡metros; suelen ser mÃ¡s eficaces que los genÃ©ticos. El
proceso de diseÃ±o es importante.

*** Tema 5. MetaheurÃ­sticas basadas en trayectorias
**** 5.1. Enfriamiento simulado
Criterio de aceptaciÃ³n de soluciones peores, basado en
termodinÃ¡mica. Variable de temperatura que regula el criterio de
aceptaciÃ³n, que tambiÃ©n depende de la diferencia de fitness.

Mecanismos de enfriamiento.

 - Temperaturas descendientes.
 - Descenso constante.
 - Descenso geomÃ©trico.
 - Criterio de Boltzmann.
 - Esquema de Cauchy.

CondiciÃ³n de parada con nÃºmero de iteraciones fijo.

**** 5.2. BÃºsqueda tabÃº (Glover)
Se introduce la posibilidad de introducir movimientos peores. Se
introducen /prohibiciones/ que evitan que se vuelva a buscar en el
mismo sitio. Hay una estructura de memoria adaptativa llamada *lista
tabÃº* que mantiene las zonas buscadas para no volver a ellas,
restringiendo el entorno de bÃºsqueda.

Es costoso mantener todos los visitados.

 - Corto plazo o lista tabÃº, memoria recientemente considerada. Los
   asÃ­ marcados son *tabÃº-activos*. No permanencen siempre en la lista
   hay un intervalo de tiempo de *tenencia tabÃº*. Puede haber niveles
   de aspiraciÃ³n.

   - Lista se soluciones tabÃº. Ya visitadas.
   - Lista de movimientos tabÃº. Vecinos que se eliminan del entorno.
   - Lista de valores tabÃº. Se eliminan los que tienen una caracterÃ­stica.

 - Largo plazo, diversificaciÃ³n a zonas nuevas. GuÃ­a a posteriori,
   despuÃ©s de varias ejecuciones con memoria a corto plazo. Se usa
   para intensificar en zonas usadas o para diversificar la bÃºsqueda 
   a zonas nuevas.  Estrategias de reinicializaciÃ³n cuando la bÃºsqueda
   se estanca.

**** 5.3. Modelos multiarranque
 
**** 5.4. GRASP
Construye soluciones greedy aleatorizadas y aplica bÃºsqueda local
sobre ellas. La construcciÃ³n de la soluciÃ³n inicial ya lleva informaciÃ³n
del problema eligiendo de una lista de candidatos greedy.

**** 5.5. ILS
BÃºsqueda local reiterada. Hay una mutaciÃ³n fuerte que aplica un cambio
brusco para poder seguir buscando.

 - Criterios de aceptaciÃ³n [TODO]

**** 5.6. BÃºsqueda de entorno variable. VNS

 - VND. BÃºsqueda descendente en entornos variables. Se amplÃ­a el
   entorno si no hay nadie mejor.
 - VNS. Es un ILS donde la mutaciÃ³n cambia si la soluciÃ³n mutada
   tras la bÃºsqueda local no es mejor que la actual.

Otros modelos de entornos variables.

***** Diferencia VNS/ILS
La VNS quiere cambiar de entorno a lo largo de la bÃºsqueda. La ILS
quiere construir un camino de soluciones Ã³ptimas locales.

*** Tema 6. MetaheurÃ­sticas basadas en adaptaciÃ³n social
**** Basados en adaptaciÃ³n social: particle swarm optimization
Â¿CÃ³mo se mueven las partÃ­culas por el espacio de bÃºsqueda?

**** Colonias de hormigas
Es bueno al meterlo:

 - Local search.
 - Multiarranque.
 - La reinicializaciÃ³n viene muy bien.

Colonias de hormigas sin local search se estancan y no son
competitivos.

**** Similitudes y diferencias PSO y Colonia de hormigas

 - Uno resuelve problemas reales, colonia de hormigas resuelve
   problemas en grafos.

 - Hormigas basado en estimergia, PSO estÃ¡ basado en trayectorias
   mÃºltiples y en paralelo donde cada elemento es una soluciÃ³n y
   donde uno guÃ­a la bÃºsqueda.

**** PSO pierde frente a Differential Evolution
Differential evolution es la mÃ¡s consistente para variables reales,
despuÃ©s de muchos aÃ±os de desarrollo.

*** Tema 7. Aspectos avanzados en metaheurÃ­sticas
**** 7.1. IntensificaciÃ³n y diversificaciÃ³n

 * IntensificaciÃ³n (explotaciÃ³n), bÃºsqueda local.
 * DiversifiaciÃ³n (exploraciÃ³n), bÃºsqueda global.

Pueden presentarse en componentes *intrÃ­nsecas* (del algoritmo)
o *estratÃ©gicas* (aÃ±adidas al algoritmo). Suelen aplicarse de forma
oscilatoria.

|----------------+-----------------+-----------------------------|
| MetaheurÃ­stica | IntensificaciÃ³n | DiversificaciÃ³n             |
|----------------+-----------------+-----------------------------|
| ES             | BÃºsqueda local  | ModificaciÃ³n de temperatura |
| BT             | BÃºsqueda local  | Lista tabÃº                  |
| GRASP          | BÃºsqueda local  | Lista candidatos            |
| ILS            | BÃºsqueda local  | PerturbaciÃ³n                |
| AGs            | SelecciÃ³n       | Operadores genÃ©ticos        |
|----------------+-----------------+-----------------------------|

***** Equilibrio
 * ES: hay una primera fase de diverisficaciÃ³n y una segunda de
   intensificaciÃ³n, es dinÃ¡mico.
 * BT: es estÃ¡tico, el cuanto mÃ¡s tamaÃ±o mÃ¡s diversidad y menos
   intensificaciÃ³n.
 * ILS: hay intensificaciÃ³n si hay aceptaciÃ³n del mejor y mÃ¡s
   diversificaciÃ³n si se acepta el Ãºltimo local.
 * VNS: balance osiclatorio.
 * AGs: Modelo dinÃ¡mico con diversidad.

***** Componentes estratÃ©gicas
Operadores genÃ©ticos especializados. BÃºsquedas locales especializadas
en el espacio mÃ©trico. Los genÃ©ticos necesitan componentes especÃ­ficas
estratÃ©gicas.

***** Equilibrio oscilatorio
 * Algoritmo genÃ©tico: modelo CHC.
 * BÃºsqueda tabÃº: memoria de frecuencias.

**** 7.2. Algoritmos genÃ©ticos. ExploraciÃ³n y explotaciÃ³n
 * Convergencia: presiÃ³n selectiva, competiciÃ³n padres e hijos.
 * Diversidad: mutaciÃ³n fuerte, dispersiÃ³n de soluciones.

***** Algoritmo CHC
Usa un operador de cruce que crea hijos muy diferentes a los padres.
Componentes estratÃ©gicas

 * *selecciÃ³n elitista*, introduce convergencia al seleccionar sÃ³lo
   las soluciones buenas;

 * *cruce uniforme (HUX)*, intercambia exactamente la mitad de alelos entre
   padres e hijos, garantizando que los hijos tengan distancia Hamming
   mÃ¡xima a los padres; esto puede ralentizar convergencia;

 * *prevenciÃ³n de incesto*, solo cruza parejas que se diferencian en mÃ¡s
   de un nÃºmero determinado de bits; previene la convergencia rÃ¡pida.
   Normalmente se toma un umbral de diferencia a $l/4$ genes y se baja
   en cada generaciÃ³n en la que no se han generado nuevos.
   
 * *reinicializaciÃ³n*, cuando el umbral de cruce baja suficiente, se
   reinicializa; esto da muchÃ­sima diversidad;

   * modificando parcialmente la mejor soluciÃ³n,
   * generando aleatoriamente laa mayorÃ­a de la poblaciÃ³n.

AdemÃ¡s, no usa mutaciÃ³n.

***** Propuesta del TSP
Introducir diversidad cuando no la haya.

 * SelecciÃ³n aleatoria adyacente que potencia diversidad.
 * Probabilidad de cruce 1.
 * Sin mutaciÃ³n.
 * CompeticiÃ³n padres-hijos para tener mÃ¡s presiÃ³n en poblaciÃ³n.
 * DiversificaciÃ³n voraz de la poblaciÃ³n en cada iteraciÃ³n.

Esto mantiene la diversidad constante y alta en la poblaciÃ³n
frente a la implementaciÃ³n normal. Las soluciones repetidas se
eliminan.

**** 7.3. MÃºltiples soluciones. Problemas multimodales
**** 7.4. Comentarios finales
*** Tema 8. MetaheurÃ­sticas paralelas
*** PrÃ¡cticas
**** Elephant Search Algorithm (ESA)

*** PrÃ¡ctica alternativa
El usar focos de luz es un ejemplo de Estimergia.

** Ecuaciones diferenciales I
*** 1. MÃ©todos de integraciÃ³n
**** 1.1. MÃ©todo de variables separadas
**** 1.2. Ecuaciones homogÃ©neas
\[
x' = f\left(\frac{x}{t}\right)
\]
**** 1.3. Ecuaciones reducibles a homogÃ©neas
\[
x' = f \left( \frac{a+bt+cx}{\alpha + \beta t + \gamma x} \right)
\]

segÃºn la posiciÃ³n de las rectas

**** 1.4. Ecuaciones exactas
\[
M + Nx' = 0
\]

tal que existe $F \in {\cal C}^1(D)$ con $\frac{\partial F}{\partial t} = M$ y  $\frac{\partial F}{\partial x} = N$. Tienen
soluciÃ³n trivial $F(t,x) = k$ para $k$ constante.

** Ãlgebra conmutativa y computacional
# Exportaba con config.setup

*** 1. Anillos e ideales
**** La categorÃ­a CRing
***** CategorÃ­a
Consideramos la categorÃ­a de los *anillos conmutativos*, que consta de
los anillos conmutativos como objetos y de los homomorfismos de
anillos; respetando suma, producto y unidad; como morfismos.

***** Z es objeto inicial
El anillo $\mathbb{Z}$ es inicial en =CRing=. El homomorfismo Ãºnico
queda unÃ­vocamente definido con $f(1) = 1$ y $f(n) = nf(1)$.

***** Subanillos
*Subanillo*. Subconjunto cerrado para la suma y el producto
conteniendo a 1.

***** Monomorfismos y epimorfismos
En =CRing= coinciden la inyectividad con ser monomorfismo y la
sobreyectividad con ser epimorfismo.

**** Ideales
***** DefiniciÃ³n
Un *ideal* es un subconjunto cerrado para la suma y el producto de cualquier 
elemento desde $R$.

***** RetÃ­culo de ideales
Los ideales forman un retÃ­culo con la suma y la intersecciÃ³n.
Dos ideales son *primos relativos* cuando suman el anillo.

***** GeneraciÃ³n de ideales
Llamamos $(X)$ al *ideal generado* por $X$; el menor ideal que 
contiene a $X$:

\[ (X) = \bigcap_{X \subset \alpha \text{, ideal}} \alpha\]

Lo llamamos *ideal finitamente generado* cuando $X$ es finito,
cumpliÃ©ndose:

\[ (X) = \left\{ \sum^{finita} r_ix_i \mid r_i \in R, x_i \in X\right\}\]

E *ideal principal* cuando $X$ tiene un sÃ³lo elemento.

***** Producto de ideales
Se define para $\alpha, \beta$ ideales como:

\[ \alpha\beta = \left\{ xy \mid x\in\alpha, y\in\beta \right\}\]

***** Ideales primos relativos
Dos ideales son *primos relativos* cuando $\alpha+\beta = R$. Cuando son 
primos relativos se cumple que $\alpha\beta = \alpha \cap \beta$.

**** Anillo cociente
***** DefiniciÃ³n
Sea $R$ anillo y $\alpha \subset R$ ideal, tomamos la relaciÃ³n de equivalencia
$x \sim y \Leftrightarrow x-y \in \alpha$, para obtener:

\[ R/\alpha = \{x+\alpha \mid x\in R\}\]

***** Ideales del anillo cociente
Los ideales del anillo cociente sobre $\alpha$ estÃ¡n en correspondencia biyectiva
con los ideales que lo contienen, $\beta \supset \alpha$. Son siempre de la forma $\beta/\alpha$.

***** Primer Teorema de IsomorfÃ­a
Para cualquier homomorfismo de anillos $f$:

\[ R/\ker(f) \cong \mathrm{img}(f)\]

Y ademÃ¡s, los ideales estÃ¡n en correspondencia biyectiva por $f_\ast$ y $f^{-1}$:

\[ \{ \alpha \mid \ker(f)\subset\alpha \} 
\longleftrightarrow 
\{ \beta \mid \beta \subset \mathrm{img}(f)\}\]

****** DemostraciÃ³n
Trivial desde la descomposiciÃ³n de morfismos en una categorÃ­a arbitraria.
AdemÃ¡s, se comprueba que $f^{-1}$ preserva ideales y que $f_\ast$ preserva ideales en su 
imagen.

**** Ideales primos y maximales
***** DefiniciÃ³n
$P$ es ideal *primo* si no es el total y $xy\in P \Rightarrow x \in P \vee y \in P$.
$P$ es ideal *maximal* si $M \neq R$ y es maximal en el retÃ­culo de ideales 
sin $R$.

***** CaracterizaciÃ³n de ideales primos y maximales
En relaciÃ³n a su cociente en el anillo:

 - $P$ es primo ssi $R/P$ es un dominio de integridad no trivial.
 - $M$ es maximal ssi $M/P$ es un cuerpo no trivial.

****** DemostraciÃ³n trivial
***** PreservaciÃ³n de ideales primos y maximales
Dado un homomorfismo $f$,

 1. Si $\Pi$ es ideal primo, entonces $f^{-1}(\Pi)$ es ideal primo.
 2. La imagen y preimagen de ideales preserva primos y maximales 
    entre el nÃºcleo y sobre la imagen del anillo.

Como consecuencia los ideales primos (resp. maximales) de un
anillo $R/\alpha$, son los ideales de la forma $\Pi/\alpha$ con $\alpha\subset\Pi$ primo (resp.
maximal).

****** DemostraciÃ³n
*1* es trivial. *2* tenemos que demostrarlo en cuatro pasos:

 - Si $M$ es maximal en $\mathrm{img} f$, entonces $f^{-1}(M)$ es maximal entre los 
   ideales que contienen a $\ker f$.
 - Si $M$ es maximal, entonces $f(M)$ es maximal en $\mathrm{img} f$.
 - Si $\Pi$ es primo en $\mathrm{img} f$, entonces $f^{-1}(\Pi)$ es primo, ya demostrado.
 - Si $\ker f \subset \Pi$ es primo, entonces $f(\Pi)$ es primo en $\mathrm{img} f$.

***** Teorema de Krull
Dados $\alpha \subseteq R$ ideal y $S$ multiplicativamente cerrado con $\alpha\cap S = \varnothing$, 
existe un ideal $M$ tal que:

  - $\alpha \subset M$ 
  - $M \cap S = \varnothing$
  - $M$ es maximal respecto a estas condiciones.

AdemÃ¡s, $M$ es un ideal primo.

****** Subconjunto multiplicativamente cerrado
Es un subconjunto $S$ con:

 - $1 \in S$
 - $x,y\in S \implies xy \in S$

****** DemostraciÃ³n
Dada una cadena de ideales cumpliendo la condiciÃ³n su uniÃ³n tambiÃ©n 
la cumple y es cota de la cadena. Aplicando lema de Zorn obtenemos 
un maximal.

Supongamos $xy \in M$, pero $x,y \notin M$. Entonces $M+(x) \cap S \neq \varnothing$ y
$M + (y) \cap S \neq \varnothing$ por maximalidad, y deben existir $xz,yt \in S$; luego
se tendrÃ­a $xzyt \in M \cap S$, contradicciÃ³n.

***** Corolario al teorema de Krull
Tomando $S=\{1\}$ tenemos que; dado un ideal, existe un ideal maximal que 
lo contiene. En particular, todo elemento no unidad estÃ¡ contenido en un 
ideal maximal, tomando $S = (x)$.

**** Anillos locales
***** InclusiÃ³n en ideales primos
Sean ideales $\alpha_1,\dots,\alpha_n$ y $\pi$ un ideal primo. Si $\bigcap \alpha_i \subset \pi$, entonces,

$\exists i: \alpha_i \subset \pi$.

****** DemostraciÃ³n
Supongamos que $\forall i: \alpha_i \nsubseteq \Pi$; entonces tenemos $x_i \in \alpha_i - \Pi$ tales que ninguno
estÃ¡ en $\Pi$, pero su producto debe estar, contraviniendo que sea primo.

***** InclusiÃ³n de ideales primos
Sean ideales primos $\pi_1,\dots,\pi_n$, (salvo quizÃ¡ 2) y $\alpha$ un ideal. Si $\alpha \subset \bigcup \pi_i$, 
entonces,

$\exists i: \alpha \subset \pi_i$.

****** DemostraciÃ³n
En el caso $n=2$, supongamos $\alpha \subset \pi_1 \cup \pi_2$; y tomemos $x \in \alpha, x\in\pi_1$, $y\in\alpha, y\in\pi_2$.
Entonces tendrÃ­a $x+y \in \alpha$, pero no estarÃ­a en $\pi_1 \cup \pi_2$. NÃ³tese que no usamos
todavÃ­a que sean primos.

En el caso inductivo, aplico la hipÃ³tesis de inducciÃ³n para obtener, para cada $k$:

\[\exists x_k \in \alpha : x_k \notin \bigcup_{i\neq k} \pi_i\]

Luego debe tenerse $x_k \in \alpha_k$; sea ahora $x = x_1x_2\dots x_{n-1}+x_n \in \alpha$; 
luego $\exists r: x\in\alpha_r$. Si $r=n$, tendrÃ­amos $x_1x_2\dots x_{n-1} \in \alpha_r$, y por primalidad deberÃ­a
estar alguno; si no, tendrÃ­amos $x_n \in \alpha_r$.

***** DefiniciÃ³n de anillo local
Un *anillo local* es aquel con un Ãºnico ideal maximal. A $R/M$ se le llama 
*cuerpo residual*.

***** CaracterizaciÃ³n de anillos locales
Un anillo $R$ es local ssi $\{x\in R \mid x \text{ no unidad}\}$ es un ideal.

Sea $M$ ideal propio:

 - $R$ es local con maximal $M$ ssi $R-M \subset {\cal U}(R)$.
 - Si $M$ es maximal y $\{1+x \mid x\in M\}\subset {\cal U}(R)$ entonces es $R$ local y $M$ su maximal.

****** DemostraciÃ³n
1. Una no unidad debe estar contenida en el Ãºnico maximal que hay, que no contiene
   a las unidades y ademÃ¡s es ideal.
   Por otro lado, por Krull, el ideal de las no unidades deberÃ­a estar contenido
   en un ideal maximal que tampoco tuviera unidades, luego debe ser Ã©l mismo.
2. Por la caracterizaciÃ³n anterior tenemos una implicaciÃ³n. Sea $R-M \subset {\cal U}(R)$,
   si tenemos $M \not\subset \beta$, entonces tendrÃ­amos un $x \in \beta,x \notin M$, luego $x \in {\cal U}(R)$
   y entonces $\beta = R$. Si tenemos otro $M'$ maximal, entonces $\exists x\in M': x \notin M$,
   pero eso me vuelve a dar $M' = R$. Luego $M$ es el Ãºnico ideal y maximal.
3. Por la caracterizaciÃ³n anterior tenemos una implicaciÃ³n. Sea $x \notin R-M$,
   tenemos por maximalidad: $rx+m = 1$, luego $rx = 1-m \in {\cal U}(R)$.
   En conclusiÃ³n, $R-M \subset {\cal U}(R)$ y es local.

**** Radicales
***** Nilradical
Sus elementos se llaman *nilpotentes*:

\[\operatorname{Nil}(R) = \{x \in R \mid \exists n: x^n = 0\}\]

El nilradical es un ideal.

***** Anillo reducido
Un anillo es *reducido* si $\operatorname{Nil}(R) = 0$. Los dominios de integridad son 
reducidos. AdemÃ¡s, podemos reducir un anillo dividiendo por su 
nilradical $R/\operatorname{Nil}(R)$.

***** Espectro
El *espectro* de un anillo R es el conjunto de sus ideales primos:

\[\spec(R) = \{\pi\subset R \mid \pi \text{ es un ideal primo}\}\]

***** CaracterizaciÃ³n del nilradical
El nilradical de $R$ es la intersecciÃ³n de los ideales del espectro:

\[ \operatorname{Nil}(R) = \bigcap_{\pi \in \operatorname{Spec}(R)} \pi\]

****** DemostraciÃ³n
Sea $x^n = 0$, entonces $x^n \in \pi, \forall \pi \in \operatorname{Spec}(R)$; y, por primalidad, $x \in \pi$.
Sea $x \notin \operatorname{Nil}(R)$, entonces $S = \{1,x,x^2,\dots\}$ es multiplicativamente cerrado 
con $S \cap \operatorname{Nil}(R) = \varnothing$. Por Krull, tenemos un $\pi$ tal que $\pi \cap S = \varnothing$.

***** Radical de un ideal
Se define como:

\[\sqrt{\alpha} = \{x \in R \mid \exists n \geq 1 : x^n \in \alpha\}\]

Tenemos que $\operatorname{Nil}(R/\alpha) = \sqrt{\alpha}/\alpha$. Cuando $\alpha = \sqrt{\alpha}$ decimos que es *ideal radical*.
Se caracteriza como:

\[\sqrt{\alpha} = \bigcap_{\alpha \subset \pi \in Spec(R)} \pi\]

****** DemostraciÃ³n
Tenemos claramente $\pi^{-1}(\operatorname{Nil}(R/\alpha)) = \sqrt{\alpha}$, pero entonces:

\[\sqrt{\alpha} = 
\pi^{-1}(\operatorname{Nil}(R/\alpha))=
\pi^{-1} \left( 
\bigcap_{\alpha \subset \pi \in \operatorname{Spec}(R/\alpha)} \pi/\alpha 
\right) = \bigcap_{\alpha \subset \pi \in \operatorname{Spec}(R)} \pi\]

***** Radical de Jacobson
Se define el *radical de Jacobson* como:

\[{\cal J}(R) = \bigcap_{{\cal M} \text{ maximal}} {\cal M}\]

****** CaracterizaciÃ³n del radical de Jacobson
Tenemos que $x \in {\cal J}(R)$ ssi $1-xy \in U(R)$ para cualquier $y$.

******* DemostraciÃ³n
Un elemento $1-xy$ para $x \in {\cal J}(R)$ no puede estar en ningÃºn ideal maximal, 
porque estarÃ­a entonces el $1$. Por corolario de Krull, debe ser unidad.

Supongamos $x \notin M$, entonces $(x) + M = R$, y por tanto $1-xy \in M$. Pero 
un maximal no puede contener una unidad.

**** Ideales residuales y anulador
***** Ideales residuales
Definimos el *ideal residual* (a veces llamado *cociente*) de dos 
ideales como:

\[ (\alpha : \beta) = \{x\in R\mid x\beta \subset\alpha\}\]

***** Anulador
\[\operatorname{Ann}(\beta) = \{x \in R \mid x\beta = 0\} = (0 : \beta)\]

**** Ideales contraÃ­dos y extendidos
***** Ideal extendido
Dado un homomorfismo $f : R \longrightarrow S$ llamamos *ideal extendido* de $\alpha$ al ideal:

\[ \alpha^e = (f(\alpha)) = \left\{ \sum s_i f(x_i) \mid s_i \in S, x_i \in \alpha\right\}\]

***** Ideal contraÃ­do
Dado un homomorfismo $f$ llamamos *ideal contraÃ­do* de $\beta$ al ideal:

\[ \beta^c = f(\beta) \]

***** BiyecciÃ³n entre ideales
Para $f : S \longrightarrow R$ homomorfismo y $\alpha,\beta$ ideales:

  1. $\alpha \subset \alpha^e^c$, y $\beta \supset \beta^c^e$.
  2. $\alpha^e = \alpha^e^c^e$, y $\beta^c = \beta^c^e^c$.
  3. Hay una biyecciÃ³n con $(-)^e,(-)^c$ entre ideales contraÃ­dos y extendidos, 
     que ademÃ¡s pueden escribirse como $\{\alpha \subset R \mid \alpha = \alpha^e^c\}$ y $\{\beta \subset S \mid \beta = \beta^c^e\}$.

****** DemostraciÃ³n
Pueden escribirse asÃ­ porque si tengo $\beta = \alpha^e$, entonces $\beta^c^e = \alpha^e^c^e = \alpha^e = \beta$. 
La biyecciÃ³n es trivial desde la definiciÃ³n y las proposiciones anteriores.

**** Producto directo de anillos
***** Producto directo
Para $R_1,R_2,\dots,R_n$ tomamos como su producto directo a:

\[R_1\times \dots \times R_n = \prod_{i=1}^n R_i\]

con las operaciones definidas componente a componente.

***** Proyecciones e inclusiones
Las *proyecciones*, $p_k$, a cualquier factor son homomorfismos.
Las *inclusiones*, $u_k$, *no* son homomorfismos, ya que no respetan
el uno del anillo, que en este caso es $(1,\dots,1)$. Cumplen ademÃ¡s:

 - $p_k \circ u_k = \delta_{kj}id$
 - $\sum u_i \circ p_i = id$
 - $\ker(p_j) = \sum \mathrm{img}(u_j)$

***** Propiedad universal
El producto con las proyecciones es el *producto categÃ³rico* de la
categorÃ­a de los anillos:

\[ \begin{tikzcd}
& & S \dar[dashed]{\exists!} \arrow[bend right]{ddll} \arrow[bend right]{ddl} \arrow[bend left]{ddr} \arrow[bend left]{ddrr} & &\\
& & \prod R \arrow{dll}[swap]{\pi_1} \arrow{dl}{\pi_2} \arrow{dr}[swap]{\pi_3} \arrow{drr} & & \\
R_1 & R_2 &  & R_3 & \dots
\end{tikzcd} \]

***** Teorema Chino del Resto
En la situaciÃ³n de la propiedad universal sobre el cociente por unos
ideales $\alpha_1,\dots,\alpha_n$:

\[ \begin{tikzcd}
\prod_{i=1}^n R/\alpha_i \rar{\pi_i} &  R/\alpha_i \\
R \uar{\exists! f} \urar{p_i}
\end{tikzcd} \]

Tenemos que:

  1. Si los $\alpha_i$ son primos entre sÃ­, $\prod \alpha_i = \bigcap \alpha_i$.
  2. La $f$ es sobreyectiva ssi los $\alpha_i$ son primos entre sÃ­.
  3. La $f$ es inyectiva ssi $\bigcap \alpha_i = 0$. De hecho, $\ker(f) = \bigcap \alpha_i$.

****** DemostraciÃ³n
1. El caso $n=2$ es conocido. En el caso $n>2$, aplicamos la hipÃ³tesis
   de inducciÃ³n a $\alpha_1\alpha_2\dots\alpha_{n-1}$ y a $\alpha_n$, que se demuestran primos relativos.
2. Doble implicaciÃ³n:
   - Si $f$ es sobreyectiva, existe $f(x) = (1,0,0,\dots)$, que me da $x \in \alpha_i$
     para cualquier $i$, y ademÃ¡s, $x-1 \in \alpha_1$; luego $1 \in \alpha_i+\alpha_1$.
   - Si son primos relativos, existen $x_i + y_i = 1$ con $x_i \in \alpha_1$, $y_i \in \alpha_i$.
     $y = y_2y_3\dots y_n = 1 + x$, con $x \in \alpha_1$; luego $y \equiv_{\alpha_1} 1$ pero $y \equiv_{\alpha_i} 0$; luego
     puedo crear una base del anillo.
3. Trivial.

*** 2. Bases de GrÃ¶bner
**** R-Ã¡lgebras
***** R-mÃ³dulo
Dado $R$ anillo. Un *R-mÃ³dulo* (izquierdo) es un grupo abeliano $M$ junto
a una operaciÃ³n $\cdot : (R,M) \longrightarrow M$ verificando:

  - $r(x+y) = rx+ry$
  - $(r+s)x = rx+sx$
  - $r(sx) = (rs)x$
  - $1x = x$

***** R-Ã¡lgebras
Una *R-Ã¡lgebra* $S$ es un anillo con estructura de R-mÃ³dulo, tal que:

\[\forall r\in R, x,y\in S:\; (rx)y = r(xy) = x(ry)\]

***** Homomorfismo de estructura
Equivalentemente, una R-Ã¡lgebra es un anillo $S$ junto a un 
*homomorfismo de estructura* $\lambda : R \longrightarrow S$.

****** Equivalencia
NÃ³tese que puedo pasar de una a otra definiciÃ³n tomando $\lambda(r) = r1$.

***** Homomorfismos de R-Ã¡lgebras
Un homormorfismo de R-Ã¡lgebras $f : S_1 \longrightarrow S_2$ es un homomorfismo de 
anillos que sea tambiÃ©n homomorfismo de R-mÃ³dulos.

\[ f(rs) = rf(s) \]

***** Anillo de polinomios
Definimos el anillo de polinomios en varias variables recursivamente:

\[ R[X_1,X_2,\dots,X_n] = R[X_1,X_2,\dots,X_{n-1}][X_n] \]

Y forma una R-Ã¡lgebra con la inclusiÃ³n desde $R$.

***** Propiedad universal del anillo de polinomios
Sea $S$ anillo y $f : R \longrightarrow S$ homomorfismo de anillos. Sean $s_1,\dots,s_n \in S$
elementos arbitrarios; entonces existe un Ãºnico homomorfismo de 
R-Ã¡lgebras $f_{s_1,\dots,s_n} : R[X_1,\dots,X_n] \longrightarrow S$ tal que:

\[ \begin{tikzcd}
R \rar[hook]{i} \drar[swap]{f} & R[X_1,X_2,\dots,X_n] \dar[dashed]{f_{s_1,\dots,s_n}} \\
  & S
\end{tikzcd} \]

Llevando $f(X_i) = s_i$.

****** TODO DemostraciÃ³n

***** R-Ã¡lgebras finitamente generadas
Una R-Ã¡lgebra $S$ es finitamente generada si existe un epimorfismo de
R-Ã¡lgebras $f : R[X_1,\dots,X_n] \longrightarrow S$.

**** Ãrdenes monomiales
***** RepresentaciÃ³n recursiva de un polinomio
Llamamos representaciÃ³n recursiva a la siguiente:

\[
F = \sum_{j=0}^t F_j X^j_n
\]

donde $F_j \in K[X_1,\dots,X_{n-1}]$. NÃ³tese que es la representaciÃ³n natural una
vez asumimos la definiciÃ³n recursiva del anillo de polinomios.

***** RepresentaciÃ³n distributiva de un polinomio
Llamamos representaciÃ³n distributiva a la siguiente:

\[ p = \sum a_{(\alpha_1,\dots,\alpha_n)} 
X_1^{\alpha_1} X_2^{\alpha_2} \dots X_n^{\alpha_n} \]

Si escribimos los monomios como $X^{\alpha}$ para cada $\alpha \in \mathbb{N}^n$, nos
queda:

\[p = \sum_{\alpha \in \mathbb{N}^n} a_\alpha X^\alpha\]

***** Base de los polinomios
Los monomios forman una K-base vectorial del espacio de polinomios:

\[\{ X^\alpha \mid \alpha \in \mathbb{N}^n \}\]

****** DemostraciÃ³n
Son claramente sistema de generadores y, por definiciÃ³n, un polinomio
con algÃºn coeficiente no nulo no puede ser nunca nulo. NÃ³tese que puede
haber cuerpos donde los polinomios evalÃºen a cero en cualquier punto
del cuerpo, como $X(X-1)$ en $\mathbb{F}_2$, pero ese polinomio no se considera 
nulo.

***** Grado de un monomio
El grado de un monomio $X^\alpha$ con $\alpha \in \mathbb{N}^n$ es la suma:

\[gr(X^\alpha) = \sum^n_{i=1} \alpha_i\]

***** Ãrdenes compatibles
Un orden $\leq$ en $\mathbb{N}^n$ es compatible si:

\[\alpha \leq \beta \Longrightarrow \alpha + \gamma \geq \beta + \gamma\]

***** Ãrdenes monÃ³tonos
Un orden $\leq$ es monÃ³tono si:

\[ 0 \leq \alpha \]

***** Ãrdenes monomiales
Un orden monomial es compatible, monÃ³tono y total.

***** Orden lexicogrÃ¡fico
Definimos $\alpha \geq_{lex} \beta$ cuando para el primer $\alpha_i \neq \beta_i$, se tiene $\alpha_i \geq \beta_i$.

***** Orden lexicogrÃ¡fico graduado
Definimos $\alpha \geq_{grlex} \beta$ cuando $\sum \alpha > \sum \beta$ Ã³ se da la igualdad
y se tiene $\alpha \geq_{lex} \beta$.

***** Orden lexicogrÃ¡fico graduado inverso
Definimos $\alpha \geq_{invgrlex} \beta$ cuando $\sum \alpha > \sum \beta$ Ã³ se da la igualdad
y para el primer $\alpha_i \neq \beta_i$ a la derecha, se tiene $\alpha_i < \beta_i$.

***** PreÃ³rdenes
Un preorden es una relaciÃ³n binaria transitiva y reflexiva.
Equivalentemente, es un orden sin la antisimetrÃ­a.

***** RelaciÃ³n de equivalencia en preÃ³denes
Dado un preorden $\sqsubseteq$, se define la relaciÃ³n de equivalencia $x \equiv y$,
que se tiene cuando $x \sqsubseteq y \wedge y \sqsubseteq x$.

***** Producto lexicogrÃ¡fico de preÃ³rdenes
Definimos el producto lexicogrÃ¡fico de dos preÃ³rdenes $\sqsubseteq_1,\sqsubseteq_2$
como:

\[x \sqsubseteq_{12} y = \left\{
\begin{array}{l} 
x \sqsubseteq_1 y \wedge y \not\sqsubseteq_1 x \\ 
\text{  Ã³} \\
x \equiv_1 y \wedge x \sqsubseteq_2 y
\end{array}\right.\]

***** Propiedades del producto
El producto de preÃ³rdenes cumple:

  1. $\sqsubseteq_{12}$ es un preorden.
  2. $\sqsubseteq_2$ orden $\Rightarrow$ $\sqsubseteq_{12}$ orden
  3. $\sqsubseteq_1,\sqsubseteq_2$ totales $\Rightarrow$ $\sqsubseteq_{12}$ total
  4. $\sqsubseteq_1,\sqsubseteq_2$ compatible $\Rightarrow$ $\sqsubseteq_{12}$ compatible
  5. $\sqsubseteq_1,\sqsubseteq_2$ monÃ³tono $\Rightarrow$ $\sqsubseteq_{12}$ monÃ³tono

****** DemostraciÃ³n
******* Punto 1
Es reflexivo trivialmente. La transitividad se obtiene analizando 
por casos.

******* Punto 2
Cuando $x \equiv_{12} y$, se tiene $x \equiv_1 y$ y por tanto $x \equiv_2 y$; lo que llevarÃ­a
a $x = y$.

******* Punto 3
Si ambos son totales, suponemos s.p.g. que $x \sqsubseteq_1 y$. Si no tuviÃ©ramos
que $y \sqsubseteq_1 x$, entonces $x \equiv_1 y$ y como son totales, podemos suponer s.p.g
que $x \sqsubseteq_2 y$, llgando a $x \sqsubseteq_{12} y$.

******* Punto 4
# No parece obvio si no son Ã³rdenes. AdemÃ¡s, tenemos definido lo que
# es ser compatible o monÃ³tono sÃ³lo para Ã³rdenes.

******* Punto 5
Tenemos $0 \sqsubseteq_1 x$; si tuviÃ©ramos $x \sqsubseteq_1 0$, entonces $x \equiv_1 0$; pero como
sabemos que $0 \sqsubseteq_2 x$, tenemos finalmente $0 \sqsubseteq_{12} x$.

***** Los Ã³rdenes son monomiales
Los Ã³rdenes $\leq_{lex},\leq_{grlex},\leq_{invgrlex}$ son monomiales.

****** DemostraciÃ³n
******* El orden lexicogrÃ¡fico es monomial
Una forma de definir el orden lexicogrÃ¡fico es con el signo del primer
elemento de la resta. Sabemos que es total. La monotonÃ­a y la 
compatibilidad se tienen por:

\[(\alpha+\gamma)-(\beta+\gamma) = (\alpha-\beta)\]

Tenemos que $\alpha - 0 = \alpha$, positivo.

******* El resto de Ã³rdenes son monomiales
Simplemente notando que el grado es un preorden y que son [[*Propiedades del producto][producto]]
de preorden con el lexicogrÃ¡fico o con un lexicogrÃ¡fico inverso.

**** Lema de Dickson
***** Lema de Dickson
Para $S \subseteq \mathbb{N}^n$ no vacÃ­o, existe $G \subseteq S$ finito tal que $S \subseteq G + \mathbb{N}^n$.

****** DemostraciÃ³n
******* Caso base
Cuando $n=1$, como los naturales estÃ¡n bien ordenados, podemos tomar
el mÃ­nimo

******* Caso inductivo
Tomamos un $y \in S$, y tenemos dos casos, o bien $x \in \{y\} + \mathbb{N}^n$, o bien
se tiene $x \in S_{i|j}$ para $j < y_i$. Donde definimos:

\[
S_{i|j} 
= 
\{ x \in S \mid x_i = j\}
\]

Y una familia de conjuntos:

\[
S_{i|j}'
=
\{ 
x \in \mathbb{N}^n
\mid
x_i=0, (x_1,\dots,j,\dots,x_n) \in S
\}
\]

Que por hipÃ³tesis de inducciÃ³n dejando nula a cada paso una de las
coordenadas, dan lugar a $G_{i|j}' \subseteq S_{i|j}'$ finito con $S'_{i|j} = G_{i|j}'+\mathbb{N}^{n-1}$.

\[
G_{i|j} = 
\{x \in S_{i|j} \mid (x_1,\dots,0,\dots,x_n) \in G'_{ij} \}
\]

Sea ahora $x \in S_{i|j}$. Si hacemos un cero en $i$, se tiene 
$(x_1,\dots,0,\dots,x_n) \in S'_{i|j} \subseteq G_{i|j}'+ \mathbb{N}^{n-1}$. Luego

\[\begin{aligned}
(x_1,\dots,0,\dots,x_n) &= z + \alpha \\
(x_1,\dots,j,\dots,x_n) &= (z_1,\dots,j,\dots,z_n) + (\alpha_1,\dots,0,\dots,\alpha_n)
\end{aligned}\]

Por lo tanto, $S_{i|j} \subseteq G_{i|j} + \mathbb{N}^{n}$; y tenemos finalmente $S \subseteq G + \mathbb{N}^n$ si
definimos:

\[G = \{y\} \cup \bigcup_{i,j<y_i} G_{i|j}\]

Tenemos que cada uno de ellos es finito.

***** Orden monomial es buen orden
Todo orden monomial en $\mathbb{N}^n$ es buen orden.

****** DemostraciÃ³n
Por Dickson, cualquier subconjunto tiene un $G$ finito con $S \subseteq G + \mathbb{N}^n$.
El mÃ­nimo de $G$ existe por finitud y ser orden total. Es el mÃ­nimo de
$S$ porque cualquier otro $s \in S$ cumple $s = g + \gamma$ para $g \in G$, lo que lleva
por monotonÃ­a y compatibilidad, a $s \geq g$.

***** Monoideales
Un subconjunto $E \subseteq \mathbb{N}^n$ es monoideal cuando $E = E + \mathbb{N}^n$.

***** Sistemas de generadores
Si $E$ es monoideal, existe $G \subset E$ finito con $E = G + \mathbb{N}^n$.
Llamamos a $G$ sistema de generadores de $E$.

****** DemostraciÃ³n
Por Dickson tenemos $E \subset G + \mathbb{N}^n$, luego $E = E + \mathbb{N}^n = G + \mathbb{N}^n$. 

***** Sistema de generadores minimal
Un sistema de generadores es minimal si ningÃºn subconjunto
suyo es sistema de generadores.

***** Unicidad del sistema de generadores minimal
El sistema de generadores minimal de un monoideal es Ãºnico.

****** DemostraciÃ³n
Supongamos que hubiera dos $G,G'$, con $\beta \in G' \setminus G$. Entonces tendrÃ­a una
representaciÃ³n con $g \in G, g' \in G', \gamma \neq 0$ como:

\[
\beta = g + \gamma = g' + \delta + \gamma
\]

Con lo cual, $\beta \in G'\setminus \{\beta\} + \mathbb{N}^n$, contraviniendo minimalidad.

**** DivisiÃ³n de polinomios
***** Diagrama de Newton
El conjunto de exponentes para un polinomio $p = \sum a_\alpha x^\alpha$:

\[N(p) = \{\alpha\in\mathbb{N}^n \mid a_\alpha \neq 0\}\]

***** Exponente
El exponente de un polinomio, fijado un orden monomial, es el mÃ¡ximo
exponente de su diagrama:

\[exp(p) = \max \{\alpha\in\mathbb{N}^n \mid a_\alpha \neq 0\} \]

***** Grado total
Definimos el grado total, fijado un orden monomial, como el mÃ¡ximo
grado de los exponentes:

\[Grtot(p) = \max \{gr(\alpha) \mid a_\alpha \neq 0\} \]

***** TÃ©rmino lÃ­der
Llamamos tÃ©rmino lÃ­der al que aporta el exponente, $a_{Exp(p)}X^{Exp(p)}$.
Su coeficiente lÃ­der es $a_{Exp(p)}$ y su monomio lÃ­der, $X^{Exp(p)}$.

***** Propiedades del exponente
Dados $F,G \in K[X_1,\dots,X_n]$ no nulos: 

  1. $exp(FG) = exp(F) + exp(G)$.
  2. $exp(F+G) \leq \max\{exp(F),exp(G)\}$.
  3. Si $exp(F) < exp(G)$; entonces $exp(F+G) = exp(G)$.

****** DemostraciÃ³n
En el fondo, parte sÃ³lo de la observaciÃ³n de que son K-espacios 
vectoriales y tienen de base a los distintos monomios.

******* Punto 1
Primero notamos que si estamos en un cuerpo, el producto de dos
polinomios tendrÃ¡ en su diagrama de Newton a la suma de cualesquiera
exponentes de ambos polinomios. Supongamos la suma mÃ¡xima $\gamma + \delta$, y
la suma de los mÃ¡ximos $\alpha+\beta$. Usamos que son mÃ¡ximos y orden monomial
para tener:

\[\alpha + \beta \geq \gamma + \delta\]

******* Punto 2
Si un exponente no aparece en $F$ ni en $G$, tendrÃ¡ coeficiente nulo
tambiÃ©n en $F+G$.

******* Punto 3
La suma tendrÃ¡ como mucho los exponentes de $F$ y de $G$. Pero ademÃ¡s,
conserva los exponentes que sÃ³lo estuvieran en uno de los dos con los
coeficientes intactos.

***** ParticiÃ³n de generadores
Dada una lista de elementos $a_1,\dots,a_k$, tenemos una particiÃ³n que
definimos inductivamente como:

  1. Los elementos que genera el primer generador: 

     \[\Delta^1 = a_1 + \mathbb{N}^n\]

  2. Los elementos nuevos que aporta cada nuevo generador:

     \[\Delta^i = (a_i + \mathbb{N}^n) \setminus \bigcup_{j < i} \Delta^j \]

  3. Todos los demÃ¡s elementos:

     \[ \overline{\Delta} = \mathbb{N}^n \setminus \bigcup_{j \leq k} \Delta^j\]

***** Teorema de la divisiÃ³n
Dado un orden monomial y una lista de polinomios $G_i$; consideramos la
particiÃ³n $\Delta_1,\dots,\overline{\Delta}$ dada por los exponentes $exp(G_i)$. Tenemos que para
cada $F \in K[X_1,\dots,X_n]$, existen $Q_1,\dots,Q_t,R$ Ãºnicos tales:

  1. $F = Q_1G_1 + \dots + Q_tG_t + R$.
  2. $R = 0$ Ã³ $N(R) \subseteq \overline\Delta$.
  3. $exp(G_i) + N(Q_i) \subseteq \Delta^i$.

****** DemostraciÃ³n
******* Caso base
Aplicamos inducciÃ³n a $exp(F)$ aprovechando el [[*Orden monomial es buen orden][buen orden]]. Sea 
$exp(F) = (0,\dots,0)$.

******** Aparece en algÃºn elemento de la particiÃ³n
Si $exp(F) \in \Delta^i$, entonces forzosamente $exp(G_i)=0$. Entonces tomamos
$Q_i = {F}/{G_i}$, y $Q_j=0$ para $j \neq i$.

******** SÃ³lo aparece en el resto de la particiÃ³n
Simplemente tomamos $R = F$.

******* Caso inductivo
Sabiendo que se tiene el resultado para todo $G$ con $exp(G) < exp(F)$,
tenemos de nuevo dos casos.

******** Aparece en algÃºn elemento de la particiÃ³n
Si $exp(F) \in \Delta^i$, entonces $exp(F) = exp(G) + \gamma$. Tomando $H=X^\gamma G$,
aplicamos inducciÃ³n sobre:

\[F - \frac{cl(F)}{cl(H)} H 
=
F'
= 
\sum Q'_iG_i + R'
\]

Pero si tomamos $Q_i = Q_i' + \frac{cl(F)}{cl(H)} X^\gamma$ y $Q_j = Q_j'$ para todos los demÃ¡s:

\[F = \sum Q_i G_i + R\]

******** SÃ³lo aparece en el resto de la particiÃ³n
Aplicamos hipÃ³tesis de inducciÃ³n sobre:

\[F-tl(F) = F' = \sum Q_i'G_i + R'\]

Entonces tomamos $R = R' + tl(F)$ y se tiene:

\[F = \sum Q_i'G_i + R\]

***** Ejemplo de divisiÃ³n
Dividimos $F = X^4Y^2+X^2Y^3Z-2XY^2Z^3-3X^2YZ^4$ entre los polinomios:

  - $G_1 = X^3Y-2X^2Z^2$
  - $G_2 = X^2Y^3+XZ^3$
  - $G_3 = XY^2Z - X^2YZ - 3XYZ^2$

Usando el orden lexicogrÃ¡fico.

****** Exponentes
Con el orden dado, tenemos exponentes:

  - $exp(G_1) = (3,1,0)$
  - $exp(G_2) = (2,3,0)$
  - $exp(G_3) = (2,1,1)$

Con ellos creamos las particiones $\Delta_i$.

****** Desarrollo
A cada paso tomamos el exponente mayor, lo encuadramos en un subconjunto
de la particiÃ³n y dividimos para el siguiente elemento.

\[F = X^4Y^2+X^2Y^3Z-2XY^2Z^3-3X^2YZ^4\]

Tenemos $(4,2,0) \in \Delta_1$, asÃ­ que restamos $XYG_1$:

\[F = 2X^3YZ^2+X^2Y^3Z-2XY^2Z^3-3X^2YZ^4\]

Tenemos $(3,1,2) \in \Delta_1$, asÃ­ que restamos $2Z^2G_1$:

\[F = 4X^2Z^4+X^2Y^3Z-2XY^2Z^3-3X^2YZ^4\]

Tenemos $(2,3,1) \in \Delta_2$, asÃ­ que restamos $ZG_2$:

\[F = 4X^2Z^4 - 2XY^2Z^3 - 3X^2YZ^4 - XZ^4\]

Tenemos $(2,1,4) \in \Delta_3$, asÃ­ que restamos $3Z^3G_3$:

\[F = 4X^2Z^4 - 2XY^2Z^3 - XZ^4 - 3XY^2Z^4 + 9XYZ^5\]

Tenemos $(2,0,4) \in \overline\Delta$, asÃ­ que tomamos lo demÃ¡s como resto. Nos acabarÃ¡
quedando que:

\[R = 4X^2Z^4 - 2XY^2Z^3 - XZ^4 - 3XY^2Z^4 + 9XYZ^5\]

Por lo tanto:

\[F = (XY+2Z^2)G_1 + ZG_2 + 3Z^3G_3 + R\]

****** CÃ¡lculo en Sage
Calculamos el resto de la divisiÃ³n en sage. NÃ³tese cÃ³mo se ve afectado
por el orden de los polinomios que escojamos para la divisiÃ³n.

#+BEGIN_SRC sage
  R.<x,y,z> = PolynomialRing(QQ, 3, 'xyz', order='lex')
  f = x^4*y^2 + x^2*y^3*z - 2*x*y^2*z^3 - 3*x^2*y*z^4
  g1 = x^3*y - 2*x^2*z^2
  g2 = x^2*y^3 + x*z^3
  g3 = x*y^2*z - x^2*y*z - 3*x*y*z^2
  f.reduce(Ideal([g1])).reduce(Ideal([g2])).reduce(Ideal([g3]))
  f.reduce(Ideal([g3])).reduce(Ideal([g2])).reduce(Ideal([g1]))
#+END_SRC

#+RESULTS:
: 4*x^2*z^4 - 3*x*y^2*z^4 - 2*x*y^2*z^3 + 9*x*y*z^5 - x*z^4
: x^2*y^3*z - 3*x^2*y*z^4 + 4*x^2*z^4 - 2*x*y^2*z^3

**** Ideales monomiales
***** Ideales monomiales
Un ideal es monomial si estÃ¡ generado por monomios. Es decir, si es de la 
forma:

\[ I = (X^\alpha \mid \alpha\in A)\]

Para algÃºn $A \subset \mathbb{N}^n$.

***** Pertenencia a ideal monomial
Si $X^\beta \in (X^\alpha \mid \alpha\in A)$ es de la forma: $X^\beta = FX^\alpha$.

****** DemostraciÃ³n
Los monomios forman una K-base del espacio. Como:

\[X^\beta = \sum F_iX^\alpha\]

Todo monomio de la expresiÃ³n de la derecha es divisible por algÃºn $X^\alpha$,
y en particular, $X^\beta$ debe serlo.

***** Monomios en ideal monomial
Sea $I = (X^\alpha \mid \alpha\in A)$ monomial, equivalen:

  1. $F \in I$.
  2. Todo monomio de $F$ estÃ¡ en $I$.
  3. $F$ es combinaciÃ³n lineal de monomios de $I$.

Y ademÃ¡s, si para cualquier polinomio todos sus monomios estÃ¡n en
el ideal, es monomial.

****** DemostraciÃ³n
Los monomios forman una K-base del espacio. Como:

\[F = \sum F_iX^\alpha\]

Todo monomio de $F$ debe ser mÃºltiplo de algÃºn $X^\alpha$.

El resto de implicaciones son triviales. NÃ³tese que podemos escribir
el ideal como $(X^\alpha \mid \alpha \in Exp(I))$.

***** Exponente de un ideal
Dado un ideal $I$ no nulo, definimos su exponente como el subconjunto:

\[Exp(I) = \{Exp(F) \mid 0\neq F \in I\} \subseteq \mathbb{N}^n\]

***** El exponente es monoideal
$Exp(I)$ es un monoideal de $\mathbb{N}^n$.

****** DemostraciÃ³n
Sea $F \in I$, tenemos $X^\alpha F \in I$ para cualquier $\alpha$ y:

\[exp(X^\alpha F) = \alpha + exp(F)\]

Gracias a que los Ã³rdenes son compatibles.

***** Lema de Dickson para ideales monomiales
Todo ideal monomial tiene un sistema de generadores finito y formado
por monomios.

****** DemostraciÃ³n
Como $Exp(I)$ es [[*Monoideales][monoideal]], tiene [[*Sistemas de generadores][sistema de generadores]] $Exp(I) = G + \mathbb{N}^n$.
Dado un exponente $\alpha \in Exp(I)$, [[*Monomios en ideal monomial][tenemos]] $X^\alpha \in I$, luego tomamos como sistema
de generadores:

\[ (X^\alpha \mid \alpha \in G) \]

Dado cualquier $F \in I$, sus monomios estarÃ¡n en $I$ y serÃ¡n mÃºltiplo
de ellos.

***** Corolario de generaciÃ³n de ideales
Sean $I,J \subseteq K[X_1,\dots,X_n]$ ideales monomiales:

\[I=J \iff Exp(I)=Exp(J)\]

****** DemostraciÃ³n
Si tienen el mismo exponente, tienen el mismo generador.

***** Unicidad del sistema de generadores minimal
Sea $I$ monomial no nulo. Existe un Ãºnico sistema de generadores minimal
formado por monomios, es decir, ningÃºn subconjunto suyo es generador.

****** DemostraciÃ³n
Dado $G$ el [[*Unicidad del sistema de generadores minimal][Ãºnico]] sistema generador minimal de $Exp(I)$. Entonces, sus
monomios generan al ideal y si:

\[H \subseteq (X^\alpha\mid \alpha \in G)\]

genera al ideal, en particular se tendrÃ­a $Exp(I) = H + \mathbb{N}^n$, contraviniendo
minimalidad.

***** Otros sistemas de generadores
Sea $G = \{\alpha_1,\dots,\alpha_n\}$ sistema de generadores de $Exp(I)$. Si $exp(F_i)=\alpha_i$,
entonces $\{F_1,\dots,F_n\}$ es sistema de generadores de $I$.

****** DemostraciÃ³n
Para $F\in I$, por [[*Teorema de la divisiÃ³n][divisiÃ³n]] tenemos:

\[F = \sum Q_iF_i + R\]

Como $R\in I$, $exp(R) \in Exp(I)$ y debe tenerse $R = 0$.

**** Bases de GrÃ¶bner
***** Bases de GrÃ¶bner
Sea $I \subseteq K[X_1,\dots,X_n]$ un ideal no nulo. Una base de GrÃ¶bner es un conjunto
$\mathbb{G} = \{G_1,\dots,G_n\} \subseteq I$ cumpliendo $Exp(I) = \{exp(G_1),\dots,exp(G_n)\} + \mathbb{N}^n$.

***** Propiedades de bases de GrÃ¶bner
Las bases de GrÃ¶bner en $K[X_1,\dots,X_n]$ cumplen:

  1. Todo ideal no nulo tiene una base de GrÃ¶bner.
  2. Toda base de GrÃ¶bner de un ideal es base de generadores del ideal.
  3. *Teorema de la base de Hilbert*: todo ideal es finitamente generado.

****** DemostraciÃ³n 
******* Punto 1
Sabemos que $Exp(I)$ es [[*El exponente es monoideal][monoideal]] y por tanto tiene un sistema de
[[*Sistemas de generadores][generadores]]. Existen entonces polinomios cumpliendo:

\[Exp(I) = \{exp(G_1),\dots,exp(G_n)\} + \mathbb{N}^n\]

Que forman una base de GrÃ¶bner

******* Punto 2
Por el algoritmo de la [[*Teorema de la divisiÃ³n][divisiÃ³n]], tenemos que todo polinomio del ideal
se divide entre ellos dando un resto tal que $exp(R) \in \overline{\Delta} \cap Exp(I) = \varnothing$.
Por tanto, $R=0$.

******* Punto 3
Todo ideal estÃ¡ generado por su base de GrÃ¶bner, que es finita.

***** Resto en bases de GrÃ¶bner
Sea $I$ ideal con bases de GrÃ¶bner $\mathbb{G},\mathbb{G}'$. Para $F \in K[X_1,\dots,X_n]$:

\[R(F;\mathbb{G}) = R(F;\mathbb{G}')\]

****** DemostraciÃ³n
Tenemos que, al coincidir los exponentes:

\[Exp(I) = \bigcup_{i=1}^t \Delta^i\]

Por lo tanto, $\overline{\Delta} = \overline{\Delta}'$, y tenemos ${\cal N}(R-R') \subseteq {\cal N}(R) \cup {\cal N}(R') \subseteq \overline{\Delta}$.
Pero como $R-R' \in I$, y tenemos $exp(R-R') \in Exp(I)$, debe ser nulo.

***** CaracterizaciÃ³n de bases de GrÃ¶bner
Sea $I$ ideal no nulo, equivalen:

  1. $\mathbb{G}$ es base de GrÃ¶bner de $I$.
  2. $R(F,\mathbb{G}) = 0$, para todo $F \in I$.

****** DemostraciÃ³n
******* Primer punto al segundo
Tenemos $exp(R) \in Exp(I) \cap \overline{\Delta}$, luego debe ser $exp(R) = 0$.

******* Segundo punto al primero
Por la divisiÃ³n, cualquier $F \in I$ es de la forma:

\[ F = \sum_{i=0}^n Q_iG_i\]

Donde $exp(Q_iG_i) \in \Delta^i$, y como forman una particiÃ³n disjunta, se tiene 
que el $exp(F) = max\{exp(Q_iG_i)\} \notin \overline{\Delta}$. Por tanto $Exp(I) \cap \overline{\Delta} = \varnothing$.

***** Semizigia
Sean $F,G \in K[X_1,\dots,X_n]$ no nulos. Definimos el S-polinomial o *semizigia*
como el siguiente polinomio:

\[{\cal S}(F,G) 
= \frac{1}{cl(F)}X^{\gamma-\alpha}F - \frac{1}{cl(G)}X^{\gamma-\beta}G\]

donde $\alpha = exp(F)$, $\beta = exp(G)$, y $\gamma_i = \max\{\alpha_i,\beta_i\}$, para $\gamma = (\gamma_1,\dots,\gamma_n)$.

***** Teorema de Buchberger
Sea $I$ ideal no nulo de $K[X_1,\dots,X_n]$ y $\mathbb{G} = \{G_1,\dots,G_n\}$ un sistema de
generadores. Equivalen:

  1. $\mathbb{G}$ es base de GrÃ¶bner.
  2. Para cualquier ordenaciÃ³n de $\mathbb{G}$, se tiene $R({\cal S}(G_i,G_j); \mathbb{G}) = 0$
     para cualesquiera $i \neq j$.
  3. Para alguna ordenaciÃ³n de $\mathbb{G}$, se tiene $R(S(G_i,G_j); \mathbb{G}) = 0$.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Trivialmente por la [[*CaracterizaciÃ³n de bases de GrÃ¶bner][caracterizaciÃ³n]] una vez que vemos que las 
semizigias son combinaciones lineales.

******* Tercera implicaciÃ³n
No es trivial. Puede consultarse [[http://people.math.aau.dk/~diego/Libro.pdf][aquÃ­]].

***** Algoritmo de Buchberger
Sea $I$ ideal no nulo y $\mathbb{G} = (G_1,\dots,G_n)$ sistema de generadores. Es posible 
construir una base de GrÃ¶bner con los siguientes pasos:

  1. $\mathbb{G}_0 = \{G_1,\dots,G_n\}$
  2. $\mathbb{G}_{n+1} = \mathbb{G}_n \cup \{R(S(F,G);\mathbb{G}_n) \neq 0 \mid F,G \in \mathbb{G}_n, F \neq G\}$

Entonces cuando $\mathbb{G}_i = \mathbb{G}_{i+1}$, podemos asegurar que $\mathbb{G}_i$ es base de GrÃ¶bner del
ideal $I$.

****** DemostraciÃ³n
Cuando el algoritmo termina, la base que tenemos es claramente de 
GrÃ¶bner debido a la caracterizaciÃ³n que nos da [[*Teorema de Buchberger][Buchberger]].

Hay que demostrar que el algoritmo termina en algÃºn punto. Esto se
deduce de el hecho de que, dado un monoideal, el conjunto de monoideales
que lo contienen es finito. Pero si un elemento es resto de una divisiÃ³n
por $\mathbb{G}$ no nulo, no puede estar en el monoideal generado por los exponentes.
AsÃ­, al aÃ±adirlo tendremos un monoideal generado mayor, y esto sÃ³lo puede
hacerse un nÃºmero finito de pasos.

***** Retirando un polinomio de una base de GrÃ¶bner
Sea $\mathbb{G}$ una base de GrÃ¶bner de un ideal no nulo $I \subseteq K[X_1,\dots,X_n]$ y sea
$F \in \mathbb{G}$ tal que $exp(F) \in \{exp(G) \mid G \in \mathbb{G}, F \neq G \}+\mathbb{N}^n$. Entonces, $\mathbb{G}\setminus\{F\}$
es tambiÃ©n una base de GrÃ¶bner de $I$.

****** DemostraciÃ³n
# Esto no parece necesario. Es trivial desde la definiciÃ³n.
Si $R(F;\mathbb{G}\setminus \{F\})$ estÃ¡ en $\overline\Delta$, que es el mismo que generarÃ­an
con $F$. Pero como estÃ¡ en el ideal, deberÃ­a ser $0$.

Por lo tanto $F$ estÃ¡ generado por $\mathbb{G}$, que es generadora y por tanto,
base de GrÃ¶bner.

***** Base de GrÃ¶bner minimal
Una base de GrÃ¶bner de un ideal no nulo $I \subseteq K[X_1,\dots,X_n]$ se dice minimal
si:

  1. $cl(F) = 1,\quad\forall F\in\mathbb{G}$.
  2. $exp(F) \notin \{exp(G) \mid G \in \mathbb{G}, G \neq F\} + \mathbb{N}^n$.

Es claro que todo ideal tiene una base de GrÃ¶bner minimal, simplemente
[[*Retirando un polinomio de una base de GrÃ¶bner][retirando]] polinomios.

***** CaracterizaciÃ³n de bases minimales
Sea $\mathbb{G}$ un sistema de generadores de $I$. Equivalen:

  1. $\mathbb{G}$ es una base de GrÃ¶bner minimal de $I$.
  2. $\{exp(G_1),\dots,exp(G_t)\}$ es sistema minimal de generadores de $Exp(I)$
     y se tiene $cl(G_i) = 1$.

Los tÃ©rminos lÃ­deres de los polinomios de una base de
GrÃ¶bner minimal estÃ¡n determinados de forma Ãºnica y, ademÃ¡s, dos bases
de GrÃ¶bner minimales tienen el mismo nÃºmero de elementos.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Si no fuera sistema minimal, un subconjunto suyo lo serÃ­a y generarÃ­a
tambiÃ©n $Exp(I)$. Pero entonces, podemos quitar elementos de la base
de GrÃ¶bner y seguirÃ­an generando lo mismo; contraviniendo minimalidad.

******* Segunda implicaciÃ³n
Si no fuera minimal, algÃºn $exp(F)$ serÃ­a generado por los demÃ¡s,
y por tanto los exponentes no formarÃ­an un sistema mnimal.

NÃ³tese que hay que considerar los coeficientes lÃ­deres para que sean
$1$.

******* Unicidad
Como el sistema minimal de generadores de $Exp(I)$ es Ãºnico, los
tÃ©rminos lÃ­deres de los polinomios de una base de GrÃ¶bner minimal
deben ser Ãºnicos.

******* Cardinalidad invariante
Dos bases minimales darÃ­an lugar a dos sistemas minimales de 
generadores, que deben ser iguales y tener la misma cardinalidad.

***** Base de GrÃ¶bner reducida
Una base de GrÃ¶bner $\mathbb{G}$ de $I$ es reducida si para cualquier $\forall F \in \mathbb{G}$:

  1. $cl(F) = 1$.
  2. ${\cal N}(F) \cap \{exp(G) \mid G \in \mathbb{G}, G \neq F\} + \mathbb{N}^n = \varnothing$.

***** Una base de GrÃ¶bner reducida es minimal
Toda base de GrÃ¶bner reducida es minimal.

****** DemostraciÃ³n
Trivialmente, si todo el diagrama de Newton estÃ¡ fuera de lo que
generan los demÃ¡s; el exponente estÃ¡ fuera de lo que generan los demÃ¡s.

***** Existencia y unicidad de la base reducida
Todo ideal no nulo $I$ tiene una Ãºnica base de GrÃ¶bner reducida.

****** DemostraciÃ³n
******* Existencia
Dada una base de GrÃ¶bner minimal, $F \in \mathbb{G}$, veremos que podemos tomar
otra base minimal en la que ${\cal N}(F)$ no estÃ© en los exponentes que 
generan los demÃ¡s. Si dividimos $F$ entre los demÃ¡s polinomios
de la base de GrÃ¶bner:

\[
F = \sum Q_iG_i + F'
\]

Como $exp(F) = \max(\max\{Q_iG_i\},exp(F'))$ y sabemos que por ser minimal
tiene que ser distinto del exponente de los $Q_iG_i$; luego
$exp(F) = exp(F')$.

Entonces sustituyendo $F$ por $F'$ tenemos otra base de GrÃ¶bner minimal
en la que $F$ estÃ¡ reducido. Podemos repetir esto para todos los 
elementos obteniendo una base reducida.

******* Unicidad
Sean bases de GrÃ¶bner reducidas $\mathbb{G}$ y $\mathbb{G}'$. Sea $F \in \mathbb{G}$ y sea $F' \in \mathbb{G}'$ el
que tiene el mismo exponente. Tenemos que si intentamos dividir
por $\mathbb{G}$, por ser ambas reducidas:

\[{\cal N}(F-F') \subseteq \overline{\Delta}\]

Entonces $R(F-F';\mathbb{G}) = F-F'$, pero como $F - F' \in I$, debe 
ser $F - F' = 0$.

**** TeorÃ­a de eliminaciÃ³n
***** Ideal de eliminaciÃ³n
Sea $I$ ideal no nulo. Definimos el j-Ã©simo ideal de eliminaciÃ³n como:

\[ I_j = I \cap K[X_{j+1},\dots,X_n]\]

***** Base del ideal de eliminaciÃ³n
Sea $I$ ideal no nulo y $\mathbb{G}$ base de GrÃ¶bner del orden lexicogrÃ¡fico
dado por $X_1 > X_2 > \dots > X_n$. Entonces:

\[\mathbb{G}_j = \mathbb{G} \cap I_j\]

es base de GrÃ¶bner de $I_j$.

****** DemostraciÃ³n
Sea $F \in I_j$, veremos que $exp(F) = exp(G) + \gamma$ para algÃºn $G \in \mathbb{G}_j$.
Tenemos ya $exp(F) = exp(G) + \gamma$ para $G \in \mathbb{G}$. Como el exponente de $F$ es
nulo en las $j$ primeras variables, entonces $exp(G)$ tambiÃ©n lo hace.
Como estamos usando orden lexicogrÃ¡fico ${\cal N}(G)$ entero se anula en las
$j$ primeras variables. Luego $G \in \mathbb{G}$.

***** ExtensiÃ³n de un ideal
Sea $I$ ideal de $K[X_1,\dots,X_n]$. Un sistema de generadores suyo es
tambiÃ©n sistema de generadores de:

\[
I^e = \left\{
\sum_{i=1}^k Q_iF_i \;\middle|\; Q_i \in K[T,X_1,\dots,X_n], F_i \in I
\right\}
\]

en el espacio $K[T,X_1,\dots,X_n]$.

****** DemostraciÃ³n
Trivialmente por ser generador de $I$.

***** CÃ¡lculo de la intersecciÃ³n
Sean $I,J$ dos ideales no nulos de $K[X_1,\dots,X_n]$. Sea:

\[ H = TI^e + (1-T)J^e \subseteq K[T,X_1,\dots,X_n]\]

Entonces $I\cap J = H \cap K[X_1,\dots,X_n]$, primer ideal de eliminaciÃ³n de $H$.
Esto nos permite calcular la intersecciÃ³n usando bases de GrÃ¶bner.

****** TODO DemostraciÃ³n
Si estÃ¡ en $I\cap J$ tenemos $TF + (1-T)F = F \in H$. Si estÃ¡ en 
$H \cap K[X_1,\dots,X_n]$, entonces es de la forma:

\[TF+(1-T)G = G + (F-G)T\]

Y por tanto debe tenerse $F = G \in I \cap J$.

***** Cociente de ideales
Dados $I,J$ ideales no nulos, definimos el ideal cociente como:

\[(I : J) 
= 
\{F \in K[X_1,\dots,X_n] \mid FJ \subset I\}
=
\bigcap_{i=1}^n (I : G_i)\]

Siendo $J = (G_1,\dots,G_n)$.

****** DemostraciÃ³n
Trivialmente, si incluye al ideal $J$ incluye a todos sus generadores.
Si estÃ¡ en la intersecciÃ³n incluye a todos los generadores por tanto
a todo el ideal.

***** CaracterizaciÃ³n del cociente
Para $G \in K[X_1,\dots,X_n]$ se verifica que $G(I:G) = I \cap (G)$. Luego:

\[(I:G) = \frac{1}{G}(I \cap (G))\]

***** MÃ¡ximo comÃºn divisor y mÃ­nimo comÃºn mÃºltiplo
Sean $F,G \in K[X_1,\dots,X_n]$ con $D = \operatorname{mcd}(F,G)$ y $M = \text{mcm}(F,G)$. Entonces:

  1. $(F) \cap (G) = (M)$
  2. $D = \frac{FG}{M}$

NÃ³tese que la intersecciÃ³n puede calcularse como el primer ideal de
eliminaciÃ³n de:

\[
H = T(F)^e + (1-T)(G)^e = (TF,(1-T)G)
\]

*** 3. Variedades afines
**** Funciones polinÃ³micas
***** Espacio afÃ­n de un Ã¡lgebra
Llamamos $\mathbb{A}^n(K)$ al espacio afÃ­n sobre $K^n$ con la funciÃ³n afÃ­n
$\varphi : K^n \times K^n \longrightarrow K^n$ dada por $\varphi(u,v) = v-u$.

***** Funciones polinÃ³micas
A partir de cada polinomio $F \in K[X_1,\dots,X_n]$, tenemos definida una funciÃ³n
polinÃ³mica $F^\ast : \mathbb{A}^n(K) \longrightarrow K$.

****** Ãlgebra de funciones polinÃ³micas
Al conjunto de funciones polinÃ³micas lo denotamos por $P(\mathbb{A}^n(K))$.
Forma una K-Ã¡lgebra con las operaciones usuales:

\[\begin{aligned}
(F^\ast+G^\ast)(a_1,\dots,a_n) 
&= 
F^\ast(a_1,\dots,a_n)+G^\ast(a_1,\dots,a_n)
\\
(F^\ast G^\ast)(a_1,\dots,a_n) 
&= 
F^\ast(a_1,\dots,a_n)G^\ast(a_1,\dots,a_n)
\\
(\alpha F^\ast)(a_1,\dots,a_n) 
&= 
\alpha F^\ast(a_1,\dots,a_n)
\end{aligned}\]

***** Epimorfismo de los polinomios a las funciones polinÃ³micas
Por definiciÃ³n se tiene un epimorfismo $\Delta : K[X_1,\dots,X_n] \longrightarrow P(\mathbb{A}^n(K))$ 
dado por:

\[\Delta(F) = F^\ast\]

****** No es inyectivo en general
La funciÃ³n dada por un polinomio no nulo puede ser nula. Por ejemplo:

\[F = X(X+1)\]
\[G = 0\]

Dan lugar a la misma funciÃ³n en $\mathbb{F}_2$, pero son polinomios distintos.

***** Isomorfismo de los polinomios en cuerpos infinitos
Sea $K$ cuerpo infinito. Sus polinomios verifican:

  1. $F^\ast = 0 \iff F = 0$.
  2. $F^\ast=G^\ast \iff F = G$.

****** DemostraciÃ³n
Un polinomio que de la funciÃ³n constante $0$ en un cuerpo infinito 
deberÃ­a tener infinitas raÃ­ces. Eso sÃ³lo puede darlo el polinomio
constantemente $0$.

**** Variedades afines
***** Variedad de un polinomio
Dado $F \in K[X_1,\dots,X_n]$, denotamos por $\mathbb{V}(F)$ al conjunto de sus ceros:

\[\mathbb{V}(F) = \Big\{(a_1,\dots,a_n) \in \mathbb{A}^n(K) \;\Big|\; F(a)=0 \Big\}\]

Dado un ideal ${\cal F}$, denotamos por $\mathbb{V}({\cal F})$ al conjunto de sus ceros:

\[\mathbb{V}({\cal F})
=
\Big\{(a_1,\dots,a_n) \in \mathbb{A}^n(K) \;\Big|\; F(a)=0\; \forall F \in {\cal F} \Big\}\]

***** Variedad algebraica afÃ­n
Un conjunto es una *variedad algebraica afÃ­n* cuando existe
${\cal F} \subseteq K[X_1,\dots,X_n]$ tal que es de la forma $\mathbb{V}({\cal F})$.

***** Variedad de un ideal
Sea ${\cal F} \subseteq K[X_1,\dots,X_n]$ y sea ${\cal I} = ({\cal F})$ el ideal generado. Se tiene:

  1. $\mathbb{V}({\cal F}) = \mathbb{V}({\cal I})$
  2. $\exists F_1,\dots,F_t : \mathbb{V}({\cal F}) = \mathbb{V}(F_1,\dots,F_t)$

****** DemostraciÃ³n
******* Punto 1
Si los generadores se anulan en un punto, sus combinaciones lineales
tambiÃ©n.

******* Punto 2
Todo ideal en el anillo de polinomios es finitamente [[*Propiedades de bases de GrÃ¶bner][generado]]
gracias a las bases de GrÃ¶bner.

***** Propiedades de las variedades
Las variedades dadas por ideales cumplen:

  1. \[\bigcap_{\lambda \in \Lambda} \mathbb{V}(J_\lambda)
     = \mathbb{V}\left(
     \sum_{\lambda \in \Lambda} J_\lambda
     \right)\]

  2. $\mathbb{V}(J_1) \cup \mathbb{V}(J_2) = \mathbb{V}(J_1J_2) = \mathbb{V}(J_1 \cap J_2)$

  3. $\mathbb{V}(0) = \mathbb{A}^n(K)$

  4. $\mathbb{V}(K[X_1,\dots,X_n]) = \varnothing$

****** DemostraciÃ³n
******* Punto 1
Si un punto se anula para todos los polinomios de cada $J_\lambda$, en
particular estÃ¡ en cada $\mathbb{V}(J_\lambda)$. Si estÃ¡ en cada $\mathbb{V}(J_\lambda)$, se anula
para todos sus polinomios y por tanto para sus combinaciones
lineales.

******* Punto 2
Si $x \notin \mathbb{V}(J_1) \cup \mathbb{V}(J_2)$, entonces existirÃ­an polinomios en cada ideal
para los que no serÃ­a cero, y no serÃ­a cero de su producto, luego
$x \notin \mathbb{V}(J_1J_2)$.

El resto de los contenidos son triviales, viendo:

\[\mathbb{V}(J_1) \cup \mathbb{V}(J_2) \subseteq
\mathbb{V}(J_1\cap J_2) \subseteq
\mathbb{V}(J_1J_2)\]

******* Punto 3
Todos los puntos son raÃ­ces del constante $0$.

******* Punto 4
NingÃºn punto es raÃ­z de todos los polinomios, porque existen los
polinomios constantes no nulos en cualquier cuerpo.

***** TopologÃ­a de Zariski
Las variedades son los cerrados de una topologÃ­a sobre $\mathbb{A}^n(K)$.

\[\{
V \subseteq \mathbb{A}^n(K) 
\mid
V \text{ es variedad algebraica afÃ­n}
\}\]

****** DemostraciÃ³n
Desde las propiedades de una [[*Variedad de un ideal][variedad]] tenemos que la 
intersecciÃ³n arbitraria y la uniÃ³n finita de variedades son variedad.
AdemÃ¡s, lo son el vacÃ­o y el total.

***** Ejemplos
****** Puntos discretos
Dado un conjunto discreto de puntos:

\[\{(a_1,\dots,a_n)\} = \mathbb{V}(X_1-a_1,\dots,X_n-a_n)\]

****** Hipersuperficies
Toda variedad generada por un polinomio no constante:

\[
\mathbb{V}(F) = \Big\{ 
\alpha \in \mathbb{A}^n(K) \mid F(\alpha) = 0
\Big\}
\]

Toda variedad afÃ­n es una intersecciÃ³n finita de hipersuperficies.

****** Hiperplano
La variedad de un polinomio de grado total 1 es un hiperplano.
Para $F = a_0 + a_1X_1 +a_2X_2 + \dots + a_nX_n$, tenemos:

\[
\mathbb{V}(F) = \Big\{
\alpha \in \mathbb{A}^n(K) 
\mid 
a_0 + a_1\alpha_1 + a_2\alpha_2 + \dots + a_n\alpha_n = 0
\Big\}
\]

La intersecciÃ³n finita de hiperplanos es una variedad afÃ­n lineal.

**** Teorema de los ceros de Hilbert
***** Representaciones paramÃ©tricas
Una *representaciÃ³n paramÃ©trica racional* de $\mathbb{V}(F_1,\dots,F_t)$ es un conjunto
de funciones $G_i,H_i \in K[X_1,\dots,X_n]$ cumpliendo:

\[\left\{ 
\frac{G_1}{H_1}(t_1,\dots,t_n),
\frac{G_2}{H_2}(t_1,\dots,t_n),
\dots,
\frac{G_n}{H_n}(t_1,\dots,t_n)
\;\middle|\;
t_1,\dots,t_n \in \mathbb{K}
\right\}
\subseteq
V\]

Cuando $H_i = 1$, la llamamos *representaciÃ³n polinomial*.

****** Ejemplos
******* RepresentaciÃ³n no racional del cÃ­rculo
El cÃ­rculo es una variedad de la que puede darse una representaciÃ³n
no paramÃ©trica.

\[V = \{(sen(t), cos(t)) \midt \in \mathbb{R}\}\]

Sabemos que es variedad porque puede expresarse como:

\[ V = \mathbb{V}(X^2+Y^2-1) \]

Y podemos darle ademÃ¡s una representaciÃ³n paramÃ©trica racional
no trivial:

***** Ideal de un conjunto
Sea $S \subseteq \mathbb{A}^n(K)$. Definimos el ideal asociado a $S$ como:

\[
\mathbb{I}(S)
=
\Big\{F \in K[X_1,\dots,X_n] \mid \forall a \in S: F(a) = 0\Big\}
\]

****** Es ideal
Trivialmente sabiendo que se conservan los ceros por suma y producto
externo.

***** Propiedades de variedades e ideales
Los ideales y sus variedades cumplen:

  1. $S_1 \subseteq S_2 \implies \mathbb{I}(S_1) \supseteq \mathbb{I}(S_2)$.
  2. $\mathbb{I}(\varnothing) = K[X_1,\dots,X_n]$.
  3. $S \subseteq \mathbb{V}\mathbb{I}(S)$ y $J \subseteq \mathbb{I}\mathbb{V}(J)$.
  4. $\mathbb{I}(S) = \mathbb{I}\mathbb{V}\mathbb{I}(S)$ y $\mathbb{V}(J) = \mathbb{V}\mathbb{I}\mathbb{V}(J)$.
  5. $\mathbb{I}(S)$ es ideal radical.
  6. Cuando $V \subseteq \mathbb{A}^n(K)$ es variedad afÃ­n, $V = \mathbb{V}\mathbb{I}(V)$.
  7. $V_1 \subseteq V_2 \iff \mathbb{I}(V_1) \supseteq \mathbb{I}(V_2)$.
  8. $\mathbb{I}(S_1 \cup S_2) = \mathbb{I}(S_1) \cap \mathbb{I}(S_2)$.
  9. $V_1 \cup V_2 = \mathbb{V}(\mathbb{I}(V_1)\mathbb{I}(V_2)) = \mathbb{V}(\mathbb{I}(V_1) \cap \mathbb{I}(V_2))$.
  10. \[\bigcap_{\lambda \in \Lambda} V_\lambda = \mathbb{V}\left(\sum_{\lambda\in\Lambda} \mathbb{I}(V_\lambda)\right)\].

****** DemostraciÃ³n
******* Punto 1, 2 y 3
Triviales desde la definiciÃ³n. NÃ³tese que las inclusiones vienen
dadas directamente.

******* Punto 4
Uniendo las dos inclusiones anteriores.

******* Punto 5
Si $f^n \in \mathbb{I}(S)$ entonces para cualquier punto en $S$ se cumple $f^n(s) = 0$, 
lo que lleva a $f(s) = 0$.

******* Punto 6
Al igual que el punto 4 uniendo las dos inclusiones anteriores.

******* Punto 7
Usando la igualdad y por definiciÃ³n se tiene.

******* Punto 8
Trivial por definiciÃ³n.

******* Punto 9
La uniÃ³n de variedades es variedad y se tiene por la igualdad y por 
el punto anterior.

******* Punto 10
Por propiedades de las variedades y la igualdad.

***** No todo ideal radical es de esa forma
La funciÃ³n $\mathbb{{I}}$ considerada como:

\[ \mathbb{I} : \left\{ V \in \mathbb{A}^n(K) \mid V \text{ es v.a.}\right\}
\longrightarrow
\left\{ J \subseteq K[X_1,\dots,X_n] \mid J = \sqrt{J}\right\}\]

Es inyectiva, se tiene $\mathbb{V}\mathbb{I} = 1$ y en general no se tiene $\mathbb{I}\mathbb{V} = 1$.

***** Teorema de los ceros de Hilbert
Cuando $K$ es algebraicamente cerrado, para cualquier ideal
$J \subseteq K[X_1,\dots,X_n]$, se verifica que:

\[\mathbb{I}\mathbb{V}(J) = \sqrt{J}\]

Y por tanto hay una biyecciÃ³n entre ideales radicales y variedades.

\[ \{ V \in \mathbb{A}^n(K) \mid V \text{ es v.a.}\}
\cong
\{ J \subseteq K[X_1,\dots,X_n] \mid J = \sqrt{J}\}\]

****** TODO DemostraciÃ³n                                                                                   :extra:
**** Anillos de coordenadas
***** Aplicaciones polinÃ³micas
Un *morfismo entre variedades* de $\mathbb{A}^n$ y $\mathbb{A}^m$ es una aplicaciÃ³n $f : V \longrightarrow W$
tal que existen polinomios $F_1,\dots,F_n$ tales que:

\[f(a) = (F_1(a),F_2(a),\dots,F_n(a)) \quad \forall a \in V\]

Los llamamos tambiÃ©n *aplicaciones polinÃ³micas*.

****** Isomorfismos de variedades afines
Un isomorfismo entre variedades afines es un morfismo entre variedades
cuya inversa es un morfismo entre variedades.

***** Anillo de coordenadas
Sea $V \subset \mathbb{A}^n(K)$ una variedad algebraica afÃ­n. Definimos el anillo de 
coordenadas de $V$ como:

\[
K[V] = \frac{K[X_1,\dots,X_n]}{\mathbb{I}(V)}
\]

Estos anillos son K-Ã¡lgebras.

****** Ejemplos de anillo de coordenadas
Sea $V = \mathbb{V}(X^3-Y^2) \subseteq \mathbb{A}^2(K)$. 

******* En caracterÃ­stica 0
Si $car(K)=0$, entonces $\mathbb{I}(V) = (X^3-Y^2)$. Entonces:

\[K[V] = \frac{K[X_1,\dots,X_n]}{(X^3-Y^2)\]

******* En caracterÃ­stica 2
En caracterÃ­stica 2 tenemos $V = \{(0,0),(1,1)\}$, luego
$\mathbb{I}(V) = \mathbb{I}(\{(0,0\} \cup \{(1,1)\}) = (X,Y) \cap (X-1,Y-1)$, y en este caso:

\[\mathbb{F}_2[V]
= 
\frac{\mathbb{F}_2[X,Y]}{(X,Y) \cap (X-1,Y-1)}
= 
\frac{\mathbb{F}_2[X,Y]}{(X,Y)} +
\frac{\mathbb{F}_2[X,Y]}{(X-1,Y-1)}
=
\mathbb{F}_2 \times \mathbb{F}_2
\]

***** Funtor entre anillos de coordenadas y aplicaciones polinÃ³micas
Sea $V \subseteq \mathbb{A}^n(K)$ y $W \subseteq \mathbb{A}^m(K)$ variedades algebraicas afines.

  1. Toda aplicaciÃ³n polinÃ³mica $f : V \longrightarrow W$ define un homomorfismo de
     K-Ã¡lgebras $\tilde{f} : K[W] \longrightarrow K[V]$.
  2. Para cada homomorfismo de K-Ã¡lgebras $h : K[W] \longrightarrow K[V]$ existe una
     Ãºnica aplicaciÃ³n polinÃ³mica $f : V \longrightarrow W$ Ãºnica tal que $\tilde{f} = h$.
  3. Si $V_1 \overset{f}\longrightarrow V_2 \overset{g}\longrightarrow V_3$ son aplicaciones polinÃ³micas, $\widetilde{f} \circ \widetilde{g} = \widetilde{g \circ f}$.
  4. $f$ es isomorfismo ssi $\widetilde{f}$ es isomorfismo.

Es decir, hay un funtor contravariante entre ambas categorÃ­as.

****** DemostraciÃ³n
******* Punto 1
Sea $f(a) = (F_1(a),\dots,F_m(a))$. Por propiedad universal del anillo de
polinomios tenemos un $f'$ cumpliendo $f'(Y_j) = F_j$:

\[\begin{tikzcd}
K \rar[hook]\drar[hook]& K[Y_1,\dots,Y_n] \dar{\exists! f'} \\
& K[X_1,\dots,X_n]
\end{tikzcd}\]

Comprobamos que baja al cociente. Para $G \in \mathbb{I}(W)$, veamos que $f'(G) \in \mathbb{I}(V)$.
Sea $a \in V$, entonces $f(a) \in W$ y por tanto:

\[ f'(G)(a) = G(F_1(a),\dots,F_n(a)) = G(f(a)) = 0\]

Acabamos de ver que estÃ¡ bien definida una funciÃ³n:

\[\widetilde{f}(G + \mathbb{I}(W)) = G(F_1,\dots,F_n)+\mathbb{I}(V)\]

******* Punto 2
******** Existencia
Dada $h$, calculamos:

\[h(Y_i + \mathbb{I}(W)) = F_i + \mathbb{I}(V)\]

Y definimos $f : V \longrightarrow \mathbb{A}^m(K)$ como:

\[f(a) = (F_1(a),\dots,F_m(a))\]

********* Bien definida
Si $a \in V$, veremos que $f(a) \in W = \mathbb{VI}(W)$. Para $G \in \mathbb{I}(W)$:

\[h(0) = h(G+\mathbb{I}(W)) = G(F_1,\dots,F_n) + \mathbb{I}(V)\]

Luego $G(f(a)) = G(F_1,\dots,F_n)(a) + 0 = 0$, tenemos una funciÃ³n
polinÃ³mica $f : V \longrightarrow W$.

********* Cumple el requisito
Si $h(Y_j + \mathbb{I}(W) = F_j'+\mathbb{I}(V)$, entonces $F_j + \mathbb{I}(V) = F_j' + \mathbb{I}(V)$.
Y $F_j(a) = F_j'(a)$ para cualquier $a$. Es claro por tanto que $\widetilde{f} = h$.

******** Unicidad
Supongamos una $g : V \longrightarrow W$ tal que $\widetilde{g} = h$ con $g(a) = (G_1(a),\dots,G_m(a))$.
Entonces $\widetilde{g} : K[W] \longrightarrow K[V]$:

\[\tilde{g}(Y_j + \mathbb{I}(W)) = G_j + \mathbb{I}(V)\]
\[h(Y_j + \mathbb{I}(W)) = F_j + \mathbb{I}(V)\]

Por lo tanto $G_j - F_j \in \mathbb{I}(V)$ para cualquier $j$.

******* Punto 3
Por construcciÃ³n.

******* Punto 4
Por el punto anterior, comprobando que el funtor respeta la identidad.

****** Ejemplo de polinÃ³mica biyectiva no isomorfismo
Sea $V = \mathbb{A}^1(\mathbb{R})$ y $W = \mathbb{V}(X^3-Y^2) \subseteq \mathbb{A}^2(\mathbb{R})$. Tenemos una aplicaciÃ³n
del tipo $f : \mathbb{A}^1(\mathbb{R}) \longrightarrow W$:

\[ f(a) = (a^2,a^3)
\]

Que es claro que es polinÃ³mica. Tenemos $f$ biyectiva. Pero vemos que no
es isomorfismo de variedades con $\widetilde f(X) = T^2$, $\widetilde f(X) = T^3$:

\[\widetilde{f} : \frac{K[X,Y]}{(X^3-Y^2)} \longrightarrow K[T]\]

Ya que $\tilde{f}$ no es sobreyectiva porque $T \notin \mathrm{img}(\tilde{f})$.

***** NÃºcleos e imÃ¡genes de morfismos de anillos de coordenadas
Sea $h : K[W] \longrightarrow K[V]$ un morfismo de K-Ã¡lgebras dado por
$h(Y_j + \mathbb{I}(W)) = F_j+\mathbb{I}(V)$.

  1. Sea el ideal de $K[X_1,\dots,X_n,Y_1,\dots,Y_n]$ dado por
     $C = (Y_1-F_1,\dots,Y_m-F_m) + \mathbb{I}(V)$, entonces:

     \[\ker(h) = 
     ((C \cap K[Y_1,\dots,Y_m]) + \mathbb{I}(W)) / \mathbb{I}(W)\]

  2. Sea $\mathbb{G}$ una base de GrÃ¶bner reducida de $C$ con orden lexicogrÃ¡fico:

     \[F + \mathbb{I}(V) \in \mathrm{img}(h) 
   \iff
   R(F;\mathbb{G}) \in K[Y_1,\dots,Y_n]
   \]

     AdemÃ¡s, en tal caso, $F+\mathbb{I}(V) = h(R(F;G) + \mathbb{I}(W))$.

***** Corolario: caracterizaciÃ³n de inyectividad y sobreyectividad
Se cumple:

  1. $h$ inyectiva $\iff$ $C \cap K[Y_1,\dots,Y_m] \subseteq \mathbb{I}(W)$.
  2. $h$ sobreyectiva $\iff$ existen $M_i \in K[Y_1,\dots,Y_m]$ tal que $X_i-M_i \in \mathbb{G}$.

****** TODO DemostraciÃ³n
***** Clausura de Zariski
Dada una variedad $V$ y una representaciÃ³n paramÃ©trica polinomial suya:

\[
U = \left\{
(F_1(t_1,\dots,t_k),F_2(t_1,\dots,t_k), \dots, F_n(t_1,\dots,t_k))
\mid
t_1,t_2,\dots,t_k \in K
\right\}
\subseteq V
\]

su clausura en la topologÃ­a de Zariski es:

\[
\overline{U} =
\mathbb{V}(J \cap K[X_1,\dots,X_n])
\]

donde $J = (X_1-F_1,\dots,X_m-F_m) \subseteq K[X_1,\dots,X_n,T_1,\dots,T_n]$.

*** Temas de teorÃ­a
**** 1. Ideales maximales y primos. Teorema de Krull.
***** Ideales                                                                                               :extra:
Un ideal de $R$ es un subconjunto cerrado para la suma y
el producto por elementos $R$.

****** Anillo cociente
Para un $R$ anillo con $\alpha$ ideal, se define el *anillo cociente*:

\[
R/\alpha = \{x+\alpha\mid x \in R\}
\]

Obtenido por la relaciÃ³n $x \sim y \iff x-y\in\alpha$, con la suma y el
producto proyectados.
***** RetÃ­culo de ideales                                                                                   :extra:
Los ideales forman un retÃ­culo con la suma y la intersecciÃ³n.
Se tiene:

  - $I,J \subseteq I + J$
  - $I \cap J \subseteq I,J$

****** Son ideales
Claramente son cerrados para la suma y el producto externo por serlo
ambos.

***** Anillo cociente                                                                                       :extra:
Dado $R$ anillo con $\alpha$ ideal, tomamos la relaciÃ³n de equivalencia 
$x \sim y \iff x-y\in\alpha$, para obtener el ideal:

\[
R/\alpha = \{x+\alpha \mid x \in R\}
\]

***** Ideales primos y maximales
Un ideal propio $P$ es:

  - Primo, si $xy \in P \implies x \in P \vee y \in P$.
  - Maximal, si es maximal en el retÃ­culo de ideales.
 
****** CaracterizaciÃ³n de ideales primos y maximales
Un ideal $P$ propio es:

  - Primo ssi $R/P$ es dominio de integridad.
  - Maximal ssi $R/P$ es un cuerpo.
 
AsÃ­, tenemos que primo implica maximal.

******* DemostraciÃ³n: primos
VÃ©ase que es equivalente a pedir que a $(x+P)(y+P) \neq 0$ equivalga
$x+P = 0$ Ã³ $y+P = 0$.

******* DemostraciÃ³n: maximales
Ser maximal equivale a que no podamos aÃ±adir ningÃºn elemento sin
llegar al ideal total. Esto es, si intentamos aÃ±adir $y$, tendremos
algÃºn $z$ tal que $(y+P)(z+P) = 1+P$, que es equivalente a la
condiciÃ³n de cuerpo.

***** Teorema de Krull
Dados $\alpha \subset R$ ideal y $S$ multiplicativamente cerrado con $\alpha\cap S=\varnothing$,
existe un ideal $M$ tal que:

  - $\alpha \subset M$
  - $M \cap S = \varnothing$
  - $M$ es maximal respecto a esta condiciÃ³n.

AdemÃ¡s, $M$ es un ideal primo.

****** Multiplicativamente cerrado
Un subconjunto $S$ es multiplicativamente cerrado si:

  - $1 \in S$
  - $a,b \in S \implies ab \in S$

****** DemostraciÃ³n
Dada una cadena de ideales que cumple $\alpha \subset I$ y $I \cap S = \varnothing$, su uniÃ³n
tambiÃ©n lo cumple y es cota de la cadena. Aplicando lema de Zorn,
sabemos que debe haber un ideal maximal $M$ respecto a las condiciones.

Supongamos $xy \in M$ pero $x,y \notin M$. Por maximalidad, $(M+(x)) \cap S$ y
$(M+(y)) \cap S$ son no vacÃ­os y existen $xz,yt \in S$. Entonces tenemos 
que $xzyt \in M \cap S$, contradicciÃ³n.

****** Corolario al teorema de Krull
Todo ideal tiene un ideal maximal que lo contiene. AdemÃ¡s, todo elemento
no unidad tiene un ideal maximal que lo contiene.

******* DemostraciÃ³n
Tomando $S = \{1\}$.

***** InclusiÃ³n en ideales primos                                                                           :extra:
Sean ideales $\alpha_1,\dots,\alpha_n$ y $\pi$ un ideal primo. Si $\bigcap \alpha_i \subseteq \pi$, entonces,

\[
\exists \alpha_i \subseteq \pi
\]

**** 2. Nilradical y radical de Jacobson.
***** Nilradical
El *nilradical* es el ideal dado por los elementos nilpotentes:

\[
\operatorname{Nil}(R) = \{x \in R \mid \exists n : x^n = 0\}
\]

****** Es un ideal
Trivial usando el binomio de Newton.

****** Anillo reducido
Un anillo se dice reducido si $\operatorname{Nil}(R) = 0$. Los dominios de integridad son
trivialmente reducidos. AdemÃ¡s, podemos reducir un anillo dividiÃ©ndolo
por su nilradical, $R/\operatorname{Nil}(R)$.

***** CaracterizaciÃ³n del Nilradical
El nilradical es la intersecciÃ³n de ideales del espectro, los ideales
primos del anillo.

\[
\operatorname{Nil}(R) = \bigcap_{{\cal \pi}\text{ primo}} \pi
\]

****** DemostraciÃ³n
******* Nilpotente en la intersecciÃ³n
Si $x^n = 0 \in \pi$, por primalidad, $x \in \pi$ para cualquier ideal primo.

******* En la intersecciÃ³n es nilpotente
Si $x \notin \operatorname{Nil}(R)$, $S=\{1,x,x^2,\dots\}$ es multiplicativamente cerrado con
$S \cap \operatorname{Nil}(R) = \varnothing$, y por Krull, existe un primo con $\pi \cap S = \varnothing$.

***** Radical de un ideal                                                                                   :extra:
El *radical de un ideal* se define como:

\[
\sqrt{\alpha} = \{x \in R \mid x^n \in \alpha\}
\]

Cuando $\alpha = \sqrt{\alpha}$, lo llamamos *ideal radical*.

****** CaracterizaciÃ³n
Tenemos que $\operatorname{Nil}(R/\alpha) = \sqrt{\alpha}/\alpha$ por construcciÃ³n. De esa forma ademÃ¡s 
llegamos a caracterizar al radical como:

\[
\sqrt{\alpha} = \bigcap_{\alpha \subset \pi \in \operatorname{Spec}(R)} \pi
\]

***** Radical de Jacobson
El *radical de Jacobson* es el dado por:

\[
{\cal J}(R) = \bigcap_{{\cal M} \text{ maximal}} {\cal M}
\]

***** CaracterizaciÃ³n del radical de Jacobson
Tenemos que $x \in {\cal J}(R)$ ssi $1-xy \in U(R)$ para cualquier $y$.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Si $x \in {\cal J}(R)$, entonces $1-xy$ no puede estar en ningÃºn ideal maximal,
porque estarÃ­a tambiÃ©n el $1$. Pero todo elemento no unidad estÃ¡ en un
ideal maximal por teorema de Krull.

******* Segunda implicaciÃ³n
Si $x\notin M$ maximal, $(x)+M = R$. Luego $1-xy \in M$, y un maximal
no puede contener una unidad.

**** 3. Algoritmo de la divisiÃ³n en K[X1,â¦,Xn].
***** Ãrdenes monomiales                                                                                    :extra:
Un orden monomial $\leq$ en $\mathbb{N}^n$ es un orden compatible, monÃ³tono y total.

  1. *Compatible:* si $x\leq y$, entonces $\gamma+x \leq \gamma+y$.
  2. *MonÃ³tono:* $0 \leq x$.
  3. *Total:* o bien $x \leq y$, o bien $y \leq x$.

El orden lexicogrÃ¡fico, el orden lexicogrÃ¡fico graduado y el orden 
lexicogrÃ¡fico graduado inverso son Ã³rdenes monomiales.

***** Componentes de la divisiÃ³n                                                                            :extra:
Definimos los siguientes componentes para un polinomio
expresado como $p = \sum a_\alpha X^\alpha$:

  - ${\cal N}(p) = \{\alpha \in \mathbb{N}^n \mid a_\alpha \neq 0\}$, diagrama de Newton.
  - $exp(p) = \max\{\alpha \in \mathbb{N}^n \mid a_\alpha \neq 0\}$, exponente.
  - $tl(p) = a_{exp(p)}X^{exp(p)}$, tÃ©rmino lÃ­der.

****** Propiedades del exponente
Dados $F,G \in K[X_1,\dots,X_n]$ no nulos, se cumple:

  1. $exp(FG) = exp(F)+exp(G)$
  2. $exp(F+G) \leq \max\{exp(F),exp(G)\}$
  3. Si $exp(F) < exp(G)$, entonces $exp(F+G) = exp(G)$.

******* DemostraciÃ³n
Se demuestran usando que los polinomios forman un K-espacio vectorial
teniendo de base a los monomios.

***** ParticiÃ³n de generadores
Dada una lista de elementos $a_1,\dots,a_k$, tenemos una particiÃ³n que
definimos inductivamente como:

  1. Los elementos que genera el primer generador:

     \[\Delta^1 = a_1 + \mathbb{N}^n\]

  2. Los elementos que aporta cada nuevo generador:

     \[
     \Delta^i = (a_i+\mathbb{N}^n) \setminus \bigcup_{j<i} \Delta^j
     \]

  3. Todos los demÃ¡s elementos:

     \[
     \overline{\Delta} = \mathbb{N}^n \setminus \bigcup_{j \leq k} \Delta^j
     \]

***** Teorema de la divisiÃ³n
Dado un orden monomial y una lista de polinomios $\{G_1,\dots,G_t\}$; consideramos
la particiÃ³n $\Delta_1,\dots,\Delta_t,\overline{\Delta}$ dada por los generadores $exp(G_i)$. Tenemos que,
para cada $F \in K[X_1,\dots,X_n]$, existen $Q_1,\dots,Q_t,R$ Ãºnicos tales que:

  1. $F = Q_1G_1+\dots+Q_tG_t+R$.
  2. $R=0$ Ã³ $N(R) \subset \overline{\Delta}$.
  3. $exp(G_i)+N(Q_i) \subseteq \Delta^i$.

****** DemostraciÃ³n
Procedemos por inducciÃ³n sobre $exp(F)$ con el orden monomial que
tenemos. Distinguimos siempre segÃºn si aparece en $\Delta_i$ o en $\overline{\Delta}$.
NÃ³tese que en cada caso debemos comprobar que se cumplen trivialmente
las condiciones del teorema.

******* Caso base
******** Aparece en algÃºn elemento de la particiÃ³n
Si $exp(F) \in \Delta^i$, entonces forzosamente $exp(G_i)=0$. Entonces
tomamos $Q_i = F/G_i$ y $Q_j=0$ para $j \neq i$.

******** Aparece en el resto de la particiÃ³n
Simplemente tomamos $R=F$.

******* Caso inductivo
Sabiendo ahora que podemos dividir todo $G$ con $exp(G) < exp(F)$.

******** Aparece en algÃºn elemento de la particiÃ³n
Si $exp(F) \in \Delta^i$ entonces $exp(F) = exp(G_i) + \gamma$. Tomando $H=X^\gamma G$,
aplicamos inducciÃ³n sobre:

\[
F -\frac{cl(F)}{cl(H)}H = F' = \sum Q_i'G_i+R'
\]

Ahora tomamos $Q_i = Q_i'+\frac{cl(F)}{cl(H)}X^\gamma$ y $Q_j = Q_j'$, para llegar a:

\[
F = \sum Q_iG_i+R'
\]

******** Aparece en el resto de la particiÃ³n
Aplicamos inducciÃ³n sobre:

\[
F - tl(F) = \sum Q_i'G_i + R'
\]

Y tomamos entonces $R=R'+tl(F)$, que sigue cumpliendo las 
condiciones.

**** 4. Ideales monomiales.
***** Exponente de un ideal                                                                                 :extra:
El exponente de un ideal es el conjunto de exponentes de sus polinomios:

\[
Exp(I) = \{
exp(F) \mid 0\neq F \in I
\} \subseteq \mathbb{N}^n
\]

El exponente es un monoideal, es decir, $Exp(I) + \mathbb{N}^n = Exp(I)$.

****** DemostraciÃ³n
NÃ³tese que tenemos $exp(X^\gamma F) = \gamma + exp(F)$.

***** Lema de Dickson y sistemas generadores                                                                :extra:
Para $S \subseteq \mathbb{N}^n$ no vacÃ­o, existe $G \subseteq S$ finito tal que $S \subseteq G + \mathbb{N}^n$.

****** Monoideal
Un subconjunto $E \subseteq \mathbb{N}^n$ es monoideal cuando $E = E+\mathbb{N}^n$.

****** Sistemas de generadores
Si $E$ es monoideal, existe un $G \subset E$ finito con $E = G + \mathbb{N}^n$. Llamamos
a $G$ sistema de generadores de $E$.

Esto es debido al lema de Dickson.

***** Ideales monomiales
Un *ideal monomial* estÃ¡ generado por monomios, es de la forma:

\[
I = (X^{\alpha}\mid \alpha \in A)
\]

para algÃºn $A \subseteq \mathbb{N}$.

***** Monomios en un ideal monomial
Sea $I = (X^\alpha \mid \alpha \in A)$ monomial, equivalen:

  1. $F\in I$.
  2. Todo monomio de $F$ estÃ¡ en $I$.
  3. $F$ es combinaciÃ³n lineal de monomios de $I$.

Y ademÃ¡s, si para cualquier polinomio de un ideal todos sus monomios 
estÃ¡n en el ideal, es monomial.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Si $F \in I$, serÃ¡ de la forma:

\[
F = \sum F_iX^\alpha
\]

Como los monomios son una K-base del espacio, todo monomio de la
suma serÃ¡ mÃºltiplo de algÃºn $X^\alpha$.

******* Segunda y tercera implicaciones
Triviales desde lo anterior.

******* CaracterizaciÃ³n de ideales monomiales
NÃ³tese que podemos tomar todos los polinomios del ideal y generarlo
como: $(X^\alpha \mid \alpha \in Exp(I))$.
***** Lema de Dickson para ideales monomiales
Todo ideal monomial tiene un sistema de generadores finito y 
formado por monomios.

****** DemostraciÃ³n
Como el exponente de un ideal es siempre monoideal, podemos aplicar
el lema de Dickson para darle un sistema de generadores $Exp(I)=G+\mathbb{N}^n$.
Dado un exponente $\alpha \in Exp(I)$, tenemos $X^\alpha\in I$, luego tomamos como
sistema de generadores:

\[
(X^\alpha \mid \alpha \in G)
\]

Dado cualquier $F\in I$, cada monomio suyo estarÃ¡ en $I$, luego serÃ¡ 
generado por este sistema.
**** 5. Bases de GrÃ¶bner. Aplicaciones.
***** Base de GrÃ¶bner
Una *base de GrÃ¶bner* de un ideal $I$ es un conjunto $\mathbb{G} = \{G_1,\dots,G_n\} \subseteq I$
cumpliendo $Exp(I) = \{exp(G_1),\dots,exp(G_n)\} + \mathbb{N}^n$.

***** Propiedades de bases de GrÃ¶bner
Las bases de GrÃ¶bner en $K[X_1,\dots,X_n]$ cumplen:

  1. Todo ideal no nulo tiene base de GrÃ¶bner.
  2. Toda base de GrÃ¶bner de un ideal genera al ideal.
  3. *Teorema de la base de Hilbert*: todo ideal es finitamente generado.

****** DemostraciÃ³n
******* Primer punto
Todo ideal tiene $Exp(I)$ monoideal, y por tanto tiene un sistema de
generadores. Tomamos polinomios que tengan como exponente estos
generadores y tenemos una base de GrÃ¶bner.

******* Segundo punto
Por algoritmo de la divisiÃ³n, todo polinomio del ideal se divide
entre ellos dando un resto tal que $exp(R) \in \overline{\Delta} \cap Exp(I) = \varnothing$. Por
tanto, $R=0$.

******* Tercer punto
Todo ideal estÃ¡ generado por su base de GrÃ¶bner, que es finita.

***** CaracterizaciÃ³n de las bases de GrÃ¶bner
Sea $I$ ideal no nulo, equivalen:

  1. $\mathbb{G}$ es base de GrÃ¶bner de $I$.
  2. $R(F,\mathbb{G}) = 0$, para todo $F\in I$.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Como $exp(R) \in Exp(I)\cap\overline{\Delta}$, tenemos $exp(R) = 0$.

******* Segunda implicaciÃ³n
Por teorema de la divisiÃ³n, todo $F \in I$ es de la forma:

\[
F = \sum_{i=0}^n Q_iG_i
\]

Donde $exp(Q_iG_i) \in \Delta^i$, y como forman una particiÃ³n disjunta, se
tiene que el $exp(F) = \max\{exp(Q_iG_i)\} \notin \overline{\Delta}$. Por tanto $Exp(I)\cap\overline{\Delta} = \varnothing$.

***** CÃ¡lculo de bases de GrÃ¶bner                                                                           :extra:
Las bases de GrÃ¶bner de un ideal pueden calcularse a partir de sus
generadores utilizando el Algoritmo de Buchberger, que aÃ±ade a cada
paso los restos de las semizigias de los generadores a la base.

****** Teorema de Buchberger
Dado un sistema de generadores $\mathbb{G} = \{G_1,\dots,G_n\}$ de una base de GrÃ¶bner,
equivalen:

  1. $\mathbb{G}$ es base de GrÃ¶bner.
  2. Para alguna ordenaciÃ³n de $\mathbb{G}$, se tiene $R(S(G_i,G_j);\mathbb{G}) = 0$ para $i\neq j$.

****** Semizigia
Definimos la semizigia de dos polinomios como:

\[{\cal S}(F,G) 
= \frac{1}{cl(F)}X^{\gamma-\alpha}F - \frac{1}{cl(G)}X^{\gamma-\beta}G\]

***** Base de GrÃ¶bner minimal                                                                               :extra:
Una base de GrÃ¶bner de $I$ se dice minimal si, para cualquier $F \in \mathbb{G}$:

  1. $cl(F) = 1$
  2. $exp(F) \notin \{exp(G) \mid G\in\mathbb{G}, G\neq F\} + \mathbb{N}^n$

****** Retirar polinomios
Sea $\mathbb{G}$ base de GrÃ¶bner con un ideal no nulo y sea un $F \in \mathbb{G}$ que no
cumple la segunda condiciÃ³n de minimalidad. Tenemos que $\mathbb{G}\setminus\{F\}$ es
tambiÃ©n base de GrÃ¶bner, ya sus exponentes siguen generando $Exp(I)$.
***** Base de GrÃ¶bner reducida                                                                              :extra:
Una base de GrÃ¶bner $\mathbb{G}$ de $I$ es reducida cuando para cualquier $\forall F\in\mathbb{G}$:

  1. $cl(F) = 1$
  2. ${\cal N}(F) \cap \{exp(G) \mid G \in \mathbb{G}, G \neq F\} + \mathbb{N}^n = \varnothing$

Toda base de GrÃ¶bner reducida es minimal y todo ideal no nulo $I$ tiene una
Ãºnica base de GrÃ¶bner reducida.

****** DemostraciÃ³n: existencia
Dada una base de GrÃ¶bner, podemos llegar a una base de GrÃ¶bner minimal
retirando polinomios. Y dada una base minimal, podemos llegar a una base
reducida dividiendo cada polinomio por todos los demÃ¡s y sustituyÃ©ndolo
por su resto, que tiene el mismo exponente por ser la anterior una base
minimal.

****** DemostraciÃ³n: unicidad
Si hubiera dos bases reducidas, tomarÃ­amos los dos elementos con el mismo
exponente en cada una de ellas, y dividirÃ­amos su resto $F-F' \in I$, que
debe dar resto $0$:

\[R(F-F';\mathbb{G}) = F-F'
\]

Ya que por ser una base reducida, ${\cal N}(F-F') \subseteq \overline{\Delta}$.

***** Aplicaciones: Problema de pertenencia
Sea $I$ ideal con un sistema de generadores. Podemos calcular si $F \in I$
calculando una base de GrÃ¶bner del ideal y usando:

\[F \in I \iff R(F,\mathbb{G}) = 0\]

***** Aplicaciones: Igualdad de ideales
Dados dos ideales $I,J$ con sus sistemas de generadores. Podemos comprobar
que serÃ¡n iguales calculando sus bases de GrÃ¶bner reducidas, que por
unicidad, deben ser iguales.

***** Aplicaciones: Ideal de eliminaciÃ³n
Dado $I$ ideal no nulo. Definimos el j-Ã©simo ideal de eliminaciÃ³n como:

\[
I_j = I \cap K[X_{j+1},\dots,X_n]
\]

Si $\mathbb{G}$ era base de GrÃ¶bner con el orden lexicogrÃ¡fico de $I$, entonces:

\[
\mathbb{G}_j = \mathbb{G} \cap I_j
\]

es base de GrÃ¶bner de $I_j$.

****** CÃ¡lculo de la intersecciÃ³n
Sean $I,J$ ideales no nulos de $K[X_1,\dots,X_n]$ y sea:

\[
H = TI + (1-T)J \subseteq K[T,X_1,\dots,X_n]
\]

considerando los ideales extendidos. Entonces su intersecciÃ³n es el
primer ideal de eliminaciÃ³n de $H$:

\[
I \cap J = H \cap K[X_1,\dots,X_n]
\]

**** 6. Variedades algebraicas afines. Correspondencia ideal-variedad.
***** R-Ã¡lgebras                                                                                            :extra:
Dado un anillo $R$, un R-mÃ³dulo es un grupo abeliano $M$ junto a una
operaciÃ³n $\cdot : (R,M) \longrightarrow M$ verificando:

  - $r(x+y) = rx+ry$
  - $(r+s)x = rx + sx$
  - $r(sx) = (rs)x$
  - $1x = x$

Una *R-Ã¡lgebra* $S$ es un anillo con estructura compatible de R-mÃ³dulo,
tal que:

\[
\forall r \in R; x,y \in S: (rx)y = r(xy) = x(ry)
\]

***** Variedades algebraicas afines
Dado un ideal ${\cal F} \subseteq K[X_1,\dots,X_n]$, denotamos:

\[
\mathbb{V}({\cal F}) = \left\{ (a_1,\dots,a_n) \mid 
\forall F \in {\cal F}: F(a) = 0 \right\}
\]

Y llamamos *variedad algebraica afÃ­n* a los conjuntos de esta forma.

***** TopologÃ­a de Zariski
Las variedades son los cerrados de una topologÃ­a sobre $\mathbb{A}^n(K)$, ya
que cumplen:

  1. \[\bigcap_{\lambda\in\Lambda} \mathbb{V}(J_\lambda) =
     \mathbb{V}\left(\sum_{\lambda\in\Lambda} J_\lambda\right)}\]

  2. $\mathbb{V}(J_1) \cup \mathbb{V}(J_2) = \mathbb{V}(J_1J_2) = \mathbb{V}(J_1\cap J_2)$

  3. $\mathbb{V}(0) = \mathbb{A}^n(K)$

  4. $\mathbb{V}(K[X_1,\dots,X_n]) = \varnothing$

****** DemostraciÃ³n
******* Punto 1
NÃ³tese que un punto se anula para un conjunto de ideales ssi
se anula para todas sus combinaciones lineales.

******* Punto 2
Si $x \notin \mathbb{V}(J_1) \cup \mathbb{V}(J_2)$, entonces existen polinomios en cada ideal
que no lo anulan, y su producto da $x \notin \mathbb{V}(J_1J_2)$ y $x \notin \mathbb{V}(J_1\cap J_2)$.

El resto de inclusiones son triviales.

******* Punto 3
El polinomio $0$ anula todos los puntos.

******* Punto 4
NingÃºn punto es raÃ­z de todos los polinomios. Existen los 
polinomios constantes no nulos en particular.

***** Correspondencia ideal-variedad
Se define el *ideal de un conjunto* como:

\[
\mathbb{I}(S) = 
\left\{
F \in K[X_1,\dots,X_n]
\mid
\forall a \in S: F(a) = 0
\right\}
\]

Los ideales y variedades cumplen:

  1. $\mathbb{I}(\varnothing) = K[X_1,\dots,X_n]$.

  2. $\mathbb{I}(S_1\cup S_2) = \mathbb{I}(S_1) \cap \mathbb{I}(S_2)$.

  3. $S_1 \subseteq S_2 \implies \mathbb{I}(S_1) \supseteq \mathbb{I}(S_2)$.

  4. $\mathbb{I}(S)$ es ideal radical.

  5. $S \subseteq \mathbb{VI}(S)$ y $J \subseteq \mathbb{IV}(J)$.

  6. $\mathbb{I}(S) = \mathbb{IVI}(S)$ y $\mathbb{V}(J) = \mathbb{VIV}(J)$.

  7. Cuando $V \subseteq \mathbb{A}^n(K)$ es variedad afÃ­n, $V = \mathbb{VI}(V)$.

  8. $V_1 \subseteq V_2 \iff \mathbb{I}(V_1) \supseteq \mathbb{I}(V_2)$.

  9. $V_1\cup V_2 = \mathbb{V}(\mathbb{I}(V_1)\mathbb{I}(V_2)) = \mathbb{V}(\mathbb{I}(V_1) \cap \mathbb{I}(V_2))$.

  10. \[\bigcap_{\lambda\in\Lambda} V_\lambda = \mathbb{V}\left(\sum_{\lambda\in\Lambda} \mathbb{I}(V_\lambda)\right)\]

****** DemostraciÃ³n
******* Puntos 1, 2 y 3
Triviales por definiciÃ³n.

******* Punto 4
Notamos que $f^n(s)=0 \implies f(s) = 0$.

******* Puntos 5, 6 y 7
Desde la definiciÃ³n se tienen las desigualdades. Uniendo ambas nos
da la igualdad. El siguiente es un caso particular.

******* Punto 8
Trivial por definiciÃ³n y por la igualdad anterior.

******* Punto 9
La uniÃ³n de variedades es variedad, aplicamos $\mathbb{VI}$ y las propiedades
anteriores.

******* Punto 10
La intersecciÃ³n es variedad y volvermos a aplicar $\mathbb{VI}$.

***** Teorema de los ceros de Hilbert
En general las funciones $\mathbb{I}, \mathbb{V}$ dan una correspondencia no biyectiva
entre ideales radicales y variedades, con $\mathbb{VI} = id$. Cuando $K$ es ademÃ¡s
*algebraicamente cerrado*, se tiene $\mathbb{IV}(J) = \sqrt{J}$, y hay biyecciÃ³n:

\[
\left\{ V \in \mathbb{A}^n(K) \mid V \text{ es v.a.}\right\}
\cong
\left\{
J \subseteq K[X_1,\dots,X_n] \mid J = \sqrt{J}
\right\}
\]

**** 7. Anillo de coordenadas de una variedad. Aplicaciones polinÃ³micas.
***** Espacio afÃ­n de un cuerpo                                                                             :extra:
Llamamos $\mathbb{A}^n(K)$ al espacio afÃ­n sobre $K^n$ con la funciÃ³n afÃ­n
$\varphi : K^n \times K^n \longrightarrow K^n$ dada por $\varphi(u,v) = v-u$.

***** Aplicaciones polinÃ³micas
Una aplicaciÃ³n polinomial o morfismo entre variedades es una aplicaciÃ³n
$f : V \longrightarrow W$, variedades de $\mathbb{A}^n$ y $\mathbb{A}^m$, de la forma:

\[
f(a) = (F_1(a),F_2(a),\dots,F_m(a))
\]

donde los $F_1,\dots,F_m \subseteq K[X_1,\dots,X_n]$ son polinomios.

***** Anillo de coordenadas
El anillo de coordenadas de una variedad $V \subseteq \mathbb{A}^n(K)$ es la K-Ã¡lgebra:

\[
K[V] = \frac{K[X_1,\dots,X_n]}{\mathbb{I}(V)}
\]

***** RelaciÃ³n entre aplicaciones polinÃ³micas y anillos de coordenadas
Para $V \subseteq \mathbb{A}^n(K), W \subseteq \mathbb{A}^m(K)$ variedades,

  1. Toda aplicaciÃ³n polinÃ³mica $f : V \longrightarrow W$ induce un homomorfismo de
     K-Ã¡lgebras $\widetilde f : K[W] \longrightarrow K[V]$ cumpliendo $\widetilde f(G+\mathbb{I}(W)) = G(f) + \mathbb{I}(V)$.

  2. Cada homomorfismo $h : K[W] \longrightarrow K[V]$ induce una Ãºnica aplicaciÃ³n
     polinÃ³mica $h' : V \longrightarrow W$ tal que $\tilde{h}' = h$.

  3. Dadas aplicaciones polinÃ³micas $V_1 \overset{f}\longrightarrow V_2 \overset{g}\longrightarrow V_3$, $\widetilde{f} \circ \widetilde{g} = \widetilde{g \circ f}$.

  4. $f$ es isomorfismo ssi $\widetilde f$ es isomorfismo.

Es decir, hay un funtor contravariante entre ambas categorÃ­as.

****** DemostraciÃ³n
******* Punto 1
Dado $f(a) = (F_1(a),\dots,F_m(a))$, por propiedad universal del anillo de
polinomios definimos $f'(Y_j) = F_j$. Para $G \in \mathbb{I}(W)$, si tomamos $a \in V$,
se tiene $f(a) \in W$, y por tanto:

\[
f'(G)(a) = G(F_1(a),\dots,F_n(a)) = G(f(a)) = 0
\]

AsÃ­, estÃ¡ bien definida la funciÃ³n:

\[
\widetilde f (G + \mathbb{I}(W)) = G(F_1,\dots,F_m) + \mathbb{I}(V)
\]

******* Punto 2
******** Existencia
Dada $h$, llamamos $h(Y_i+\mathbb{I}(W)) = F_i$, y definimos $f(a) = (F_1(a),\dots,F_m(a))$.
EstÃ¡ bien definida porque para $G \in \mathbb{I}(W)$,

\[
h(0) = h(G) = G(F_1,\dots,F_m) + \mathbb{I}(V)
\]

Luego $G(f(a)) = G(F_1,\dots,F_m)(a) + 0 = 0$, y $f(a) \in \mathbb{VI}(W) = W$. Es claro
ademÃ¡s que $\widetilde f = h$, por estar definida como ella en una base de los
polinomios.

******** Unicidad
Supongamos $\widetilde g = h$ con $g(a) = (G_1(a),\dots,G_m(a))$, entonces:

\[\widetilde g(Y_j+\mathbb{I}(W)) = G_j + \mathbb{I}(V)\]
\[h(Y_j+\mathbb{I}(W)) = F_j + \mathbb{I}(V)\]

Por lo tanto, $G_j-F_j \in \mathbb{I}(V)$, para cualquier $j$.

******* Punto 3
Por construcciÃ³n. Sean:

  - $f(a) = (F_1(a),\dots,F_m(a))$
  - $g(a) = (G_1(a),\dots,G_m(a))$
  - $g \circ f (a) = (G_1(F_1(a),\dots), \dots, G_m(F_1,\dots))$

Entonces comprobamos que: $\widetilde{g \circ f}(H) = \widetilde f  \circ \widetilde{g} (H)$.

******* Punto 4
Por el punto anterior y viendo simplemente que $\widetilde{id} = id$.

***** CaracterizaciÃ³n de nÃºcleos e imÃ¡genes                                                                 :extra:
Sea $h : K[W] \longrightarrow K[V]$ un morfismo de K-Ã¡lgebras dado por $h(Y_j) = F_j$.

  1. Sea $C = (Y_1-F_1,\dots,Y_m-C_m) + \mathbb{I}(V)$ ideal de $K[X_1,\dots,X_n,Y_1,\dots,Y_m]$
     se tiene:

     \[
     \ker(h) = ((C\cap K[Y_1,\dots,Y_m]) + \mathbb{I}(W))/\mathbb{I}(W)
     \]
     
  2. Sea $\mathbb{G}$ GrÃ¶bner reducida de $C$ con orden lexicogrÃ¡fico:

     \[F + \mathbb{I}(V) \in \mathrm{img}(h) \iff
     R(F;\mathbb{G}) \in K[Y_1,\dots,Y_m]\]
     
     AdemÃ¡s, en tal caso, $F + \mathbb{I}(V) = h(R(F;G)+\mathbb{I}(W))$.

****** DemostraciÃ³n
******* Primer punto
******** Primera implicaciÃ³n
Sea $G \in \ker(h)$, entonces $G(F_1,\dots,F_m) = h(G) = 0+\mathbb{I}(V) \in C$.
Dividimos ahora $H = G - G(F_1,\dots,F_m)$ entre $Y_1-Y_1,\dots,Y_m-C_m$
pero con orden lexicogrÃ¡fico $Y_1>\dots >Y_m>X_1> \dots>X_n$:

\[
H = \sum_{i=1}^m (Y_i-F_i) Q_i + R
\]

Como ${\cal N}(R) \subseteq \overline\Delta$, $R \in K[X_1,\dots,X_m]$, pero:

\[
H(F_1,\dots,F_m) = G(F_1,\dots,F_m) - G(F_1,\dots,F_m) = 0 = R
\]

Luego $H \in C$, y $G \in C \cap K[Y_1,\dots,Y_m]$.

******** Segunda implicaciÃ³n
Si escribimos $\mathbb{I}(V) = (H_1,\dots,H_s)$, tenemos entonces:

\[
G = \sum (Y_i-F_i)T_i + \sum H_jR_j
\]

luego:

\[
G(F_1,\dots,F_m) = \sum_{j=1}^s H_jR_j(X_1,\dots,X_n,F_1,\dots,F_m) 
\in \mathbb{I}(V)
\]

Y por tanto tenemos $G \in \ker(H)$:

\[
h(G) = G(F_1,\dots,F_m) + \mathbb{I}(V) = 0
\]

******* TODO Segundo punto
****** Inyectividad y sobreyectividad
Se tiene desde lo anterior que:

  1. $h$ inyectiva ssi $C \cap K[Y_1,\dots,Y_m] \subseteq \mathbb{I}(W)$.
  2. $h$ sobreyectiva ssi $\exists M_i \in K[Y_1,\dots,Y_m]$ tal que $X_i-M_i \in \mathbb{G}$.

*** Ejercicios
**** RelaciÃ³n 1
***** Ejercicio 1
#+begin_statement
Sea $R$ un anillo conmutativo. Demostrar:

 1. Los elementos $0$ y $1$ estÃ¡n determinados de forma Ãºnica.
 2. Para cada elemento $x\in R$, el opuesto $-x$ y el inverso si existe $x^{-1}$, estÃ¡n
    determinados de forma Ãºnica.
 3. El conjunto ${\cal U}(R)$ de las unidades de $R$ es un grupo abeliano.

$\quad$
#+end_statement
****** Punto 1
Supongamos que hubiera dos elementos neutros de cualquier tipo:

\[e = e \otimes e' = e'\]

****** Punto 2
Supongamos que hubiera dos inversos $x,y$ de cualquier tipo:

\[x = x \otimes (a \otimes y) = (x \otimes a) \otimes y = y\]

****** Punto 3
El anillo es conmutativo. AsÃ­ que el grupo de las unidades con el producto serÃ¡
abeliano.

***** TODO Ejercicio 2
#+begin_statement
Sea $R$ anillo conmutativo. Demostrar:

  1. $x0=0$ para todo $x\in R$.
  2. $R$ tiene mÃ¡s de un elemento ssi $0\neq 1$.
  3. $(-x)y = -(xy) = x(-y)$ para cualesquiera $x,y \in R$.
  4. $(nx)y = n(xy) = x(ny)$ para cualesquiera $x,y \in R$ y todo $n \in \mathbb{Z}$.
  5. $(nx)(my) = (nm)(xy)$ para cualesquiera $x,y \in R$ y $n,m \in \mathbb{Z}$.
  6. $\left(\sum_{i=1}^n x_i\right) \left(\sum^m_{j=1} y_j\right) = \left(\sum^{n,m}_{i=1,j=1} x_iy_j\right)$, para $x_i,y_j \in R$ y $n,m > 0$.
  7. $(x+y)^n = \sum_{i=0}^n {n \choose i} x y^{n-i}$, para cualesquiera $x,y \in R$ y $n,m \geq 0$.
  8. $(xy)^n = x^ny^n$ y $(x^n)^m = x^{nm}$, para cualesquiera $x,y\in R$ y $n,m \geq 0$.
$\quad$
#+end_statement
***** Ejercicio 3
#+begin_statement
Sea $X$ un conjunto, en ${\cal P}(X)$ se consideran las opearciones

\[A+B := (A\cup B) \setminus (A\cap B)\]
\[AB := A \cap B\]

para cualesquiera $A,B \in {\cal P}(X)$.

Prueba que ${\cal P}(X)$, con las operaciones anteriores y elemento uno igual a $X$, es un
anillo conmutativo. Â¿CuÃ¡l es el elemento cero?. Observa que este anillo es un 
*anillo de Boole*, es decir $A^2=A$ para $A\in{\cal P}(X)$ y que por tanto $2A = 0$.
#+end_statement

Demostramos que ${\cal P}(X)$ con la suma forma un grupo abeliano. Cada conjunto es su
inverso, es conmutativo y tiene al conjunto vacÃ­o como neutro. La asociatividad
se comprueba viendo que pertenecer a $A+B+C$ es pertenecer a uno o a los tres.

Que es conmutativo se tiene por:

\[0 = (A+B)^2 - (A+B) = AB + BA\]
\[0 = A+A\]

***** Ejercicio 4
#+begin_statement
Dado un anillo $R = (R,+,\times,1)$, definir sobre $R$ dos operaciones $\oplus,\otimes$ de forma
que $(R,\oplus,\otimes,0)$ sea un anillo con elemento $1$ como cero.
#+end_statement
Nos sirve tomar:

\[a \oplus b = a+b-1\]
\[a \otimes b = a + b - ab\]

Debemos comprobar las propiedades.

***** TODO Ejercicio 7
***** Ejercicio 8
#+begin_statement
Â¿Se deduce la condiciÃ³n $f(1) = 1$ en la definiciÃ³n de homomorfismo de anillos de 
las dos condiciones $f(x+y) = f(x)+f(y)$ y $f(xy) = f(x)f(y)$?
#+end_statement
Una inclusiÃ³n en la suma directa de dos anillos cumple lo pedido pero no cumple
que $i(1) = 1$.

\[i : R \longrightarrow R \oplus S,\quad i(r) = (r,0)\]

***** Ejercicio 9
#+begin_statement
Prueba que los elementos nilpotentes de un anillo forman un ideal.
#+end_statement
Sean $n^p = 0$ y $m^q = 0$, dos nilpotentes. Tenemos que $(n+m)^{p+q} = 0$, por binomio de 
Newton, y que $(rn)^p = 0$ por conmutatividad.
***** Ejercicio 10
#+begin_statement
Demuestra que todo dominio de integridad finito es un cuerpo.
#+end_statement
Dado $x \in R$, considero el endomorfismo $(\lambda a. xa)$. Por ser dominio de integridad, es
inyectivo, y siendo inyectivo y finito, es sobreyectivo. Luego $\exists a : xa = 1$.
***** Ejercicio 11
#+begin_statement
Demuestra que todo dominio de integridad con un nÃºmero finito de ideales es un
cuerpo.
#+end_statement
Dado $x\in R$, considero una aplicaciÃ³n que lleva un ideal $I$ en $(x)I$. Tenemos que es
inyectiva por ser dominio de integridad. De hecho, sean $i \in I$ con $(x)I = (x)J$:

\[xi = rxj = xrj \Rightarrow i = rj\]

Luego $I\subset J$, simÃ©tricamente $I =J$. Por ser inyectiva y ser de nÃºmero finito, es
biyectiva, luego $\exists I: (x)I = (1)$.
***** Ejercicio 12
Sea una cadena de ideales $\Pi \subset \beta\subset R$, con $x \in \beta$, $x \notin \Pi$.
Entonces $x(x^{n-1}-1) = 0 \in \Pi$, y debe tenerse $x^{n-1}-1 \in \Pi \subset \beta$.
Con eso, debe ser $\beta = R$.

***** Ejercicio 13
 Por definiciÃ³n de *radical*, tenemos que cuando es radical es intersecciÃ³n de 
 anillos primos.

 Sea $\alpha$ intersecciÃ³n de ideales primos, serÃ¡ en particular intersecciÃ³n de ideales
 primos mÃ¡s algunos que lo contienen.

***** Ejercicio 14
 Sea $x$ idempotente y sea $M$ el maximal de $R$. 

 - Sea $x \notin M$, entonces debe ser $x$ una
   unidad; por ser $R$ local. Siendo unidad $x^2 = x$ nos da $x=1$.

 - Sea $x \in M$. Sabemos ${\cal J}(R) = M$, luego $x$ estÃ¡ en el radical de Jacobson.
   Esto quiere decir que $1-xy \in U(R)$ para cualquier $y \in R$. En particular $1-x$
   estÃ¡ en las unidades del anillo. Como $x(1-x) = 0$, se tiene $x = 0$.

***** Ejercicio 15
 Aplicaremos Zorn, viendo que todo conjunto totalmente ordenado tiene cota inferior.

 Sea una cadena de ideales primos $\Pi_i$, entonces sea $ab \in \bigcap \Pi_i$, entonces, supongamos que
 $a$ no perteneciera a la intersecciÃ³n, entonces, por primalidad, si $b \notin \Pi_j$, tampoco
 pertenecerÃ­a a ninguno por debajo de Ã©l; y $a$ deberÃ­a pertenecer a todos ellos y
 por tanto a la intersecciÃ³n.

***** Ejercicio 16
****** Punto 1
 Tenemos que $0 = (2x)^2 - 2x = 4x - 2x= 2x$.

****** Punto 2
 Si $\pi$ es primo, entonces $R/\pi$ es dominio de integridad. Sea $m \in R/\pi$, tenemos que
 $m(m-1) = 0$, luego $m=0$ Ã³ $m=1$. AsÃ­, sÃ³lo puede ser isomorfo a $\mathbb{Z}_2$
 y cuerpo. $\pi$ es maximal.

 AdemÃ¡s, la primera parte se obtiene tambiÃ©n por caso particular del ejercicio 12.

****** Punto 3
 Definimos la operaciÃ³n $a \oplus b = a+b+ab$ y comprobamos que $(a,b) = (a \oplus b)$, ya que
 $a (a\oplus b) =a$ y $b(a\oplus b) = b$. Por inducciÃ³n, cada ideal generado por varios lo podemos
 generar por un elemento.

***** Ejercicio 17
Tenemos que los divisores de cero ya forman un ideal primo.
***** TODO Ejercicio 18
 Esto es equivalente a decir, ya que estamos en un anillo de ideales principales,
 que $X^3-Y^2$ es irreducible.

***** Ejercicio 19
 Vemos que $\alpha$ es ideal. Supongamos que fuera principal, deberÃ­a estar generado
 por uno de mÃ­nimo grado. Si estÃ¡ generado por una constante, como contiene a $(2)$,
 debe estar generado por $(2)$, pero no es el caso porque no contendrÃ­a a $x+2$.

***** TODO Ejercicio 21
***** Ejercicio 24
1. => 2. Sea $R$ con un ideal primo, y sea $a \in R$ no unidad. Tengo $a \in M$ para algÃºn 
   maximal, que debe ser el Ãºnico ideal primo que hay. Aplicando Krull a  
   $S = \{1,a,a^2\dots\}$ contra el ideal $(0)$ tendrÃ­a un ideal no conteniendo 
   a $S$ pero primo, lo que es imposible, asÃ­ que $S$ tiene intersecciÃ³n no vacÃ­a 
   con $(0)$.
2. => 3. Trivial.
3. => 1. Si $R/{\cal N}$ es un cuerpo, ${\cal N}$ es maximal. Si hubiera otro ideal primo, 
   lo meterÃ­amosen su maximal ${\cal M}$ y; si hubiera $m \in {\cal M}-{\cal N}$, se 
   tendrÃ­a $(m) \cup {\cal M} = R$. Entonces $km+n = 1$, luego $km$ es unidad y ${\cal M} = R$.

***** TODO Ejercicio 25
***** Ejercicio 26
 NÃ³tese que si $x$ es nilpotente, tambiÃ©n lo es $ux$ para $u$ unidad.
 Sea $x^n = 0$. Tenemos que:

 \[(1+x)(1-x+x^2-\dots+x^{n-1}) = 1 + (-x)^n = 1\]

 Luego es unidad. Dada suma de unidad y nilpotente, podemos escribirla como:

 \[(u+x) = u(1+u^{-1}x)\]

 Producto de unidades.

***** Ejercicio 27
****** Punto 1
 Por un lado, si todos los $a_i$ fueran nilpotentes se tendrÃ­an $a_iX^i$ nilpotentes.
 Y como la suma de unidad por nilpotente es unidad, la suma total es unidad.

 Sea $\sum b_iX^i$ el inverso de un polinomio $f(x) = \sum a_iX^i$. Es obvio que $a_0$ es unidad
 porque $a_0b_0 = 1$. Veamos por inducciÃ³n que  $a^{r+1}_nb_{m-r} = 0$.

  - *Caso base:* $a_nb_m = 0$ por ser coeficiente del grado mÃ¡ximo.
  - *Caso de inducciÃ³n*: El coeficiente de grado $n+m-1$ serÃ­a:
    $a_nb_{m-1} + a_{n-1}b_n = 0$, luego $a_n(a_nb_{m-1} + a_{n-1}b_m) = a_n^2b_{m-1} = 0$; y aplicarÃ­amos
    inducciÃ³n en los siguientes casos de forma similar.

 En particular, para $r=n$ tenemos que $a_n^{n+1}b_0 = 0$, luego $a_n$ es nilpotente.
 Ahora hacemos inducciÃ³n sobre el grado. TambiÃ©n es nilpotente $a_nX^n$, y 
 entonces tenemos que el polinomio siguiente tambiÃ©n es unidad, pero de menor grado 
 que el original:

 \[f - a_nX^n = \sum^{n-1} a_iX^i \in {\cal U}(R[X])\]

****** Punto 2
 Tenemos que las $a_iX^i$ son nilpotentes; y la suma de nilpotentes es trivialmente
 nilpotente. 

 Hacia el otro lado, hacemos inducciÃ³n sobre $grd(f)$:

  - Si $grd(f) = 0$, entonces $f = a_0$.
  - Sea $f = a_0 + \dots + a_nX^n$, tenemos que que si $f$ es nilpotente a la potencia $m$:
    \[ 0 = f^m = \sum_{j=0}^m {m \choose j}(a_0+\dots+a_n-1X^{n-1})^j(a_nX^n)^{m-j} = a_n^mX^{nm} + \dots\]
    Luego $a^m_n = 0$. El polinomio total es suma de esto y un polinomio de grado menor,
    que por inducciÃ³n es nilpotente.

****** Punto 3. Teorema de MaCoy.
 Una implicaciÃ³n es trivial por definiciÃ³n.

 Sea $f$ divisor de cero. Tomamos un polinomio $g = \sum b_iX^i$ de grado mÃ­nimo con

 $fg = 0$. Entonces $a_nb_m = 0$, y por tanto $a_ng$ anularÃ­a tambiÃ©n a $f$ pero tendrÃ­a grado
 menor que $m$, luego deberÃ­a ser $a_ng = 0$. Ahora procedemos por inducciÃ³n, y se 
 volverÃ­a a tener $a_{n-r}g = 0$.

 Si $a_{n-r}g = 0$ para cualquier $r$, entonces:

 \[ 0 = \sum a_{n-r}b_i X^i\]

 AsÃ­ que $a_{n-r}b_0 = 0$ y se concluye $fb_0 = 0$.

***** Ejercicio 30
#+begin_statement
Calcular el radical de cualquier ideal de $\mathbb{Z}$.
#+end_statement
Dado $(n\mathbb{Z})$ y escribiendo $n = p_1^{e_1}\dots p_n^{e_n}$, podemos ver que un nÃºmero que perteneciera
a su radical deberÃ­a ser tal que:

\[ p_1^{e_1}\dots p_n^{e_n} | (q_1^{f_1}\dots q_n^{f_n})^k\]

Para algÃºn $k$, lo que equivale a que $e_i \leq kf_i$. Es decir, necesitamos sÃ³lo $f_i \neq 0$.
En conclusiÃ³n, $\sqrt{n\mathbb{Z}} = (p_1p_2\dots p_n)\mathbb{Z}$, ideal del producto de sus factores primos.

***** Ejercicio 31
#+begin_statement
Demostrar los siguientes resultados para radicales de ideales de un anillo $R$:

  1. $\sqrt{\sqrt{\alpha}} = \sqrt{\alpha}$.
  2. $\sqrt{\alpha\beta} = \sqrt{\alpha \cap \beta} = \sqrt{\alpha} \cap \sqrt{\beta}$.
  3. $\sqrt{\alpha} = R \Leftrightarrow \alpha = R$.
  4. $\sqrt{\alpha+\beta} = \sqrt{\sqrt{\alpha}+\sqrt{\beta}}$.
  5. Si $\pi$ es un ideal primo, entonces $\sqrt{\pi} = \pi$.
  6. Si $\sqrt{\alpha} + \sqrt{\beta} = R$, entonces $\alpha+\beta = R$.

$\quad$
#+end_statement
****** Punto 1
Sea $y \in \sqrt{\sqrt{\alpha}}$, entonces $y^n \in \sqrt\alpha$, y entonces $(y^n)^m \in \alpha$, luego $y \in \sqrt{\alpha}$.
****** Punto 2
Sea $y \in \sqrt{\alpha \cap \beta}$, entonces $y^n \in \alpha \cap \beta$, y entonces $y^ny^n \in \alpha\beta$, luego 
$y \in \sqrt{\alpha\beta}$.
****** Punto 3
Sea $\sqrt{\alpha} = R$, entonces $1 = 1^n \in\alpha$.
****** Punto 4
Sea $x \in \sqrt{\sqrt\alpha+\sqrt\beta}$, entonces $x^n = u+v$, donde $u^p \in \alpha$, $v^q \in \beta$. Tengo entonces que
$(x^n)^{p+q} = (u+v)^{p+q} \in \alpha+\beta$, donde aplicamos Binomio de Newton.
****** Punto 5
Si fuera $x^n \in \pi$ para $n>1$, por primalidad, deberÃ­a tenerse $x^{n-1} \in \pi$. AsÃ­ que $x\in\pi$.
****** Punto 6
Si $1 = a+b$ con $a^n\in \alpha$, $b^m\in\beta$, entonces, por Binomio de Newton tenemos que 
$1^{n+m} = (a+b)^{n+m} \in \alpha+\beta$.

***** Ejercicio 32
#+begin_statement
Sean $\alpha,\beta$ ideales de un anillo $R$ tales que $\sqrt{\alpha},\sqrt{\beta}$ son primos entre sÃ­. Demostrar que
entonces $\alpha,\beta$ tambiÃ©n son primos entre sÃ­.
#+end_statement
Trivial por el punto 6 del ejercicio 31.

***** Ejercicio 33
#+begin_statement
Sea $R$ un anillo y $\alpha,\beta$ ideales. Demostrar que si $(\alpha)^n\subseteq\beta$ para algÃºn $n\geq 0$, entonces
$\sqrt\alpha \subseteq \sqrt\beta$.
#+end_statement
Tengo trivialmente que $\alpha \subseteq \sqrt\beta$, puedo tomar 
raÃ­ces para tener $\sqrt{\alpha} \subseteq \sqrt{\sqrt{\beta}}$.

***** Ejercicio 34
#+begin_statement
Sea $\alpha\subseteq R$ un ideal tal que $\sqrt\alpha$ es finitamente generado. Demostrar que existe
$n \in \mathbb{N}$ tal que $(\sqrt{\alpha})^n\subseteq\alpha$.
#+end_statement

Sea $(\sqrt{\alpha}) = (x_1,\dots,x_m)$ tales que $x_i^{e_i} \in \alpha$. Entonces se tiene por binomio de Newton:

\[(\sqrt\alpha)^{e_1+\dots+e_m} \subset \alpha\]

Ya que cada sumando tiene algÃºn $x_i$ elevado a mÃ¡s que $e_i$.

***** Ejercicio 35
#+begin_statement
En el anillo de polinomios $\mathbb{F}_2[X,Y]$, con $\mathbb{F}_2$ cuerpo finito de dos elementos, sean
$\alpha_1 = (X,Y)$ y $\alpha_2 = (X-1,Y-1)$. Haciendo uso del ejercicio anterior, demuestra que el
ideal producto $\alpha = \alpha_1\alpha_2$ es un ideal radical.
#+end_statement
Primero vemos que $\alpha_1$ y $\alpha_2$ son radicales. $\alpha_1$ lo forman todos los polinomios con
tÃ©rmino independiente $0$, y ningÃºn polinomio con tÃ©rmino independiente $1$ puede
elevarse hasta tener tÃ©rmino $0$. $\alpha_2$ es la imagen de $\alpha_1$ por el homomorfismo de 
anillos que lleva la indeterminada $X$ en $X+1$, asÃ­ que tambiÃ©n lo es.

Sabemos que $\alpha_1$,$\alpha_2$ son primos entre sÃ­. Luego $\alpha_1\alpha_2 = \alpha_1 \cap \alpha_2$. Ahora tenemos
que:

\[\sqrt{\alpha_1\alpha_2} = \sqrt\alpha_1 \cap \sqrt\alpha_2 = 
\alpha_1\cap\alpha_2 = \alpha_1\alpha_2\]

***** Ejercicio 36
Sea $x \in \sqrt{\alpha}$, entonces $x^n \in \alpha \subset \bigcap \pi_i$, pero $x^n \in \pi_i$ me da $x \in \pi_i$, luego $x \in \bigcap \pi_i$.

***** Ejercicio 37
***** Ejercicio 38
***** Ejercicio 39
***** Ejercicio 40
***** Ejercicio 41
****** Punto 1
 Veamos que la aplicaciÃ³n que va del producto de ideales a $R$ es un homomorfismo
 de grupos abelianos multiplicativo biyectivo.

 \[ f((x_1,\dots,x_n)) = x_1+\dots+x_n\]

 Por la definiciÃ³n de *conjunto independiente*, sabemos que dado $x\in R$ existen Ãºnicos
 $x_1 \in \alpha_1, x_2 \in \alpha_2,\dots, x_t \in \alpha_t$ tales que $x = x_1 + \dots + x_t$. Esto en el caso $2$ es trivial, y
 se puede ampliar por inducciÃ³n.

****** Punto 2
 Por el apartado primero, sabemos que existen $e_1+e_2+\dots+e_t = 1$. Puedo tomar
 $\alpha_i$ como anillo sobre la suma y el producto, pero tomando $e_i$ como unidad.

 Sea $x_i \in \alpha_i$, tenemos que $x_i = x_i(e_1+e_2+\dots+e_t)$, asÃ­ que por unicidad de la 
 descomposiciÃ³n, debe ser $x_i = e_ix_i$.

****** Punto 3
 Tenemos un conjunto de elementos que suman $1$, son idempotentes y ortogonales.

***** Ejercicio 43
****** Punto 1
 Tenemos $X \times \mathbb{Z}$ grupo abeliano con la suma por serlo $X$ y $\mathbb{Z}$. La asociatividad se 
 tiene por:

 \[(x_1,n_1)(x_2,n_2)(x_3,n_3) 
 = (x_1x_2x_3 + n_1x_2x_3+x_1n_2x_3+x_1x_2n_3+x_1n_2n_3+n_1x_2n_3+n_1n_2x_3, 
 n_1n_2n_3)\]

 Y la distributividad:

 \[\begin{aligned}
 (x,y)((a,b)+(c,d)) &= (x,y)(a+c,b+d) = (x(a+c)+y(a+c)+x(b+d)), y(b+d)) \\
 &= (xa+ya+xb,yb) + (xc+yc+xd,yd)
 \end{aligned}\]

****** Punto 2
 Vemos que es trivialmente cerrado para la suma y el producto. Cualquier
 elemento de $X\times \mathbb{Z}$ puede expresarse como $(x,0)+(0,n)$ asÃ­ que tomamos el isomorfismo
 entre $X\times \mathbb{Z}/ X$ y $\mathbb{Z}$ siguiente:

 \[\phi(x,n) = n\]

 Bien definido porque tiene a $X$ como nÃºcleo.

****** Punto 3
 El valor de $f'(x,0) = f(x)$ estÃ¡ fijado por la condiciÃ³n, y por ser homomorfismo
 de anillos debe tener $f'(0,1) = 1$; por tanto, para preservar suma, $f'(x,n) = f(x)+n$.

 Ahora, para comprobar que es homomorfismo vemos que respeta las sumas, la unidad
 y los productos:

 \[\begin{align*}
 f'((a,b)(c,d)) &= f(ac+bc+da) + bd = f(a)f(c)+bf(c)+df(a)+bd \\
		&= (f(a)+b)(f(c)+d) = f'(a,b)f'(c,d)
 \end{align*}\]

***** Ejercicio 44
 $S,T$ son R-MÃ³dulos, asÃ­ que entendemos por $S\times T$ la suma directa como mÃ³dulos.
 Tomamos como definiciÃ³n de $R \cong S \times T$ el que:

 \[\forall r \in R: \exists! s\in S, t\in T:\quad s + t = r\]

****** Descompone con un idempotente
 Primero vemos que $\forall r \in R: re + r(1-e) = r$, y es forma Ãºnica, porque si existieran
 dos formas de expresar $r$:

 \[\begin{align*}
 r &= s + t \\
 r &= s' + t'
 \end{align*}\]

 Entonces $(s-s') + (t-t') = 0$, y no es posible salvo que
 sean iguales, porque $s + t = 0$, con $ea + (1-e)b = 0$ conduce a:

 \[\begin{align*}
 0 &= eea + e(1-e)b &=& ea \\
 0 &= (1-e)ea + (1-e)(1-e)b &=& (1-e)b
 \end{align*}\]


****** Toda descomposiciÃ³n es por idempotente
 Supongamos $R \cong S \times T$. Tenemos la Ãºnica descomposiciÃ³n de $1$ como $u+v = 1$.
 Hacemos otra descomposiciÃ³n de $1$ como:

 \[1 = (u+v)(u+v) = u^2+v^2+2uv\]

 AquÃ­ tenemos que $uv \in S$ y $uv \in T$, asÃ­ que $uv = 0$ (si no, tendrÃ­a dos 
 descomposiciones); por tanto $1 = u^2 + v^2$, y por unicidad $u=u^2$ y $v=v^2$.

 Ahora veamos $S = (u)$, si tengo $s \in S$, entonces $su+sv = s$, y como $sv \in S$ y
 ademÃ¡s $sv \in T$, debe ser nulo y tenerse $s = su$.
***** Ejercicio 45
****** Punto 1
 El producto directo de ideales es trivialmente ideal.

 Sea $\alpha$ un ideal del producto directo, tomamos los siguientes ideales
 \[\beta_i = \{ e_ix \med| x \in \alpha\}\]. Siendo $\pi_i$ la proyecciÃ³n canÃ³nica, tomamos:

 \[ \alpha_i = \pi_i(\beta_i) \]

 Por ser sobreyectivo $\pi$, se tiene que es ideal. Queda probar:

 \[\alpha = \prod \alpha_i\]

****** Punto 2
 Vemos que los ideales primos son de la forma $R_1 \times \dots \times P_i \times \dots R_n$, para algÃºn
 $P_i$ primo en $R_i$.

 Sea un ideal primo del producto, serÃ¡ producto de ideales 
 \[P = \alpha_1\times\alpha_2\dots\times\alpha_n\]. Veremos que son todos el total salvo uno. Supongamos que 
 tuviÃ©ramos dos ideales propios $\alpha_i,\alpha_j$, con $x_i \in \alpha_i$ y $x_j \in \alpha_j$. Tengo:

 \[x = (0,\dots,x_i,0,\dots,1,0,\dots,0) \notin \Pi\]
 \[y = (0,\dots,1,0,\dots,x_j,0,\dots,0) \notin \Pi\]

 Y sin embargo, $xy \in \pi$.

 Para maximales, la demostraciÃ³n es anÃ¡loga.

****** Punto 3
 Por el apartado primero, tiene $2^n$ ideales.

***** Ejercicio propuesto
 Veamos que si $x \in J(R)$, entonces $1-xy$ es unidad. Tenemos que $yx \in J(R)$. Si 
 $1-xy$ no fuera unidad, habrÃ­a un maximal conteniÃ©ndolo, luego ese maximal contendrÃ­a
 a la vez a $1-xy$ y a $xy$.

 Ahora, sea $1-xy$ unidad para cualquier $y$. Si un maximal no contuviera a $x$, entonces
 contendrÃ­a a $1-xy$. TendrÃ­a que necesariamente al aÃ±adir $x$ a ese maximal obtendrÃ­a
 todo el anillo. Luego $m + xy = 1$ y entonces $m = 1-xy$ serÃ­a unidad y pertenecerÃ­a
 al ideal maximal, lo que es imposible.

**** RelaciÃ³n 2
***** DONE Ejercicio 8
***** Ejercicio 16
#+begin_statement
Sea $\leq$  un orden en $\mathbb{N}^n$ que es total y compatible. Haciendo uso de la teorÃ­a
de ideales monomiales, probad que $\leq$ es un buen orden si, y sÃ³lo si, es
monÃ³tono.
#+end_statement

Si es monÃ³tono, entonces es monomial, y los monomiales son buenos [[*Sistemas de generadores][Ã³rdenes]].

Si es buen orden, todo $\mathbb{N}^n$ tiene un mÃ­nimo. Si fuera $a$, se tendrÃ­a
que $a \leq 0$ y por tanto $2a \leq a$, llegando a $a = 2a$, lo que da $a = 0$.

***** Ejercicio 17
#+begin_statement
Sean $I,J \subset K[X_1,\dots,X_n]$ ideales monomiales generados por $\{A_1,\dots,A_s\}$ y
$\{B_1,\dots,B_n\}$, respectivamente:

  1. Demuestra que $I \cap J$ es un ideal monomial.
  2. Demuestra que $\{M_{i,j} \mid i=1,\dots,s;\;j=1,\dots,t\}$, $M_{i,j}$ es un m.c.m. de
     $A_i$ y $B_j$, es un sistema de generadores de $I \cap J$.
  3. Calcula la intersecciÃ³n de los ideales $I = (X,Y^2Z,YZ^2)$ y
     $J = (X^3YZ, X^2Y, Y^2Z^3)$ en el anillo $K[X,Y,Z]$.
#+end_statement

****** Punto 1 y punto 2
Un monomio $X^\mu \in I \cap J$ debe cumplir $A_i \mid X^\mu$, $B_j\mid X^\mu$ y por tanto
$M_{ij}\mid X^\mu$ para algunos $i,j$. Todo monomio del ideal es generado por 
los $M_{ij}$.

Pero dado un $F \in I \cap J$, sus monomios deben estar en $I \cap J$ tambiÃ©n,
asÃ­ que todo polinomio del ideal es generado por los $M_{i,j}$, y es monomial.

****** TODO Punto 3
Como ambos son ideales monomiales, tenemos que calcular sÃ³lo sus
$M_{ij}$.
 
***** Ejercicio 19
#+begin_statement
Demuestra que si $\{J_i \mid i \in I\}$ es una cadena de ideales monomiales, entonces
la uniÃ³n $\bigcup_{i} J_i$ es un ideal monomial.
#+end_statement

Sea un polinomio que estÃ¡ en la uniÃ³n. Sus monomios estÃ¡n en algÃºn $J_i$,
asÃ­ que el ideal es monomial.

***** Ejercicio 20
#+begin_statement
Demostrad que la intersecciÃ³n de ideales monomiales es un ideal monomial.
#+end_statement

Si un polinomio pertenece a la intersecciÃ³n, todos sus monomios pertenecen
a cada ideal y a la intersecciÃ³n. Por lo tanto, la intersecciÃ³n es monomial.

***** Ejercicio 22
#+begin_statement
Demuestra que:

  1. Un ideal monomial es primo ssi estÃ¡ generado por un subconjunto 
     de $\{X_1,\dots,X_n\}$.
  2. El nÃºmero de ideales monomiales primos es finito, y cada uno de ellos
     es finitamente generado.
  3. $(X_1,\dots,X_n)$ es el Ãºnico ideal maximal que es monomial.
#+end_statement

****** Punto 1
Si estÃ¡ generado de esa forma, debe ser primo, ya que es imposible que
el producto de dos polinomios que no contienen a una incÃ³gnita contenga
a la incÃ³gnita o sea nulo.

Si tengo un ideal monomial primo:

\[ I = (X^\alpha \mid \alpha \in A \subseteq \mathbb{N}^n)\]

Tengo que si $gr(\alpha) > 1$, entonces podrÃ­a escribirse como $\alpha = \beta + \gamma$
y deberÃ­a tenerse a $X^\beta$ o $X^\gamma$ en el ideal, generando a $X^\alpha$. Por
descenso infinito tengo todos los generadores de grado $1$.

****** Punto 2
Como son de la forma dada, debe tenerse.

****** Punto 3
Tenemos que es un cuerpo su cociente, luego debe ser un ideal
maximal:

\[ \frac{K[X_1,\dots,X_n]}{(X_1,\dots,X_n)} \cong K\]

Ahora bien, todo ideal maximal es primo, y por tanto de la forma
anterior, y por tanto estÃ¡ contenido en este, que debe ser el Ãºnico
maximal.

***** Ejercicio 26
#+begin_statement
Da un ejemplo de dos polinomios $F,G \in K[X_1,\dots,X_n]$ tales que
$Exp((F,G)) \not\subseteq \{exp(F), exp(G)\} + \mathbb{N}^n$. Observar que la inclusiÃ³n contraria
es siempre cierta.
#+end_statement

Incluso en una variable podemos tener $F = X^2+X$ y $G = X^2-X$, que
cumplen:

\[Exp((F,G)) = \{1\}+\mathbb{N}^n\]
$\{exp(F),exp(G)\} + \mathbb{N}^n = \{2\} + \mathbb{N}^n$

La inclusiÃ³n contraria es cierta porque podemos multiplicar por monomios
como $X^\gamma$ para cualquier $\gamma$.

***** Ejercicio 32
Vemos que en el primer caso queda invariante en el algoritmo de Buchberger
y en el segundo no. Es decir, las semizigias dan un resto distinto de cero
sÃ³lo en el segundo caso.

*** PrÃ¡cticas
**** PrÃ¡ctica 1: Anillos e ideales
***** Anillos bÃ¡sicos
#+BEGIN_SRC sage
  ZZ in Fields
  ZZ in EuclideanDomains
#+END_SRC

#+RESULTS:
: False
: True

****** Extensiones algebraicas
 #+BEGIN_SRC sage
 K = NumberField(x^2+1, 's')
 OK = K.ring_of_integers()
 OK
 SS = NumberField(x^2-5,'s').ring_of_integers()
 SS.0
 SS
 #+END_SRC

 #+RESULTS:
 : Gaussian Integers in Number Field in s with defining polynomial x^2 + 1
 : 1/2*s + 1/2
 : Maximal Order in Number Field in s with defining polynomial x^2 - 5

****** Anillos de enteros mÃ³dulo n
 #+BEGIN_SRC sage
 Z7 = Integers(7)
 Z7.is_field()
 Z8 = Integers(8)
 Z8.cardinality()
 #+END_SRC

 #+RESULTS:
 : True
 : 8

****** Anillos de polinomios
 #+BEGIN_SRC sage
 P = QQ['x']
 P
 P2 = QQ['x','y','z']
 P2
 #+END_SRC

 #+RESULTS:
 : Univariate Polynomial Ring in x over Rational Field
 : Multivariate Polynomial Ring in x, y, z over Rational Field

***** Cuerpos finitos
#+BEGIN_SRC sage
K = GF(5)
a = K.0
a+a+a+a
K.characteristic()
#+END_SRC

#+RESULTS:
: 4
: 5

****** Cuerpos de fracciones
 #+BEGIN_SRC sage
 P.<x> = QQ[]
 K = P.fraction_field()
 1/x in K
 #+END_SRC

 #+RESULTS:
 : True

***** Producto de anillos
#+BEGIN_SRC sage
CZQ = ZZ.cartesian_product(QQ)
CZQ
CZQ.one()
#+END_SRC

#+RESULTS:
: The Cartesian product of (Integer Ring, Rational Field)
: (1, 1)

***** Ideales
#+BEGIN_SRC sage
P.<x,y> = QQ[]
I = Ideal(P,[x+y,x^2+1])
I
ZZ8 = ZZ.quotient(ZZ.ideal(8))
ZZ8 == Integers(8)
#+END_SRC

#+RESULTS:
: Ideal (x + y, x^2 + 1) of Multivariate Polynomial Ring in x, y over Rational Field
: True

****** InclusiÃ³n de ideales
 Definimos la inclusiÃ³n de ideales y sobrecargamos el mÃ©todo de 
 orden para expresarla.
 #+BEGIN_SRC sage
   I = 2*ZZ
   J = 4*ZZ
   def contenido(I,J):
       return I+J == J

   contenido(I,J)
   contenido(J,I)
   sage.rings.ideal.Ideal_pid.__lt__=contenido
   I < J
   J > I
 #+END_SRC

 #+RESULTS:
 : False
 : True
 : False
 : True

****** Operaciones con ideales
 #+BEGIN_SRC sage
   # Suma de ideales
   4*ZZ + 6*ZZ

   # Producto de ideales
   (4*ZZ)*(6*ZZ)

   # IntersecciÃ³n de ideales
   def intersection(I,J):
       a = I.gen()
       b = J.gen()
       q = (a*b).quo_rem(gcd(a,b))
       return Ideal(q[0])
   sage.rings.ideal.Ideal_pid.__and__=intersection

   (4*ZZ)&(6*ZZ)

   # Cociente de ideales
   def cocienteideales(I,J):
       if not (J<I):
           raise "El cociente necesita de inclusiÃ³n"
       return I.gens()*I.ring().quo(J)
   sage.rings.ideal.Ideal_pid.__div__=cocienteideales

   (2*ZZ)/(4*ZZ)

   # Operaciones con ideales
   6*ZZ/((6*ZZ)&(4*ZZ))
 #+END_SRC

 #+RESULTS:
 : Principal ideal (2) of Integer Ring
 : Principal ideal (24) of Integer Ring
 : Principal ideal (12) of Integer Ring
 : Principal ideal (2) of Ring of integers modulo 4
 : Principal ideal (6) of Ring of integers modulo 12

****** Ideales primos y maximales
 #+BEGIN_SRC sage :file images/idealpoly.png :output both
 P.<x,y> = QQ[]
 I = (x^3-y^2)*P
 I.is_prime()
 implicit_plot(x^3-y^2, (x,-1,2), (y,-1,1))
 #+END_SRC

 #+RESULTS:
 [[file:images/idealpoly.png]]

****** Ideales radicales
 #+BEGIN_SRC sage
 R.<x,y> = GF(2)[]
 I = (x,y)*R
 J = (x-1,y-1)*R
 K = I*J
 K.radical() == K
 #+END_SRC

 #+RESULTS:
 : True

***** Homomorfismos de anillos
#+BEGIN_SRC sage
# InclusiÃ³n 
H = Hom(ZZ,QQ)
inc = H([1])
inc(1)

# Homomorfismo al cociente
f = ZZ.hom([1],ZZ.quo(4*ZZ))
f(1) == f(5)
f(6)

# ComposiciÃ³n de homomorfismos
R.<x,y,z> = QQ[]
S.<t> = QQ[]
f = R.hom([t^3,t^5,t^7],S)
g = S.hom([t^2],S)
g*f
#+END_SRC

#+RESULTS:
#+begin_example
1
True
2

Ring morphism:
  From: Multivariate Polynomial Ring in x, y, z over Rational Field
  To:   Univariate Polynomial Ring in t over Rational Field
  Defn: x |--> t^6
        y |--> t^10
        z |--> t^14
#+end_example

**** PrÃ¡ctica 2: Anillos de polinomios
***** Anillos de polinomios
    Para poder usar las variables del anillo de polinomios en 
    sage debemos insertarlas

#+BEGIN_SRC sage
P = PolynomialRing(QQ,['x','y','z','t'])
P.inject_variables()
x in P
#+END_SRC
#+RESULTS:
: Defining x, y, z, t
: True

***** FactorizaciÃ³n
#+BEGIN_SRC sage
P.<x,y> = QQ[]
c = x^3-y^2
c.factor()

f = x*y-1
g = x^2+y^2-4
#+END_SRC
#+RESULTS:
: (-1) * (-x^3 + y^2)

***** Polinomios en una variable
#+BEGIN_SRC sage
R = Zmod(2)['x']
R.inject_variables()
filter(is_prime,list(R.monics(of_degree=5)))
#+END_SRC
#+RESULTS:
: Defining x
: 
: [x^5 + x^2 + 1,
:  x^5 + x^3 + 1,
:  x^5 + x^3 + x^2 + x + 1,
:  x^5 + x^4 + x^2 + x + 1,
:  x^5 + x^4 + x^3 + x + 1,
:  x^5 + x^4 + x^3 + x^2 + 1]

***** Ejemplo: Cuerpo de 256 elementos
Vamos a tomar un cuerpo de 256 elementos y vamos a multiplicar en Ã©l usando 
logaritmos. Es decir, vamos a representar cada polinomio como un nÃºmero binario
y vamos a sumarlos para multiplicar.

#+BEGIN_SRC sage
p = R.irreducible_element(8)
Q = R.quo(p*R,'a')
Q.is_field()
a = Q.0

# Potencias de a, son todo elementos de Q
l = [a^i for i in range(2**8-1)]
l.index(a^2+1)
#+END_SRC
#+RESULTS:
: True
: 50

***** TODO Ejercicio 1
    #+begin_statement
    Demuestra que la clase de $x+1$ genera $\mathbb{Z}_2[x]/(x^8+x^4+x^3+x+1)$.
    #+end_statement

***** Ãrdenes monomiales
    #+BEGIN_SRC sage
      P = PolynomialRing(QQ,x,3,order='lex')
      P.inject_variables()

      ds = list(WeightedIntegerVectors(2,[1,1,1]))+list(WeightedIntegerVectors(1,[1,1,1]))
      ms=[P({tuple(l):1}) for l in ds]
      sorted(ms)

      P=PolynomialRing(QQ,x,3,order='degrevlex')
      P.inject_variables()

      ds = list(WeightedIntegerVectors(2,[1,1,1]))+list(WeightedIntegerVectors(1,[1,1,1]))
      ms=[P({tuple(l):1}) for l in ds]
      sorted(ms)
    #+END_SRC

    #+RESULTS:
    : Defining x0, x1, x2
    : [x2, x2^2, x1, x1*x2, x1^2, x0, x0*x2, x0*x1, x0^2]
    : Defining x0, x1, x2
    : [x2, x1, x0, x2^2, x1*x2, x0*x2, x1^2, x0*x1, x0^2]

****** Ejercicio
     Ordena mediante el orden lexicogrÃ¡fico y lexicogrÃ¡fico graduado todos los 
     monomios en tres variables de grado 3.

     #+BEGIN_SRC sage
       P=PolynomialRing(QQ,x,3)
       P.inject_variables()

       ds = list(WeightedIntegerVectors(3,[1,1,1]))
       ms=[P({tuple(l):1}) for l in ds]
       sorted(ms)
     #+END_SRC

     #+RESULTS:
     #+begin_example
     Defining x0, x1, x2

     [x2^3,
      x1*x2^2,
      x0*x2^2,
      x1^2*x2,
      x0*x1*x2,
      x0^2*x2,
      x1^3,
      x0*x1^2,
      x0^2*x1,
      x0^3]
     #+end_example

***** Algoritmo de la divisiÃ³n
**** PrÃ¡ctica 3: Bases de GrÃ¶bner
***** Bases de GrÃ¶bner
    #+BEGIN_SRC sage
      P = PolynomialRing(QQ, ['x','y','z'], order='lex')
      I = (x^4-y^4+z^3-1, x^3+y^2+z^2-1)*P
      len(I.groebner_basis())
    #+END_SRC

    #+RESULTS:
    : 5

***** Pertenencia de polinomios
    #+BEGIN_SRC sage
      P.<x,y>=QQ[]
      I = (x^2-y^3,x)*P
      I.groebner_basis()
      y**3 in I
    #+END_SRC

    #+RESULTS:
    : [y^3, x]
    : True

****** Ejercicio
     #+BEGIN_SRC sage
       P.<x,y,z>=QQ[]
       g1=x^2*y*z+y^2*z+1
       g2=x*y^2*z+y*z^2-2
       g3=x*y*z^2+z+3
       I = (g1,g2,g3)*P
       len(I.groebner_basis())
       P.quo(I).()
     #+END_SRC

     #+RESULTS:
     : 8
     : Rational Field

***** CÃºpside
    #+BEGIN_SRC sage
      P = PolynomialRing(QQ,["t","x","y"],order="lex")
      P.inject_variables()
      I = (x-t^2,y-t^3)*P
      I.groebner_basis()
      I.elimination_ideal([t])
    #+END_SRC

    #+RESULTS:
    : Defining t, x, y
    : [t^2 - x, t*x - y, t*y - x^2, x^3 - y^2]
    : Ideal (x^3 - y^2) of Multivariate Polynomial Ring in t, x, y over Rational Field

    
****** Ejercicio
     #+BEGIN_SRC sage
       P = PolynomialRing(QQ,["t","x","y","z","u"],order="lex")
       P.inject_variables()
       I = (x-t-u, y-t^2-2*t*u, z-t^3-3*t^2*u)*P
       I.elimination_ideal([t,]u)
     #+END_SRC

     #+RESULTS:
     : Defining t, x, y, z, u
     : Ideal (4*x^3*z - 3*x^2*y^2 - 6*x*y*z + 4*y^3 + z^2) of Multivariate Polynomial Ring in t, x, y, z, u over Rational Field     
** Ãlgebra III
# Exportaba con config.setup

*** 1. Polinomios simÃ©tricos
**** MotivaciÃ³n: La cÃºbica
***** Polinomios cÃºbicos
Toda ecuaciÃ³n cÃºbica polinÃ³mica puede escribirse en la forma
\(Y^3 + pY + q\), tomando un cambio de variable desde la original
\(X \mapsto - \frac{1}{3} b\). Esto se llama una cÃºbica deprimida.

****** MÃ©todo de Vieta
El mÃ©todo de Vieta toma \(t = w - \frac{p}{3w}\), y llega a la ecuaciÃ³n:

\[w^3 + q - \frac{p^3}{27w^3} = 0\]

Ahora podemos resolver esa cuadrÃ¡tica y resolver luego la ecuaciÃ³n
en $w^3$.

**** 1.1. PolinÃ³mios simÃ©tricos
***** Polinomios simÃ©tricos
Un *polinomio simÃ©trico* es aquel invariante por $f_\sigma$ para cualquier $\sigma \in S_r$, 
donde $f_\sigma (X_i) = X_{\sigma i}$. Llamamos $Sim(A[X_1\dots X_n])$ al subanillo de polinomios 
simÃ©tricos.

***** Componentes homogÃ©neas
Llamamos *componente homogÃ©nea* a cada sumando homogÃ©neo maximal de un
polinomio. Un polinomio es simÃ©trico si y sÃ³lo si cada una de sus
componentes lo es.

***** Polinomios simÃ©tricos elementales
Los polinomios simÃ©tricos elementales son aquellos de la forma:

\[e_i = \sum_{i_1 < \dots < i_i} X_{i1} X_{i2} \dots X_{ii}\]

***** Teorema fundamental de los polinomios simÃ©tricos
Los polinomios elementales generan cada polinomio $Sim(A[X_1\dots X_n])$ 
de forma Ãºnica. En particular,

\[\omega : A[X_1,\dots,X_r] \longrightarrow Sim(A[X_1,\dots,X_r])\]

con $\omega(a) = a$ y $\omega(X_i) = e_i$ es un isomorfismo.

****** DemostraciÃ³n
Damos una relaciÃ³n de orden lexicogrÃ¡fica entre los monomios de un
polinomio simÃ©trico homogÃ©neo. Al mayor de ellos, llamado 
$X_1^{k_1} \dots X_r^{k_r}$ le restamos $e^{b1}_1 e^{b2}_2 \dots e^{br}_r$, donde
$b_i = k_i - k_{i+1}$. Nos quedarÃ¡ $0$ u otro polinomio simÃ©trico de
igual grado pero menor en el orden lexicogrÃ¡fico. Este proceso debe
ser finito.

La unicidad se obtiene con $0 = h(e_1\dots e_r) - k(e_1\dots e_r) =
l(e_1 \dots e_r)$.

**** 1.2. Polinomios alternados
***** 1.2.1. Polinomio alternado
Un polinomio $f$ es alternado cuando para toda permutaciÃ³n se tiene
$\sigma(f) = sign(\sigma) f$.

**** 1.3. La resultante
***** 1.3.2. Resultante
Dados dos polinomios $f,g$ en un cuerpo $K$ en el que descomponen 
podemos escribirlos como:

\[ f = a_n(X-\alpha_1)\dots(X-\alpha_n) = a_n \prod^n_{i=1}(X-\alpha_i)\]
\[ g = b_n(X-\beta_1)\dots(X-\beta_n) = a_n \prod^n_{i=1}(X-\beta_i)\]

La *resultante* busca ser una expresiÃ³n que se anula cuando tienen
raÃ­z comÃºn, y se define como:

\[ R(f,g) = a_n^m b_m^n \prod^n_{i=1} \prod^m_{j=1} (\alpha_i - \beta_j)\]

***** TODO 1.3.3. Propiedades de la resultante
La resultante de dos polinomios $f,g$ cumple:

 1. $R(f,g) = 0$ ssi tienen una raÃ­z comÃºn.
 2. $R(g,f) = (-1)^{nm}R(f,g)$, siendo $nm$ el producto del nÃºmero de raÃ­ces.
 3. $R(f,g) = a^m_n \prod^n_{i=1} g(\alpha_i)$
 4. $R(fg,h) = R(f,h)R(g,h)$, $R(f,gh) = R(f,g)R(f,h)$
 5. Si $m=0$, entonces $R(f,k) = k^n$
 6. $R(X^k,f) = a_0^k$; con $R(f,X^k) = (-1)^{nk}a_0^k$

**** 1.4. Discriminante
***** 1.4.1. RaÃ­ces mÃºltiples
Podemos usar la resultante para caracterizar los polinomios
con raÃ­ces mÃºltiples, que son aquellos que comparten raÃ­z con
su derivada.

\[ R(f,f') = a_n^{n-1} \prod f'(\alpha_j)\]

***** 1.4.1. El discriminante
El *discriminante* de un polinomio con raÃ­ces $\alpha_1, \dots, \alpha_n$ en una 
clausura algebraica es:

\[\text{Discr}(p) = a^{2n-2}_n \prod_{i<j}(\alpha_i-\alpha_j)^2\]

***** 1.4.1. RelaciÃ³n con la resultante
\[R(p,p') = (-1)^{\frac{n(n-1)}{2}}a_n \text{Discr}(p)\]

***** 1.4.2. Propiedades del discriminante
El determinante cumple:

  1. $f_1,f_2 \in F[X] \implies D(f_1f_2) = D(f_1)D(f_2)R(f_1,f_2)^2$.
  2. $f_1,\dots,f_r \in F[X] \implies D(f_1\dots f_r) = D(f_1)\dots D(f_r)R^2$ con $R \in F$.

**** 1.5. MÃ©todos de cÃ¡lculo
***** TODO 1.5.2. MÃ©todo modular
***** TODO 1.5.3. Por el algoritmo de Euclides
***** 1.5.4. Resultante de Euler-Sylvester-Cayley
Definimos la resultante de Euler-Sylvester-Cayley:

\[
R(f,g) = \left| \begin{matrix}
a_n & a_{n-1} & \dots & a_0 & 0 & \dots &\\
0   & a_n & \dots & a_{1} & a_0 & 0 & \dots \\
0   &   0 & a_n & \dots & a_1 & a_0 & \dots \\
&      &     &\vdots & & & \\
b_m & b_{m-1} & \dots & b_0 & 0 & \dots &\\
0   & b_m & \dots & b_1 & b_0 & 0 & \dots \\
0   &   0 & b_m & \dots & b_1 & b_0 & \dots \\
\end{matrix} \right|
\]

****** Origen
La resultante se obtiene como la determinante de la
matriz del sistema de ecuaciones que dan:

\[ \begin{aligned}
X^{m-1} f &= 0 \\
X^{m-2} f &= 0 \\
& \vdots \\
1f &= 0 \\
X^{n-1}g &= 0 \\
X^{n-2}g &= 0 \\
& \vdots \\
1g &= 0 \\
\end{aligned}\]

Por Teorema de RouchÃ©, este sistema tiene soluciÃ³n ssi 
el determinante de los coeficientes es cero.

***** 1.5.5. Resultante como determinante
La *resultante* de dos polinomios $p,q$ es el determinante soluciÃ³n de 
$pq' - qp' = 0$ dados $p$ y $q$.

\[R(p,q) = \left| \begin{matrix}
a_0 & a_1 & \dots & a_n & 0 & \dots &\\
0   & a_0 & \dots & a_{n-1} & a_n & 0 & \dots \\
0   &   0 & a_0 & \dots & a_{n-1} & a_n & \dots \\
&     &     &\dots & & & \\
b_0 & b_1 & \dots & b_m & 0 & \dots &\\
0   & b_0 & \dots & b_{m-1} & b_m & 0 & \dots \\
0   &   0 & b_0 & \dots & b_{m-1} & b_m & \dots \\
\end{matrix} \right|
\]

Y llamamos *matriz resultante* a la matriz de la que es determinante.

*** 2. Series de grupos y grupos solubles
**** 2.1. Series de composiciÃ³n
***** 2.1.1. Factor
Sea $G$ grupo, llamamos factor a cualquier $H/H'$ donde $G > H \trianglerighteq H'$.

***** 2.1.2. ProyecciÃ³n
Llamamos proyecciÃ³n de $H/H'$ sobre $K/K'$, ambos factores, a:

\[\frac
{K'(H\cap K)}
{K'(H'\cap K)}\]

***** 2.1.3. Serie
Llamamos serie a toda cadena finita:

\[ G > G_1 > G_2 > \dots > G_r = 1\]

Donde llamamos a $r$ la longitud del grupo.

***** 2.1.4. Refinamiento de una serie
Dadas dos series,

\[ G > G_1 > G_2 > \dots > G_r = 1\]
\[ G > G'_1 > G'_2 > \dots > G'_r = 1\]

llamamos a la segunda refinamiento si todo grupo suyo aparece en
la primera. Es refinamiento propio si ademÃ¡s son distintas.

***** 2.1.5. Serie normal
Una serie es *normal* cuando se verfica $G_i \trianglerighteq G_{i+1}$. Llamamos a $G_{i-1}/G_i$
los *factores* de la serie.

***** 2.1.5. Serie propia
Una serie propia tiene sÃ³lo inclusiones propias $G_i \gneq G_{i+1}$.

***** 2.1.5. Isomorfismo de series
Dos series son isomorfas cuando existe una permutaciÃ³n que hace 
isomorfos sus factores:

\[\exists \sigma \in S_r : \quad 
G_{i-1}/G_i \cong H_{\sigma(i)-1}/H_{\sigma(i)}\]

***** 2.1.5. Serie de composiciÃ³n
Una *serie de composiciÃ³n* es una serie normal propia sin 
refinamientos normales. Llamamos *factores de composiciÃ³n* a sus
factores.

***** 2.1.6. Grupo simple
Un grupo simple es aquel que no admite subgrupos normales propios.

***** 2.1.7. Grupos abelianos finitos simples
Un grupo abeliano, finito y simple es isomorfo a $\mathbb{Z}_p$ para algÃºn $p$ primo.

****** TODO DemostraciÃ³n

***** 2.1.8. Los factores de composiciÃ³n son simples
Los factores de cualquier serie de composiciÃ³n son simples.

****** TODO DemostraciÃ³n

***** 2.1.9. Existencia de la serie de composiciÃ³n
Todo grupo finito posee una serie de composiciÃ³n.

***** 2.1.10. Teorema de refinamiento de Schreier
Dos series normales de un grupo tienen refinamientos isomorfos.

***** 2.1.11. Teorema de Jordan-Holder
Si un grupo admite serie de composiciÃ³n, toda serie normal propia puede
refinarse a una serie de composiciÃ³n. Las series de composiciÃ³n son 
isomorfas.

**** 2.2. El programa de Holder
***** TODO 2.2.1. Teorema de clasificaciÃ³n de grupos simples finitos
***** 2.2.2. Teorema de Abel
El grupo $A_n$ es simple para $n \geq 5$.

****** TODO DemostraciÃ³n

***** 2.2.3. Teorema de Feit-Thompson
Si $G$ es simple de orden impar, entonces $G \cong \mathbb{Z}_p$ con $p$ primo.

****** TODO DemostraciÃ³n

**** 2.3. Grupos solubles
***** 2.3.1. Serie derivada
Dado $G$, definimos la serie derivada de $G$ como:

\[G = G^0 > G' > G'' > \dots \]

donde $G^{(i+1} = [G^{(i}, G^{(i}]$ es el grupo derivado. NÃ³tese que no tiene por
quÃ© ser finita.

***** 2.3.2. CaracterizaciÃ³n de grupos solubles
Para $G$ grupo finito, equivalen:

 1. Los factores de composiciÃ³n son cÃ­clicos de orden primo.
 2. $G$ tiene serie normal con factores cÃ­clicos.
 3. $G$ tiene serie normal con factores abelianos.
 4. Se tiene $G^{(i} = 1$.

****** TODO DemostraciÃ³n

***** 2.3.3. Grupo soluble
Un grupo es soluble si tiene una serie normal con factores cÃ­clicos.

***** 2.3.4. Subgrupos de solubles
Son solubles:

 1. Los subgrupos de un grupo soluble.
 2. Los cocientes de un grupo soluble.
 3. Si $N$ y $G/N$ son solubles, $G$ es soluble.

****** TODO DemostraciÃ³n

***** 2.3.5. Producto de solubles
Todo producto finito de grupos solubles es soluble.

****** TODO DemostraciÃ³n

***** 2.3.6. Teorema de Hall
Sea $G$ soluble de orden $mk$ cumpliendo $mcd(m,k) = 1$. Entonces:

 1. $G$ posee un grupo de orden $m$.
 2. Dos subgrupos cualesquiera de orden $m$ son conjugados.
 3. Todo subgrupo de orden $m' \mid m$ estÃ¡ contenido en uno de orden $m$.
 4. El nÃºmero de subgrupos de orden $m$, $r_m$ es producto de factores
    congruentes a $1$ mÃ³dulo algÃºn factor primo de $m$. Es ademÃ¡s potencia
    de primo y divide a alguno de los factores de $G$.

****** TODO DemostraciÃ³n

***** 2.3.7. CaracterizaciÃ³n de grupo soluble
Dado $G$ grupo finito, es soluble ssi para cualquier descomposiciÃ³n $|G|=mk$
con $mcd(m,k) = 1$, existe un subgrupo de orden $m$.

*** 3. Extensiones de cuerpos
**** 3.1. Generalidades
***** 3.1.1. Extensiones de cuerpos
Una *extensiÃ³n de cuerpos* es un subcuerpo $K$ de $F$, se nota por $F/K$. 

***** 3.1.2. Grado de la extensiÃ³n
Llamamos *grado* a la dimensiÃ³n de $F$ como espacio vectorial.
Notamos por $[F : K]$.

***** 3.1.3. Cuerpo intermedio
Un cuerpo intermedio entre $F$ y $K$ es cualquier subcuerpo de $F$ 
conteniendo a $K$.

***** 3.1.4. Torre de cuerpos
Una torre es una sucesiÃ³n de subcuerpos:

\[F_0 \subset F_1 \subset \dots \subset F_n\]

***** 3.1.5. Extensiones finitas
Una extensiÃ³n es finita ssi $[F:K]$ es finito.

***** 3.1.6. Base de una torre de inclusiones
Sea $K \supset F \supset E$ una torre de inclusiones. Sean $\{u_i\}_{i\in I}$ una base de $E$
sobre $F$ y $\{v_j\}_{j\in J}$ una base de $F$ sobre $K$. Entonces $\{u_iv_j\}$ es una base
de $E$ sobre $K$.

****** DemostraciÃ³n
******* Es sistema de generadores
Si tenemos ambos sistemas de generadores, podemos escribir cada
elemento de $E$ como:

\[ e 
= \sum u_i f_i 
= \sum u_i \left(\sum v_j k_{ij}\right) 
= \sum u_iv_ik_{ij}\]

******* Son linealmente independientes
Aplicando la independencia lineal de cada una de las bases:

\[ \sum u_iv_jk_{ij} 
= \sum u_i \left( \sum v_jk_{ij}\right) = 0\]

Tenemos que $\sum v_jk_{ij} = 0$, luego $k_{ij} = 0$.

***** 3.1.7. Teorema del grado
Sean $K \subset F \subset E$, extensiones de cuerpos, se tiene que:

\[ [E:K] = [E:F][F:K] \]

****** DemostraciÃ³n
Teniendo una base de cada uno de ellos, calculamos la base
de la [[*3.1.6. Base de una torre de inclusiones][torre de inclusiones]], que nos da la dimensiÃ³n.

***** 3.1.8. Corolario al Teorema del grado: finitud
Sean $K \subset F \subset E$, la extensiÃ³n $E/K$ es finita ssi las extensiones
$E/F$ y $F/K$ lo son.

****** DemostraciÃ³n
Si ambas son finitas, podemos aplicar el [[*3.1.7. Teorema del grado][teorema del grado]]. Cuando
$E/K$ es finita, tenemos que $E/F$ tiene como sistema generador a la
base y $F/K$ es un subespacio de $E/K$.

***** 3.1.9. Corolario al Teorema del grado: torres de cuerpos
Sea $F_0 \subset F_1 \subset \dots \subset F_n$ torre de longitud $n$, entonces:

\[ [F_n : F_0] =
[F_n:F_{n-1}] \dots [F_2:F_1][F_1 : F_0]
\]

****** DemostraciÃ³n
Por inducciÃ³n sobre la longitud de la torre y aplicando el teorema
del grado a cada paso.

***** 3.1.10. Corolario al Teorema del grado: extensiones primas
Sea $F/K$ una extensiÃ³n tal que $[F:K] = p$ es primo. Entonces no
existe ningÃºn cuerpo intermedio propio.

****** DemostraciÃ³n
Usando el teorema del grado, tenemos que deberÃ­a tener grado $p$, en
cuyo caso serÃ­a un subespacio de la misma dimensiÃ³n que $F$, y por
tanto $F$. O deberÃ­a tener grado $1$, en cuyo caso serÃ­a $K$.

**** 3.2. Elementos algebraicos y extensiones algebraicas
***** 3.2.1. Homomorfismo unital
Para todo anillo $A$ existe un Ãºnico homomorfismo de anillos
$1_\mathbb{Z} : \mathbb{Z} \longrightarrow A$, llamado *homorfismo unital*.

***** 3.2.2. CaracterÃ­stica del anillo
La caracterÃ­stica de $A$ es el entero no negativo que genera
al ideal $ker(1_\mathbb{Z})$.

***** 3.2.3. CaracterÃ­stica en dominios de integridad
Si $A$ es dominio de integridad, $car(A)$ es primo o $0$.

****** DemostraciÃ³n
Trivialmente desde el homomorfismo unital. Si no fuera asÃ­,
tendrÃ­amos $ab = 0$ enteros.

***** 3.2.4. CaracterizaciÃ³n de la caracterÃ­stica
$car(A)=n$ ssi $n$ es el menor entero positivo tal que $na = 0$ para
todo $a \in A$.

****** DemostraciÃ³n
Si hubiera otro menor, deberÃ­a pertenecer al nÃºcleo del homomorfismo
unital, y no podrÃ­a ser generado por $n$. Si cumple la condiciÃ³n
y es el menor, todo el resto de elementos del nÃºcleo deben ser 
mÃºltiplos, porque si no lo fueran, podrÃ­amos crear un menor con 
Bezout.

***** 3.2.5. IntersecciÃ³n de anillos
Sea $A$ un anillo y sea $\{B_i\}_{i\in I}$ una familia de subanillos. Entonces
$\bigcap B_i$ es subanillo. AnÃ¡logo para cuerpos y subcuerpos.

****** DemostraciÃ³n
Si dos elementos pertenecen a todos los $B_i$, tenemos que su suma
y su producto pertenece a cada uno de ellos.

***** 3.2.6. Anillo primo
Llamamos subanillo primo de $A$ a la intersecciÃ³n de todos los 
subanillos de $A$.

***** 3.2.7. ClasificaciÃ³n de anillos primos
El subanillo primo de un $A$ es isomorfo a $\mathbb{Z}$ si $car(A) = 0$ y
a $\mathbb{Z}/n\mathbb{Z}$ si $car(A) = n \neq 0$.

****** DemostraciÃ³n
En ambos casos, ellos son subanillos por ser imÃ¡genes del 
homomorfismo unital, como se comprueba por primer teorema de
isomorfÃ­a.

***** 3.2.8. Subcuerpo primo
Llamamos subcuerpo primo de $K$ a la intersecciÃ³n de todos los
subcuerpos de $K$.

***** 3.2.9. ClasificaciÃ³n de subcuerpos primos
El subcuerpo primo de un cuerpo $K$ es isomorfo a $\mathbb{Q}$ cuando
$car(K)=0$ y a $\mathbb{Z}/p\mathbb{Z}$ cuando $car(K) = p \neq 0$.

****** DemostraciÃ³n
De nuevo, vuelve a tenerse una inyecciÃ³n de ambos por el 
homomorfismo unital. Cualquier subanillo contendrÃ¡ a $1$ y por
tanto a este subcuerpo.

***** 3.2.10. Subanillo generado
Sea $F/K$ extensiÃ³n con $S\subseteq F$; llamamos *subanillo generado*
$K[S]$ a la intersecciÃ³n de todos los subanillos de $F$ conteniendo
a $K$ y a $S$.

***** 3.2.10. Subcuerpo generado
Sea $F/K$ extensiÃ³n con $S \subseteq F$; llamamos *subcuerpo generado*
$K(S)$ a la intersecciÃ³n de todos los subcuerpos de $F$ conteniendo
a $K$ y a $S$.

***** 3.2.11. Propiedades de subanillos y subcuerpos generados
Para $S,T \subseteq F$ extensiÃ³n de $K$, tenemos:

 - $K[S \cup T] = K[S][T] = K[T][S]$
 - $K(S \cup T) = K(S)(T) = K(T)(S)$

****** DemostraciÃ³n
En cualquiera de los dos casos la definiciÃ³n es la intersecciÃ³n
de todos los que contienen a $K$, $T$ y $S$.

***** 3.2.12. Subcuerpo compuesto
Dados $K \subset E,F \subset L$, definimos el subcuerpo compuesto
$EF = E(F) = F(E)$.

****** DemostraciÃ³n
Son iguales trivialmente desde la definiciÃ³n.

***** 3.2.13. Conjunto de generadores
Sea $F/K$ con $S \subseteq F$, es un subconjunto de generadores si
$F = K(S)$.

***** 3.2.14. ExtensiÃ³n finitamente generada
Una extensiÃ³n $F/K$ es finitamente generada cuando tiene un
conjunto finito de generadores, $F = K(u_1,u_2,\dots,u_n)$.

***** 3.2.15. Extensiones simples y elementos primitivos
Una extensiÃ³n $F/K$ se llama simple cuando $F = K(u)$. Al $u$
se le llama *elemento primitivo* para la extensiÃ³n.

***** 3.2.16. Elementos algebraicos
$\alpha \in F$ es *algebraico* sobre $K$ si existe polinomio $f \in K[x]$ tal 
que $f(\alpha) = 0$. 

Un no algebraico es *trascendente* y una extensiÃ³n es *algebraica* 
si lo son todos sus elementos.

***** 3.2.16. Polinomios irreducibles
Dado $F/K$ con $\alpha \in F$ algebraico. Existe un Ãºnico polinomio 
irreducible del que $\alpha$ es raÃ­z salvo asociados, llamado $Irr(\alpha)$.
   
****** Existencia y unicidad del polinomio irreducible
Tomo el nÃºcleo del homomorfismo que evalÃºa un polinomio en $\alpha$. 
Por ser un ideal en PID, estarÃ¡ generado por algÃºn polinomio $f$ 
no nulo y no constante.

Este serÃ¡ irreducible, porque si no lo fuera, con $f = g_1g_2$ se 
tendrÃ­a:

\[0 = f(\alpha) = g_1(\alpha)g_2(\alpha)\]

Un polinomio de grado mÃ­nimo deberÃ­a estar dentro del ideal, 
y por tanto ser asociado de $f$, que lo genera.

***** 3.2.16. Propiedades de los polinomios irreducibles
Sea $F/K$ extensiÃ³n con $u \in F$ algebraico. Se cumple:

  1. $K(u) = K[u]$
  2. $K[u] \cong K[X]/(Irr(u,K))$
  3. $[K(u):K]$ es igual al grado de $Irr(u,K)$.
  4. $\{1,u,u^2,\dots,u^{n-1}\}$ es una base de $K[u]$ sobre $K$.
  5. $f(u)=0$ ssi $Irr(u,K) \mid f$.

Llamamos *grado* del elemento $u$ al grado de $Irr(u,K)$.

****** DemostraciÃ³n
******* Punto 1
Sabemos que el anillo generado estÃ¡ dentro del cuerpo generado,
y ademÃ¡s, $u^{-1}$ estÃ¡ en el anillo generado porque, si su polinomio
irreducible nos da $\sum a_iu^i = 0$, tenemos:

\[ u \left(a_nu^{n-1} + a_{n-1}u^{n-2} + \dots a_1 \right)\frac{1}{a_0} = 1\]

******* Punto 2
Aplicando el primer teorema de isomorfÃ­a al morfismo evaluaciÃ³n,
tenemos el resultado.

******* Punto 3
Tenemos que $1,u,u^2,\dots,u^{n-1}$ son linealmente independientes porque
una relaciÃ³n lineal entre ellos darÃ­a un polinomio menor que el
mÃ­nimo. AdemÃ¡s son base trivialmente porque $u^{-1}$ puede expresarse
linealmente como polinomio suyo como mostramos [[*Punto 1][antes]] y porque
cuaquier expresiÃ³n polinÃ³mica de grado mayor a $n$ puede dividirse 
por el polinomio irreducible para obtener otra de grado menor.

******* Punto 4
La misma demostraciÃ³n [[*Punto 3][anterior]].

******* Punto 5
Un polinomio verificando $f(u)=0$ estÃ¡ dentro del nÃºcleo del
homomorfismo evaluaciÃ³n.

***** 3.2.17. Algebraicos en una torre de cuerpos
Sea $K \subset F \subset E$ con $u \in E$ algebraico sobre $K$, entonces $u$ es
algebraico sobre $F$ y $Irr(u,F)$ divide a $Irr(u,K)$.

****** DemostraciÃ³n
Notamos que $Irr(u,K)$ es tambiÃ©n un polinomio sobre $F$ que anula
a $u$, asÃ­ que, por las propiedades de los polinomios irreducibles,
se debe tener $Irr(u,F) \mid Irr(u,K)$.

***** 3.2.18. Extensiones algebraicas
Una extensiÃ³n se llama *algebraica* si todos sus elementos lo son.
Es *trascendente* en otro caso.

***** 3.2.19. Elementos algebraicamente independientes
Los elementos $\{u_i \mid i\in I\}$ son algebraicamente independientes si el
homomorfismo de evaluaciÃ³n sobre el cuerpo de polinomios en varias
variables $K[X_i \mid i\in I]$ es inyectivo.

***** 3.2.20. Extensiones puramente trascendentes
Una extensiÃ³n $F/K$ se llama puramente trascendente si $F = K(S)$
donde $S$ un conjunto de algebraicamente independientes.

***** 3.2.21. GeneraciÃ³n finita de elementos
Para $F/K$ extensiÃ³n cualquiera $S \subseteq F$, se tiene:

 1. Para $u \in K[S]$ existe un subconjunto $\{u_1,\dots,u_n\} \subset S$ tal que
    $u \in K[u_1,\dots,u_n]$.
 2. Para $u \in K(S)$ existe un subconjunto $\{u_1,\dots,u_n\} \subset S$ tal que
    $u \in K(u_1,\dots,u_n)$.

****** DemostraciÃ³n
Tener $u \in K[S]$ nos da una expresiÃ³n polinÃ³mica finita como elementos
de $S$. Los elementos involucrados en esa expresiÃ³n crean una extensiÃ³n
finita en la que estÃ¡ $u$. AnÃ¡logo en el caso de cuerpos.

***** 3.2.22. GeneraciÃ³n del compuesto
Sean $K \subset E,K(S) \subset L$, entonces $EK(S) = E(S)$.

****** DemostraciÃ³n
Por definiciÃ³n del cuerpo compuesto, serÃ¡ $EK(S) = K(E)(S) = E(S)$.

***** 3.2.23. Grado de una extensiÃ³n compuesta
Sean $K \subset E,F \subset L$. Entonces:

\[ [EF:K] \leq [E:K][F:K]\]

****** DemostraciÃ³n
En el caso finito, el cuerpo generado por las bases de $E$ y
de $F$ multiplicadas contiene a todo elemento de $E$ y de $F$, por 
lo que es sistema de generadores de $EF$.

***** 3.2.24. Extensiones primas relativas
Sean $K \subset E,F \subset L$ con $n = [E:K]$ y $m = [F:K]$ primos relativos.
Entonces $[EF:K] = [E:K][F:K]$.

****** DemostraciÃ³n
Sea $\{f_i\}$ base de $F$ sobre $K$. Tenemos que es sistema de generadores
de $F$ sobre $E$, y por tanto, de $EF$ sobre $E$. AsÃ­ $[EF:E] \leq [F:K]$
y anÃ¡logamente $[EF:F] \leq [E:K]$.

Por otro lado, por teorema del grado tenemos:

\[[EF:K] = [EF:F][F:K] = [EF:F]m\]
\[[EF:K] = [EF:E][E:K] = [EF:E]n\] 

AsÃ­, por ser primos relativos, tenemos $n \mid [EF:E]$ y $m \mid [EF:F]$; 
teniÃ©ndose finalmente:

\[ [EF:F] = [E:K] \]
\[ [EF:E] = [F:K] \]

***** 3.2.25. ExtensiÃ³n finitamente generada por algebraicos es finita
Una extensiÃ³n $F = K(u_1,\dots,u_n)$ finitamente generada por $u_i$ algebraicos
es finita.

****** DemostraciÃ³n
Todo elemento algebraico cumple una relaciÃ³n polinÃ³mica. AsÃ­,
todo elemento de grado igual o mayor a esta relaciÃ³n, puede
expresarse como elementos de grado menor.

Si tenemos $e_i$ como el exponente mayor al que puedo elevar $u_i$ sin
que pueda ser reescrito, tenemos un sistema de generadores de $F$ 
finito como:

\[\{ 1, u_1^1, u_1^2, \dots u_1^{e_i}, u_2^1, \dots, u_2^{e_j}, u_3^1,\dots\}\]

***** 3.2.26. ExtensiÃ³n generada por algebraicos es algebraica
Una extensiÃ³n $K(S)/K$ es algebraica sobre $K$ ssi todo $u \in S$ es 
algebraico sobre $K$.

****** DemostraciÃ³n
Si es algebraica, en particular lo es cada elemento de $S$.
Si lo son los elementos de $S$, podemos ver que cualquier 

***** 3.2.27. Finita es algebraica y finitamente generada
Una extensiÃ³n es finita ssi es algebraica y finitamente generada

****** DemostraciÃ³n
Tenemos que extensiÃ³n finitamente generada por algebraicos es
[[*3.2.25. ExtensiÃ³n finitamente generada por algebraicos es finita][finita]]. Por otro lado, si es finita, tendrÃ¡ una base finita que
la genera; y para cada elemento de la base, $\{1,u,\dots,u^n\}$ no
serÃ¡ linealmente independiente. Luego serÃ¡ finita.

***** 3.2.28. CaracterizaciÃ³n de elementos algebraicos
Un elemento $u \in F$ es algebraico sobre $K$ ssi existe una extensiÃ³n
finita intermedia $E/K$ donde $u \in E$.

****** DemostraciÃ³n
Si es algebraico de grado $n$, tenemos $K(u)$ algebraica y finitamente
generada, [[*3.2.27. Finita es algebraica y finitamente generada][luego finita]]. Si existe una extensiÃ³n finita intermedia, 
serÃ¡ algebraica.

***** 3.2.29. Torre algebraica
Sean $K \subset F \subset E$, $E/K$ es algebraica ssi $E/F$ y $F/K$ son ambas 
algebraicas.

****** DemostraciÃ³n
******* Si es algebraica, lo son sus partes
Un elemento algebraico sobre $K$ lo serÃ¡ sobre $F$. Y todo elemento
de $F$ estÃ¡ en $E$, luego serÃ¡ algebraico sobre $K$.

******* Si las partes son algebraicas, es algebraica
Todo elemento $e \in E$ es algebraico sobre $F$, luego cumple algÃºn
polinomio con elementos en $F$. Los elementos que generan el polinomio
en $F$ son todos algebraicos sobre $K$, luego $K$ extendido con esos
elementos es finito. Si lo extiendo con $e$, que es algebraico sobre
ellos, llego a otra extensiÃ³n finita. Toda finita es [[*3.2.27. Finita es algebraica y finitamente generada][algebraica]].

***** 3.2.30. Clausura algebraica relativa
Dada $F/K$ extensiÃ³n, el conjunto de elementos algebraicos forman un
subcuerpo de $F$. Llamado *clausura algebraica relativa* de $K$ en $F$.

****** DemostraciÃ³n
Sean $a,b \in F$ algebraicos; entonces $K(a,b)$ es finito, luego $a+b$ y
$ab$ son tambiÃ©n algebraicos.

****** DemostraciÃ³n constructiva para la suma
Se puede [[http://mathoverflow.net/a/81640/45365][encontrar constructivamente]] un polinomio que tenga como
raÃ­z a la suma de dos algebraicos.

***** DONE Existencia de clausura
*Teorema de Steinitz*. Todo cuerpo tiene una extensiÃ³n algebraicamente 
cerrada.

***** DONE Homomorfismos sobre un cuerpo
     Un *homomorfismo sobre cuerpos* $K,K'$ es un homomorfismo $\phi$ sobre extensiones
     $F,F'$ con un isomorfismo $\omega : K \longrightarrow K'$ debe cumplir: $\phi|_K = \omega$. 
     Cuando no se especifica, se asume la identidad.

     \[ \phi : F/K \longrightarrow F'/K' \]

***** DONE Automorfismos entre extensiones
     Un automofismo de extensiones es un homomorfismo sobre el cuerpo $K$, 
     $\phi : F/K \longrightarrow F/K$ que es isomorfismo.

*** 4. Cuerpos de descomposiciÃ³n
**** 4.1. Cuerpo de descomposiciÃ³n
***** 4.1.1. Teorema de Kronecker
Sea $f$ de grado no nulo sobre $K$, entonces existe una extensiÃ³n $F/K$ 
tal que existe $u \in F$ con $f(u) = 0$.

****** DemostraciÃ³n
Puedo descomponer en irreducibles $f = f_1f_2\dots f_m$; y tener una 
extensiÃ³n cumpliendo lo pedido:

\[ F = \frac{K[X]}{(f_1)}\]

Para el elemento $u = x + (f_1)$.

***** 4.1.2. ExtensiÃ³n de un homomorfismo
Dadas extensiones $F_1/K_1, F_2/K_2$, decimos que $\tau : F_1 \longrightarrow F_2$ es una extensiÃ³n
de $\sigma : K_1 \longrightarrow K_2$, ambos homomorfismos de cuerpos, cuando $\tau|_{K_1} = \sigma$.

***** 4.1.3. Isomorfismo extendido a polinomios
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos. Existe una Ãºnica extensiÃ³n a
un isomorfismo $\sigma : K_1[X] \longrightarrow K_2[X]$ cumpliendo que $\sigma(X) = X$.

****** DemostraciÃ³n
Por la propiedad universal por el anillo de polinomios. Las imÃ¡genes
de todos los elementos estÃ¡n fijas excepto la de $X$ y sus mÃºltiplos.

***** 4.1.4. Isomorfismos respetan irreducibilidad
Sea $f_1 \in K_1[X]$ irreducible sobre $K_1$, entonces $\sigma(f)$ es irreducible 
sobre $K_2$; donde $\sigma$ es la [[*4.1.3. Isomorfismo extendido a polinomios][extensiÃ³n]] a polinomios de un isomorfismo.

****** DemostraciÃ³n
Si tuviÃ©ramos $f_1 = gh$ dos polinomios no triviales, su grado se 
conservarÃ­a al aplicar el isomorfismo y tendrÃ­amos
$\sigma(f)=\sigma(g)\sigma(h)$.

***** 4.1.5. Isomorfismo de raÃ­ces de polinomios
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos y sean $F_1/K_1, F_2/K_2$ 
extensiones algebraicas. Dado un homomorfismo $\tau : F_1 \longrightarrow F_2$ sobre $\sigma$;
si $u$ es raÃ­z de $p$ entonces $\tau(u)$ es raÃ­z de $\sigma(p)$.

****** DemostraciÃ³n
Por simple cÃ¡lculo tomando $p(x) = a_nx^n + \dots + a_1x + a_0$:

\[ \begin{aligned}
\sigma(p)(\tau(u)) &= \sigma(a_n)\tau(u)^n + \dots + \sigma(a_1)\tau(u) + \sigma(a_0) \\ 
                   &= \tau(a_n)\tau(u)^n + \dots + \tau(a_1)\tau(u) + \tau(a_0) \\
                   &= \tau(a_nu^n + \dots + a_1u + a_0) \\
                   &= \tau(0) = 0
\end{aligned} \]

***** 4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos
Sea $F/K$ una extensiÃ³n algebraica y $\tau : F/K \longrightarrow F/K$ un endomorfismo
sobre $K$. Entonces $\tau$ es un automorfismo.

****** DemostraciÃ³n
Tenemos que es una extensiÃ³n de la identidad y que preserva [[*4.1.5. Isomorfismo de raÃ­ces de polinomios][raÃ­ces]].
Por eso, dado un elemento $u \in F$, tomamos su $f = Irr(u,K)$. Y tenemos
que $\{\tau^n(u)\}_{n\in \mathbb{N}}$ es una sucesiÃ³n de raÃ­ces del polinomio que, por 
inyectividad, deberÃ¡ repetir los elementos en algÃºn momento.

Tendremos $\tau^m(u)=u$ y serÃ¡ sobreyectiva. NÃ³tese que estamos usando
la inyectividad de todo homomorfismo de cuerpos.

***** 4.1.7. Isomorfismo intercambiando conjugadas
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos. Sea $p$ irreducible
y $u_1,u_2$ raÃ­ces de $p$ y $\sigma(p)$ en extensiones $F_1,F_2$. Entonces existe un 
Ãºnico isomorfismo $\tau : K_1(u_1) \longrightarrow K_2(u_2)$ sobre $\sigma$ tal que $\tau(u_1) = u_2$.

****** DemostraciÃ³n
El isomorfismo buscado sale como composiciÃ³n:

\[K_1(u_1) \cong 
  \frac{K_1[X]}{(f_1)} \cong 
  \frac{K_2[X]}{(f_2)} \cong
  K_2(u_2)\]

***** 4.1.8. NÃºmero de extensiones
El nÃºmero de extensiones $\tau : K_1[u_1] \longrightarrow F_2$ sobre $\sigma$ es el nÃºmero de
raÃ­ces distintas de $\sigma(p)$ en $F_2$.

****** DemostraciÃ³n
Por la [[*4.1.7. Isomorfismo intercambiando conjugadas][proposiciÃ³n anterior]], una para cada raÃ­z de $\sigma(p)$. NÃ³tese
que no puede haber mÃ¡s porque la imagen de $u_1$ determina completamente
el morfismo y porque la imagen de raÃ­z [[*4.1.5. Isomorfismo de raÃ­ces de polinomios][debe ser]] una raÃ­z.

***** 4.1.9. Cuerpo de descomposiciÃ³n
Un $E/K$ es un cuerpo de descomposiciÃ³n de $f$ ssi existen $u_1,\dots,u_n \in E$ 
tales que $f = (X-u_1)\dots(X-u_n)$ y $E = K(u_1,\dots,u_n)$.

****** CaracterizaciÃ³n
NÃ³tese que es el mÃ­nimo en el que factoriza linealmente. Cualquier
otro en el que factorice linealmente necesita contar con sus raÃ­ces.

***** 4.1.10. Cuerpo de descomposiciÃ³n en una torre
Sean $K \subset F \subset E$. Si $E$ es cuerpo de descomposiciÃ³n de $f$ sobre $K$, 
tambiÃ©n lo es de $f$ sobre $F$.

****** DemostraciÃ³n
Se cumple trivialmente:

\[ E = K(u_1,\dots,u_n) \subset F(u_1,\dots,u_n) \subset E\]

***** 4.1.11. Existencia del cuerpo de descomposiciÃ³n
Cualquier $f \in K[X]$ de grado $n$ tiene un cuerpo de descomposiciÃ³n, 
que ademÃ¡s verifica $[F:K] \leq n!$

****** DemostraciÃ³n
******* InducciÃ³n: caso base
Cuando $n=1$, tenemos una raÃ­z en el cuerpo.

******* InducciÃ³n: caso inductivo
En otro caso, por [[*4.1.1. Teorema de Kronecker][Kronecker]], tenemos que existe una extensiÃ³n
en la que hay una raÃ­z del polinomio. Tomamos esa raÃ­z para crear
$K(u)$. Por hipÃ³tesis de inducciÃ³n, hay un cuerpo de descomposiciÃ³n
de $f/(X-u)$ sobre $K(u)$, llamado $F$. Por Ãºltimo, podemos construir 
un cuerpo de descomposiciÃ³n con sus raÃ­ces.

Tenemos ademÃ¡s que la raÃ­z tiene menos grado que el polinomio:

\[ [F:K] = [F:K(u)][K(u):K] \leq (n-1)!n = n! \]

***** 4.1.12. Isomorfismo entre cuerpos de descomposiciÃ³n
Sean $F_1/K_1, F_2/K_2$ cuerpos de descomposiciÃ³n de $f \in K_1[X]$ y 
$\sigma(f) \in K_2[X]$; para $\sigma : K_1 \longrightarrow K_2$ isomorfismo. Entonces son
isomorfos.

****** DemostraciÃ³n
******* Caso base
Si ambos son de grado $1$ tenemos extensiones triviales y 
hemos terminado.

******* Caso inductivo
Sea $u \in F_1$ raÃ­z de $f$. Como $Irr(u,K) \mid f$, tenemos que 
$\sigma(Irr(u,K)) \mid \sigma(f)$. Si tomamos una raÃ­z $v$ de $\sigma(Irr(u,K))$ y 
aplicamos el [[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo intercambiando conjugadas]] anterior,
tenemos una extensiÃ³n $\tau : K_1(u) \longrightarrow K_2(v)$ que lleva una en 
otra. Ahora tomamos $f=g(X-u)$ y por inducciÃ³n tenemos
un isomorfismo extendiendo hasta el cuerpo de descomposiciÃ³n.

***** 4.1.13. Unicidad del cuerpo de descomposiciÃ³n
Dos cuerpos de descomposiciÃ³n de $f \in K[X]$ sobre $K$ son isomorfos.

****** DemostraciÃ³n
Trivial aplicando lo [[*4.1.12. Isomorfismo entre cuerpos de descomposiciÃ³n][anterior]] al isomorfismo igualdad.

***** 4.1.23. Cuerpo de descomposiciÃ³n de una familia
Sea ${\cal P} \subseteq K[X]$ una familia de polinomios no constantes. Una
extensiÃ³n $E/K$ es cuerpo de descomposiciÃ³n suyo si todo polinomio
factoriza linealmente y es ademÃ¡s se tiene $E = K(S)$ con:

\[ S = \{ u \in E \mid \exists f \in {\cal P}: f(u)=0\} \]

***** 4.1.24. Existencia del cuerpo de descomposiciÃ³n de una familia
Para toda familia de polinomios existe un cuerpo de descomposiciÃ³n.

****** DemostraciÃ³n
******* Caso finito
En el caso finito, multiplicamos toda la familia para aplicar
la [[*4.1.11. Existencia del cuerpo de descomposiciÃ³n][existencia del cuerpo de descomposiciÃ³n]] al producto.

******* Caso infinito
Cuando tenemos una familia $\{ f_\lambda \mid \lambda \in \Lambda\}$. Si tomamos $I \subset \Lambda$ finito, 
podemos asignarle un cuerpo de descomposiciÃ³n $F_I$ tal que $I\subset J$ 
implique $F_I \subset F_J$.

Tomamos:

\[F = \bigcup_{J\text{ finito}} F_J \]

Las operaciones entre dos elementos de $F$ se definen en el menor
cuerpo que contenga a los dos.

Esto es un cuerpo de descomposiciÃ³n porque todo polinomio ya
descompone linealmente en cualquier $F_J$ que lo contenga; y cada
$F_J$ era ya cuerpo de descomposiciÃ³n de los polinomios que contenÃ­a,
asÃ­ que el cuerpo de descomposiciÃ³n debe al menos contener a todos
los $F_J$.

***** 4.1.25. Unicidad (esencial) del cuerpo de descomposiciÃ³n de una familia
El cuerpo de descomposiciÃ³n de una familia de polinomios es Ãºnico
salvo isomorfismos.

****** DemostraciÃ³n
******* Caso finito
Aplicamos que los cuerpos de descomposiciÃ³n de un polinomio
[[*4.1.13. Unicidad del cuerpo de descomposiciÃ³n][son isomorfos]] al polinomio producto.

******* Caso infinito
Sean $F_1,F_2$ dos cuerpos de descomposiciÃ³n sobre $K$ de una familia
de polinomios.

Ordenamos el siguiente conjunto por inclusiÃ³n y por extensiÃ³n del
isomorfismo. 

\[ (E,\tau) \leq (F,\psi) \Leftrightarrow 
(E \subset F) \wedge (\psi|_E = \tau)\]

Es una ordenaciÃ³n inductiva porque la uniÃ³n arbitraria da una 
cota maximal de cualquier cadena:

\[ {\cal E} = 
\left\{ (E,\tau) 
\mid F_1 \supset E \supset K;\;
\tau : E/K \longrightarrow F_2/K
\right\}\]

Que sabemos no vacÃ­o por el caso finito. Sea $(F,\sigma)$ maximal. Y 
supongamos que $F \subsetneq F_1$, entonces existe alguna raÃ­z de alguno
de los polinomios que no estÃ¡ en $F$. Creando un [[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo]]
sobre $F$ que la llevara a su conjugada, contravendrÃ­amos maximalidad.

Ahora, todo $f$ de la familia descompondrÃ­a en $F$ y por $\sigma$, se las
llevarÃ­an a $F_2$.

**** 4.2. Clausura algebraica
***** 4.2.1. Algebraicamente cerrado
Un cuerpo tal que toda extensiÃ³n algebraica suya sea trivial es un
cuerpo algebraicamente cerrado.

***** 4.2.2. CaracterizaciÃ³n de los algebraicamente cerrados
Equivalen las siguientes propiedades:

 1. Todo polinomio no constante tiene raÃ­z en $K$.
 2. Todo polinomio descompone linealmente en $K$.
 3. Un polinomio es irreducible en $K$ ssi es de grado $1$.
 4. Toda extensiÃ³n algebraica de $K$ es trivial.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Como $f$ tiene raÃ­z en $K$, podemos dividirlo por $(x-u)$, para
obtener un nuevo polinomio de grado menor.

******* ImplicaciÃ³n 2 a 3
Trivialmente.

******* ImplicaciÃ³n 3 a 4
Sea un elemento en la extensiÃ³n algebraica, su irreducible
debe ser de grado $1$, luego debe estar en el cuerpo base.

******* ImplicaciÃ³n 4 a 1
Si algÃºn polinomio no constante no tuviera raÃ­z, aplicamos
[[*4.1.1. Teorema de Kronecker][Teorema de Kronecker]] para crear una extensiÃ³n algebraica
no trivial.

***** 4.2.3. Infinitud de algebraicamente cerrados
Todo cuerpo algebraicamente cerrado es infinito.

****** DemostraciÃ³n
Si tengo un cuerpo finito $K$ formo el polinomio irreducible
siguiente, que no tiene raÃ­ces en $K$:

\[ 
f(x) = \prod_{k \in K} (x-k) + 1
\]

***** 4.2.4. Cuerpo algebraicamente cerrado de elementos algebraicos
Sea $E/K$ con $E$ algebraicamente cerrado. Los elementos algebraicos
de $E$ forman un cuerpo algebraicamente cerrado.

****** DemostraciÃ³n
Sabemos que [[*3.2.30. Clausura algebraica relativa][forman un cuerpo]]. Para ver que es algebraicamente cerrado
vemos que todo polinomio sobre ellos tiene una raÃ­z en $E$, por ser
este algebraicamente cerrado. Como una raÃ­z es algebraica, tiene
una raÃ­z en el subcuerpo de los algebraicos.

***** 4.2.5. Clausura algebraica absoluta
Una extensiÃ³n algebraica $E/K$ es la clausura algebraica (absoluta) 
si es una extensiÃ³n algebraica y $E$ es algebraicamente cerrado.

***** 4.2.6. CaracterizaciÃ³n de la clausura algebraica
Equivalen:

 1. $E/K$ clausura algebraica.
 2. $E/K$ algebraica y todo polinomio no constante $f \in K[X]$ descompone
    en factores lineales en $E[X]$.
 3. $E$ es cuerpo de descomposiciÃ³n de todos los polinomios no 
    constantes de $K$.
 4. $E/K$ algebraica y todo no constante tiene una raÃ­z en $E$.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Como $E$ es algebraicamente cerrado, [[*4.2.2. CaracterizaciÃ³n de los algebraicamente cerrados][sabemos que]] todos sus polinomios
descomponen en factores lineales en $E[X]$.

******* ImplicaciÃ³n 2 a 3
Todo polinomio descompone. AdemÃ¡s, todo elemento de $E$ es algebraico;
asÃ­ que $K(S) = E$.

******* ImplicaciÃ³n 3 a 1
$E$ estÃ¡ generado por elementos algebraicos, luego es algebraico.
AdemÃ¡s, todo polinomio no constante descompone en Ã©l, luego
es algebraicamente cerrado por la [[*4.2.2. CaracterizaciÃ³n de los algebraicamente cerrados][caracterizaciÃ³n]].

******* TODO Equivalencia con 4
***** 4.2.7. Transitividad de la clausura algebraica
Sean $K \subset F \subset E$ torre de cuerpos, con $F/K$ algebraica. 
Entonces $E$ es clausura algebraica de $F$ ssi lo es de $K$.

****** DemostraciÃ³n
Ser algebraicamente cerrado es independiente del cuerpo base de
la extensiÃ³n. Ser algebraico [[*3.2.29. Torre algebraica][equivale]] a que lo sean las dos partes.

***** 4.2.8. Teorema de Steinitz
Para todo cuerpo existe una clausura algebraica.

****** DemostraciÃ³n
Sabemos que [[*4.1.24. Existencia del cuerpo de descomposiciÃ³n de una familia][existe]] el cuerpo de descomposiciÃ³n de todos los polinomios
no constantes. Por la caracterizaciÃ³n de [[*4.2.6. CaracterizaciÃ³n de la clausura algebraica][clausura algebraica]] sabemos
que lo es.

***** 4.2.9. Unicidad esencial de la clausura algebraica
Dos clausuras algebraicas $E_1,E_2$ del mismo cuerpo $K$ son isomorfas 
sobre $K$.

****** DemostraciÃ³n
Tenemos unicidad de esencial de los cuerpos de descomposiciÃ³n,
y estos son cuerpos de descomposiciÃ³n de todos los polinomios
no constantes.

***** 4.2.10. ExtensiÃ³n de homomorfismos a algebraicas
Sea $K \subset F \subset E$ con $E/K$ algebraica. Entonces todo $\sigma : F \longrightarrow \overline{K}$ tiene
una extensiÃ³n $\tau : E \longrightarrow \overline{K}$.

****** DemostraciÃ³n
Aplicamos Zorn sobre el siguiente conjunto ordenado para la inclusiÃ³n,
sabiendo que toda cadena de cuerpos estÃ¡ acotada por su uniÃ³n y que cada
$\sigma_i$ es extensiÃ³n del anterior:

\[{\cal S} = \{(E_i,\sigma_i) \mid 
F \subset E_i \subset E;\; \sigma_i : E_i \longrightarrow \overline{K};\;
\sigma_i|_F = \sigma\}\]

Esto nos da el maximal $E_1$. Si $E_1 \subsetneq E$, tomo $u \in E - E_1$; y busco un
[[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo]] intercambiando $u$ por una conjugada tambiÃ©n raÃ­z de $Irr(u,K)$.
Esto me da $(E_1,\sigma_1) \leq (E_1(u),\tau)$.

***** 4.2.11. Cardinalidad de la clausura algebraica
Sea $K$ cuerpo y $\overline{K}$ su clausura.

 1. Si $K$ es finito, $\overline{K}$ es infinito numerable
 2. Si $K$ es infinito, $\#K = \#\overline{K}$.

****** DemostraciÃ³n
******* Caso finito
Cuando $K$ es finito, el nÃºmero de polinomios irreducibles
sobre Ã©l es infinito numerable.
******* Caso infinito
En el caso infinito, como mÃ¡ximo se tendrÃ¡ la acotaciÃ³n que
da el nÃºmero de polinomios, que es una uniÃ³n numerable:

\[ \#\overline{K} \leq \#\left(\bigcup K^n\right) = \#K \]

*** 5. Extensiones normales y separables
**** 5.1. Elementos conjugados y extensiones conjugadas
***** 5.1.1. Elementos conjugados
Sean $u,v \in \overline{K}$, clausura algebraica. Equivalen:

  1. $Irr(u,K) = Irr(v,K)$
  2. $\exists \tau : K(u) \longrightarrow K(v)$ *isomorfismo* con $\tau(u) = v$.
  3. $\exists\sigma : K(u) \longrightarrow \overline{K}$ *homomorfismo* con $\sigma(u) = v$.
  4. $\exists \sigma : \overline{K} \longrightarrow \overline{K}$ *automorfismo* con $\sigma(u) = v$.

Morfismos manteniendo $K$. Estos elementos se llaman 
*elementos conjugados*.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Supongamos que tienen el mismo polinomio irreducible $f(X)$, 
entonces podemos construir un isomorfismo que lleve $u,v$ a 
$X + (f(X))$ para tener:

\[K(u) \cong \frac{K[X]}{(f(X))}\cong K(v)\]

******* ImplicaciÃ³n 2 a 3 y 4
Dado el isomorfismo, lo podemos prolongar a un homomorfismo.
Y dado un homomorfismo, lo podemos [[*4.2.10. ExtensiÃ³n de homomorfismos a algebraicas][prolongar]] a un automorfismo 
por ser una extensiÃ³n algebraica.

******* ImplicaciÃ³n 4 a 1
Supongamos que existe el automorfismo de $\overline{K}$. Sea $f(X) = Irr(u,K)$,
y tenemos $0 = \phi(f(u)) = f(\phi(u)) = f(v)$; luego $Irr(u,K) = Irr(v,K)$.

***** 5.1.3. Extensiones conjugadas
Sean $F_1/K$, $F_2/K$ extensiones algebraicas, equivalen:

 1. $\exists \sigma: F_1 \longrightarrow F_2$ *isomorfismo* sobre $K$.
 2. $\exists \sigma : F_1 \longrightarrow \overline{K}$, *homomorfismo* sobre $K$ con $\sigma(F_1) = F_2$.
 3. $\sigma : \overline{K} \longrightarrow \overline{K}$, *isomorfismo* tal que $\sigma(F_1) = F_2$.

Estas extensiones se llaman *extensiones conjugadas*.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Extendiendo el isomorfismo se pasa de 1 a 2.

******* ImplicaciÃ³n 2 a 3
Dado el homomorfismo, lo podemos [[*4.2.10. ExtensiÃ³n de homomorfismos a algebraicas][prolongar]] a un homomorfismo
por ser $\overline{K}$ algebraica. Tenemos entonces un endomorfismo de un
cuerpo algebraico, que [[*4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos][debe ser]] un automorfismo.

******* ImplicaciÃ³n 3 a 1
La restricciÃ³n a $F_1$ es isomorfismo.

**** 5.2. Extensiones normales
***** 5.2.1. Extensiones normales
Sea $F/K$ extensiÃ³n algebraica, subcuerpo de $\overline{K}$, equivalen:

  - $\sigma : F \longrightarrow \overline{K}$ sobre $K$, me da $\sigma(F) = F$.
  - Todo irreducible de $K[X]$ con una raÃ­z en $F$ descompone en 
    lineales en $F[X]$.
  - $F$ es cuerpo de descomposiciÃ³n de una familia de polinomios
    sobre $K[X]$.

 A una extensiÃ³n de este tipo se le llama *extensiÃ³n normal*.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Dos raÃ­ces del mismo irreducible son conjugadas, luego existe
un [[*5.1.1. Elementos conjugados][homomorfismo]] que lleva una en otra, como debe llevar $F$ en $F$,
la otra debe estar en $F$.

******* ImplicaciÃ³n 2 a 3
Para cada elemento de $F$ puedo tomar su irreducible, como tiene
una raÃ­z en $F$ tiene todas. $F$ es el cuerpo de descomposiciÃ³n de la
familia de todos los irreducibles de sus elementos.

******* ImplicaciÃ³n 3 a 1
Tenemos que $F = K(\alpha_1,\alpha_2,\dots)$, raÃ­ces de los polinomios. La imagen
de la raÃ­z de un polinomio sobre $K$ debe ser raÃ­z de ese mismo 
polinomio porque este debe quedar invariante sobre $\sigma$. AsÃ­ tenemos
que $\sigma(F) \subset F$, y por ser algebraico, todo [[*4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos][endomorfismo es automorfismo]].

***** 5.2.3. Propiedades de las extensiones normales
Sea $E$ extensiÃ³n normal. Cumple:

 1. Si $A/K$ es algebraica, $EA/A$ es normal.
 2. Si $K \subset F \subset E$, entonces $E/F$ es normal.
 3. Si $E_1/K, E_2/K$ son normales, $E_1E_2/K$ es normal.
 4. Sea $E_\lambda$, familia de extensiones normales; $F = \bigcap E_\lambda$ es normal.

****** DemostraciÃ³n de 1
Dado $\sigma : EA/A \longrightarrow \overline{K}/A$, tengo que $\sigma(EA) = \sigma(E)\sigma(A) = EA$.

****** DemostraciÃ³n de 2
Sea $\sigma : E/F \longrightarrow \overline{K}/F$, como deja fijo $K$, debe tenerse $\sigma(E)=E$.

****** DemostraciÃ³n de 3
Sea $\sigma: E_1E_2/K \longrightarrow \overline{K}/K$, las restricciones dejan fijos ambos cuerpos
y $\sigma(E_1E_2) = \sigma(E_1)\sigma(E_2) = E_1E_2$.

****** DemostraciÃ³n de 4
Si un homomorfismo de la intersecciÃ³n lo extiendo y lo restrinjo a
cada uno, debe dejar todos los cuerpos fijos. Por tanto, un elemento
en la intersecciÃ³n debe quedarse en la intersecciÃ³n.

****** Contraejemplo: subcuerpo de normal no es normal
El Ãºltimo es cuerpo de descomposiciÃ³n de $x^3-2$,
pero el primero no es normal, porque no estÃ¡n todas las
raÃ­ces del polinomio irreducible de $\sqrt[3]{2}$.

$\mathbb{Q} 
\subset \mathbb{Q}(\sqrt[3]{2}) 
\subset \mathbb{Q}(\sqrt[3]{2}, i)$

***** 5.2.4. Clausura normal
Llamamos clausura normal de $F/K$ a:

\[ E = \bigcap \{ H \mid H \supset F;\; H/K\text{ normal}\}\]

***** 5.2.5. Existencia de la clausura normal
Para toda extensiÃ³n algebraica existe una clausura normal. 

****** DemostraciÃ³n
Trivial porque la clausura algebraica es normal.

***** 5.2.6. Unicidad de la clausura normal
La clausura normal es Ãºnica salvo isomorfismos.

****** DemostraciÃ³n
Sean las dos clausuras sobre la clausura algebraica absoluta.
Como son intersecciÃ³n de normales y ambas lo son, deben ser
la misma.

***** 5.2.7. ExtensiÃ³n de homomorfismos a extensiÃ³n normal
Sea $K \subset F \subset E$ con $E/K$ normal. Todo $\tau : F \longrightarrow E$ extiende a
un $\tau : E \longrightarrow E$. 

****** DemostraciÃ³n
Tenemos que extiende a $\tau : F \longrightarrow \overline{K}$ y de ahÃ­, por ser $E$ algebraica,
se [[*4.2.10. ExtensiÃ³n de homomorfismos a algebraicas][prolonga]] a $\tau : E \longrightarrow \overline{K}$. Por ser normal, $\tau(E) = E$.

***** 5.2.8. Clausura de una extensiÃ³n finita
Sea $F = K(u_1,u_2,\dots,u_n)$ y $f_i = Irr(u_i,K)$; entonces la clausura normal
es el cuerpo de descomposiciÃ³n de $f = f_1f_2\dots f_n$.

****** DemostraciÃ³n
Es normal por ser cuerpo de descomposiciÃ³n de un polinomio, es
la mÃ­nima porque debe contener las raÃ­ces de $f_i = Irr(u_i,K)$
para ser normal, y si las contiene, debe contener al cuerpo de
descomposiciÃ³n de $f$.

***** 5.2.9. La clausura normal de extensiÃ³n finita es finita
Sea $F/K$ finita, su clausura normal es finita.

****** DemostraciÃ³n
El cuerpo de descomposiciÃ³n anterior es finito porque puedo
crearlo insertando las raÃ­ces de cada polinomio.

***** 5.2.10. Polinomio normal
Un polinomio irreducible es normal si en toda extensiÃ³n
algebraica $F/K$ con una raÃ­z de $f$, descompone en factores 
lineales.

***** 5.2.11. CaracterizaciÃ³n de polinomios normales
Sea $f$ un polinomio en $K[X]$, equivalen:

 1. $f$ es normal sobre $K$.
 2. El cuerpo de descomposiciÃ³n de $f$ es $K(u)$, una raÃ­z de $f$.
 3. Todas las raÃ­ces de $f$ es expresan como polinomios de una de ellas.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Tenemos que en $K(u)$, $f$ descompone en factores lineales,
luego es cuerpo de descomposiciÃ³n.

******* ImplicaciÃ³n 2 a 3
Trivial.

******* ImplicaciÃ³n 3 a 1
Una extensiÃ³n con una raÃ­z $u$ contendrÃ­a a $K(u)$, y como todas
las raÃ­ces se expresan como polinomios de $u$, contendrÃ­a a todas
las raÃ­ces y $f$ descompondrÃ­a en polinomios lineales.

**** 5.3. Extensiones separables
***** 5.3.1. Elemento separable
Un elemento algebraico $u$ es separable en $K$ si $Irr(u,K)$ no tiene 
raÃ­ces mÃºltiples.

***** 5.3.2. Extensiones separables
Una extensiÃ³n algebraica $F/K$ es *separable* si todos sus elementos 
lo son.

****** Ejemplo de extensiÃ³n normal no separable
Existen [[http://math.stackexchange.com/questions/982702/example-of-a-non-separable-normal-extension][ejemplos]] de extensiones normales no separables.

***** 5.3.3. Torres separables
Si $E \supset F \supset K$ es extensiÃ³n separable, lo son $E/F$ y $F/K$.

****** DemostraciÃ³n
Que $F/K$ es separable es trivial. Y $E/F$ es separable porque
$Irr(u,F) \mid Irr(u,K)$, y si el segundo no tiene raÃ­ces mÃºltiples,
no puede tenerlas el primero.

***** 5.3.4. Grado separable
El *grado separable* es cardinal del conjunto de homomorfismos
de $F/K \longrightarrow \overline{K}/K$.

\[ [F : K]_s = \#\{F/K \longrightarrow \overline{K}/K\}\]

***** 5.3.5. Grado separable en torres
Sea $K \subset F \subset E \subset \overline{K}$ entonces $[E:K]_s = [E:F]_s[F:K]_s$.

****** DemostraciÃ³n
******* Con dos homomorfismos construyo el mayor
Dados $\sigma : F/K \longrightarrow \overline{K}/K$ y $\psi : E/F \longrightarrow \overline{K}/F$, puedo extender $\sigma$ al
algebraico $E$ para tener $\sigma^\ast : \overline{K}/K \longrightarrow \overline{K}/K$. Ahora, la composiciÃ³n
$\sigma^\ast \circ \psi$, me da el homomorfismo buscado.

******* Y es Ãºnico
Supongamos que $\sigma^\ast \circ \psi = \sigma'^\ast \circ \psi'$; como deben ser iguales para 
cualquier $f \in F$, se tiene $\sigma = \sigma'$. Ahora, si tomamos la misma extensiÃ³n
para cada elemento, $\sigma^\ast = \sigma'^\ast$ y entoces por inyectividad $\psi = \psi'$.

AsÃ­ tenemos ya:

\[ [E:K]_S \geq [E:F]_S[F:K]_S \]

******* Con el mayor construyo los dos menores
Dado un $\sigma : E/K \longrightarrow \overline{K}/K$, para cada $\sigma|_F$ construyo una extensiÃ³n
a la clausura, $\tau : \overline{K}/K \longrightarrow \overline{K}/K$, que es isomorfismo. Como 
es isomorfismo podemos construirle inversa para tener 
$\tau^{-1} : \overline{K}/K \longrightarrow \overline{K}/K$. A la vez, podemos extender todos los $\sigma$ a
la clausura algebraica como $\sigma^\ast$.

Ahora bien, como $\tau|_F = \sigma|_F$, tenemos que $\sigma^\ast \circ \tau^{-1}$ deja fijo a $F$. 
AsÃ­, hemos partido $\sigma$ en $\sigma^\ast \circ \tau^{-1}|_E$ y $\sigma|_F$.

******* Y son Ãºnicas
Supongamos que tenemos $\sigma|_F = \sigma'|_F$, entonces las dos extensiones $\tau$
las hemos tomado iguales. Si tenemos $\sigma \circ \tau^{-1}|_E = \sigma' \circ \tau^{-1}|_E$, como $\tau^{-1}$
es sobreyectiva, tenemos $\sigma^\ast|_E = \sigma'^\ast|_E$, y por tanto $\sigma = \sigma'$.

AsÃ­ tenemos que:

\[ [E:K]_S \leq [E:F]_S[F:K]_S \]

***** 5.3.6. RelaciÃ³n de grado y grado separable
Sea $F/K$ extensiÃ³n finita, entonces $[F:K]_s \mid [F:K]$.

****** DemostraciÃ³n
******* Caso simple
Sea $K(u)$ la extensiÃ³n, con polinomio $Irr(u,K)$ de grado $n$. 
Si la raÃ­z $u$ tiene multiplicidad $m$ en el polinomio, todas las
raÃ­ces del polinomio tienen la misma multiplicidad y hay
$n/m$ raÃ­ces distintas.

Cada homormofismo quedarÃ¡ determinado por la imagen de $u$, y
tenemos $n/m$ imÃ¡genes distintas para $u$.

******* Caso compuesto
Ahora procedemos por inducciÃ³n sobre el grado de la extensiÃ³n. 
Tomamos un elemento de la base y hacemos:

\[ [F:K(u)]_S[K(u):K]_S  \mid  [F:K(u)][K(u):K] \]

Lo primero es divisible por el caso simple y lo segundo por
hipÃ³tesis de inducciÃ³n.

***** 5.3.7. Igualdad de grados en torres
Sea $K \subset F \subset E$, con $E/K$. Entonces $[E:K]_s = [E:K]$ ssi 
$[E:F]_s = [E:F]$ y $[F:K]_s = [F:K]$.

****** DemostraciÃ³n
Tenemos que deben tenerse ambos casos de igualdad en:

\[
[E:K]_S = [E:F]_S[F:K]_S \leq [E:F][F:K] = [E:K]
\]

***** 5.3.8. CaracterizaciÃ³n de extensiÃ³n separable
La extensiÃ³n finita $E/K$ es separable ssi $[E:K]_s = [E:K]$.

****** DemostraciÃ³n
Usamos la idea de la demostraciÃ³n de la relaciÃ³n con el grado.
En el caso simple tenemos $[K(u):K]_S = [K(u):K]$ solo cuando la
multiplicidad de cada raÃ­z es uno. En el caso compuesto exigimos
eso a cada paso.

***** 5.3.9. ExtensiÃ³n por un conjunto separable
Para $K(S)/K$ algebraica es separable ssi todo elemento de $S$ es
separable sobre $K$.

****** DemostraciÃ³n
La suma o producto de elementos separables [[http://math.stackexchange.com/a/82837/85067][es separable]].

***** 5.3.10. Propiedades de extensiones separables
Las extensiones separables cumplen:

 1. Para $K \subset F \subset E$, se tiene $E/K$ separable ssi $E/F$ y $F/K$ 
    separables.
 2. Sea $E/K$ algebraica separable y $H/K$ extensiÃ³n, entonces $EH/H$ 
    es separable.
 3. Sean $E/K$ y $F/K$ separables, entonces $EF/K$ es separable.

****** DemostraciÃ³n
******* Punto 1
Simplemente usar que la igualdad de grados de separabilidad
se da en [[*5.3.7. Igualdad de grados en torres][torres]] y que equivale a la [[*5.3.8. CaracterizaciÃ³n de extensiÃ³n separable][separabilidad]].

******* Punto 2
Tenemos $EH/H = H(E)/H$. Que sea separable [[*5.3.9. ExtensiÃ³n por un conjunto separable][equivale]] a que
cada elemento de $E$ lo sea. Pero $Irr(e,H) \mid Irr(e,K)$, y si uno
tiene sÃ³lo raÃ­ces simples, el otro las tendrÃ¡ tambiÃ©n.

******* Punto 3
Tenemos $K(E,F)/K$ separable por serlo todos los elementos en 
$E,F$.

***** 5.3.11. La clausura normal de una separable es separable
Sea $F/K$ separable y $E/K$ su clausura normal. Entonces $E/K$ es
separable.

****** DemostraciÃ³n
Si extiendo $F$ con todas las raÃ­ces de los irreducibles de sus
elementos, tengo la clausura normal; porque debe contenerlas
y es normal, asÃ­ que es la mÃ­nima. Como cada una de ellas es
separable porque tiene como irreducible el mismo irreducible
de un elemento de $F$, la [[*5.3.9. ExtensiÃ³n por un conjunto separable][extensiÃ³n entera]] es separable.

***** 5.3.12. Clausura separable
El conjunto de todos los elementos de $\overline{K}$ separables sobre $K$ forman
un subcuerpo $K^{sep}$ que se llama *clausura separable*.

****** DemostraciÃ³n
La suma o producto de elementos separables es separable, como
demostramos [[*5.3.9. ExtensiÃ³n por un conjunto separable][anteriormente]].

***** 5.3.13. Teorema del elemento primitivo
Sea $E/K$ una extensiÃ³n finita. Es simple ssi el conjunto de
cuerpos intermedios $\{ F \mid K \subset F \subset E\}$ es finito.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Sea $K(\alpha)$ simple, como es finita es algebraica, y $Irr(a,K)$ tiene 
finitos divisores en la clausura. 

Para cada divisor del polinomio irreducible creo $K[|p|]$, el subcuerpo
generado por los coeficientes del polinomio. Todo cuerpo intermedio $E$,
en el que el irreducible de $\alpha$ sea $p$ contendrÃ¡ a $K[|p|]$, pero como:

\[ [K(\alpha): E] = [K(\alpha) : K[|p|]] \]

Se tendrÃ¡ forzosamente $E = K[|p|]$.

******* Segunda implicaciÃ³n
******** Cuerpo base finito
Como la extensiÃ³n es finita, tenemos que hay finitos elementos.
El grupo multiplicativo de un cuerpo finito es [[http://mathoverflow.net/a/54741/45365][cÃ­clico]], luego es 
simple.

******** Cuerpo base infinito
Siendo $E = K(a_1,a_2,\dots,a_n)$, nos limitamos a probar $K(a,b)$ simple.
Consideramos las $K(a+xb)$ para $x \in F$. Como los elementos de $F$ son 
infinitos y las extensiones intermedias finitas, se tienen $x \neq y$
tales que $K(a+xb) = K(a+yb)$, y por tanto:

\[
b = \frac{ a +bx - (a + by)}{x-y} \in K(a+bx)
\]

****** Contraejemplo:
Hay extensiones finitas de cuerpos que no son simples

***** 5.3.13. Finita y separable es simple
Una extensiÃ³n finita y separable es simple.

****** DemostraciÃ³n
Si es finita y separable, podemos tomar su clausura normal.
La clausura normal de una extensiÃ³n finita es [[*5.2.8. Clausura de una extensiÃ³n finita][finita]], asÃ­ que
tengo una extensiÃ³n de Galois finita. HabrÃ¡ finitas subextensiones
porque habrÃ¡ finitos subgrupos de Galois.

****** Contraejemplo: cuerpo finito no simple
Tenemos [[https://en.wikipedia.org/wiki/Primitive_element_theorem#Counterexamples][contraejemplos]] en caracterÃ­stica $p$ con cuerpos de
dimensiÃ³n $p^2$.

***** 5.3.14. Endomorfismo de Frobenius
Sea $K$ de caracterÃ­stica de $p$. Llamamos *endomorfismo de Frobenius* 
a $\phi(u) = u^p$.

***** 5.3.15. Cuerpos perfectos
Para $K$ cuerpo equivalen:

  1. Todo $f \in K[X]$ irreducible tiene raÃ­ces simples.
  2. Toda $E/K$ algebraica es separable.
  3. Toda $E/K$ finita es separable.
  4. $car(K)=0$ Ã³ $car(K)=p$, y el endomorfismo de Frobenius es 
     sobreyectivo.

Y en ese caso, llamamos a $K$ *cuerpo perfecto*.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Todo elemento de la extensiÃ³n cumple algÃºn polinomio,
y son todos separables, luego es separable.

******* ImplicaciÃ³n 2 a 3
Trivial

******* ImplicaciÃ³n 3 a 4
Supongamos que no fuera sobreyectivo, existe $y \neq x^p$.
Si tomamos $(x^p-y)$, tenemos que es irreducible. Y si creamos
entonces $z^p = y$ tendrÃ­amos una extensiÃ³n no finita no
separable.

# TODO: Â¿Por quÃ© es irreducible? Por Eisenstein en algÃºn cuerpo 
# raro podrÃ­a tenerse.

******* ImplicaciÃ³n 4 a 1 en caracterÃ­stica 0
En caracterÃ­stica $0$, un polinomio no puede dividir a su derivada,
que serÃ¡ de grado menor, asÃ­ que un irreducible no puede tener raÃ­ces
dobles.

******* ImplicaciÃ³n 4 a 1 en caracterÃ­stica p
Para que un irreducible divida a la derivada, que debe ser de grado
menor, se debe tener que sea $0$. El polinomio original debe ser
de la forma $f(x^p)$ para que al derivarlo se anule.

Cuando Frobenius es automorfismo, podemos escribir:

\[f(x^p) = \sum a_i x^{ip} = \left(\sum \sqrt[p]{a_i} x^i\right)^p\]

contraviniendo irreducibilidad. Todos los polinomios deben tener
raÃ­ces simples.

***** 5.3.17. Ejemplos de cuerpos perfectos
Son perfectos:

  1. Todo cuerpo de caracterÃ­stica cero.
  2. Todo cuerpo finito.
  3. Todo cuerpo algebraicamente cerrado.
    
****** DemostraciÃ³n
******* Punto 1
Trivial desde la [[*5.3.15. Cuerpos perfectos][caracterizaciÃ³n]].
******* Punto 2
En todo cuerpo finito, un endomorfismo es sobreyectivo.
******* Punto 3
En un algebraicamente cerrado, todo irreducible es lineal y
tiene raÃ­ces simples.

**** 5.4. Derivada y raÃ­ces mÃºltiples
***** 5.4.1. RaÃ­ces
Sea $f\in F[X]$ y $u \in F$. Es raÃ­z de multiplicidad $k$ cuando $f = (X-u)^kf_0$,
con $f_0(u) \neq 0$.

***** 5.4.2. Derivada
Se define la derivada de $f$ como:

\[ f' = \sum_{i=1}^n ia_iX^{i-1} = na_nX^{n-1} + \dots + a_1\]

***** 5.4.3. Propiedades de la derivada
La derivada verifica:

 1. $(f+g)' = f'+g'$
 2. $(f \cdot g)' = f' \cdot g'$
 3. $(f^m)' = mf^{m-1}\cdot f'$

***** 5.4.4. CondiciÃ³n de raÃ­ces simples
Las raÃ­ces de $f$ son simples ssi $mcd(f,f') = 1$.

***** 5.4.5. CondiciÃ³n de raÃ­ces simples en irreducibles
Sea $f$ irreducible con $f' \neq 0$, las raÃ­ces de $f$ son simples.

***** 5.4.6. Propiedades de las raÃ­ces simples
Se cumple:

 1. En caracterÃ­stica $0$, todo irreducible tiene raÃ­ces simples.
 2. En caracterÃ­stica $p$ prima, un irreducible tiene raÃ­ces mÃºltiples
    ssi $f(x) = g(x^p)$.

****** TODO DemostraciÃ³n
***** 5.4.7. Polinomio separable
Un polinomio $f \in K[X]$ se llama separable $K$ si sus factores 
irreducibles tienen sÃ³lo raÃ­ces simples.

*** 6. TeorÃ­a de Galois Finita
**** 6.1. Grupos de automorfismos
***** 6.1.1. Espacio vectorial de las aplicaciones de un conjunto
Sea $S$ un conjunto. Las aplicaciones $Fun(S,F)$ con la suma y producto
de escalares elevados desde $F$ forman un espacio vectorial de dimensiÃ³n
$|S|$.

****** DemostraciÃ³n
Simplemente comprobar que cumplen las propiedades de espacio 
vectorial.

***** 6.1.2. ProposiciÃ³n al lema de Dedekind
Sean $\sigma_1,\dots, \sigma_m : G \longrightarrow F^\times$ homomorfismos desde un grupo $G$. 
Son distintos ssi son linealmente independientes sobre $F$.

****** DemostraciÃ³n
******* Caso base
Procedemos por inducciÃ³n. Cuando $n=1$, es trivial.

******* Caso inductivo
Tomemos un subconjunto mÃ­nimo de linealmente dependientes, 
$\sigma_1,\dots,\sigma_s$. TendrÃ­amos $b_i \in F^\times$:

\[\sigma_s = b_1\sigma_1 + \dots + b_{s-1}\sigma_{s-1}\]

Si tomamos ahora $y$ tal que $\sigma_1(y) \neq \sigma_s(y)$, evaluamos en $xy$ y 
multiplicamos por $\sigma_s(y)$ obtenemos dos ecuaciones que restadas
dan:

\[0 = b_1(\sigma_1(y)-\sigma_s(y))\sigma_1 + \dots + b_{s-1}(\sigma_{s-1}(y) - \sigma_s(y)) \sigma_{s-1}\]

RelaciÃ³n de dependencia no nula menor que la anterior.

***** 6.1.3. Lema de Dedekind
Un conjunto de homomorfismos de cuerpos $F_1 \longrightarrow F_2$ distintos
son linealmente independientes sobre $F_2$.

****** DemostraciÃ³n
Desde el lema anterior, tenemos que son linealmente independientes 
una vez restringidos a $F_1^\times \longrightarrow F_2^\times$, el aÃ±adirles el $0$ no los vuelve 
dependientes.

***** 6.1.4. AcotaciÃ³n del nÃºmero de homomorfismos sobre K
Existen como mÃ¡ximo $[F:K]$ morfismos distintos sobre $K$ 
hacia cualquier otro cuerpo:

\[|Hom(F/K,E/K)| \leq [F:K]\]

****** DemostraciÃ³n
Sea $\{u_1,\dots,u_n\}$ base de $F$, y supongamos $n+1$ homomorfismos
distintos. El siguiente sistema de ecuaciones tiene soluciÃ³n 
no trivial porque tiene $n+1$ incÃ³gnitas y tiene $n$ ecuaciones.
      
\[ X_1\sigma_1(u_j) + \dots + X_{n+1}\sigma_{n+1}(u_j) = 0 
\qquad j = 1,\dots,n\]
      
Pero una soluciÃ³n es una relaciÃ³n de dependencia sobre toda la
base de $F$. Si son dependientes, por Dedekind son [[*6.1.3. Lema de Dedekind][iguales]].

***** 6.1.5. Grupo de extensiÃ³n
Para toda extensiÃ³n finita $F/K$ llamamos *grupo de la extensiÃ³n* a:

\[ G(F/K) = \{ \sigma \in  Aut(F) \mid \forall u \in K : \sigma(u) = u\}\]

***** 6.1.6. AcotaciÃ³n de elementos del grupo
Para toda extensiÃ³n finita $F/K$ se verifica que $|G(F/K)| \leq [F : K]$.

****** DemostraciÃ³n
Caso particular de la [[*6.1.4. AcotaciÃ³n del nÃºmero de homomorfismos sobre K][acotaciÃ³n]] del nÃºmero de homomorfismos
sobre el cuerpo.

***** 6.1.7. Cuerpo fijo
Sea $G < Aut(E)$, llamamos *cuerpo fijo* por $G$ al conjunto al que 
dejan fijos todos los elementos de $G$:

\[ E^G = \{ u \in E \mid \forall\sigma\in G: \sigma(u) = u\}\]

Es un *subcuerpo* de $E$.

****** DemostraciÃ³n: es un subcuerpo
Trivialmente desde la definiciÃ³n de automorfismo de cuerpos.

***** 6.1.8. Teorema de Artin
Para $G < Aut(E)$ finito, $[E : E^G] = |G|$.

****** DemostraciÃ³n
Ya tenemos $n = |G| \leq [E : E^G]$. Supongamos la desigualdad estricta 
con $u_1,\dots,u_{n+1} \in E$ independientes sobre $E^G$. Y tenemos el sistema 
de $n+1$ incÃ³gnitas y $n$ ecuaciones, sobre los $\sigma_j$ de $G$:

\[ X_1\sigma_j(u_1) + \dots + X_{n+1}\sigma_j(u_{n+1}) = 0 \qquad
j = 1,\dots,n\]
      
Sea $a_1,\dots,a_{n+1}$ una soluciÃ³n no trivial con nÃºmero mÃ­nimo de 
elementos no nulos. Suponemos s.p.g. que $a_1 \neq 0$ y despejamos para 
tener:

\[\sigma_j(u_1) = b_2\sigma_j(u_2) + \dots + b_n\sigma_j(u_{n+1}) 
\qquad j = 1,\dots,n\]
      
En particular, en el caso $\sigma_j = id$, tenemos:

\[u_1 = b_2u_2 + \dots + b_{n+1}u_{n+1}\]

que obliga a que uno de los coeficientes no estÃ© en $E^G$. Supongamos 
s.p.g. que $b_2 \notin E^G$, y sea $\tau \in G$ tal que $\tau(b_2) \neq b_2$. Si aplicamos ahora
$\tau$ a cada una de las ecuaciones y restamos tenemos:

\[0 = (b_2-\tau(b_2))\sigma_j(u_2) + \dots + (b_n-\tau(b_n)\sigma_j(u_{n+1}))
\qquad j = 1,\dots,n\]

Esta soluciÃ³n es no trivial porque $(b_2-\tau(b_2)) \neq 0$, pero tiene mÃ¡s 
elementos nulos que la anterior.

***** 6.1.9. ExtensiÃ³n de Galois
Una extensiÃ³n finita $E/K$ es *de Galois* cuando:
     
\[\exists Gal(E/K) < Aut(E): E^{Gal(E/K)} = K\]

Llamamos a $Gal(E/K)$ el *grupo de Galois* de la extensiÃ³n.

***** 6.1.10. CaracterizaciÃ³n de extensiÃ³n de Galois
Una extensiÃ³n finita es de Galois ssi es normal y separable.

****** DemostraciÃ³n
Sea $Gal(E/K) = G$, entonces cada automorfismo se extiende a un 
homomorfismo $\sigma' : E/K \longrightarrow \overline{K}/K$, luego $[E:K]_S \geq |G| = [E:K] \geq [E:K]_S$, 
y por tanto la extensiÃ³n es separable. Como ademÃ¡s el nÃºmero total 
de homomorfismos es $[E:K]_S = |G|$, son todos automorfismos y la 
extesiÃ³n es normal.

Sea $E/K$ normal y separable. Tenemos $n = [E:K]_S = [E:K]$ 
homomorfismos $\tau : E \longrightarrow \overline{K}$ sobre $K$, con $\tau(E) = E$. Entonces 
$G = G(E/K)$ tiene orden $n$. Por Teorema de Artin, 
$[E:E^G] = n = [E:K]$, luego $[E^G:K] = 1$.

***** 6.1.11. CaracterizaciÃ³n para extensiones finitas
Sea $F/K$ una extensiÃ³n separable finita y sea $E/K$ su clausura 
normal. Entonces $E/K$ es una extensiÃ³n finita de Galois.

****** DemostraciÃ³n
La clausura normal de una extensiÃ³n separable es [[*5.3.11. La clausura normal de una separable es separable][separable]].

**** 6.2. Correspondencia de Galois, caso finito
***** Correspondencia
Definimos una correspondencia entre los subgrupos de una 
extensiÃ³n de Galois y los cuerpos intermedios como:

\[ H^\ast = E^H = \{u \in E \mid \forall\sigma\in H: \sigma(u)=u\}\]
\[F^\ast = Gal(E/F) = \{\sigma\in G \mid \forall u\in F: \sigma(u) = u\}\]

***** 6.2.1. Contenidos de cuerpos
Sean $F_i$ cuerpos intermedios de $E/K$ y $H_i$ subgrupos de $Gal(E/K)$.
Se cumple:

 1. Si $F_1 \subset F_2$, entonces $F_1^\ast \supset F_2^\ast$.
 2. Si $H_1 \subset H_2$, entonces $H_1^\ast \supset H_2^\ast$.
 3. $F \subset F^\ast^\ast$; $H < H^\ast^\ast$.
 4. $F^\ast = F^\ast^\ast^\ast$; $H^\ast = H^\ast^\ast^\ast$.

****** DemostraciÃ³n
1. Trivial.
2. Trivial.
3. Trivial.
4. Componiendo el apartado 3 con el 2 y el 1.

***** 6.2.2. Correspondencia de Galois
El par de aplicaciones $\ast$ se llama *correspondencia de Galois*.

***** 6.2.3. Teorema fundamental
Sea $E/K$ una extensiÃ³n de Galois finita con $G = Gal(E/K)$:

  1. La correspondencia es biyecciÃ³n, ${\cal F}(E/K) \cong {\cal S}(G)$.
  2. $A \supset B$ ssi $B^\ast \subset A^\ast$.
  3. La correspondencia es un antiisomorfismo de retÃ­culos, 
     $(F_1 \cdot F_2)^\ast = F_1^\ast \cap F_2^\ast$ y $(F_1 \cap F_2)^\ast = F_1^\ast \vee F_2^\ast$.
  4. Las extensiones $F_1/K$ y $F_2/K$ son conjugadas ssi los subgrupos
     $F_1^\ast$ y $F_2^\ast$ son conjugados en $G$.
  5. La extensiÃ³n $F/K$ es normal ssi $F^\ast$ es un subgrupo normal de $G$.
     En este caso $Gal(F/K) \cong G/F^\ast$.
  6. Para $H<G$ se verifica $|H| = [E:H^\ast]$ y $[G:H]=[H^\ast:K]$. 
     Para $F \in {\cal F}(E/K)$ se verifica $[E:F] = |F^\ast|$ y $[F:K] = [G:F^\ast]$.
 
****** DemostraciÃ³n de 6 para grupos
Por [[*6.1.8. Teorema de Artin][teorema de Artin]] tenemos $|G| = [E:K]$ y $|H| = [E : H^\ast]$.
Por teorema de Lagrange y teorema del grado tenemos:

\[ |G| = [G:H] |H| \]
\[ [E:K] = [E:H^\ast][H^\ast : K] \]

Simplificando obtenemos $[G:H] = [H^\ast : K]$.

****** DemostraciÃ³n de 6 para cuerpos intermedios
Como $E/F$ es de Galois, $|F^\ast| = [E:F]$ y sabemos $|G| = [E:K]$.
Volvemos a aplicar Lagrange y teorema del grado.

****** DemostraciÃ³n de 1
Tenemos la torre $G > H^\ast^\ast > H$ y:

\[ [G : H^\ast^\ast] = [H^\ast^\ast^\ast : K] = [H^\ast : K] = [G : H] \]

Tenemos la torre $E \supset F^\ast^\ast \supset F$ y:

\[ [E:F] = |F^\ast| = |F^\ast^\ast^\ast| = [E : F^\ast^\ast] \]

****** DemostraciÃ³n de 2 y 3
Se cumple por ser biyecciÃ³n y la proposiciÃ³n anterior. Trivial
desde esto el antiisomorfismo de retÃ­culos.

****** DemostraciÃ³n 4
Sean $F_2 = \sigma(F_1)$, para $\tau \in F_1^\ast$, tenemos $\sigma\tau\sigma^{-1} \in F_2^\ast$.
Luego $\sigma F_1^\ast \sigma^{-1} \subset F_2^\ast$. Aplicando lo mismo sobre $\sigma^{-1}$ llegamos
a la otra igualdad. Dando la vuelta al razonamiento, tenemos
que $\sigma F_1^\ast \sigma^{-1} = F_2^\ast$ nos da $F_2 = \sigma(F_1)$.

****** DemostraciÃ³n 5
Desde el cuarto apartado, se conserva normalidad.

Si aplicamos primer Teorema de IsomorfÃ­a a la restricciÃ³n
$\Phi : Gal(E/K) \longrightarrow Gal(F/K)$:

\[ \frac{Gal(E/K)}{F^\ast} 
= \frac{Gal(E/K)}{\ker(\Phi)} 
\cong \im(\Phi) 
= Gal(F/K) \]

El que $\im(\Phi) = Gal(F/K)$ usa la normalidad de $F$.

**** 6.4. Propiedades de las extensiones de Galois
***** 6.4.1. Subgrupos en Galois
Sean $E \supset F \supset K$ con $E/K$ Galois finita. Entonces $E/F$ es Galois
finita y $Gal(E/F)$ es subgrupo de $Gal(E/K)$.

****** DemostraciÃ³n
La finitud se tiene trivialmente. La normalidad se tiene
sobre cuerpos [[*5.2.3. Propiedades de las extensiones normales][intermedios]] y la separabilidad [[*5.3.10. Propiedades de extensiones separables][tambiÃ©n]]. Y sabemos
que normal y separable es de [[*6.1.10. CaracterizaciÃ³n de extensiÃ³n de Galois][Galois]].

Que uno es subgrupo de otro estÃ¡ claro por las propiedades de la
[[*6.2.3. Teorema fundamental][correspondencia]].

***** 6.4.2. Extensiones abelianas, cÃ­clicas y solubles
Una extensiÃ³n finita se dice *abeliana, cÃ­clica o soluble* si es
de Galois y su grupo lo es.

***** 6.4.3. Subgrupos abelianos, cÃ­clicos y solubles
Sea $E\supset F\supset K$ con $E/K$ finita y *abeliana, cÃ­clica o soluble*,
entonces $E/F$ tambiÃ©n es abeliana, cÃ­clica o soluble, respectivamente.

****** DemostraciÃ³n
Tenemos que el subgrupo de un abeliano, cÃ­clico o soluble
es tambiÃ©n abeliano, cÃ­clico o soluble.

***** 6.4.4. Subextensiones finitas abelianas y cÃ­clicas
Sean $K \subset F \subset E$ torre de extensiones *finitas*:

 1. Si $E/K$ es Galois abeliana, $F/K$ es Galois abeliana.
 2. Si $E/K$ es Galois cÃ­clica, $F/K$ es Galois cÃ­clica.

****** TODO DemostraciÃ³n

***** 6.4.5. Galois para cuerpos compuestos
Sea $E/K$ Galois finita y $F/K$ extensiÃ³n con $E,F \subset L$.
Entonces $EF/F$ y $E/(E\cap F)$ son extensiones finitas de Galois.
AdemÃ¡s la aplicaciÃ³n restricciÃ³n $\sigma \mapsto \sigma|_E$ define un isomorfismo:

\[ \bullet|_E : Gal(EF/F) \longrightarrow Gal(E/(E\cap F)) < Gal(E/K) \]

****** TODO DemostraciÃ³n

***** 6.4.6. RelaciÃ³n de Galois con el grado
Sea $E/K$ extensiÃ³n finita de Galois y sea $F/K$ con $E,F \subset L$.
Entonces $[EF:F] \mid [F:K]$.

****** TODO DemostraciÃ³n

***** 6.4.8. ComposiciÃ³n de extensiones de Galois
Sean $E_1/K$, $E_2/K$ extensiones Galois finitas con $E_1,E_2 \subset L$.
Entonces $E_1E_2/K$ es extensiÃ³n finita de Galois y existe un
monomorfismo restricciÃ³n:

\[
\lambda : Gal(E_1E_2/K) 
\longrightarrow 
Gal(E_1/K) \times Gal(E_2/K)
\]

Cuando ademÃ¡s tenemos $E_1 \cap E_2 = K$, $\lambda$ es un isomorfismo.

****** TODO DemostraciÃ³n
***** 6.4.9. ExtensiÃ³n del producto
Sean $E_i/K$ extensiones contenidas en un $L$ con grupos de Galois
$G_1,G_2,\dots,G_n$. Si cumplendemÃ¡s $E_i \cap (E_1E_2\dots E_{i-1}) = K$, entonces:

\[Gal(E_1E_2\dots E_n/K) \cong G_1 \times G_2 \times \dots \times G_n\]

****** TODO DemostraciÃ³n
***** 6.4.10. Cuerpos fijos del producto
Sea $E/K$ una extensiÃ³n finita de Galois con grupo $G = G_1 \times \dots \times G_n$.
Sea $E_i$ el cuerpo fijo de $G_1 \times \dots \times \{1\} \times \dots \times G_n$. Entonces $E_i/K$ es
de Galois con grupo $Gal(E_i/K) = G_i$, $E_i \cap (E_1\dots E_{i-1})$ y $E=E_1\dots E_n$.

****** TODO DemostraciÃ³n

*** 7. Cuerpos finitos
**** 7.1. Estructura de los cuerpos finitos
***** 7.1.1. Propiedades de un cuerpo finito
Sea $F$ cuerpo finito con $|F| = q$,

  1. $car(F) = p$ es un primo.
  2. El cuerpo primo es $\mathbb{Z}/p\mathbb{Z}$.
  3. $F/\mathbb{Z}_p$ es extensiÃ³n finita.
  4. $[ F : \mathbb{Z}_p] = n$, entonces $|F| = p^n$.
  5. $F^\times$ es cÃ­clico de orden $|F|-1$.
 
****** DemostraciÃ³n
******* Punto 1
No puede tener caracterÃ­stica nula por ser finito, luego debe
ser un primo.

******* Punto 2
Ya que tiene caracterÃ­stica prima.

******* Punto 3
Ya que $F$ es finito.

******* Punto 4
Si tiene una base de $n$ elementos, debe tener $p^n$ combinaciones
de elementos bÃ¡sicos.

******* Punto 5
Todo subgrupo finito del grupo multiplicativo de un cuerpo es 
cÃ­clico.

***** 7.1.2. ClasificaciÃ³n de cuerpos finitos
Dos cuerpos finitos del mismo cardinal son isomorfos. De hecho,
son el cuerpo de descomposiciÃ³n de $X^{|F|} - X$ sobre $\mathbb{F}_p$.

****** DemostraciÃ³n
Al ser [[*7.1.1. Propiedades de un cuerpo finito][cÃ­clico]] $u^{q-1} = 1$ nos da $u^q-u = 0$ con todo el cuerpo
como raÃ­ces.

***** 7.1.3. Existencia de cuerpos finitos
Dado $p$ primo, existe cuerpo de $p^n$ elementos.

****** DemostraciÃ³n
Sea $f(x) = x^{p^n}-x$ polinomio en $\mathbb{Z}_p$. Su derivada, $-1$, no tiene raÃ­ces,
luego tiene sÃ³lo raÃ­ces simples. Veamos que con sÃ³lo aÃ±adir esas
raÃ­ces del polinomio, llega a ser cuerpo de descomposiciÃ³n.

Sean $u,v$ raÃ­ces:

 - $(u+v)^{p^n} - (u+v) = u^{p^n}-u+v^{p^n}-v = 0$
 - $(uv)^{p^n}-uv = u^{p^n}v^{p^n} - uv = 0$
 - $(-u)^{p^n} - (-u) = 0$
 - $(u^{-1})^{p^n} - u^{-1} = 0$

***** 7.1.4. Teorema de Moore
Para cada $p^n$ existe exactamente un cuerpo con $p^n$ elementos; 
que es el cuerpo de descomposiciÃ³n de $x^{p^n}-x$ sobre $\mathbb{Z}_p$. 
No existen otros cuerpos finitos.

****** DemostraciÃ³n
Sabemos los cuerpos [[*7.1.1. Propiedades de un cuerpo finito][deben]] tener cardinal $p^n$ y que dos cuerpos
con el mismo cardinal son [[*7.1.2. ClasificaciÃ³n de cuerpos finitos][isomorfos]]. AdemÃ¡s, sabemos que
[[*7.1.3. Existencia de cuerpos finitos][existe]].

***** 7.1.5. Cuerpos de Galois
Notamos por $GF(p^n)$ o $\mathbb{F}_{p^n}$ al Ãºnico cuerpo con esa cardinalidad,
lo llamamos cuerpo de Galois de orden $p^n$.

***** 7.1.6. Cuerpo extensiÃ³n
Para $\mathbb{F}_q$ cuerpo finito, exÃ­ste un Ãºnico cuerpo de extensiÃ³n de grado $n$,
que es $\mathbb{F}_{q^n}$.

****** DemostraciÃ³n
Por el teorema de Moore, $q = p^m$, y sÃ³lo hay uno de $p^{nm}$ elementos.

***** 7.1.7. Grupo de automorfismos
El grupo $Aut(\mathbb{F}_{p^n}) \cong \mathbb{Z}_n$ es cÃ­clico y estÃ¡ generado por el [[*5.3.14. Endomorfismo de Frobenius][Endomorfismo de 
Frobenius]].

****** DemostraciÃ³n
El cuerpo fijo bajo el endomorfismo de Frobenius son los $p$ 
elementos cumpliendo $a^p = a$, es decir, el cuerpo base $\mathbb{F}_p$.

El generado debe ser su grupo de Galois, ya que tiene orden $n$.

***** 7.1.8. Grupo de automorfismos relativo
Un $\mathbb{F}_{p^n}$ es subcuerpo de $\mathbb{F}_{p^m}$ ssi $n\;|\;m$. En este caso, $\mathbb{F}_{p^m}/\mathbb{F}_{p^n}$ es cÃ­clica
con $Gal(\mathbb{F}_{p^m}/\mathbb{F}_{p^n}) = \langle \phi^n \rangle$.

****** DemostraciÃ³n
******* Si es subcuerpo, divide
Por Lagrange, $[\mathbb{F}_{p^n} : F] | [\mathbb{F}_{p^m} : F]$.

******* Si divide, es subcuerpo
El $Gal(\mathbb{F}_{p^m}/\mathbb{F}) \cong \mathbb{Z}_m$ tiene un Ãºnico subgrupo de orden $n$. Y debe
ser el grupo de Galois de un cuerpo de orden $m/n$, que tiene por
Moore que ser $\mathbb{F}_{p^n}$.

**** 7.2. FactorizaciÃ³n de polinomios
***** 7.2.1. Listado de polinomios irreducibles
Los factores irreducibles de $X^{p^n} - X$ son exactamente los polinomios
irreducibles de $\mathbb{F}_p[X]$ con grado divisor de $n$.

****** DemostraciÃ³n
******* Los factores irreducibles tienen grado n
Cuando $g(\alpha) = 0$, por ser factor tenemos $\alpha^{p^n} = \alpha$, luego $\alpha \in \mathbb{F}_{p^n}$.
AsÃ­, $\mathbb{F}_{p^n} \supseteq \mathbb{F}_p(\alpha) \supseteq \mathbb{F}_p$, y por teorema del grado, $gr(g) = [\mathbb{F}_p(\alpha) : \mathbb{F}_p]$
divide a $[\mathbb{F}_{p^n} : \mathbb{F}_p]$.

******* Los irreducibles de grado divisor de n son factores
Sea $g(\alpha) = 0$, tomamos $m = [\mathbb{F}_p(\alpha) : \mathbb{F}_p] = gr(g)$, y [[*7.1.8. Grupo de automorfismos relativo][sabemos]] que
debe tenerse $\mathbb{F}_{p^m} \supset \mathbb{F}_p(\alpha)$. Por Moore $f(\alpha) = 0$ y $g \mid f$.

***** 7.2.2. NÃºmero de mÃ³nicos irreducibles
Para $n$ primo, el nÃºmero de mÃ³nicos irreducibles de $\mathbb{F}_p[X]$ de grado $n$
es $(p^n-p)/n \neq 0$.

****** TODO DemostraciÃ³n

***** 7.2.3. RaÃ­ces simples
Para $f \in \mathbb{F}_p[X]$, $f_1 = f / mcd(f,f')$ tiene todas las raÃ­ces simples y
$f(\alpha) = 0 \iff f_1(\alpha) = 0$.

****** DemostraciÃ³n
Calculando vemos que $f = \prod_i(X-\alpha_i)^{k_i}$ da $f_1 = \prod_{i} (X- \alpha_i)$.

***** 7.2.4. Producto de factores irreducibles de grado n
El producto de todos los factores irreducibles distintos de un $f$
y cuyo grado divida a $n$ es $mcd(f, X^{p^n} - X)$.

****** TODO DemostraciÃ³n

**** TODO 7.3. Ilustraciones
*** 8. Extensiones ciclotÃ³micas
**** 8.1. RaÃ­ces de la unidad
***** 8.1.1. Subgrupos finitos del grupo multiplicativo
Todo subgrupo finito del grupo multiplicativo $K^\times$ es cÃ­clico.

****** DemostraciÃ³n
Si $n = mcm\{ord(\alpha) \mid \alpha \in G\} = \max\{ord(\alpha) \mid \alpha \in G\}$, tenemos que
debe ser $n = |G|$. En caso contrario se tendrÃ­a $X^n-1$ con mÃ¡s de
$n$ raÃ­ces.

***** 8.1.2. RaÃ­ces n-Ã©simas
Si llamamos $\mu_n(K) = \{ \zeta \in K \mid \zeta^n = 1 \}$, $\mu_n(K)$ es un grupo cÃ­clico finito
con $|\mu_n(K)| \leq n$.

****** DemostraciÃ³n
Trivial por el orden del polinomio $\zeta^n-1$.

***** 8.1.3. Grupo de raÃ­ces n-Ã©simas
Llamamos *grupo de raÃ­ces n-Ã©simas* de la unidad al grupo siguiente. 
Cualquier generador del grupo se llama una *raÃ­z primitiva* de la 
unidad.

\[ \mu_n(\overline{K}) = \{\zeta\in \overline{K} \mid \zeta^n = 1\} \]

***** 8.1.4. Lema de divisiÃ³n
Tenemos $d\mid n$ ssi $\mu_d \subset \mu_n$.

****** TODO DemostraciÃ³n
**** 8.2. Polinomios ciclotÃ³micos
***** 8.2.1. Polinomio ciclotÃ³mico
Se llama *polinomio ciclotÃ³mico* al polinomio:

\[\Phi_n = \prod_{\zeta \text{ primitiva}} (X -\zeta)\]

***** 8.2.2. Grado del polinomio ciclotÃ³mico
Se cumple que $\operatorname{grad}(\Phi_n) = \phi(n)$.

****** DemostraciÃ³n
El grupo de las raÃ­ces n-Ã©simas es cÃ­clico y de orden $n$, y tendrÃ¡
exactamente $\phi(n)$ generadores.

***** 8.2.3. Lema al cÃ¡lculo de polinomios ciclotÃ³micos
Tenemos:

\[X^n-1 = \prod_{d \mid n} \Phi_d\]

****** TODO DemostraciÃ³n
***** 8.2.4. CÃ¡lculo de los polinomios ciclotÃ³micos
 Tenemos:

 \[\Phi_n  = \frac{X^n-1}{\prod_{d|n, d\neq n} \Phi_d}\]

****** DemostraciÃ³n
Aplicando la fÃ³rmula [[*8.2.3. Lema al cÃ¡lculo de polinomios ciclotÃ³micos][anterior]].

***** 8.2.7. El polinomio ciclotÃ³mico es mÃ³nico
El polinomio ciclotÃ³mico es mÃ³nico $\Phi_n$ y tiene coeficientes en el
anillo primo.

****** TODO DemostraciÃ³n
***** 8.2.8. FunciÃ³n de MÃ¶bius
Se define la *funciÃ³n de MÃ¶bius*, $\mu : \mathbb{N} \longrightarrow \mathbb{Z}$, como:

\[\mu(n) = \threepartdef
{0}{\exists p: \text{ primo con } p^2|n}
{(-1)^r}{n = p_1p_2\dots p_n \text{ primos distintos}}
{1}{n=1}\]

***** 8.2.8. FunciÃ³n de MÃ¶bius en la unidad
La funciÃ³n de MÃ¶bius verifica que:

\[ \sum_{d \mid n} \mu(d) =
\left\{\begin{array}{ll} 
1 & \mbox{if } n=1  \\
0 & \mbox{if } n \neq 1 
\end{array}
\right.
\]

****** TODO DemostraciÃ³n

***** 8.2.10. Reglas de cÃ¡lculo de polinomios ciclotÃ³micos
1. Si $p$ es primo $\Phi_p(X) = X^{p-1} + X^{p-2} + \dots + X + 1$.
2. $\Phi_{p^e}(x) = \Phi_p(x^{p^{e-1}})$
3. $\Phi_{p_1^{e_1}p_2^{e_2}\dots p_r^{e_r}}(x) = \Phi_{p_1\dots p_r}(x^{p_1^{e_1-1}\dots p_r^{e_r-1}})$
4. $\Phi_n = \prod_{d\mid n} (x^{n/d}-1)^{\mu(d)}$

**** 8.3. Extensiones ciclotÃ³micas
***** 8.3.1. ExtensiÃ³n ciclotÃ³mica
Llamamos n-Ã©sima extensiÃ³n ciclotÃ³mica de $K$ a $K(\zeta)/K$, donde
$\zeta$ es una raÃ­z primitiva de la unidad en la clausura.

***** 8.3.2. Grado en los racionales
En los racionales, $[\mathbb{Q}(\zeta) : \mathbb{Q}] = \phi(n)$.

****** TODO DemostraciÃ³n

***** 8.3.3. Irreducibilidad racional del polinomio ciclotÃ³mico
$\Phi_n$ es irreducible en $\mathbb{Z}[X]$ (y en $\mathbb{Q}[X]$).

****** TODO Contraejemplo general
****** TODO DemostraciÃ³n
***** 8.3.4. Irreducibilidad en cuerpo base primo
El cuerpo $\mathbb{F}_p$ contiene una raÃ­z n-Ã©sima primitiva de la unidad
ssi $p \equiv_n 1$.

****** DemostraciÃ³n
Si contiene una raÃ­z n-Ã©sima primitiva, debe cumplir $\zeta^n = 1$, y a su
vez tener orden $p-1$, luego $n \mid p-1$. Si $n\mid p-1$, como $\mathbb{Z}_p^\times$ es
cÃ­clico, tiene un elemento de orden $n$, que serÃ¡ raÃ­z primitiva de la
unidad.

***** 8.3.5. Primos congruentes a 1
Para todo $n$ existen infinitos primos congruentes a $1$ mÃ³dulo $n$.

****** TODO DemostraciÃ³n
***** 8.3.6. Torre de extensiones ciclotÃ³micas
Sean $n \mid m$, entonces $\mathbb{Q}(\zeta_n) \subset \mathbb{Q}(\zeta_m)$.

****** TODO DemostraciÃ³n
***** 8.3.7. Producto de extensiones ciclotÃ³micas
Sean $n,m$ y raÃ­ces primitivas distintas:

\[\mathbb{Q}(\zeta_n)\mathbb{Q}(\zeta_m) = \mathbb{Q}(\zeta_{mcm(n,m)})\]

****** TODO DemostraciÃ³n

***** 8.3.8. IntersecciÃ³n de extensiones ciclotÃ³micas
Sean $n,m$ primos relativos, $\mathbb{Q}(\zeta_n) \cap \mathbb{Q}(\zeta_m) = \mathbb{Q}$.

****** TODO DemostraciÃ³n
**** 8.4. Grupos de Galois
***** 8.4.1. ExtensiÃ³n ciclotÃ³mica es de Galois
La extensiÃ³n ciclotÃ³mica $K(\zeta)/K$ es una extensiÃ³n de Galois.

****** DemostraciÃ³n
Es normal porque estÃ¡ generada por $X^n-1$, ya que la raÃ­z primitiva
genera a todas sus raÃ­ces. Como trabajamos con la hipÃ³tesis de que
$car(K) \nmid n$, todas las raÃ­ces de la unidad son distintas y el polinomio
es separable.

***** 8.4.2. Grupo de galois de la extensiÃ³n ciclotÃ³mica
El grupo de Galois de la extensiÃ³n ciclotÃ³mica es un subgrupo
de un multiplicativo, $Gal(K(\zeta)/K) < \mathbb{Z}_n^\times$.

****** DemostraciÃ³n
Todo $\sigma \in Gal(K(\zeta)/K)$ estÃ¡ determinado por su efecto sobre $\sigma(\zeta) = \zeta^a$,
que debe llevar una raÃ­z primitiva en otra. Comprobamos que $\Lambda(\sigma) = a$
es un homomorfismo inyectivo y aplicamos Primer teorema de IsomorfÃ­a:

\[\Lambda(\sigma) = a;\; \Lambda(\tau) = b; 
\qquad
\tau\sigma(\zeta) = \zeta^{ab}\]

Donde $a \in \mathbb{Z}^\times_n$ porque debe poder invertirse por otra funciÃ³n.

***** 8.4.3. Grupo abeliano de la extensiÃ³n ciclotÃ³mica
En general $Gal(K(\zeta)/K)$ es abeliano.

****** DemostraciÃ³n
Es subgrupo de un abeliano.

***** 8.4.4. Grupo de la extensiÃ³n ciclotÃ³mica en los racionales
Tenemos $Gal(\mathbb{Q}(\zeta)/\mathbb{Q}) \cong \mathbb{Z}_n^\times$.

****** TODO DemostraciÃ³n

***** 8.4.11. Teorema de Kronecker-Weber
Toda extensiÃ³n abeliana de $\mathbb{Q}$ estÃ¡ contenida en una extensiÃ³n 
ciclotÃ³mica.

***** 8.4.12. Existencia de extensiones
Sea $G$ un grupo abeliano finito arbitrario. Existe una extensiÃ³n $K/\mathbb{Q}$
tal que $Gal(K/\mathbb{Q}) \cong G$.

*** 10. Extensiones cÃ­clicas y radicales
**** 10.1. Extensiones cÃ­clicas
***** 10.1.1. ExtensiÃ³n cÃ­clica
Una extensiÃ³n es cÃ­clica si es de Galois con grupo cÃ­clico.

***** 10.1.2. Teorema de Lagrange
Sea $K$ cuerpo y $n$ un primo relativo a $car(K)$. Supongamos que existe
una raÃ­z n-Ã©sima de la unidad en $K$:

  1. Dada $E/K$ extensiÃ³n cÃ­clica de grado $n$, existe $\alpha \in E$ tal que
     $E = K(\alpha)$ y $Irr(\alpha,K) = X^n-a$ para algÃºn $a \in K$.
  2. Para $a \in K$, con $\alpha$ raÃ­z de $X^n-a$, se tiene $K(\alpha)/K$ cÃ­clica
     de grado $d \mid n$ y $\alpha^d \in K$.

****** DemostraciÃ³n
******* Primer punto
Sea $E = K(u)$, $Gal(E/K) = \langle\sigma\rangle$ y $\zeta$ raÃ­z n-Ã©sima primitiva de la 
unidad. Llamamos *resolvente de Lagrange* a:

\[
\alpha = 
u + \zeta\sigma(u) + \zeta^2\sigma^2(u) + \dots + \zeta^{n-1}\sigma^{n-1}(u)
\]

Y por independencia lineal de los homomorfismos, sabemos $\alpha \neq 0$.
Aplicando $\sigma$ obtenemos:

\[\begin{aligned}
\sigma(\alpha) &= 
\zeta^{n-1}\sigma^n(u) + \sigma(u) + 
\zeta\sigma^2(u) + \dots + \zeta^{n-2}\sigma^{n-1}(u)
\\&=
\zeta^{-1}\alpha
\end{aligned}\]

Luego $\sigma^{i}(\alpha) = \zeta^{-i}\alpha$ y $\alpha$ tiene $n$ conjugados. Como $[K(\alpha):K] \geq n$,
tenemos $E = K(\alpha)$. AdemÃ¡s $\sigma(\alpha^n) = (\zeta\alpha)^n = \alpha^n$, luego $a = \alpha^n$ es
fijo bajo $\sigma$ y debe ser $a \in K$.

******* Segundo punto
******** Es de Galois
Todas las $\zeta^i\alpha$ son raÃ­ces disintas, luego $K(\alpha)$ es el cuerpo de
descomposiciÃ³n del polinomio $X^n-a$ y es normal y separable por
ser $n$ primo relativo a $car(K)$.

******** Es cÃ­clica
Cada $\sigma \in Gal(K(\alpha)/K)$ se escribe como $\sigma(\alpha) = \omega_\sigma\alpha$ para alguna raÃ­z
de la unidad, ya que debe llevar $\alpha$ en otra raÃ­z y queda determinado
por ella. AsÃ­ $\sigma \longrightarrow \omega_\sigma$ es un monomorfismo en el grupo de las raÃ­ces 
n-Ã©simas, que sÃ³lo tiene como subgrupos cÃ­clicos de orden $d \mid n$. 

Si $Gal(K(\alpha)/K)$ es cÃ­clico de orden $d$ generado por $\langle\sigma\rangle$, entonces $\omega_\sigma$ 
es raÃ­z d-Ã©sima primitiva de la unidad. Entonces:

\[\sigma(\alpha^d) = (\omega_\sigma\alpha)^d = \alpha^d\]

Quedando fijo sobre el grupo de Galois, $\alpha^d \in K$.

**** 10.2. Extensiones solubles y radicales
***** 10.2.1. ExtensiÃ³n soluble
Una extensiÃ³n separable $F/K$ es soluble si hay una extensiÃ³n $E/K$ de 
Galois con grupo soluble y $K < F < E$.

****** DefiniciÃ³n equivalente
Una extensiÃ³n separable es soluble si su clausura normal tiene grupo
de Galois soluble.

***** 10.2.2. ExtensiÃ³n soluble por radicales
Una extensiÃ³n $F/K$ finita separable es soluble por radicales cuando
$mcd([F:K],car(K)) = 1$ y hay una torre:

\[K = E_0 < E_1 < \dots < E_m = E\]

cumpliendo $F < E$ y siendo cada $E_{i+1}/E_i$ de una de estas dos formas:

  1. $E_{i+1} = E_i(\zeta)$ raÃ­z de la unidad.
  2. $E_{i+1} = E_i(\alpha)$ raÃ­z de un polinomio $X^n-a \in E_i[X]$, con 
     $mcd(n,car(K)) = 1$.

***** 10.2.3. Propiedades de las solubles por radicales
Sea $F/K$ soluble por radicales:

  1. Dada $L > K$, $FL/L$ es soluble por radicales.
  2. Dada $L > F > K$, $L/K$ soluble por radicales ssi $F/K, L/F$ solubles
     por radicales.
  3. $F/K, L/K$ solubles por radicales da $FL/K$ soluble por radicales.

****** TODO DemostraciÃ³n

***** 10.2.4. CaracterizaciÃ³n de solubilidad por radicales
Sea $F/K$ separable finita. Es soluble por radicales ssi existe:

\[L_0 < L_1 < \dots < L_m = L > F\]

Con cada paso soluble por radicales y $L/K$ Galois.

****** TODO DemostraciÃ³n

***** 10.2.5. Teorema de Galois
Sea $F/K$ separable finita. Es soluble por radicales ssi es soluble.

****** TODO DemostraciÃ³n

***** 10.2.6. Propiedades de las solubles
Cumplen:

  1. Dada $L > F > K$, $L/K$ soluble ssi $F/K,L/F$ solubles.
  2. Dada $F/K$ soluble $L/K$ arbitraria, $FL/L$ es soluble.
  3. Dadas $F/K,L/K$ solubles, $FL/K$ es soluble.

****** TODO DemostraciÃ³n
Aplicando el [[*10.2.5. Teorema de Galois][Teorema de Galois]] a las propiedades de [[*10.2.3. Propiedades de las solubles por radicales][solubles por 
radicales]].

*** 11. Polinomios de grado 3 y 4
**** 11.1. El grupo de un polinomio
***** 11.1.1. Grupo de un polinomio
Llamamos grupo del polinomio $f$ al grupo $Gal(f/K)$, dado por las 
permutaciones de raÃ­ces del polinomio que da el grupo de Galois sobre
su cuerpo de descomposiciÃ³n. Se tiene:

\[Gal(f/K) \lhook\joinrel\relbar\joinrel\rightarrow S_n\]

**** 11.2. Los teoremas clasificatorios
***** 11.2.1. Ceros bajo la misma Ã³rbita
Sea un polinomio con raÃ­ces simples factorizado $f = f_1f_2\dots f_r \in K[X]$.
Dos ceros $\alpha_i,\alpha_j$ son raÃ­ces del mismo $f_k$ ssi estÃ¡n en la misma Ã³rbita
bajo $Gal(f/K)$.

****** DemostraciÃ³n
Si estÃ¡n en la misma Ã³rbita, hay morfismo que lleva una en otra, luego
son conjugadas y tienen el mismo irreducible. Si son conjugadas existe
el morfismo que lleva una en otra y estÃ¡n bajo la misma Ã³rbita.

***** 11.2.2. Criterio de irreducibilidad de Galois
El polinomio $f$ es irreducible sobre $K$ ssi $Gal(f/K)$ es un subgrupo
transitivo de $S_n$.

****** DemostraciÃ³n
SerÃ¡n todas sus raÃ­ces del mismo irreducible ssi sÃ³lo hay una Ã³rbita.

***** 11.2.3. Discriminante fijo bajo permutaciones pares
Supongamos que $f$ no tiene raÃ­ces mÃºltiples y sea $car(K) \neq 2$. Sea:

\[\delta = \prod_{i < j} (\alpha_i - \alpha_j)\]

El cuerpo fijo bajo las permutaciones pares de $Gal(f/K)$ es $K(\delta)$.

****** RelaciÃ³n con el discriminante
Observamos que $(a_n^{n-1}\delta)^2 = Disc(f)$.

****** DemostraciÃ³n
Llamamos $F$ al cuerpo fijo bajo las permutaciones pares.
Para cualquier permutaciÃ³n impar $\rho(\delta) = -\delta$ y para cualquiera
par $\sigma(\delta) = \delta$, asÃ­ que $K \subset K(\delta) \subset F$. Tenemos dos casos:

  - $K = F$, que darÃ­a el resultado.
  - $K \neq F$, donde por Galois, $G \neq G^+$ y deberÃ­a tenerse $[G : G^+] = 2$,
    luego $[F : K] = 2$. Pero como una permutaciÃ³n impar cambia el signo
    del determinante, que no es nulo por definiciÃ³n, $\delta \notin K$, y por
    tanto $F = K(\delta)$.

NÃ³tese que en $car(K)=2$, el cambio de signo no hubiera movido a $\delta$.

***** 11.2.4. Determinante marcando paridad
El grupo de un polinomio es sÃ³lo par $Gal(f/K) < A_n$ ssi $\sqrt{Disc(f)} \in K$.

****** DemostraciÃ³n
Es lo que marca la distinciÃ³n de casos en la anterior demostraciÃ³n.

**** 11.3. Polinomios de grado pequeÃ±o
***** 11.3.1. Polinomios de grado 2
En un polinomio de grado 2 el discriminante vale $b^2 - 4ac$. Hay sÃ³lo
dos posibilidades:

  - $\sqrt{b^2-4ac} \in K$, entonces $Gal(f/K) = 1$, y tiene dos raÃ­ces en el 
    cuerpo. 
  - $\sqrt{b^2-4ac} \notin K$, entonces $Gal(f/K) = S_2$, y el polinomio es 
    irreducible.

El cuerpo de descomposiciÃ³n es $E = K\left(\sqrt{Disc(f)}\right)$.

****** DemostraciÃ³n
NÃ³tese que en ambos casos el cuerpo que dejan fijo las permutaciones
pares es todo el cuerpo de descomposiciÃ³n del polinomio.

***** 11.3.2. Polinomios de grado 3
Un polinomio mÃ³nico $f = X^3+b_2X^2+b_1X +b_0$ puede:

  - Ser reducible, con algÃºn factor lineal que nos lleva al caso de
    grado 2.
  - Ser irreducible.

En el caso irreducible:

  - $\sqrt{Disc(f)} \notin K$ ssi $Gal(f/K)=S_3$.
  - $\sqrt{Disc(f)} \in K$ ssi $Gal(f/K) = A_3$.

***** 11.3.3. Polinomios de grado 4
Tenemos un polinomio de grado 4 dado por:

\[
f = X^4 + a_3X^3+ a_2X^2 + a_1X + a_0 = \prod_{i=1}^4 (X - \alpha_i)
\]

****** CuÃ¡rtica reducible
Pueden darse dos casos:

  1. Se descompone en polinomios de grado 3 y 1; y tenemos una cÃºbica
     como en el caso anterior.
  2. Se descompone en $f = f_1f_2$, por lo que debe ser subgrupo de
     $\langle (1\ 2),(3\ 4) \rangle$. Se distinguen dos casos usando discriminante:

     - $H = \langle (1\ 2)(3\ 4) \rangle$ cuando el discriminante estÃ¡
       en el cuerpo $\sqrt{Disc(f_1)Disc(f_2)} \in K$.

     - $H = \langle (1\ 2),(3\ 4)\rangle$ cuando el discriminante no estÃ¡
       en el cuerpo $\sqrt{Disc(f_1)Disc(f_2)} \notin K$.

****** CuÃ¡rtica irreducible
El grupo es uno de los grupos transitivos de $S_4$:

\[\begin{tikzcd}
& S_4 \dlar[no head]\drar[no head] & & \\
A_4 \drar[no head] & & D_4 \dlar[no head]\drar[no head] & \\
& V & & C_4
\end{tikzcd}\]

Para ayudarnos a resolver el polinomio debemos creamos su 
*resolvente cÃºbica*, dados:

  - $\beta_1 = \alpha_2\alpha_3 + \alpha_1\alpha_4$
  - $\beta_2 = \alpha_1\alpha_3 + \alpha_2\alpha_4$
  - $\beta_3 = \alpha_1\alpha_2 + \alpha_3\alpha_4$

El polinomio que los tiene como raÃ­ces es de grado 3 y tiene
el mismo determinante. Puede calcularse con polinomios simÃ©tricos:

\[\begin{aligned}
g &= (X-\beta_1)(X-\beta_2)(X-\beta_3) \\
  &= X^3 - a_2X^2 + (a_1a_3 - 4a_0)X + (4a_2a_0 - a_3^2a_0 - a_1^2)
\end{aligned}\]

NÃ³tese que $H < D_4$ o conjugado si $\beta_2 \in K$ o alguno de los otros.
Usando eso y que el discriminante nos da las permutaciones pares:

\[\begin{array}{ccc|c}
\sqrt{Disc(g)} & \beta_i & Gal(g/K) & Gal(f/K) \\
\hline
\notin K & \notin K & S_3 & S_4 \\
\in K & \notin K & A_3 & A_4 \\
\in K & \in K & 1 & V \\
\notin K & \in K & S_3 & D_4,C_4
\end{array}\]

******* DistinciÃ³n de los Ãºltimos casos
Observamos que $D_4 \cap A_4 = V$ es transitivo pero $C_4 \cap A_4 = \langle (13)(24) \rangle$ 
no lo es. AsÃ­ que $Gal(f/K) = D_4$ ssi $f$ es irreducible sobre
$K\left(\sqrt{Disc(f)}\right)$.

**** TODO 11.4. CÃ³mo resolver una ecuaciÃ³n soluble

*** A. Norma y traza
**** Norma y traza
***** Norma
Se define la *norma* de $\alpha$ relativa a $F/K$ separable como:

\[N_{F/K} = \prod \{\sigma_i(\alpha) \mid 1\leq i\leq n\} \]

para los $n$ homomorfismos que da $[F:K]_S$.

***** Traza
Se define la *traza* de $\alpha$ relativa a $F/K$ separable como:

\[T_{F/K} = \sum \{\sigma_i(\alpha) \mid 1 \leq i \leq n\}\]

para los $n$ homormorfismos que da $[F : K]_S$.

***** Norma y traza estÃ¡n en el normal
Si $F/K$ es de Galois, $N_{F/K}(\alpha), T_{F/K}(\alpha) \in F$.

****** DemostraciÃ³n
Al ser de Galois, los homomorfismos tienen imÃ¡genes en $F$.

***** Propiedades de norma y traza
Sea $F/K$ finita y separable de grado $n$:

  1. $N(\alpha\beta) = N(\alpha)N(\beta)$
  2. $T(\alpha+\beta) = T(\alpha) + T(\beta)$
  3. $N(a\alpha) = a^nN(\alpha)$ para cada $a \in K$
  4. $T(a\alpha) = naT(\alpha)$ para cada $a \in K$
  5. $N(\alpha) \in K$ para cada $\alpha \in F$
  6. $T(\alpha) \in K$ para cada $\alpha \in F$

****** DemostraciÃ³n
******* Puntos 1 y 2
Simplemente por ser homomorfismos los que componen la suma y el
producto.

******* Puntos 3 y 4
Los homomorfismos dejan fijos los elementos del cuerpo.

******* TODO Puntos 5 y 6
Para $E$ una clausura normal $Gal(E/K)$ actÃºa sobre $\frac{Gal(E/K)}{Gal(F/K)}$.

***** FÃ³rmulas de transitividad
Para una torre $K \subset F \subset E$ y $E/K$ finita y separable:

  1. $N_{F/K}(N_{E/F}(\alpha)) = N_{E/K}(\alpha)$
  2. $T_{F/K}(T_{E/F}(\alpha)) = T_{E/K}(\alpha)$

****** TODO DemostraciÃ³n
***** Elemento de traza no nula
Sea $F/K$ finita y separable, existe $\alpha \in F$ tal que $T_{F/K}(\alpha) \neq 0$.

****** TODO DemostraciÃ³n

***** Discriminante de una extensiÃ³n
Sea $F/K$ extensiÃ³n finita y separable. Dados $\alpha_1,\dots,\alpha_n \in F$ equivalen:

  1. $\{\alpha_1,\dots,\alpha_n\}$ base de $F$ como espacio vectorial sobre $K$.
  2. Son linealmente independientes sobre $F$ los elementos:

     \[\begin{aligned}
     \beta_1 &= (\sigma_1(\alpha_1), \dots, \sigma_1(\alpha_n)) \\
     \vdots &\\
     \beta_n &= (\sigma_n(\alpha_1), \dots, \sigma_n(\alpha_n)) \\
     \end{aligned}\]

  3. El determinante de la matriz $(T_{F/K}(\alpha_i\alpha_j))_{ij}$ es no nulo.

A este determinante lo llamamos *discriminante de la extensiÃ³n* relativo
a la base $\alpha_1,\dots,\alpha_n$.

****** TODO DemostraciÃ³n

*** B. Lista de temas de teorÃ­a
**** 1. Galois es normal y separable
Si $E/K$ es una extensiÃ³n finita, son equivalentes:

  1. $E/K$ extensiÃ³n de Galois.
  2. $E/K$ extensiÃ³n normal y separable.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Como cada $\sigma \in G = Gal(E/K)$ nos da un $\sigma : E/K \longrightarrow \overline{K}/K$, se tiene, por 
Teorema de Artin:

\[
[E:K]_S \geq |G| = [E:K] \geq [E:K]_S
\]

Por tanto, es separable. AdemÃ¡s, cada homomorfismo de ese tipo estÃ¡
en $G$ luego es un automorfismo y el cuerpo queda fijo, siendo una
extensiÃ³n normal.

****** Segunda implicaciÃ³n
Sea $n = [E:K]_S = [E:K]$ por separabilidad. Cada uno de esos 
homomorfismos nos da $\sigma : E/K \longrightarrow E/K$, luego $Gal(E/K) = G$ tiene
orden $n$. Por Teorema de Artin:

\[ [E : E^G] = n = [E:K]\]

luego $[E^G : K] = 1$.

**** 2. Teorema del elemento primitivo de Steinitz
En $F/K$ extensiÃ³n finita equivalen:

  1. $F/K$ tiene elemento primitivo.
  2. Existe un nÃºmero finito de cuerpos intermedios.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Sea $F = K(\alpha)$. $Irr(\alpha,K)$, tiene un nÃºmero finito de factores. Para 
cada uno de ellos, $p$, definimos el subcuerpo generado por sus 
coeficientes $K[|p|]$. Cualquier cuerpo intermedio $E$ en el que
$Irr(\alpha,E)=p$, contendrÃ¡ a los coeficientes $E \supset K[|p|]$, pero ademÃ¡s:

\[ [K(\alpha) : K[|p|] = [K(\alpha) : E]\]

Luego $E = K[|p|]$ y sÃ³lo hay finitos cuerpos intermedios.

****** Segunda implicaciÃ³n
******* Caso de cuerpo base finito
Como la extensiÃ³n es finita, sÃ³lo habrÃ¡ finitos elementos en ella. El
grupo multiplicativo de cualquier cuerpo es simple, asÃ­ que tendrÃ¡ un
elemento primitivo.

******* Caso de cuerpo base infinito
Nos limitamos a probar $K(a,b)$ simple. Si consideramos los subcuerpos
de la forma $K(a+bx)$ para cada $x\in F$, como sÃ³lo habrÃ¡ finitos,
deberÃ¡n coincidir algunos dos $K(a+bx) = K(a+by)$ con $x\neq y$. 

Entonces:

\[
b = \frac{(a+bx) - (a+by)}{x-y} \in K(a+bx) = K(a,b)
\]

**** 3. Caracterizaciones de extensiones normales
Para $E/K$ extensiÃ³n finita equivalen:

  1. $E/K$ es una extensiÃ³n normal. Es cuerpo de descomposiciÃ³n de un
     polinomio.
  2. Para cada $\sigma : E/K \longrightarrow \overline{K}/K$, donde $\overline{K}$ es una clausura algebraica,
     se tiene $\sigma(E) = E$.
  3. Todo polinomio irreducible $f \in K[X]$ con una raÃ­z en $E$ descompone
     en $E$.

***** DemostraciÃ³n
****** ImplicaciÃ³n 1 a 2
Como es una extensiÃ³n finita serÃ¡ de la forma $E = K(\alpha_1,\dots,\alpha_n)$.
La imagen de la raÃ­z de un polinomio deberÃ¡ ser una conjugada suya,
luego $\sigma(E) \subseteq E$. Como ademÃ¡s un endomorfismo entre extensiones
algebraicas es automorfismo, $\sigma(E) = E$.

****** ImplicaciÃ³n 2 a 3
Si una raÃ­z del polinomio estÃ¡, existen homomorfismos que la llevan
en cada una de las conjugadas. Como cumplen $\sigma(E) = E$, se tiene que
todas las conjugadas estÃ¡n en $E$ y el polinomio descompone.

****** ImplicaciÃ³n 3 a 1
La extensiÃ³n finita es de la forma $E = K(\alpha_1,\dots,\alpha_n)$. Podemos tomar
los irreducibles de cada uno de los $\alpha_i$ y multiplicarlos. Como si una
raÃ­z estÃ¡ en el cuerpo todas lo estÃ¡n, el cuerpo de descomposiciÃ³n
de ese producto estÃ¡ contenido en $E$. Como tiene a todos sus $\alpha_i$, es $E$.

**** 4. CaracterizaciÃ³n de la separabilidad
Sea $F/K$ una extensiÃ³n finita y $\overline{K}$ una clausura algebraica de $K$ conteniendo
a $F$, entonces son equivalentes:

  1. $F/K$ es separable.
  2. $|Hom(F/K,\overline{K}/K)| = [F:K]$.

***** DemostraciÃ³n
Llamamos $[F:K]_S = |Hom(F/K,\overline{K}/K)|$.

****** Caso simple
En un caso simple $K(\alpha) / K$ cada homomorfismo a la clausura queda
determinado por la imagen de $\alpha$, que debe ser a un elemento conjugado.
Tenemos que todas las raÃ­ces de $Irr(\alpha,K)$ tienen la misma multiplicidad
$m$, luego si es de grado $n$ habrÃ¡ $n/m$ raÃ­ces distintas.

Se da la igualdad sÃ³lo si cada uno de los elementos es distinto,
esto es, si la extensiÃ³n es separable.

****** Caso compuesto
Procedemos por inducciÃ³n sobre el grado. Si tomamos un elemento en la
extesiÃ³n $F \supseteq K(\alpha) \supseteq K$, sabemos:

\[
[F : K]_S = [F : K(\alpha)]_S [K(\alpha) : K]_S
\]

Sabemos que una extensiÃ³n es separable si lo son sus subextensiones. 
Por hipÃ³tesis de inducciÃ³n $[F : K(\alpha)]_S = [F : K(\alpha)]$ y por el caso base
$[K(\alpha) : K]_S = [K(\alpha) : K]$, por ser ambas separables.

**** 5. Teorema de Artin
Sea $E$ cuerpo y $G \subseteq Aut(E)$ un subgrupo finito, prueba que
$E^G = \{e \in E \mid \forall \sigma \in G: \sigma(e) = e\}$ es un subcuerpo y $[E : E^G] = |G|$.

***** DemostraciÃ³n
****** Es un subcuerpo
Por definiciÃ³n de morfismo de cuerpos.

****** Igualdad
Sabemos por Lema de Dedekind que $n = |G| \leq [E:E^G]$. Supongamos la
desigualdad estricta con $u_1,\dots,u_{n+1}$ independientes sobre $E^G$ y
creamos el siguiente sistema de $n+1$ incÃ³gnitas y $n$ ecuaciones:

\[
X_1\sigma_j(u_1) + \dots + X_{n+1}\sigma_j(u_{n+1}) = 0
\qquad
j = 1,\dots,n
\]

Sea $a_1,\dots,a_{n+1}$ soluciÃ³n no trivial con nÃºmero mÃ­nimo de elementos
no nulos. Suponemos s.p.g. que $a_1 \neq 0$, despejamos:

\[\sigma_j(u_1) = b_2\sigma_j(u_2) + \dots + b_{n+1}\sigma_j(u_{n+1})\]

En particular, cuando $\sigma_j = id$:

\[
u_1 = b_2u_2 + \dots + b_{n+1}u_{n+1}
\]

Por lo que alguno de los coeficientes no estÃ¡ en $E^G$ si queremos
mantener la independencia lineal. Sea $b_2 \notin E^G$ y $\tau(b_2) \neq b_2$. Si aplicamos
$\tau$ a cada una de las ecuaciones y restamos nos queda, sabiendo
que $\tau G = G$:

\[
0 = (b_2-\tau(b_2))\sigma_j(u_2) + \dots + (b_n-\tau(b_n))\sigma_j(u_{n+1})
\qquad
j = 1,\dots,n
\]

Esta soluciÃ³n es no trivial pero tiene mÃ¡s elementos nulos que la
anterior, llevando a contradicciÃ³n.

**** 6. Lema de independencia de Dedekind
Sea $F/K$ y $E/K$ extensiones de cuerpos, para cada familia no vacÃ­a ${\cal F}$ de
elementos de $Hom(F/K,E/K)$ prueba que son equivalentes:

  1. ${\cal F}$ es linealmente independiente en $Hom_K(F,E)$ sobre $E$.
  2. Todos los elementos de ${\cal F}$ son distintos.

***** DemostraciÃ³n
Demostraremos que, en general $\sigma_1,\dots,\sigma_m : G \longrightarrow F^\times$ homomorfismos desde un
grupo $G$ son distintos ssi son linealmente independientes sobre $F$. El caso
$m = 1$ es trivial, y, en otro caso, podemos tomar un subconjunto mÃ­nimo
de linealmente dependientes:

\[
\sigma_s = b_1\sigma_1 + b_2\sigma_2 + \dots + b_{s-1}\sigma_{s-1}
\]

Sea $y$ tal que $\sigma_1(y) \neq \sigma_s(y)$. Evaluamos en $xy$, multiplicamos por $\sigma_s(y)$
por otro lado y restamos para tener:

\[
0 = b_1(\sigma_1(y)-\sigma_s(y))\sigma_1 + 
\dots + 
b_{s-1}(\sigma_{s-1}(y)-\sigma_{s-1}(y))\sigma_{s-1}
\]

Contraviniendo minimalidad de los linealmente independientes. NÃ³tese
que el aÃ±adirles $0$ para tener $\sigma_1,\dots,\sigma_m : F_1 \longrightarrow F_2$ no los vuelve
linealmente dependientes o iguales.

**** 7. CaracterizaciÃ³n de cuerpo algebraicamente cerrado
Para $K$ cuerpo, equivalen:

  1. Todo polinomio no constante $f \in K[X]$ tiene al menos una raÃ­z en $K$.
  2. Todo polinomio no constante $f \in K[X]$ descompone en $K$.
  3. Los polinomios no constantes irreducibles en $K[X]$ son de grado 1.
  4. $K$ no tiene extensiones algebraicas propias.

Los cuerpos que lo cumplen son *algebraicamente cerrados*.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Si el polinomio tiene una raÃ­z, puede descomponerse como $f = (X-\alpha)f'$.
Si $f'$ es constante, tenemos una descomposiciÃ³n de $f$, si no lo es, podemos
descomponerlo a su vez.

****** Segunda implicaciÃ³n
Un polinomio de grado distinto de $1$ descompone linealmente y por tanto
no puede ser irreducible.

****** Tercera implicaciÃ³n
Sea $\alpha \in F/K$ algebraica. Como $Irr(\alpha,K)$ es de grado $1$, $\alpha \in K$. No puede
ser una extensiÃ³n propia.

****** Cuarta implicaciÃ³n
Si algÃºn polinomio no tuviera ninguna raÃ­z en $K$, entonces Ã©l o un
divisor suyo serÃ­an irreducibles sin raÃ­z en $K$. Con ellos se genera
una extensiÃ³n algebraica propia.

**** 8. Teorema de Kronecker
Sea $f \in K[X]$ un polinomio no constante, entonces existe una extensiÃ³n
$F/K$ en la que $f(X)$ tiene al menos una raÃ­z.

***** DemostraciÃ³n
Podemos descomponer en polinomios irreducibles $f = f_1 f_2\dots f_{n}$ y crear
la extensiÃ³n siguiente con la inclusiÃ³n trivial de $K$, que es cuerpo
por ser $(f_1)$ un ideal maximal:

\[
F = \frac{K[X]}{(f_1)} \supset K
\]

Y donde $X + (f_1)$ es raÃ­z del polinomio original por ser raÃ­z de la
inclusiÃ³n de $f_1$:

\[
f_1(X+(f_1)) = f_1 + (f_1) = 0
\]

**** 9. Extensiones ciclotÃ³micas de Galois
Sea $n$ entero positivo, $K$ un cuerpo y $F/K$ una extensiÃ³n ciclotÃ³mica, cuerpo
de descomposiciÃ³n del polinomio $X^n-1$, prueba que se verifica:

  1. $F/K$ es una extensiÃ³n de Galois.
  2. $Gal(F/K)$ es isomorfo a un subgrupo del grupo multiplicativo de las
     unidades de $\mathbb{Z}_n$, por lo tanto su orden es un divisor de $\varphi(n)$.

***** DemostraciÃ³n
****** Primer punto
Es el cuerpo de descomposiciÃ³n de un polinomio, por lo tanto, es
normal. AdemÃ¡s, trabajamos con la hipÃ³tesis de que $car(K) \nmid n$, por lo
que el polinomio debe tener todas sus raÃ­ces distintas y ser separable.

****** Segundo punto
NÃ³tese que si tenemos una raÃ­z primitiva, todas las raÃ­ces de la unidad
quedan generadas por ella, $F = K(\zeta)$, y cada elemento del grupo de Galois
queda determinado por a quÃ© otra raÃ­z primitiva envÃ­a la $\zeta$.

Sea $\sigma(\zeta) = \zeta^a$. Tenemos que $\sigma^{-1}(\zeta) = \zeta^b$ y que $\zeta^{ab} = \zeta$, luego debe ser $a \in \mathbb{Z}^\times_n$.
Creamos entonces la funciÃ³n inyectiva $\Lambda(\sigma) = a$, que trivialmente es
un homomorfismo inyectivo:

  - $\Lambda(\sigma) = a \wedge \Lambda(\tau) = b \implies \Lambda(\sigma\tau) = ab$.
  - $\Lambda(\sigma) = 1 \implies \sigma = id$.

Y por Primer Teorema de isomorfÃ­a tenemos lo pedido. Comprobar que
ademÃ¡s divide a $\varphi(n)$ es trivial por ser el orden de $\mathbb{Z}_n^\times$.

**** 10. Teorema de Moore
Para cada $p^n$ potencia de primo, existe un cuerpo con $p^n$ elementos;
que es el cuerpo de descomposiciÃ³n de $x^{p^n}-x$ sobre $\mathbb{F}_p$. AdemÃ¡s,
todo cuerpo finito de $p^n$ elementos es isomorfo a Ã©l.

***** DemostraciÃ³n
Dos cuerpos finitos con el mismo cardinal son isomorfos y que ademÃ¡s
existe siempre un cuerpo de $p^n$ elementos.

****** Dos cuerpos finitos del mismo cardinal son isomorfos
Todo subgrupo finito del grupo multiplicativo de un cuerpo es cÃ­clico,
luego $F^\times$ serÃ¡ cÃ­clico de orden $p^n-1$, y sus elementos cumplen el 
polinomio:

\[x^{p^n-1} - 1 = 0\]

AsÃ­, $x^{p^n} - x$ tiene exactamente como raÃ­ces los $p^n$ elementos de $F$.

****** Existe un cuerpo con ese nÃºmero de elementos
El polinomio $x^{p^n}-x$ tiene derivada $-1$, y por tanto, sÃ³lo raÃ­ces
simples. AÃ±adiÃ©ndolas a $\mathbb{F}_p$, se tiene ya un cuerpo de descomposiciÃ³n.
Sean $u = u^{p^n}$ y $v = v^{p^n}$:

  - $(u+v)^{p^n} - (u+v) = 0$
  - $(uv)^{p^n} - uv = 0$
  - $(-u)^{p^n} - (-u) = 0$
  - $(u^{-1})^{p^n} - u^{-1} = 0$

**** 11. Teorema 90 de Hilbert
Sea $E/K$ extensiÃ³n cÃ­clica de grado $n$ con grupo $G = Gal(E/K) = \langle \sigma\rangle$, y
sea $\beta \in E$, prueba que son equivalentes:

  1. $N_{E/K}(\beta) = 1$
  2. Existe $0 \neq \alpha \in E$ tal que $\beta = \frac{\alpha}{\sigma(\alpha)}$.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Los automorfismos $1,\sigma,\dots,\sigma^n$ son distintos y por tanto linealmente
independientes, luego no es la aplicaciÃ³n cero:

\[
\tau 
= 
1 + \beta\sigma + (\beta \sigma(\beta))\sigma^2 + 
\dots +
(\beta \sigma(\beta)\dots \sigma^{n-2}\beta) \sigma^{n-1}
\]

Y existe $\tau(\theta) \neq 0$. Evaluando $\sigma(\tau(\theta))$ y multiplicando por $\beta$ nos queda
que $\beta \sigma(\tau(\theta)) = \tau(\theta)$.

****** Segunda implicaciÃ³n
Usando que la norma es homomorfismo:

\[
N_{E/K}(\beta) = \frac{N_{E/K}(\alpha)}{N_{E/K}(\sigma(\alpha))} = 1
\]

**** 12. RaÃ­ces simples y derivada
Sea $f \in K[X]$ no constante. Equivalen:

  1. Todas las raÃ­ces de $f$ son simples.
  2. $f$ y $Df$ son primos relativos.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
En el cuerpo de descomposiciÃ³n de $f$ podemos escribir:

\[Df(X) = a \sum 
(X-\alpha_1)\dots(X-\alpha_{i-1})(X-\alpha_{i+1})\dots(X-\alpha_n)\]

Que claramente no comparte ningÃºn factor primo $(X-\alpha_i)$ de $f$.

****** Segunda implicaciÃ³n
En el cuerpo de descomposiciÃ³n de $f$ podemos tomar una raÃ­z de 
multiplicidad $m > 1$. Y entonces $f = (X-\alpha)^m g$ mientras:

\[ Df = (X-\alpha)^m Dg+ m(X-\alpha)^{m-1}g\]

Por lo que no serÃ­an primos relativos en el cuerpo de descomposiciÃ³n,
y por tanto, tampoco en $K$; ya que tenemos:

\[ Irr(\alpha,K) \mid f, Df\]

**** 13. Teorema de Lagrange
Sea $K$ un cuerpo, $n$ un entero positivo que es primo con la caracterÃ­stica
de $K$ (o es nula) y existe una raÃ­z n-Ã©sima primitiva de la unidad
$\xi$ en $K$, prueba que se verifica:

  1. Si $E/K$ es una extensiÃ³n cÃ­clica de grado $n$, existe $\alpha \in E$ tal que
     $E = K(\alpha)$ e $Irr(\alpha,K) = X^n-a$ para algÃºn $a \in K$.
  2. A la inversa, sea $a \in K$. Si $\alpha$ es una raÃ­z de $X^n-a$, entonces $K(\alpha)/K$
     es una extensiÃ³n cÃ­clica de grado $d$, con $d\mid n$ y $a^d \in K$.

***** DemostraciÃ³n
****** Primer punto
Sea $E = K(u)$ con $Gal(E/K) = \langle \sigma \rangle$ y $\zeta$ la raÃ­z n-Ã©sima primitiva de la
unidad. Creamos el *resolvente de Lagrange*:

\[
\alpha 
= 
u + \zeta\sigma(u) + \zeta^2\sigma^2(u) + \dots + \zeta^{n-1}\sigma^{n-1}(u)
\]

Que sabemos que no es nula por independencia lineal de homomorfismos
distintos, y que cumple que: $\sigma(\alpha) = \zeta^{-1}\alpha$. Luego $\sigma^i(\alpha) = \zeta^{-i}\alpha$, que son
las $n$ raÃ­ces conjugadas de $\alpha$. AdemÃ¡s $\sigma(\alpha^n) = \alpha^n$, por lo que $\alpha^n$ es un
punto fijo bajo $\sigma$ y debe ser $\alpha^n \in K$.

****** Segundo punto
******* Es extensiÃ³n de Galois
Todas las $\zeta^i \alpha$ son raÃ­ces distintas, luego $K(\alpha)$ es el cuerpo de 
descomposiciÃ³n del polinomio $X^n-\alpha$ y es normal y separable por ser
$n$ un primo relativo de $car(K)$.

******* Es cÃ­clica
Cualquier automorfismo debe llevar una raÃ­z del polinomio en otra y
queda determinado por ella. AsÃ­, es de la forma $\sigma(\alpha) = \omega_\sigma \alpha$ para alguna 
raÃ­z de la unidad.

**** 14. Automorfismo de Frobenius
Sea $K$ cuerpo de caracterÃ­stica $p \neq 0$, prueba que son equivalentes:

  1. Todo polinomio irreducible no constante $f \in K[X]$ tiene todas sus 
     raÃ­ces simples.
  2. El endomorfismo de Frobenius es automorfismo.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Dado $a \in K$. Sea $X^p-a$, con una raÃ­z $\alpha$ en un cuerpo de descomposiciÃ³n.
Tenemos $X^p-a = (X-\alpha)^p$. Como todos los irreducibles tienen raÃ­ces
simples, el Ãºnico irreducible factor del polinomio puede ser $X-\alpha$.

AsÃ­ $\alpha \in K$ y $a = \alpha^p$, siendo Frobenius sobreyectivo.

****** Segunda implicaciÃ³n
Para que un irreducible tenga raÃ­z mÃºltiple, debe dividir a su derivada,
que debe ser de grado menor, luego debe ser $0$. El polinomio debe ser
entonces de la forma $f(x^p)$ para que al derivarlo se anule.

Cuando Frobenius es automorfismo, podemos escribir:

\[f(x^p) = \sum a_i x^{ip} = \left( \sum \sqrt[p]{a_i} x^i \right)^{p}\]

Lo que contraviene irreducibilidad. Todos los polinomios deben tener
raÃ­ces simples.

**** 15. Galois es descomposiciÃ³n de separable
Sea $E/K$ una extensiÃ³n finita, prueba que son equivalentes:

  1. $E/K$ extensiÃ³n de Galois.
  2. $E$ cuerpo de descomposiciÃ³n de un polinomio separable sobre $K$.

***** DemostraciÃ³n
****** Primera implicaciÃ³n
Galois es normal y separable. Luego es cuerpo de descomposiciÃ³n de
algÃºn polinomio y este debe ser separable. Si no fuera separable,
tendrÃ­a algÃºn factor con alguna raÃ­z que no serÃ­a separable.

****** Segunda implicaciÃ³n
El cuerpo de descomposiciÃ³n de un polinomio separable debe ser normal
por cuerpo de descomposiciÃ³n y separable porque todos los elementos
que lo generan lo son y la suma y producto de separables es separable.

*** Ejercicios
**** RelaciÃ³n 1
***** Ejercicio 1.1
 Los polinomios simÃ©tricos en cuatro variables serÃ¡n de la forma:

 \[\sum X_a = X_1+X_2+X_3+X_4\]
 \[\sum X_aX_b = X_1X_2+X_1X_3+X_1X_4+X_2X_3+X_2X_4+X_3X_4\]
 \[\sum X_aX_bX_c = X_1X_2X_3 + X_1X_2X_4 + X_1X_3X_4 + X_2X_3X_4\]
 \[\sum X_aX_bX_cX_d = X_1X_2X_3X_4\]

***** Ejercicio 1.2
 Usamos el polinomio recÃ­proco relacionando $p(x)$ con $p(\frac{1}{x})$ y usamos
 la derivada $p(x)$ para raÃ­ces dobles. Sea:

 \[\hat p(x) = \frac{1}{x^n}(1+a_nx+\dots+a_0x^n)\]

 Usamos entonces $\hat{\hat{p}'}$ para relacionar las raÃ­ces.
   
***** Ejercicio 1.3
***** Ejercicio 1.5
 Vamos a usar una ecuaciÃ³n general que escribe una suma de potencias con
 otras variables en funciÃ³n de sumas de potencias de grado menor.

 \[\sum X_1^nX_2 \dots X_k = e_k\sum X_1^{n-1} - \sum X_1^{n-1}X_2 \dots X_kX_{k+1}\]

 para concluir que si la aplicamos repetidamente y llamamos 
 $s_n = \sum X^n$ obtendremos:

 \[ s_n = e_1s_{n-1} - e_2s_{n-2} + \dots + e_{n-1}s_1 - ne_n \]

 Por tanto, en los casos hasta $5$, tenemos:

 \[s_1 = e_1\]

 \[\begin{align*}
 s_2 &= e_1s_1 - 2e_2 \\
 &= e_1^2 - 2e_2
 \end{align*}
 \]

 \[\begin{align*}
 s_3 &= e_1s_2 - e_2s_1 + 3e_3 \\
 &= e_1^3 - 3e_1e_2 + 3e_3
 \end{align*}
 \]

 \[\begin{align*}
 s_4 &= e_1s_3 - e_2s_2 + e_3s_1 - 4e_4 \\
 &= e_1^4 - 3e_1^2e_2 + 3e_1e_3 - e_1^2e_2 + 2e_2^2 + e_1e_3 - 4e_4 \\
 &= e_1^4 - 4e_1^2e_2 + 4e_1e_3 - 4e_4 + 2e_2^2 
 \end{align*}
 \]

 \[\begin{align*}
 s_5 &= e_1s_4 - e_2s_3 + e_3s_2 - e_4s_1 + 5e_5 \\
 &= e_1^5 - 5e_1^3e_2 + 5e_1^2e_3 - 5e_1e_4 + 5e_1e^2_2 - 5e_3e_2 + 5e_5
 \end{align*}
 \]

 NÃ³tese que aquÃ­ tomamos $e_m = 0$ en el caso de que haya menos de $m$ variables.

****** SoluciÃ³n por grados
 Cada polinomio simÃ©trico de grado $n$ es combinaciÃ³n lineal de los que surgen
 como productos de elementales de grado $n$.

 \[x^3+y^3+z^3 = \alpha e_1^3 + \beta e_1e_2 + \gamma e_3\]

 Calculando se llega a $\alpha = 1$, $\beta = -3$, $\gamma = 3$.

***** Ejercicio 10
 Necesitamos uno que sea raÃ­z de $p'$ y de $p$.
 Sacamos las raÃ­ces de $p'$, que son $1$ y $\frac{5}{3}$.

 Tenemos que $1$ es raÃ­z doble cuando $k= -2$. Dividiendo por $(x-1)^2$ tenemos

 \[p(x) = (x-1)^2(x-2)\]

 Lo mismo se puede hacer con $\frac{5}{3}$, pero saldrÃ­a $k \notin \mathbb{Z}$.

***** Ejercicio 11

**** Semana 1
***** Ejercicio 1.24
 Sea $M$ el conjunto de posibles /monomios/ en las variables y exponentes dados. 
 Podemos definir una funciÃ³n $S_n \longrightarrow M$ como sigue:

 \[\sigma(X_1^{e_1} X_2^{e_2} \dots X_n^{e_n}) = 
 X_{\sigma(1)}^{e_1} X_{\sigma(2)}^{e_2} \dots X_{\sigma(n)}^{e_n}
 \]

 Esto es una acciÃ³n transitiva pero no fiel. El estabilizador para
 el monomio inicial, por ejemplo, son las permutaciones que mueven variables a
 variables del mismo exponente. Si hay $m_i$ variables de exponente $i$, podemos
 intercambiarlas de $m_i!$ formas distintas quedando un monomio igual. El estabilizador
 de ese monomio tiene por tanto orden $k = |Stab(x)| = m_1!m_2!\dots m_n!$. Aplicando ahora el
 Teorema de Lagrange para Ã³rbitas y estabilizadores obtenemos el nÃºmero de
 monomios distintos:

 \[|M| = |S_n(M)| = \frac{|S_n|}{|Stab(x)|} = \frac{n!}{m_1!m_2!\dots m_n!}\]

***** Ejercicio 1.25
 Sea el polinomio $p(x) = x^3-5x-5$ con raÃ­ces $\alpha, \beta, \gamma$. Tenemos que el polinomio $p(x-1)$
 tendrÃ¡ raÃ­ces $\alpha+1,\beta+1,\gamma+1$:

 \[p(x-1) = (x+1)^3-5(x+1)-5 = x^3 - 3x^2 - 2x - 1\]

 Y que el polinomio recÃ­proco a Ã©l tendrÃ¡ raÃ­ces  $\frac{1}{\alpha+1}, \frac{1}{\beta+1}, \frac{1}{\gamma+1}$:

 \[q(x) = 1 - 3x - 2x^2 - x^3\]

 Trabajando con $-q(x)$, que es mÃ³nico, y con los polinomios de Cardano-Vieta sobre
 sus raÃ­ces tenemos que si estas fueran $u,v,w$, tendrÃ­amos:

 \[(x-u)(x-v)(x-w) = x^3 - (u+v+w)x^2 +(uv+vw+wu)x - uvw\]

 Y desde aquÃ­ obtenemos el valor de los polinomios simÃ©tricos elementales sobre
 las raÃ­ces

 \[\begin{align*}
 e_1 &= u+v+w = -2 \\
 e_2 &= uv+vw+wu = 3 \\
 e_3 &= uvw = 1
 \end{align*}\]

 Ahora, expresamos el valor de $u^3+v^3+w^3$ como suma de polinomios elementales
 mediante el algoritmo de orden lexicogrÃ¡fico de la demostraciÃ³n:

 \[\begin{align*}
 \sum X_1^3 &= e_1^3 - (3\sum X_1^2X_2 + 6\sum X_1X_2X_3) \\
            &= e_1^3 - 3(e_1e_2 - 3\sum X_1X_2X_3 + 6\sum X_1X_2X_3) \\
            &= e_1^3 - 3e_1e_2 + 3e_1
 \end{align*}\]

 Y asÃ­, finalmente tenemos:

 \[u^3+v^3+w^3 = (-2)^3 - 3(-2)3 + 3 = 13\]

 # Â¿Existen cuerpos infinitos de caracterÃ­stica no nula?

**** Semana 2
***** Ejercicio 2.14
****** Punto 1
 Supongamos que se tiene $f(x) = ax+b$ con $a$ una unidad del anillo. Entonces podrÃ­amos
 tomar como inversa de $\phi$ el homomorfismo de anillos que cumple $g(x) = a^{-1}(x - b)$ y que
 sobre los elementos del anillo es la identidad. SerÃ­a un isomorfismo.

 Estudiamos el caso de que $f(x)$ fuera de otra forma pero fuera isomorfismo. 
 Trivialmente su grado no podrÃ­a ser $0$ para ser inyectivo sobre los elementos del
 anillo. Si $f(x)$ tuviera monomio lÃ­der $b_kx^k$
 y el monomio lÃ­der de $p$ fuera $a_mx^m$. Su imagen serÃ­a:

 \[\phi(p(x)) = a_0 + a_1f(x) + a_2f(x)^2 + \dots + a_mf(x)^m\]

 tendrÃ­a un Ãºnico coeficiente lÃ­der de grado $km$ que serÃ­a 
 $a_mb_kx^{m+k} \neq 0$ por ser dominio de integridad.

 AsÃ­,tenemos que $f$ no puede tener grado mayor que $1$ y debe tener un coeficiente lÃ­der unidad
 si queremos que $a_mb_kx^{m+k} = x \in img(\phi)$.

****** Punto 2
 El coeficiente lÃ­der puede anularse y la condiciÃ³n ya no es suficiente.

 Sea $e$ en el nilradical de un anillo, con $e^n = 0$. Entonces se pueden tomar
 los dos homomorfismos cumpliendo:

 \[\phi(x) = x - e^{n-1}x^n\]
 \[\phi'(x) = x + e^{n-1}x^n\]

 NÃ³tese ahora que $\phi\phi'(x) = \phi'\phi(x) = x$ y que para cualquier polinomio se comprobarÃ¡
 que son dos automorfismos inversos entre sÃ­. De hecho, usando que son
 homomorfismos:

 \[\phi\phi'(p(x)) = p(\phi\phi'(x)) = p(x)\]

***** Ejercicio 2.15
****** Punto 1
 Si $f$ es irreducible en $\mathbb{Z}$, es en particular primitivo.
 Supongamos que $f = gh$ en $\mathbb{Q}$, factorizaciÃ³n no trivial. Puedo escribir $g$ y $h$ como
 polinomios primitivos por una unidad de $\mathbb{Q}$: $f = ug_0h_0$. Como el producto de primitivos
 es primitivo, $g_0h_0$ lo es. Supongamos que tuviÃ©ramos $u = \frac{a}{b}$, con:

 \[bf = ag_0h_0\]

 Llegamos a que $a|b$, $b|a$, ya que ninguno de los dos puede dividir a un 
 polinomio primitivo; y obtenemos $u$ unidad de $\mathbb{Z}$. Con lo cual, $f$ no serÃ­a
 irreducible en $\mathbb{Z}$.

****** Punto 2
 Como es irreducible sobre $\mathbb{Q}$, el ideal que genera es maximal y $F$ es
 por tanto un cuerpo. Las inclusiones son las triviales.

****** Punto 3
 Sea el polinomio $f(y) \in F[y]$. Tenemos que:

 \[f(X + (f(X))) = f(X) + (f(X)) = 0 + (f(X)) \]

 Por lo que es raÃ­z.

 La primera igualdad se obtiene del hecho de que las potencias y el producto
 por elementos del cuerpo respetan las clases de equivalencia; y por tanto,
 la evaluaciÃ³n de un polinomio lo hace:

 \[(X + (f(X)))^n = X^n + X^{n-1}(f(X)) + \dots + (f(X)) = X^n + (f(X))\]
 \[a(X+(f(X))) = aX + (f(X))\]

***** Ejercicio 2.16
 Tenemos una extensiÃ³n sobre $\mathbb{F}_2$ generada por un polinomio de grado 3. 
 Sus elementos son clases de equivalencia sobre polinomios de hasta 
 grado dos, habiendo 8 elementos. Abusando de la notaciÃ³n, los escribimos como
 los representantes de su clase de equivalencia:

 \[\{0,1,x,x+1,x^2,x^2+1,x^2+x,x^2+x+1\}\]

 Tenemos las siguientes tablas para la suma y el producto:

 #+BEGIN_SRC sage :exports none
 R.<t> = PolynomialRing(GF(2),'t')
 I = R.ideal(t^3+t+1)
 S.<x> = R.quotient_ring(I)
 #+END_SRC
 #+RESULTS:

 #+BEGIN_SRC sage :exports results
 S.addition_table(
     names=["0","1","x","x+1","x^2","x^2+1","x^2+x","x^2+x+1"],
     elements=[0,1,x,x+1,x^2,x^2+1,x^2+x,x^2+x+1]
 )
 #+END_SRC
 #+RESULTS:
 #+begin_example

       +        0       1       x     x+1     x^2   x^2+1   x^2+x x^2+x+1
	+----------------------------------------------------------------
       0|       0       1       x     x+1     x^2   x^2+1   x^2+x x^2+x+1
       1|       1       0     x+1       x   x^2+1     x^2 x^2+x+1   x^2+x
       x|       x     x+1       0       1   x^2+x x^2+x+1     x^2   x^2+1
     x+1|     x+1       x       1       0 x^2+x+1   x^2+x   x^2+1     x^2
     x^2|     x^2   x^2+1   x^2+x x^2+x+1       0       1       x     x+1
   x^2+1|   x^2+1     x^2 x^2+x+1   x^2+x       1       0     x+1       x
   x^2+x|   x^2+x x^2+x+1     x^2   x^2+1       x     x+1       0       1
 x^2+x+1| x^2+x+1   x^2+x   x^2+1     x^2     x+1       x       1       0
 #+end_example

 #+BEGIN_SRC sage :exports results
 S.multiplication_table(
     names=["0","1","x","x+1","x^2","x^2+1","x^2+x","x^2+x+1"],
     elements=[0,1,x,x+1,x^2,x^2+1,x^2+x,x^2+x+1]
 )
 #+END_SRC
 #+RESULTS:
 #+begin_example

       *        0       1       x     x+1     x^2   x^2+1   x^2+x x^2+x+1
	+----------------------------------------------------------------
       0|       0       0       0       0       0       0       0       0
       1|       0       1       x     x+1     x^2   x^2+1   x^2+x x^2+x+1
       x|       0       x     x^2   x^2+x     x+1       1 x^2+x+1   x^2+1
     x+1|       0     x+1   x^2+x   x^2+1 x^2+x+1     x^2       1       x
     x^2|       0     x^2     x+1 x^2+x+1   x^2+x       x   x^2+1       1
   x^2+1|       0   x^2+1       1     x^2       x x^2+x+1     x+1   x^2+x
   x^2+x|       0   x^2+x x^2+x+1       1   x^2+1     x+1       x     x^2
 x^2+x+1|       0 x^2+x+1   x^2+1       x       1   x^2+x     x^2     x+1
 #+end_example

**** Semana 3
***** Ejercicio 2.17
****** Punto 1
 Tenemos el siguiente diagrama con las extensiones

 \[ \begin{tikzcd}
  & \mathbb{Q}({\sqrt{2}},\sqrt{3},\sqrt{5}) \drar[dash] \dar[dash] \dlar[dash] & \\
 \mathbb{Q}(\sqrt{2}) & \mathbb{Q}(\sqrt{3}) & \mathbb{Q}(\sqrt{5}) \\
 & \mathbb{Q} \urar[dash,swap]{2} \uar[dash]{2} \ular[dash]{2} &
 \end{tikzcd} \]

 Donde $\sqrt{2},\sqrt{3},\sqrt{5}$ son irracionales y sus polinomios irreducibles
 en $\mathbb{Q}$ son $x^2-2 = 0$, $x^2-3=0$ y $x^2-5=0$; por lo que son extensiones
 de grado $2$.

 Ahora mostramos que $\sqrt{3} \notin \mathbb{Q}(\sqrt{2})$, ya que:

 \[\sqrt{3} = a + b \sqrt{2}\]

 Y sabiendo que no puede tenerse $\sqrt{3} = a$ o $\sqrt{3} = b\sqrt{2}$:

 \[\sqrt{2} = \frac{3-a^2-b^2}{2ab}\]

 Y entones $\sqrt{2}$ serÃ­a racional. AsÃ­, $\mathbb{Q}(\sqrt{2},\sqrt{3})$, sabiendo que ademÃ¡s 
 $\sqrt{3}^2 \in \mathbb{Q}$, debe ser una extensiÃ³n de grado $2$ sobre $\mathbb{Q}(\sqrt{2})$.

 Ahora mostramos que $\sqrt{5} \notin \mathbb{Q}(\sqrt{2})(\sqrt{3})$, ya que, siendo $a,b \in \mathbb{Q}(\sqrt{2})$:

 \[\sqrt{5} = a + b\sqrt{3}\]
 \[ 5 = a^2 + 2ab\sqrt{3} + 3b^2\]

 Ahora, $ab = 0$, ya que, si no fuera asÃ­, $\sqrt{3} \in \mathbb{Q}(\sqrt{2})$; ademÃ¡s, $a=0$ o $b=0$, llegando
 a uno de los siguientes casos:

 \[ 5 = a^2 = x^2+y^2+2xy\sqrt{2}\]
 \[ 5 = 3b^2 = 3(x^2+y^2+2xy\sqrt{2})\]

 Donde, anÃ¡logamente, llegarÃ­amos a contradicciÃ³n con $\frac{5}{3} \notin \mathbb{Q}$.
 Como ademÃ¡s $\sqrt{5}^2 \in \mathbb{Q}$, es una extensiÃ³n de grado $2$.

 Resumiendo, tenemos:

 \[ \begin{tikzcd}
  & \mathbb{Q}({\sqrt{2}},\sqrt{3},\sqrt{5}) & \\
  \mathbb{Q}(\sqrt{2},\sqrt{3}) \urar[dash]{2} & & \\
 \mathbb{Q}(\sqrt{2}) \uar[dash]{2} & \mathbb{Q}(\sqrt{3}) \ular[dash]{2} & \mathbb{Q}(\sqrt{5}) \arrow[uul,dash] \\
 & \mathbb{Q} \urar[,dash,swap]{2} \uar[dash]{2} \ular[dash]{2} &
 \end{tikzcd} \]

 Y aplicando la fÃ³rmula de grado de las extensiones llegamos
 a que $[\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}) : \mathbb{Q}] = 8$.

****** Punto 2
 Tenemos a $1,\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{6},\sqrt{10},\sqrt{15},\sqrt{30}$ sistema de generadores del espacio,
 y por ser de dimensiÃ³n $8$, sabemos que forman una base. Veamos que $1,\alpha,\alpha^2,\dots,\alpha^7$ es una base 
 del mismo espacio comprobando independencia lineal sobre la base inicial.

 #+BEGIN_SRC sage :exports none
   A = FreeAlgebra(QQ,3,'i')
   F = A.monoid()
   a,b,c = F.gens()
   mons = [ F(1), a,b,c,a*b,a*c,b*c,a*b*c ]
   M = MatrixSpace(QQ,len(mons))
   mats = [
       M([0,1,0,0,0,0,0,0,
          2,0,0,0,0,0,0,0,
          0,0,0,0,1,0,0,0,
          0,0,0,0,0,1,0,0,
          0,0,2,0,0,0,0,0,
          0,0,0,2,0,0,0,0,
          0,0,0,0,0,0,0,1,
          0,0,0,0,0,0,2,0
       ]),
       M([0,0,1,0,0,0,0,0,
          0,0,0,0,1,0,0,0,
          3,0,0,0,0,0,0,0,
          0,0,0,0,0,0,1,0,
          0,3,0,0,0,0,0,0,
          0,0,0,0,0,0,0,1,
          0,0,0,3,0,0,0,0,
          0,0,0,0,0,3,0,0
       ]),
       M([0,0,0,1,0,0,0,0,
          0,0,0,0,0,1,0,0,
          0,0,0,0,0,0,1,0,
          5,0,0,0,0,0,0,0,
          0,0,0,0,0,0,0,1,
          0,5,0,0,0,0,0,0,
          0,0,5,0,0,0,0,0,
          0,0,0,0,5,0,0,0
       ])
   ]
   P3.<a,b,c> = A.quotient(mons,mats)
 #+END_SRC

 #+RESULTS:

 Para ello escribo los coeficientes de cada $\alpha^n$ en la
 base inicial formando la siguiente matriz y compruebo que son linealmente independientes.

 #+BEGIN_SRC sage :exports results
 M = matrix([ ((a+b+c)^n).vector() for n in range(0,8) ]).transpose()
 M
 #+END_SRC

 #+RESULTS:
 : 
 : [    1     0    10     0   224     0  6160     0]
 : [    0     1     0    26     0   784     0 23024]
 : [    0     1     0    24     0   664     0 18976]
 : [    0     1     0    20     0   520     0 14720]
 : [    0     0     2     0    80     0  2448     0]
 : [    0     0     2     0    64     0  1904     0]
 : [    0     0     2     0    56     0  1584     0]
 : [    0     0     0     6     0   200     0  5936]

 Si calculamos el rango de esta matriz, obtenemos que es invertible.

 #+BEGIN_SRC sage :exports both
 M.rank()
 #+END_SRC

 #+RESULTS:
 : 8

****** Punto 3
 Usando la matriz anterior, obtengo un polinomio que tiene por raÃ­z
 a $\sqrt{2}+\sqrt{3}+\sqrt{5}$. Para ello escribo los coeficientes de $\alpha^8$ en
 funciÃ³n de la base anterior de la matriz.

 #+BEGIN_SRC sage :exports both
 M.solve_right(vector(QQ,((a+b+c)^8).vector()))
 #+END_SRC

 #+RESULTS:
 : (-576, 0, 960, 0, -352, 0, 40, 0)

 AsÃ­ que tengo el polinomio siguiente del que $\alpha$ es raÃ­z:

 \[t^8-40*t^6+352*t^4-960*t^2+576\]

 # Checking the answer
 #+BEGIN_SRC sage :exports none
 t = sqrt(2)+sqrt(3)+sqrt(5)
 (-40*t^6+352*t^4-960*t^2+576+t^8).expand()
 #+END_SRC

 #+RESULTS:
 : 0

****** Punto 4
 Por el diagrama de extensiones del ejercicio $1$ sabemos que $\mathbb{Q}(\sqrt{2},\sqrt{3})$ es una extensiÃ³n
 de grado 4. Como ya sabemos que $\mathbb{Q}(\sqrt{2}+\sqrt{3}) = \mathbb{Q}(\sqrt{2},\sqrt{3})$, tenemos que $\sqrt{2}+\sqrt{3}$ es 
 un elemento de grado 4 sobre $\mathbb{Q}$.
**** Semana 4
***** Ejercicio 3.11
****** Punto 1
 Sea el polinomio $f\in \mathbb{F}_2[X]$ con una raÃ­z $\lambda$; comprobamos que tambiÃ©n
 es raÃ­z $\lambda^2$:

 \[\begin{aligned}
 f(\lambda^2) 
 &= a_n\lambda^{2n} + a_{n-1}\lambda^{2(n-1)} + \dots + a_1\lambda^2 + a_0 \\
 &= (a_n\lambda^n + a_{n-1}\lambda^{n-1} + \dots + a_1\lambda + a_0)^2 \\
 &= f(\lambda)^2
 \end{aligned}
 \]

 Donde usamos que:

 \[a_p\lambda^{2p} + a_q\lambda^{2q} = 
 a^2_p\lambda^{2p} + 2a_pa_q\lambda^p\lambda^q + a^2_q\lambda^{2q} =
 (a_p\lambda^p + a_q\lambda^q)^2\]

 Aplicando esto varias veces llegamos a que $\lambda,\lambda^2,\lambda^4,\dots$ son raÃ­ces.

****** Punto 2
 Sea el polinomio $f(x) = x(x^2+x+1) = x^3+x^2+x$, que tiene como raÃ­z en 
 $K \cong \frac{\mathbb{F}_2[X]}{(x^2+x+1)}$ a $\lambda = x + (x^2+x+1)$; nÃ³tese que tiene como raÃ­z tambiÃ©n a $0$, que no es
 potencia de $\lambda$.

****** Punto 3
 Siendo $\beta$ una raÃ­z primitiva, genera el cuerpo y una base de $K$ sobre su cuerpo
 base debe estar generada por $\beta$ y formada por  
 $\{1,\beta,\beta^2,\dots,\beta^{n-1}\}$; supongamos que el grado de $f$ fuera
 menor que $n$, entonces tendrÃ­a una relaciÃ³n de dependencia lineal entre la base:

 \[ 0 = f(\beta) = a_0 +a_1\beta + \dots + a_{n-1}\beta^{n-1} \]

 Lo que nos darÃ­a una contradicciÃ³n.

**** Semana 5
***** Ejercicio 4.17
#+begin_statement
Sea $K \subseteq E \subseteq F$ una torre de cuerpos y supongamos que $\alpha_1,\dots,\alpha_r$ son algunas de las 
raÃ­ces de $f(X) \in K[X]$ y $E = K(\alpha_1,\dots,\alpha_r)$. Demuestra que $F$ es el cuerpo de 
descomposiciÃ³n de $f(X)$ sobre $K$ si, y sÃ³lo si, $F$ es el cuerpo de descomposiciÃ³n de
$f(X)$ sobre $E$.
#+end_statement

Antes que nada, podemos descomponer $f$ en la clausura algebraica $\overline{K}$ como factores
lineales con raÃ­ces $\alpha_1,\dots,\alpha_n$. Vamos a fijarnos en el hecho de que el cuerpo de 
descomposiciÃ³n de un polinomio sobre un cuerpo es el cuerpo resultante de aÃ±adirle
sus raÃ­ces en la clausura; esto es debido que cumple trivialmente la propiedad del 
cuerpo de descomposiciÃ³n, y ademÃ¡s es minimal porque cualquier otro debe contener a 
sus raÃ­ces y, por tanto, contenerlo Ã©l.

Ahora veamos que ambos cuerpos de descomposiciÃ³n que se plantean en el ejercicio 
son iguales. Por un lado, el cuerpo de descomposciÃ³n de $f$ sobre $K$ debe ser
$K(\alpha_1,\dots,\alpha_n)$; y por otro, el cuerpo de descomposiciÃ³n de $f$ sobre
$E$, que tambiÃ©n tiene como clausura algebraica a $K$, debe ser: 

\[\begin{aligned}
E(\alpha_1,\dots,\alpha_n) &= K(\alpha_1,\dots,\alpha_r)(\alpha_1,\dots,\alpha_n) \\
&= K(\alpha_1,\dots,\alpha_r)(\alpha_{r+1},\dots,\alpha_n) \\
&= K(\alpha_1,\dots,\alpha_n) 
\end{aligned}\]

***** Ejercicio 4.18
#+begin_statement
Sea $a \in \mathbb{Q}$ y $n$ un nÃºmero entero positivo impar tal que
$\sqrt[n]{a} \in \mathbb{R}/\mathbb{Q}$. Demuestra que la extensiÃ³n $\mathbb{Q}(\sqrt[n]{a})/\mathbb{Q}$ no es normal.
#+end_statement

Tengo que $\sqrt[n]{a}$ es raÃ­z de $x^n-a$. Eso quiere decir que serÃ¡ mÃºltiplo de 
$\operatorname{Irr}(\sqrt[n]{a},K)$, y que por tanto, toda raÃ­z de su polinomio irreducible serÃ¡ raÃ­z 
de $x^n-a$. En $\mathbb{C}$, las raÃ­ces de ese polinomio son de la forma $\{\zeta^i_n\sqrt[n]{a}\}$,
con los $\zeta$ raÃ­ces de la unidad; y, siendo $n$ impar, sÃ³lo una serÃ¡ real.

Por otro lado, si queremos que sea una extensiÃ³n normal, el polinomio irreducible
deberÃ­a tener todas sus raÃ­ces en la extensiÃ³n y factorizar linealmente en ellas;
pero como sÃ³lo hay una real y la extensiÃ³n estÃ¡ contenida en los reales, sÃ³lo
podrÃ­a tener una raÃ­z, el polinomio serÃ­a lineal y entonces se tendrÃ­a $\sqrt[n]{a} \in \mathbb{Q}$.

***** Ejercicio 4.19
#+begin_statement
Sea $E/K$ una extensiÃ³n normal y $f(X) \in K[x]$ un polinomio (mÃ³nico) irreducible. Si
$f(X)$ se factoriza en $E$ como producto de dos polinomios (mÃ³nicos) irreducibles 
$f_1(x)$ y $f_2(x)$. Demuestra que existe un homomorfismo $\sigma : E/K \longrightarrow E/K$ tal que
$f^\sigma_1(x) = f_2(x)$.
#+end_statement

Sea $\alpha$ raÃ­z de $f_1$ y $\beta$ raÃ­z de $f_2$. Ambos son los polinomios irreducibles de sus raÃ­ces
en $E$. Como ambas ademÃ¡s son raÃ­ces de $f$, son conjugadas sobre $K$
y existe un automorfismo sobre $K$ que lleva $\sigma(\alpha) = \beta$. Ese isomorfismo cumple 
que $\sigma(E) = E$ por normalidad. Ahora, por irreducibilidad:

\[f_2 | f_1^\sigma\]

Y tenemos que:

\[ f_1f_2 = f = f^\sigma = f_1^\sigma f_2^\sigma\]

Esto quiere decir que $f_1 = f_2^\sigma$ y que $f_2 = f_1^\sigma$; ya que estamos en un dominio de 
factorizaciÃ³n Ãºnica y ambos son mÃ³nicos.
**** Semana 6
***** Ejercicio 5.10
#+begin_statement
Sea $K$ cuerpo de caracterÃ­stica $p \neq 0$ y $t$ una indeterminada sobre $K$. Prueba que el
polinomio $X^p-t^p \in K(t^p)[X]$ es irreducible.
#+end_statement

Por binomio de Newton, en $K(t)[X]$ tenemos $x^p-t^p = (x-t)^p$. Sus divisores son de la
forma $(x-t)^q$ para $q<p$; y ninguno puede estar en $K(t^p)[X]$ porque implicarÃ­a que
estuviera su Ãºltimo coeficiente $(-t)^q \in K(t^p)[X]$, con lo que ya no serÃ­a una 
indeterminada porque se podrÃ­a escribir $t^q$ relacionado con $t^p$.

***** Ejercicio 5.11
#+begin_statement
Estudiar si son o no ciertas las siguientes afirmaciones:

 - $\sqrt[3]{-1}$ es separable sobre $\mathbb{F}_9$.
 - $\sqrt[3]{-1}$ es separable sobre $\mathbb{F}_{49}$.
 - $\sqrt[7]{5}$ es separable sobre $\mathbb{F}_{7^7}$.
 - $t$ es separable sobre $\mathbb{F}_{p^2}(t^p)$, siendo $p$ un nÃºmero entero primo positivo y $t$ una
   indeterminada sobre $\mathbb{F}_{p^2}$.

$\quad$
#+end_statement

Las tres primeras son extensiones finitas sobre cuerpos perfectos (por ser finitos),
luego todas son separables. Para el Ãºltimo caso, $x^p-t^p = 0$ es irreducible y por 
tanto polinomio mÃ­nimo de $t$, pero tiene raÃ­ces mÃºltiples, luego $t$ es un elemento 
no separable.

***** Ejercicio 5.12
#+begin_statement
Sea $E$ un cuerpo y $\{\varphi_1,\dots,\varphi_n\}$ un conjunto de $n$ automorfismos distintos de $E$.
Llamamos $K = \{e \in E \mid \varphi_i(e) = e, 1 \leq i \leq n\}$. Demuestra que $[E:K]\geq n$.
#+end_statement

Dada la extensiÃ³n $E/K$. Tenemos dos casos:

 - $E$ es infinita sobre $K$, luego $[E:K] \geq n$.
 - $E$ es finita sobre $K$, por el corolario al lema de Dedekind, tenemos:
   \[ [E:K] \geq |Hom(E/K,E/K)| \geq n\]

**** Semana 7
***** Ejercicio 7.25
#+begin_statement
Sea $f \in K[X]$ un polinomio sin raÃ­ces mÃºltiples; y
$G = \operatorname{Gal}(f/K)$. Prueba que son equivalentes:

 1. $f(X)$ es irreducible.
 2. $G$ actÃºa transitivamente sobre las raÃ­ces de $f$.

$\quad$
#+end_statement

Sea $f$ irreducible. Cualesquiera dos de sus raÃ­ces tienen a $f$ como polinomio 
irreducible; luego son conjugadas y existe un automorfismo de la clausura que
lleva una en otra, $\sigma : \overline{K} \longrightarrow \overline{K}$. Como la extensiÃ³n $f/K$ es normal ser la extensiÃ³n
de descomposiciÃ³n de un polinomio irreducible $f$, tenemos que $\sigma|_{f/K} \in G$.

Sea $G$ actuando transitivamente sobre las raÃ­ces de $f = gh$, descomposiciÃ³n
en $K[X]$. Una raÃ­z no puede estar
repetida en $g$ y en $h$, porque conllevarÃ­a raÃ­ces mÃºltiples. Sea $a$ raÃ­z de $g$, y
sea un $\sigma : f/K \longrightarrow f/K$ que la lleve en $b$ raÃ­z de $h$. Entonces

\[ h(a) = h(\sigma(b)) = \sigma(h(b)) = 0\]

contraviniendo que no existan raÃ­ces mÃºltiples.
**** Semana 8
***** Ejercicio 7.26
    #+begin_statement
    Se considera el producto semidirecto $G = \mathbb{Z}_8\rtimes_\theta\mathbb{Z}_2$, siendo $\theta(1)(1)=3$.
    Observa que $G$ es un grupo de orden 16. Supongamos que $G$ es el grupo de Galois
    de una extensiÃ³n $E/K$.

    1. Â¿CuÃ¡ntos cuerpos intermedios $F/K$, con $K\subset F\subset E$, existen de grado $8$?
       Â¿CuÃ¡ntos con un grupo de Galois $Gal(F/K)$ isomorfo a $\mathbb{Z}_8$?
    2. Â¿CuÃ¡ntos cuerpos intermedios $F/K$, con $K\subset F\subset E$, existen de grado $4$?
       Â¿CuÃ¡ntos con un grupo de Galois $Gal(F/K)$ isomorfo a $\mathbb{Z}_4$? y
       Â¿cuÃ¡ntos con grupo de Galois isomorfo al grupo de Klein?
    3. Â¿CuÃ¡ntos cuerpos intermedios $F/K$, con $K\subset F\subset E$, existen de grado $2$?
       Â¿CuÃ¡ntos con grupo de Galois $Gal(E/F)$ isomorfo a $\mathbb{Z}_8$?
    4. Determina el retÃ­culo de subgrupos de $\mathbb{Z}_8 \rtimes_\theta\mathbb{Z}_2$.
    #+end_statement
    
    Lo que estamos buscando en cada uno de esos casos, gracias a la correspondencia
    de Galois, son subgrupos. Si calculamos el orden de los elementos en este
    producto semidirecto, tenemos:

    | Elemento | Orden |
    |----------+-------|
    | (0,0)    |     0 |
    | (1,0)    |     8 |
    | (2,0)    |     4 |
    | (3,0)    |     8 |
    | (4,0)    |     2 |
    | (5,0)    |     8 |
    | (6,0)    |     4 |
    | (7,0)    |     8 |
    | (0,1)    |     2 |
    | (1,1)    |     4 |
    | (2,1)    |     2 |
    | (3,1)    |     4 |
    | (4,1)    |     2 |
    | (5,1)    |     4 |
    | (6,1)    |     2 |
    | (7,1)    |     4 |

****** Punto 1
     Isomorfo a $\mathbb{Z}_8}$ es:

     \[<(1,0)>\]

     Que como contiene a todos los demÃ¡s elementos de orden $8$, nos asegura que no 
     hay mÃ¡s isomorfos a $\mathbb{Z}_8$.

     Tenemos ademÃ¡s uno isomorfo a $\mathbb{Z}_2\times\mathbb{Z}_4$, que es:

     \[<(2,0),(0,1)>\]
     
     Y otro isomorfo al grupo de los cuaternios, que es:

     \[<(4,0),(2,0),(1,1),(7,1)>\]

     No encontramos ninguno isomorfo a $\mathbb{Z}_2\times\mathbb{Z}_2\times\mathbb{Z}_2$, que necesitarÃ­a de $7$ elementos
     de orden $2$.

****** Punto 2
     Isomorfos al grupo de Klein:

     \[\{(0,0),(2,1),(4,0),(6,1)\}\]
     \[\{(0,0),(0,1),(4,1),(4,0)\}\]

     CÃ­clicos de grado $4$:

     \[<(2,0)>\]
     \[<(1,1)>\]
     \[<(3,1)>\]

****** Punto 3
     Subgrupos de grado $2$ son los que genera cada elemento de grado $2$. Hay
     cinco elementos de grado $2$.

***** Ejercicio 7.26 (Usando sage)
    Usamos Galois para tener correspondencia con un problema de grupos

    #+BEGIN_SRC sage
sage: C8 = CyclicPermutationGroup(8)
sage: alpha = PermutationGroupMorphism(C8,C8,[C8.gen()**3])
sage: phi = [[(1,2)],[alpha]]
sage: G = CyclicPermutationGroup(2).semidirect_product(C8,phi)
sage: G
Permutation Group with generators [(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]
sage: G.order()
16
sage: G.subgroups()

[Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [()],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,7)(4,8)(5,9)(6,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(1,2)(4,6)(5,9)(8,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(1,2)(3,5)(4,8)(7,9)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(1,2)(3,7)(4,10)(6,8)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(1,2)(3,9)(5,7)(6,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,5,7,9)(4,6,8,10), (3,7)(4,8)(5,9)(6,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,7)(4,8)(5,9)(6,10), (1,2)(4,6)(5,9)(8,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,7)(4,8)(5,9)(6,10), (1,2)(3,9)(5,7)(6,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,7)(4,8)(5,9)(6,10), (1,2)(3,4,7,8)(5,10,9,6)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,7)(4,8)(5,9)(6,10), (1,2)(3,10,7,6)(4,5,8,9)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,5,7,9)(4,6,8,10), (3,7)(4,8)(5,9)(6,10), (1,2)(4,6)(5,9)(8,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,4,5,6,7,8,9,10), (3,5,7,9)(4,6,8,10), (3,7)(4,8)(5,9)(6,10)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,5,7,9)(4,6,8,10), (3,7)(4,8)(5,9)(6,10), (1,2)(3,4,7,8)(5,10,9,6)],
 Subgroup of (Permutation Group with generators 
[(3,4,5,6,7,8,9,10), (1,2)(4,6)(5,9)(8,10)]) generated by [(3,4,5,6,7,8,9,10), (3,5,7,9)(4,6,8,10), (3,7)(4,8)(5,9)(6,10), (1,2)(4,6)(5,9)(8,10)]]
    #+END_SRC

**** Semana 9
***** Ejercicio 7.27
    #+begin_statement
    Prueba que los subgrupos transitivos de $S_4$ son los subgrupos siguientes:
    
      1. $S_4$, que es normal.
      2. $A_4$, que es normal.
      3. $D_4 = \langle(1234),(13) \rangle$, y todos sus conjugados.
      4. $C_4 = \langle (1234) \rangle$, y todos sus conjugados.
      5. $V = \{1,(12)(34),(13)(24),(14)(24)\}$, que es normal.
    
    El retÃ­culo de subgrupos transitivos de $S_4$ es:

    \[ \begin{tikzcd}
    & & S_4 & \\
    & A_4 \urar & & D_4 \ular \\
    V \urar & & C_4 \ular \urar &
    \end{tikzcd} \]

    Como consecuencia, si $f(X)\in \mathbb{Q}[X]$ es un polinomio irreducible de grado cuatro,
    el grupo de Galois de $\mathbb{Q}(f)/\mathbb{Q}$ es isomorfo a uno de Ã©stos.
    #+end_statement

    El grupo de Galois de un polinomio *irreducible* de grado cuatro debe poder
    permutar entre las $4$ raÃ­ces del polinomio, por lo que debe ser un subgrupo
    transitivo de $S_4$. Para que un subgrupo sea transitivo, su orden debe ser mayor
    que $4$.

    Comprobamos que:

      - De orden 24, el Ãºnico es $S_4$.
      - De orden 12, el Ãºnico es $A_4$.
      - De orden 8, el Ãºnico es $D_4$, con tres conjugados.
      - De orden 6, el Ãºnico es isomorfo a $S_3$, con cuatro conjugados.
      - De orden 4, los Ãºnicos son $C_4$ y $V$, con seis conjugados.
	
    De esta lista retiramos a $\langle (12),(23),(13) \rangle$ y sus cuatro conjugados por no ser
    transitivos. El resto se comprueban transitivos.

***** Ejercicio 7.28
    #+begin_statement
    Sea $f(X)\in K[X]$ un polinomio separable y $g$ un factor irreducible de $f$. Â¿ActÃºa
    transitivamente $G = \operatorname{Gal}(f/K)$ sobre las raÃ­ces de $g$?
    #+end_statement
    
    Como $g$ es el polinomio irreducible de cualesquiera dos raÃ­ces suyas, estas son
    conjugadas y existe un isomorfismo $\sigma : \overline{K} \longrightarrow \overline{K}$. Como $f/K$ entendemos que es
    normal, ese isomorfismo se restringe a $f/K$.
**** Semana 10
***** Ejercicio 7.29
****** Punto 1
Si es irreducible, tenÃ­amos por el ejercicio de Semana 9 que
su grupo serÃ­a un subgrupo transitivo de $S_4$.

Como si $\alpha$ es raÃ­z lo es $-\alpha$, tenemos que como mucho habrÃ¡ $8$ automorfismos.
Fijada la imagen de $\alpha$ entre las cuatro posibles, queda fijada la imagen
de $-\alpha$, asÃ­, sÃ³lo quedan dos posibles imÃ¡genes para $\beta$. No puede ser
isomorfo por tanto a $S_4$ o a $A_4$.

****** Punto 2
Estamos en el caso del punto anterior por el polinomio $(x^2-n)(x^2-m)$.
El grupo no es trivial porque $\sqrt{n},\sqrt{m}$ no son racionales. No puede
tener automorfismos que lleven $\sqrt{n}$ en $\sqrt{m}$ porque $\sqrt{nm}$ no es un
cuadrado y entonces se tiene $n \neq m$.

Fijada la imagen de $\sqrt{n}$ entre dos posibles, sÃ³lo queda la imagen de 
$\sqrt{m}$ entre dos posibles. No puede ser por tanto $D_4$, y no puede ser
un grupo cÃ­clico porque tiene elementos de orden $2$. Debe ser $V$.

****** Punto 3
Como $\mathbb{Q}(\sqrt{n}+\sqrt{m})$ genera una extensiÃ³n de grado 4, serÃ¡ de grado 4
su polinomio irreducible, ya que hemos dicho que $\sqrt{nm}$ no es racional
y tenemos que:

\[ \frac{\sqrt{nm}}{\sqrt{n}+\sqrt{m}} = \frac{1}{\sqrt{1/n}+\sqrt{1/m}}\]

Ambos irreducibles por serlo $\sqrt{n}+\sqrt{m}$.

Por Ãºltimo, comprobamos que es un polinomio que lo tiene como raÃ­z:

\[ x^4 - 2(n+m) x^2 + n^2 -2nm + m^2\]

A este polinomio llegamos simplemente manipulando algebraicamente
las representaciones de $(\sqrt{n}+\sqrt{m})^2$ y $(\sqrt{n}+\sqrt{m})^4$.

****** Punto 4
Si tomamos un elemento en $u \in F-\mathbb{Q}$, tenemos que $[F:\mathbb{Q}(u)]$ podrÃ­a
ser $2$ o $1$. Si fuera $1$, entonces $\{1,u,u^2,u^3\}$ es base y $[F : Q(u^2)]$ serÃ­a
de grado $2$. TendrÃ­amos un cÃ­clico. Si fuera $2$, tenemos $u^2 \in \mathbb{Q}$,
como buscamos.

****** Punto 5
Tomamos $f_1(x) = (x^2-2)(x^2-3)$, que hemos demostrado en el punto
2 que funciona. $f_2(x) = x^4+3$ tiene grupo isomorfo a $D_3$.
En el punto 7 tenemos un ejemplo para $f_3$ en $x^4+5x^2+5$, que es
irreducible por Eisenstein en mÃ³dulo 2.

****** Punto 6
Si tengo raÃ­ces $u,-u,v,-v$, tengo $\sqrt{c} = \pm uv$ racional. Cualquier
automorfismo de grado cuatro de los posibles llevarÃ­a $u \mapsto v$
$v \mapsto -u$, por lo que cambiarÃ­a el signo de un racional.

****** Punto 7
Si resolvemos la ecuaciÃ³n de grado $4$ tenemos:

\[ \sqrt{c} = uv \]
\[ \sqrt{b^2-4c} = (u^2-v^2)\]

El producto de ambas es racional, pero uno de los automorfismos de
grado dos que estÃ¡ en $V$ y en $D_4$, que lleva $u \mapsto v$, $v \mapsto u$, cambiarÃ­a
el signo de un real.
** TopologÃ­a II
*** 1. El grupo fundamental
**** 1. Espacios conexos por arcos
***** ArcoconexiÃ³n
*ArcoconexiÃ³n*. Las curvas definen relaciÃ³n de equivalencia cuando
unen dos puntos.

 \[\exists f\ \text{arco}: f(0) = x, f(1) = y \Rightarrow x \sim y\]

Cada componente de esta particiÃ³n es una *componente arcoconexa*. Un
espacio es *arcoconexo* cuando tiene una sola componente.

***** Operaciones en arcos y arcoconexiÃ³n como equivalencia
Las propiedades de la relaciÃ³n de equivalencia se cumplen por:

 - 1. *Reflexividad*. Arco constante, $f(t) = x$, lleva a $x \sim x$.
 - 2. *SimetrÃ­a*. Arco inverso, $\widetilde f(t) = f(1-t)$, lleva a $x\sim y \Rightarrow y \sim x$.
 - 3. *Transitividad*. ComposiciÃ³n de arcos $f \ast g$.

La composiciÃ³n de arcos se define como:

\[f \ast g = \twopartdef{f(2t)}{t \leq \frac{1}{2}}{g(2t-1)}{t \geq \frac{1}{2}}\]

***** ArcoconexiÃ³n y conexiÃ³n
Un espacio *arcoconexo es conexo*. Un espacio *conexo localmente arcoconexo 
es arcoconexo*, que es conexo y donde todo punto posee un entorno arcoconexo.

****** DemostraciÃ³n
Dado un punto en un espacio arcoconexo, los caminos a los demÃ¡s serÃ¡n conexos y
compartirÃ¡n un punto, luego su uniÃ³n serÃ¡ conexa. Por otro lado, en un localmente
arcoconexo, todo punto estÃ¡ dentro de un abierto (tiene un entorno arcoconexo),
luego cada componente arcoconexa serÃ¡ abierta y si hubiera varias, contravendrÃ­a
la conexiÃ³n.

****** Contraejemplo del recÃ­proco
Hay un contraejemplo de espacio conexo no arcoconexo en el
[[https://es.wikipedia.org/wiki/Seno_del_top%25C3%25B3logo][seno del topÃ³logo]].

**** 2. Grupo fundamental
***** HomotopÃ­a de lazos
Un arco con $f(0) = f(1) = x$ es un *lazo* alrededor de $x$. Dos lazos son *homotÃ³picos*
y escribimos $f \sim g$ cuando $\exists H: [0,1] \times [0,1] \longrightarrow X$, cumpliendo:

  - $H$ continua.
  - $H(t,0) = f(t)$
  - $H(t,1) = g(t)$
  - $H(0,s) = H(1,s) = x$

lo escribimos como $H : f \simeq g$.

***** Clases de homotopÃ­a
La homotopÃ­a es una relaciÃ³n de equivalencia entre lazos:

 - *Reflexividad*. Definiendo $H(t,s) = f(t)$, tenemos $H : f \simeq f$.
 - *SimetrÃ­a*. Dada $G : g \simeq f$, definimos $H(t,s) = G(t,1-s)$, tenemos $H : f \simeq g$.
 - *Transitividad*. Dadas $F : f \simeq g$, $G : g \simeq h$, definimos
    \[H(t,s) = \twopartdef{F(t,2s)}{0\leq s \leq 1/2}{G(t,2s-1)}{1/2 \leq s \leq 1}\]
    Y tenemos $H : f \simeq h$.

Llamamos *clase de homotopÃ­a* $[f]$ a la clase de equivalencia de un lazo $f$.

***** Producto de clases de homotopÃ­a
El producto de lazos estÃ¡ bien definido entre las clases de homotopÃ­a. Sean 
$H : f_1 \simeq f_2$ y $G: g_1 \simeq g_2$; entonces definimos $F: f_1 \ast g_1 \simeq f_2 \ast g_2$ como:

\[ F(t,s) = \twopartdef{H(2t,s)}{0\leq t \leq 1/2}{G(2t-1,s)}{1/2 \leq t \leq 1}\]

***** El grupo fundamental
Comprobamos que las clases de homotopÃ­a sobre un $x$ forman un grupo con el producto:

\[\Pi(X,x) = \{[f] \mid f \text{ lazo alrededor de } x\}\]

****** Asociatividad
Para la *asociatividad*, definimos $H : (f \ast g) \ast h \simeq f \ast (g \ast h)$:

\[H(t,s) = \threepartdef
{f\left(t \frac{4}{1+s}\right)}{0\leq t\leq \frac{1+s}{4}}
{g\left(4t-1-s\right)}{\frac{1+s}{4}\leq t \leq \frac{2+s}{4}}
{h\left(t\frac{4}{2-s}-1\right)}{\frac{2+s}{4}\leq t\leq 1}\]

****** Elemento neutro
Como *elemento neutro* tomaremos el lazo constante $f_x(t)=x$. Y definimos  
$H : f_x \ast f \simeq f$:

\[ H(t,s) = \twopartdef
{f(t\frac{2t}{1+s})}{0\leq t\leq \frac{1+s}{2}}
{x}{\frac{1+s}{2}\leq t \leq 1}\]

****** Elemento inverso
Como *elemento inverso* tendremos el lazo $\hat{f}(t) = f(1-t)$. Y definimos 
$H : f \ast \hat{f} \simeq f$:

\[H(t,s) = \threepartdef
{f(2t)}{0\leq t\leq\frac{1-s}{2}}
{f(1-s)}{\frac{1-s}{2}\leq t\leq \frac{1+s}{2}}
{\hat{f}(2t-1)}{\frac{1+s}{2}\leq t\leq 1}\]

***** El grupo fundamental como funtor
Sea $\Phi : (X,x) \longrightarrow (Y,y)$ /continua/, entonces tenemos una aplicaciÃ³n bien definida 
por:

\[\Phi_\ast ([f]) = [\Phi \circ f] \]

Que es un homomorfismo de grupos. Podemos comprobar fÃ¡cilmente que $Id_\ast = Id$ y que
$(\Psi\circ\Phi)_\ast = \Psi_\ast\circ\Phi_\ast$. Es decir, el grupo fundamental es un funtor de la categorÃ­a de 
los espacios topolÃ³gicos punteados a la de los grupos.

****** DemostraciÃ³n
Sean $H: f \simeq g$, veamos que $\Phi\circ H: \Phi f \simeq \Phi g$. Tenemos que:

  - $\Phi\circ H$ continua
  - $\Phi\circ H(t,0) = \Phi\circ f(t)$
  - $\Phi\circ H(t,1) = \Phi\circ g(t)$
  - $\Phi \circ H(0,s) = \Phi \circ H(1,s) = y$

El que respeta el producto se tiene por $\Phi\circ (f \ast g) = \Phi\circ f \ast \Phi\circ g$.

***** HomotopÃ­a de arcos
Decimos que dos arcos cumpliendo $f(0)=g(0)$, $f(1)=g(1)$ son homotÃ³picos cuando 
existe una funciÃ³n continua con caracterÃ­sticas similares a la dada para lazos.

****** Propiedades
De demostraciÃ³n similar al caso de lazos:

  - Hay un arco simÃ©trico $\tilde{f}$ tal que $f\ast \tilde{f} \simeq f_x$.
  - El producto es asociativo por homotopÃ­a $f \ast (g \ast h)\simeq (f\ast g)\ast h$-
    
***** Isomorfismo entre grupos fundamentales en distintos puntos
Sea un arco $\gamma : [0,1] \longrightarrow X$ cumpliendo $\gamma(0) = x$, $\gamma(1)=y$. Entonces se tiene un 
isomorfismo entre $\Pi(X,x)$ y $\Pi(X,y)$:

\[ F_\gamma([f]) = [\tilde\gamma\ast f\ast\gamma]\]

Por tanto, el grupo fundamental es el mismo en cualquier punto de la componente
arcoconexa.

****** DemostraciÃ³n
Para ver que estÃ¡ bien definido basta notar:

\[f \simeq g \Rightarrow 
\gamma\ast f\ast\tilde\gamma\simeq \gamma\ast g\ast\tilde\gamma\]

Que es homomorfismo se tiene viendo $F([f]\ast [g]) = F([f]) \ast F([g])$, y que es biyectivo
porque tiene inversa $G_\gamma([f]) = [\gamma\ast f\ast \tilde\gamma]$.

***** El grupo fundamental del producto
Dado el producto de dos espacios topolÃ³gicos $X\times Y$ con proyecciones $\pi_1,\pi_2$. Tenemos
un isomorfismo entre grupos fundamentales:

\[F: \Pi(X\times Y, (x,y)) \longrightarrow \Pi(X,x)\times\Pi(Y,y)\]

Dado por:

\[F([f]) = (\pi_1([f]), \pi_2([f]))\]

**** 3. El grupo fundamental del cÃ­rculo
***** CÃ¡lculo del grupo fundamental del cÃ­rculo
Usaremos la teorÃ­a de recubridores que se probarÃ¡ luego.

Probamos que $\pi : \mathbb{R} \longrightarrow \mathbb{S}$ es un recubridor con $\pi(t) = e^{it}$; podemos tomar como entorno
fundamental de $p$ a $\mathbb{S}-\{p\}$, que tiene en la preimagen componentes arcoconexas 
homeomorfas a Ã©l. Siendo $t_0 \in \pi^{-1}(p)$:

\[V_m = (t_0 + 2\pi m, t_0 + 2\pi(m+1))\]

Definimos ahora el grado de un lazo en el cÃ­rculo como:

\[\operatorname{deg}(f) = \frac{\hat{f}(1) - \hat{f}(0)}{2\pi} \]

Y comprobamos que estÃ¡ bien definido entre las clases de homotopÃ­a levantando
las homotopÃ­as y viendo que debe ser constante el punto final para que se proyecte
en un punto constante. Es decir $f \simeq g \Rightarrow \hat{f}(0) = \hat{g}(0)
\Rightarrow \hat{f}(1) = \hat{g}(1)$.

Como el levantamiento de la composiciÃ³n es la composiciÃ³n de levantamientos
y levantando de forma que $\hat{g}(0) = \hat{f}(1)$:

\[ \operatorname{deg}(f \ast g) =
   \frac{\widehat{f\ast g}(1) - \widehat{f\ast g}(0)}{2\pi} = 
   \frac{\hat{f}(1) - \hat{g}(0) + \hat{g}(1) - \hat{f}(0)}{2\pi} =
   \operatorname{deg}(f) + \operatorname{deg}(g) \]

Siendo un homomorfismo del grupo fundamental del cÃ­rculo con $\mathbb{Z}$.

***** Teorema fundamental del Ã¡lgebra
*Teorema fundamental del Ã¡lgebra*. Todo polinomio con coeficientes
en $\mathbb{C}$ de grado $n$ tiene $n$ raÃ­ces en $\mathbb{C}$.

****** DemostraciÃ³n
Supongamos un $P$ que no tuviera raÃ­z en $\mathbb{C}$.

Fijado un $r$, restringimos el polinomio desde la circunferencia de
radio $r$ a la circunferencia unidad, girÃ¡ndolo
ademÃ¡s para que en $0$ valga $1$.

\[f_r(t) = \frac{|P(r)|}{P(r)} \frac{P(re^{2\pi it})}{|P(re^{2\pi it})|}\]

Tenemos una homotopÃ­a de este lazo al constante, definida como:

\[H(t,s) = f_{(1-s)r}(t)\]

Y por tanto, $deg(f_r) = 0$.

Por otro lado, sea ahora $R > 1, \sum |a_i|$, y tomemos $|z| = R$, tenemos
por un lado:

\[|z^n| 
 = R^n > R^{n-1}\left(\sum |a_i|\right) 
 \geq |a_1z^{n-1} + \dots + a_n|\]

Y por otro lado, si tomo un $t \in [0,1]$, puedo definir una familia
de polinomios $P_s(z) = z^n + t(a_1z^{n-1} + \dots + a_n) = 0$, que no tienen raÃ­ces
porque, por otro lado:

\[|z^n| = |t||a_1z^{n-1} + \dots + a_n| \leq |a_1z^{n-1} + \dots + a_n|\]

Luego esta familia de polinomios no tiene raÃ­ces en el cÃ­rculo 
de radio $R$. Podemos ahora definir otra homotopÃ­a:

\[H(t,s) = \frac{P_s(Re^{2\pi it})}{|P_s(Re^{2\pi it})|}  \frac{|P_s(R)|}{P_s(R)}\]

Que es homotopÃ­a entre $e^{2\pi int}$ y $f_R(t)$. Luego $deg(f_R) = n$.

***** Lema al punto fijo de Brower
No existe aplicaciÃ³n continua $f : D^2\longrightarrow \mathbb{S}$ tal que $f|_\mathbb{S} = id$.

****** DemostraciÃ³n
Estamos buscando una aplicaciÃ³n cumpliendo:

\[ \begin{tikzcd}
\mathbb{S} \rar[hook]{i} & D^2 \rar{f} & \mathbb{S}
\end{tikzcd} \]

Aplicando el funtor obtendrÃ­amos:

\[ \begin{tikzcd}
\mathbb{Z} \rar[hook] & 0 \rar & \mathbb{Z}
\end{tikzcd} \]

Pero asÃ­ es imposible obtener la identidad como composiciÃ³n.

***** Teorema del punto fijo de Brower
Toda funciÃ³n continua $f : D^2 \longrightarrow D^2$ tiene un punto fijo.

****** DemostraciÃ³n
Si no lo hubiera, para cada $x$ tomo $f(x)$ y la intersecciÃ³n de la recta que los une
con el cÃ­rculo mÃ¡s cercana a $x$. Tengo una aplicaciÃ³n cuya restricciÃ³n es la 
identidad.

***** Grupos topolÃ³gicos
Un grupo topolÃ³gico es un grupo en la categorÃ­a de espacios punteados. Esto es,
tal que la funciÃ³n producto y la funciÃ³n inverso son continuas.
***** Grupos topolÃ³gicos y grupo fundamental
El producto en un grupo topolÃ³gico respeta clases de homotopÃ­a:

\[ [f]\cdot [g] = [f\cdot g]\]

Y ademÃ¡s, actÃºa sobre ellas igual que la composiciÃ³n:

\[ [f]\ast [g] = [f] \cdot [g]\]

**** 4. Tipo de homotopÃ­a. Equivalencias homotÃ³picas
***** Aplicaciones homotÃ³picas
Sean $F,G : X \longrightarrow Y$ continuas. Las decimos *homotÃ³picas* si existe 
$H : X \times [0,1] \longrightarrow Y$ tal que $H(x,0) = F(x)$ y $H(x,1) = G(x)$. Lo notamos
por $H: F\simeq G$.

***** Grupo fundamental entre dos puntos
Entre cualesquiera puntos de dos funciones homotÃ³picas $F(x_0)$, $G(x_0)$; tenemos un 
arco $\gamma = H(x_0,t)$. Se cumple que:

\[ \begin{tikzcd}
& \Pi(Y,F(x_0)) \arrow[leftrightarrow]{dd}{F_\gamma}\\
\Pi(X,x_0) \urar{F_\ast}\drar{G_\ast} \\
& \Pi(Y,G(x_0))
\end{tikzcd} \]

****** DemostraciÃ³n
Sea $f \in \Pi(X,x_0)$. Si defino la funciÃ³n $\hat{H}(t,s) = H(f(t),s)$ tengo una homotopÃ­a como
la siguiente:

\begin{tikzpicture}
\draw (A) -- node [above] {$G f$} ++ (-1, 0)
-- node [left]  {$\gamma$} ++ (0, -1)
-- node [below] {$F f$} ++ (1, 0)
-- node [right] {$\gamma$} ++ (0, 1);
\end{tikzpicture}

RotÃ¡ndola obtengo:

\begin{tikzpicture}
\draw (A) -- node [above] {$G f \circ \gamma$} ++ (-1, 0)
-- node [left]  {$F(x_0)$} ++ (0, -1)
-- node [below] {$\gamma \circ F f$} ++ (1, 0)
-- node [right] {$G(x_0)$} ++ (0, 1);
\end{tikzpicture}

Lo que me da $[Gf \circ \gamma] = [\gamma \circ Ff]$, y por tanto $[Gf] = [\gamma \circ Ff \circ \tilde{\gamma}]$.

***** Equivalencia homotÃ³pica
Una *equivalencia homotÃ³pica* es una aplicaciÃ³n continua $F$, para la que
existe $G$ cumpliendo $F \circ G \simeq G \circ F \simeq Id$.

***** Propiedades de la equivalencia homotÃ³pica
Cumple:

1. Los homeomorfismos son equivalencias homotÃ³picas.
2. La inversa de una equivalencia homotÃ³pica es equivalencia homotÃ³pica.
3. La composiciÃ³n de equivalencias homotÃ³picas es equivalencia homotÃ³pica.

***** ConservaciÃ³n del grupo fundamental por equivalencia homotÃ³pica
Sea $F: X\longrightarrow Y$ equivalencia homotÃ³pica:

\[\forall x\in X : F_\ast : \Pi(X,x) \longrightarrow \Pi(Y,F(x))\]

Es un isomorfismo

****** DemostraciÃ³n
Por ser equivalencia homotÃ³pica tengo que $F \circ G \simeq Id$, luego se cumple:

\[ \begin{tikzcd}
& \Pi(X,F\circ G(x_0)) \arrow[leftrightarrow]{dd}{\cong}\\
\Pi(X,x_0) \urar{(F\circ G)_\ast}\drar{Id} \\
& \Pi(X,x_0)
\end{tikzcd} \]

AsÃ­, $(F\circ G)_\ast$, y de la misma forma $(G\circ F)_\ast$ son isomorfismos. Tenemos por tanto:

\[ \begin{tikzcd}
\Pi(X,x) \rar{F_\ast} \arrow[bend left=20]{rr}{\cong}
& \Pi(Y,F(x)) \rar{G_\ast} \dlar{\cong}
& \Pi(X,(G\circ F)(x)) \dlar{\cong}
\\
\Pi(Y,F(x)) \rar{F_\ast} \arrow[bend right=20]{rr}{\cong}
& \Pi(X,(G\circ F)(x)) \rar{F_\ast}
& \Pi(X,(G\circ F)(x))
\end{tikzcd} \]

Demostrando ambas filas que $F_\ast$ y $G_\ast$ son isomorfismos.

***** Lema a Borsuk-Ulam
No existe $F: \mathbb{S}^2 \longrightarrow \mathbb{S}^1$ continua respetando antÃ­podas.

\[F(-x) = -F(x)\]

****** TODO DemostraciÃ³n
***** Teorema de Borsuk-Ulam
Sea $F:\mathbb{S}^2 \longrightarrow \mathbb{R}^2$ continua, entonces:

\[\exists x\in\mathbb{S}^2 : F(-x) = F(x)\]

****** DemostraciÃ³n
Supongamos que no se cumpliera, definimos:

\[G(x) = \frac{F(x)-F(-x)}{|F(x)-F(-x)|}\]

Y entonces $G$ serÃ­a continua respetando antÃ­podas.

***** Teorema del sandwich de jamÃ³n
Sean $A,B\subset \mathbb{R}^2$ compactos y conexos. Existe una recta dividiendo a ambos en dos
trozos de igual Ã¡rea.

****** TODO DemostraciÃ³n
***** Espacio proyectivo
En $\mathbb{S}^n$ defino la ralaciÃ³n de equivalencia $p \sim q$ ssi $p = \pm q$. Definimos el 
espacio proyectivo como el cociente bajo esta relaciÃ³n:

\[\mathbb{RP}^n = \mathbb{S}^n}/\sim\]

**** 5. Teorema de Seifert-Van Kampen
***** Producto libre de grupos
Si definimos el *producto libre* de grupos en tÃ©rminos de palabras; podemos 
llamar *grupo libre sobre un conjunto de generadores* al producto libre del 
grupo libre generado por cada uno de ellos.

***** Subgrupo normal generado
El *subgrupo normal generado* por un subgrupo $B$ o por un conjunto de 
generadores es:

\[ \{g \cdot b \cdot g^{-1} \mid g\in G, b\in B\}\]

***** Producto libre amalgamado
Sean tres grupos $A$, $G_1$, $G_2$ y dos proyecciones de $A$ en ellos, llamadas 
$\Phi_1,\Phi_2$. Definimos el *producto libre amalgamado* como:

\[G_1 \ast_{A} G_2 =
\frac{G_1 \cdot G_2}{N} = \frac{G_1 \cdot G_2}{\{\Phi_1(a)=\Phi_2(a)\}}\]

Donde estamos dividiendo por $N$, el subgrupo normal generado por 
$\{\Phi_1(a)\Phi_2(a)^{-1} \mid a \in A\}$. Es decir, imponemos la relaciÃ³n $\Phi_1(a) = \Phi_2(a)$.

***** Teorema de Seifert-Van Kampen
Sea $X$ espacio topolÃ³gico con $U,V$ abiertos arcoconexos no vacÃ­os de $X$ tales 
que $X = U \cup V$ y $U\cap V$ son arcoconexos. Existe un isomorfismo:

\[\Theta : \Pi(U,x) \ast_{\Pi(U\cap V,x)} \Pi(V,x) \longrightarrow \Pi(X,x)\]

donde se amalgama usando $i_\ast$, $j_\ast$, homomorfismos dados por las inclusiones.

***** Seifert-Van Kampen para intersecciÃ³n simplemente conexa
En las condiciones del teorema, con $U\cap V$ simplemente conexo, se tiene:

\[\Pi(X,x) \cong \Pi(U,x)\ast\Pi(V,x)\]

***** Seifert-Van Kampen para abierto simplemente conexo
En las condiciones del teorema, con $V$ simplemente conexo, se tiene:

\[\Pi(X,x) \cong \Pi(U,x)/N\]

Con $N$ es el subgrupo normal generado por $i_\ast(\Pi(U\cap V,x))$.

**** Extra: TeorÃ­a de categorÃ­as
***** El grupo fundamental como funtor
El grupo fundamental $\Pi$ es un funtor entre las categorÃ­as:

- =Top.= de los espacios topolÃ³gicos con un punto base, usando como morfismos
  las funciones continuas respetando punto base.
- =Grp= de los grupos con los homomorfismos de grupos.

Estamos llamando $f_\ast$ a los morfismos creados por el funtor, $\Pi(f)$.

***** Producto categÃ³rico
El producto de dos espacios con un punto base es, usando la topologÃ­a producto:

\[(X,x) \times (Y,y) \cong (X\times Y, (x,y))\]

El funtor lo lleva al producto de grupos.

***** Coproducto categÃ³rico
El coproducto de dos espacios con un punto es la suma directa de los espacios
identificando el punto. Intuitivamente, consiste pegar los dos espacios por ese
punto.

\[ X \wedge Y \cong (X \amalg Y)/(x\sim y)\]

Cuando ademÃ¡s tenemos espacios localmente contractibles, el coproducto se lleva
al coproducto de grupos, esto es, al producto libre:

\[\Pi(X \wedge Y) \cong \Pi(X) \ast \Pi(Y)\]

NÃ³tese que esto es un caso particular de Seifert-Van Kampen.

***** TODO Seifert-Van Kampen
*** 2. Recubridores
**** 1. IntroducciÃ³n
***** Localmente arcoconexo
Un espacio es *localmente arcoconexo* si todo punto posee una base de 
entornos arcoconexos.

/Durante este tema tomamos los espacios como arcoconexos y localmente 
arcoconexos/.

***** Recubridores
Un *recubridor* de $X$ es un par $(Y,p)$ donde $p : Y \longrightarrow X$ es continua; 
cumpliendo que todo $x\in X$ tiene un entorno abierto $U$, llamado *entorno 
fundamental* tal que toda componente arcoconexa de $p^{-1}(U)$ se aplica 
homeomÃ³rficamente por $p$ sobre $U$.

***** Ejemplos de recubridores
Ejemplos bÃ¡sicos de recubridores son:

- Cualquier homeomorfismo $p : Y \longrightarrow X$
- $p : \mathbb{S}^1\longrightarrow\mathbb{S}^1$, con $p(z) = z^n$

***** Homeomorfismos locales
Un *homeomorfismo local* es una aplicaciÃ³n continua $f : Y \longrightarrow X$ tal que para 
todo $y\in Y$ existe $y \in V\in\tau_Y$ tal que $f|_V$ es homeomorfismo.

***** Propiedades de un recubridores
Sea $(Y,p)$ recubridores de $X$. Entonces:

1. $p$ es sobreyectiva.
2. $p$ es una aplicaciÃ³n abierta.
3. $p$ es un homeomorfismo local.

****** DemostraciÃ³n
La sobreyectividad es trivial por la definiciÃ³n. Dado un abierto $y\in O$; 
tengo $y \in V_y \cong U_x$ su entorno abierto, luego $p(O\cap V_y)$ es abierto. De esta 
forma,

\[p(O) = \bigcup_{y\in O} p(O \cap V_y)\]

es abierto.

**** 2. Grupo fundamental y levantamiento de aplicaciones al recubridor
***** Levantamiento de arcos
Sea $(Y,p)$ recubridor con $x_0\in X$ y $y_0 \in p^{-1}(x_0)$. Sea $f : [0,1]\longrightarrow X$ arco 
continuo con $f(0) = x_0$, entonces existe un Ãºnico arco continuo $\check{f}$ cumpliendo:

  - $\check{f}(0) = y_0$
  - $p \circ \check{f} = f$

    Llamado el *levantamiento* de $f$.

****** Existencia
Tomo $\{f^{-1}(U^x) \mid x\in X\}$, que recubre por abiertos a $[0,1]$. Sabemos que 
existirÃ¡ una particiÃ³n del intervalo cumpliendo:

\[\exists 0 < t_1 <\dots < t_n < 1: f([t_i,t_{i+1}]) \subseteq U^{x_i}\]

La correspondiente $y_i \in V^{y_i}$ nos da un isomorfismo $p_i$ que nos 
deja definir $\check{f} : [t_i,t_{i+1}] \longrightarrow V^{y_i}$ mediante $\check{f} = (p|_{V^{y_i}})^{-1} \circ f$. NÃ³tese que para 
tomar cada $y_i$ necesitamos usar la componente arcoconexa de la Ãºltima $\check{f}(t_{i-1})$.

****** Unicidad
Sean dos levantamientos $g_1,g_2$. Su conjunto ecualizador es cerrado:

\[ A = \{ t \mid g_1(t) = g_2(t)\} \neq \varnothing\]

Pero tambiÃ©n es abierto porque dado un punto donde coincidan, puedo tomar la 
componente arcoconexa que es isomorfa por $p$ a un entorno abierto; y las 
curvas deben coincidir en Ã©l.

***** Levantamiento de homotopÃ­as
Sea $(Y,p)$ recubridor con $x_0\in X$ y $y \in p^{-1}(x_0)$. Sea $H : [0,1]\times[0,1] \longrightarrow X$ 
continua con $H(0,0) = x_0$, entonces existe una Ãºnica aplicaciÃ³n
continua $\check{H} : [0,1]\times [0,1] \longrightarrow Y$ cumpliendo:

  - $p \circ \check{H} = H$
  - $\check{H}(0,0) = y_0$

    Llamada el *levantamiento* de $H$.

****** Existencia
Tomamos las $\{ H^{-1}(U^x) \mid x \in X \}$ y tenemos recubrimiento por abiertos de $[0,1]^2$. 
Tendremos alguna particiÃ³n cumpliendo:

\[\exists 0 < t_1 < \dots < t_n < 1 : 
H([t_i,t_{i+1}]\times[s_i,s_{i+1}]) \subset U^{x_i}\]

En la correspondiente $y_i \in V^{y_i}$ podemos definir 
$\check{H} = (p|_{V^{y_i}}^{-1})\circ H$. NÃ³tese que tenemos que usar en cada paso la componente arcoconexa
del Ãºltimo lado unido a nuestro cuadrado, que no puede salirse de esa componente
por ser arcoconexa.

****** TODO Unicidad
***** Hojas del recubridor
Sea $p : Y\longrightarrow X$ recubridor. Los cardinales de los $p^{-1}(x)$ son un invariante 
llamado el *cardinal de hojas del recubridor*.

****** DemostraciÃ³n
Puedo definir una biyecciÃ³n entre $p^{-1}(x_1)$ y $p^{-1}(x_2)$ tomando un arco entre 
ellas $\gamma$, y levantÃ¡ndolo en cada componente. Los arcos arriba me relacionan 
los dos conjuntos. La biyecciÃ³n se obtiene levantando $\tilde\gamma$, que sÃ© que es $\tilde{g}$ 
porque ella ya es un levantamiento y es Ãºnico.

***** Los recubridores crean monomorfismos
Sea $p: Y \longrightarrow X$ recubridor, entonces $p_\ast : \Pi(Y,y) \longrightarrow \Pi(X,x)$ es monomorfismo.

****** DemostraciÃ³n
Supongamos $p_\ast[h] = 0$; tengo una homotopÃ­a $H : p \circ h \simeq f_x$ que puedo levantar tomando
$\check{H}(0,0)=y$ y sabiendo $p \circ \check{H} = H$. Tengo:

\[ p \circ \check{H} (t,0) = H(t,0) = (p\circ h)(t)\]
\[ p \circ \check{H} (t,1) = H(t,1) = f_x(t) = x\]

Luego $\check{H}(t,0) = h(t)$ porque son levantamientos de lo mismo y $\check{H}(t,1) = y$ para poder
ser levantamiento. Esto me da $\check{H} : h \simeq f_y$.

***** ConjugaciÃ³n y recubridores
Si $p : Y \longrightarrow X$ es recubridor y $x\in X$, entonces la familia de subgrupos:

\[ \left\{p_\ast(\Pi(Y,y)) \mid y\in p^{-1}(x)\right\}\]

forma exactamente una clase de conjugaciÃ³n de subgrupos de $\Pi(X,x)$.

****** DemostraciÃ³n
Sea $\gamma$ camino entre $y_1,y_2 \in p^{-1}(x)$ con el isomorfismo $F_\gamma([f]) = [\tilde\gamma \ast f \ast \gamma]$, construimos 
el diagrama:

\[ \begin{tikzcd}
\Pi(Y,y_1) \rar{F_\gamma} \dar{p_\ast} & \Pi(Y,y_2) \dar{p_\ast} \\
\Pi(X,x) \rar{F_{p \circ \gamma}}& \Pi(X,x)
\end{tikzcd} \]

Que es conmutativo:

\[ \begin{tikzcd}
\math{[f]} \rar{F_\gamma} \dar{p_\ast} & 
\math{[\tilde\gamma \ast f \ast\gamma]} \dar{p_\ast} \\
\math{[p\circ f]} \rar{F_{p \circ \gamma}} & 
\math{[p(\tilde\gamma) \ast p(f) \ast p(\gamma)]}
\end{tikzcd} \]

Y que nos da por tanto:

\[ p_\ast(\Pi(Y,y_2)) = 
F_{p\circ\gamma}(\Pi(Y,y_1)) =
[p(\tilde\gamma)] \ast \Pi(Y,y_1) \ast [p(\gamma)]
\]

Ahora, sea una clase de conjugaciÃ³n de la proyecciÃ³n de un grupo fundamental
$H = [g]^{-1} \ast p_\ast(\Pi(Y,y)) \ast [g]$. El levantamiento $\check{g}$ da un camino entre dos puntos de 
$p^{-1}(x)$, y vemos que:

\[ \begin{aligned}
H &= \{[g]^{-1} \ast [p\circ h] \ast g \mid [h] \in \Pi(Y,y)\} \\
&= \{[p\circ \tilde{\check{g}} \ast p\circ h \ast p \circ \check{g}] \mid [h] \in \Pi(Y,y)\} \\
&= p_\ast(F_{\check{g}}(\Pi(Y,y))) = p_\ast(\Pi(Y,y'))
\end{aligned} \]

***** Levntamiento de aplicaciones
Sea $\Phi : Z \longrightarrow X$ continua con $x_0 = \Phi(z_0)$, $y_0 \in p^{-1}(x_0)$. Tenemos que
$\exists! \Psi : Z \longrightarrow Y$ continua cumpliendo:

- $\Psi(z_0) = y_0$
- $p \circ \Psi = \Phi$

ssi $\Phi_\ast(\Pi(Z,z_0)) \subset p_\ast(\Pi(Y,y_0))$.

****** DemostraciÃ³n
Definimos:

\[\check\Phi(z) = \widehat{\Phi \circ f}(1)\]

Siendo el levantamiento de la imagen de una curva que tenÃ­a $f(1)=z$, $f(0)=z_0$.
Esta es una funciÃ³n que lo cumple; debemos demostrar que estÃ¡ *bien definida* y
que es continua. Esto es, $\widehat{\phi \circ g}(1) = \widehat{\phi\circ f}(1)$, para otro $g$ cumpliendo lo mismo que $f$.

Como tenemos por la condiciÃ³n que $\phi_\ast[f \ast\tilde{g}] = p_\ast(\alpha)$, tenemos que ambas
$H : \phi(f \ast \tilde{g}) \simeq p\circ \alpha$. Tomamos $[f \circ \tilde{g}]$ para ver:

\[ p(\widehat{\phi\circ f} \ast \widehat{\phi\circ g}) = 
\phi\circ f \ast \widetilde{\phi\circ g}\]

Y usando eso, levantamos la homotopÃ­a, para tener 
$\check{H} : \widehat{\phi\circ f} \ast \widehat{\phi \circ \tilde{g}} 
\simeq \alpha$. Pero como $\alpha$ es lazo, tengo que es lazo lo primero.

Para ver que es *continua*, sea $\check{\Phi}(z) \in O$, abierto. Tenemos $p(O)$ abierto y tomamos
$W$ como la arcocomponente de la preimagen de $U^{\Phi(z)}$ en la que estÃ¡ $\check{\Phi}(z)$. Ahora
sea $\Phi(z) \in p(W \cap O)$, abierto por homeomorfismo; y sea

$\Phi(\hat{O}) \subset p(W \cap O)$, que existe por continuidad de $\Phi$ y es abierto arcoconexo. 

Veamos que $\Phi(\hat{O}) \subset \check{\Phi}^{-1}(O)$. Si $\hat{z} \in \hat{O}$, entonces hay un arco $g$ que une $z$ y $\hat{z}$; y tenemos
$\Phi(g) \subset p(W\cap O)$; como hay homeomorfismo, su levantamiento estÃ¡ en $W \cap O$, y
se tiene:

\[\check{\Phi}(z) = \widehat{\Phi(g)}(1) \in O\]

***** Estructura de grupo topolÃ³gico
Sea $G$ un grupo topolÃ³gico con neutro $e$. Sea $(\check{G},p)$ un recubridor y 
$\check{e} \in p^{-1}(e)$. Entonces $\check{G}$ admite una estructura de grupo topolÃ³gico que tiene
a $\check{e}$ como elemento neutro y a $p$ como homomorfismo de grupos.

**** 3.1. Isomorfismos de recubridores
***** Homomorfismos
Sean $(Y_1,p_1)$, $(Y_2,p_2)$ recubridores. Un *homomorfismo de recubridores* es
$\Phi : (Y_1,p_1) \longrightarrow (Y_2,p_2)$ continua con $p_2 \circ \Phi = p_1$.

\[ \begin{tikzcd}
Y_1 \drar[swap]{p_1} \arrow{rr}{\Phi} & & Y_2 \dlar{p_2} \\
& X &
\end{tikzcd} \]

***** Propiedades de los homomorfismos de recubridores
Los homomorfismos de recubridores cumplen:

   1. La composiciÃ³n de homomorfismos es homomorfismo.
   2. La identidad $Id : Y \longrightarrow Y$ es homomorfismo.
   3. El inverso de isomorfismo es isomorfismo.

      Llamamos $Aut(Y,p)$ al *grupo de automorfismos* de un recubridor.
      # Â¡Forman una categorÃ­a! Debe ser algo como una "slice category".

***** Puntos fijos de los automorfismos de recubridores
Sean dos homomorfismos de recubridores $\Phi,\Psi$ con $\Phi(y) = \Psi(y)$ en algÃºn punto; 
entonces $\Phi = \Psi$. Por tanto, todo automorfismo distinto de la identidad actÃºa 
sin puntos fijos.

***** Existencia de homomorfismos
Existe un homomorfismo de recubridores $\Phi : (Y_1,p_1) \longrightarrow (Y_2,p_2)$ con $\Phi(y_1) = y_2$ ssi
$p_1_\ast(\Pi(Y_1,y_1)) \subseteq p_2_\ast(\Pi(Y_2,y_2))$. Es isomorfismo en el caso de igualdad.

***** Existencia de isomorfismos
Dos recubridores son isomorfos ssi las clases de conjugaciÃ³n asociadas a sus 
proyecciones al grupo fundamental son iguales:

\[\{ p_1_\ast(\Pi(Y_1,y)) \mid y\in p_1^{-1}(x)\}
= \{ p_2_\ast(\Pi(Y_2,y)) \mid y\in p_2^{-1}(x)\}\]

***** Ejemplos de recubridores
****** Espacios simplemente conexos
SÃ³lo se admiten a sÃ­ mismos como recubridores.
****** Circunferencia unidad
Grupo fundamental isomorfo a $\mathbb{Z}$ y abeliano. Sus subgrupos son de la forma $m\mathbb{Z}$,
asÃ­ que, salvo isomorfismos, sus recubridores son de la forma:

\[ p_0 : \mathbb{R} \longrightarrow \mathbb{S}^1,\ p(t) = e^{it} \]
\[p_m : \mathbb{S}^1 \longrightarrow \mathbb{S}^1,\ p_m(z) = z^m\]

****** Espacio proyectivo
Como $\mathbb{R}\mathbb{P}^n$ tiene grupo fundamental $\mathbb{Z}_2$, tiene sÃ³lo a $(\mathbb{RP}^n,Id)$ y a $(\mathbb{S}^n,p)$ 
como recubridores.

***** Recubridores de recubridores
Sean $(Y_1,p_1)$, $(Y_2,p_2)$ dos recubridores, y sea $\Phi : Y_1 \longrightarrow Y_2$ un homomorfismo de 
recubridores. Entonces $(Y_1,\Phi)$ es un recubridor de $Y_2$.

***** Recubridores universales
Un recubridor $(\check{X},p)$ de $X$ es *universal* si $\check{X}$ es simplemente conexo.

**** 3.2. Automorfismos de recubridores
***** AcciÃ³n del grupo fundamental
Sea $(Y,p)$ un recubridor y $x$ punto de $X$ definimos la acciÃ³n

\[ (\cdot) : p^{-1}(x) \times \Pi(X,x) \longrightarrow p^{-1}(x) \]

construyendo $y \cdot \alpha$ como sigue: sea $\alpha = [f]$, tomamos $\check{f}(0) = y$ y llamamos 
$y \cdot \alpha = \check{f}(1)$.

****** TODO EstÃ¡ bien definida
****** TODO Es una acciÃ³n transitiva
****** TODO Espacios homogÃ©neos.
***** Ãndice y hojas del recubridor
Sea $(Y,p)$ recubridor; su nÃºmero de hojas es el Ã­ndice del subgrupo  $p_\ast(\Pi(Y,y))$ 
en $\Pi(X,x)$; donde $y \in p^{-1}(x)$.

***** Automorfismos del espacio homogÃ©neo
Un *automorfismo del espacio homogÃ©neo* $p^{-1}(x)$ es una biyecciÃ³n
$\varphi : p^{-1}(x) \longrightarrow p^{-1}(x)$ tal que:

\[ \varphi(y \cdot \alpha) = \varphi(y) \cdot \alpha\]

***** Automorfismos del espacio homogÃ©neo y automorfismos de recubridores
Como dado un automorfismo de recubridores $\Phi\in Aut(Y,p)$, tenemos que 
$\Phi(y\cdot \alpha) = \Phi(y)\cdot\alpha$, tenemos $\Phi|_{p^{-1}(x)}$ un automorfismo del espacio homogÃ©neo.
De hecho, es isomorfismo de grupos:

\[ ( \bullet |_{p^{-1}(x)}) : Aut(Y,p) \longrightarrow Aut(p^{-1}(x))\]

****** DemostraciÃ³n
******* La restricciÃ³n es automorfismo en el espacio homogÃ©neo
Sea $y\in p^{-1}(x)$, tenemos $p(\Phi(y)) = p(y) = x$, luego $\Phi(y) \in p^{-1}(x)$.

******* Los automorfismos de recubridor respetan la acciÃ³n de grupos
Sea $[f]=\alpha$, y sea $\check{f}$ su levantamiento en $y$; si considero $\Phi(\check{f})$ puedo 
comprobar que $\Phi(\check{f})(0) = \Phi(y)$ y que $p \circ \Phi (\check{f}) = f$, luego es el levantamiento
de $f$ en $\Phi(y)$. AsÃ­:

\[\Phi(y)\cdot\alpha = \Phi(\check{f})(1) = \Phi(y \cdot \alpha)\]

******* Es inyectiva
Llamamos $F = (\bullet |_{p^{-1}(x)})$. Sea $\Phi\in\ker(F)$, entonces $\Phi|_{p^{-1}(x)} = Id$; pero dos
homomorfismos de recubridores coindiciendo en un punto son iguales.

******* Es sobreyectiva
Sea $\varphi\in Aut(p^{-1}(x))$. Por el lema de existencia:
       
\[\exists \phi\in Aut(Y,p): \phi(y) = \varphi(y) \Leftrightarrow
p_\ast(\Pi(Y,y)) = p_\ast(\Pi(Y,\varphi(y))\]
       
Pero tenemos la igualdad de estos dos grupos de isotropÃ­a por:
       
\[\begin{aligned}
H_{\varphi(y)} 
&= \{\alpha\in\Pi(X,x) \mid \varphi(y)\cdot\alpha = \varphi(y)\} \\
&= \{\alpha\in\Pi(X,x) \mid \varphi(y\cdot\alpha) = \varphi(y)\} \\
&= \{\alpha\in\Pi(X,x) \mid y\cdot\alpha = y\} = H_y\\
\end{aligned}\]
       
***** IdentificaciÃ³n de automorfismos de un espacio homogÃ©neo
Dado $E$ espacio homogÃ©neo sobre $G$, existe el isomorfismo:

\[ Aut(E) \cong N(H)/H \]

donde $H = \operatorname{Stab}(y)$ y $N(H)$ es su normalizador, el mayor 
grupo en el que es normal.

***** IdentificaciÃ³n de automorfismos del recubridor
Aplicando las dos identificaciones anteriores:

\[Aut(Y,p) \cong N(p_\ast(\Pi(Y,y))) / p_\ast(\Pi(Y,y))\]

***** Recubridores regulares
Un recubridor es *regular* cuando $p_\ast(\Pi(Y,y))$ es subgrupo normal de 
$\Pi(X,x)$ siendo $y \in p^{-1}(x)$. Equivalen, por conjugaciÃ³n:

- $\exists x\in X, y \in Y: p_\ast(\Pi(Y,y))$ subgrupo normal en $\Pi(X,x)$.
- $\forall x\in X, y \in Y: p_\ast(\Pi(Y,y))$ subgrupo normal en $\Pi(X,x)$.

***** Propiedades de recubridores regulares
Sea $(Y,p)$ un recubridor regular:

1. $Aut(Y,p) \cong \Pi(X,x)/p_\ast(\Pi(Y,y))$
2. Si es el universal, $Aut(Y,p) \cong \Pi(X,x)$. Y el nÃºmero de hojas es 
   el orden de $\Pi(X,x)$.

***** Ejemplos de recubridores
****** Circunferencia, recubridor universal
Tenemos el recubridor $(\mathbb{R},p)$ de $\mathbb{S}$, que tiene como automorfismos:

\[Aut(\mathbb{R},p) = \{\Phi_n(t) = t + 2\pi n \mid n \in \mathbb{N}\}\]

****** Espacio proyectivo
Siendo $(\mathbb{S}^n,p)$ un recubridor de dos hojas de $\mathbb{RP}^n$, sus automorfismos 
vienen dados por:

\[Aut(\mathbb{RP}^n,p) = \{Id, -Id\}\]

****** Circunferencia, otros recubridores
Sean los recubridores $(\mathbb{S}^1,p_n)$ de $\mathbb{S}^1$. Sus grupos de automorfismos son:

\[Aut(\mathbb{S}^1,p_n) = \left\{\Phi_k(z) = e^{\frac{2\pi k}{n}i}z 
\mid k = 0,1,\dots,n-1 \right\}\]

***** Teorema de Borsuk-Ulam
No existe una aplicaciÃ³n continua $F : \mathbb{S}^2 \longrightarrow \mathbb{S}^1$ respetando antÃ­podas, es decir,
$F(-x) = -F(x)$.

**** 4. Recubridores regulares y espacios cocientes
***** AcciÃ³n transitiva de automorfismos de recubridores regulares
$Aut(Y,p)$ actÃºa transitivamente sobre $p^{-1}(x)$ ssi $(Y,p)$ es regular.

***** Acciones discontinuas
Un $G \subset Homeo(Y)$ *actÃºa discontinuamente* si:
 
\[\forall y\in Y: \exists V_y\text{ entorno}: \forall \Phi \in G:\quad
\Phi \neq Id \Rightarrow \Phi(V) \cap V = \varnothing\]

***** Recubrimiento de cocientes
Sea $G \subset Homeo(Y)$ actuando propia y discontinuamente sobre $Y$. Si
$p : Y \longrightarrow Y/G$ es proyecciÃ³n al cociente, $(Y,p)$ es recubridor regular de $Y/G$, 
con $Aut(Y,p) = G$.

***** Espacios lente
Sean las aplicaciones $\Phi_k : \mathbb{S}^{2n+1} \longrightarrow \mathbb{S}^{2n+1}$ definidas por:

\[\Phi_k(z) = e^{\frac{2\pi ik}{p}}z\]

Entonces $G_p = \{\Phi_0,\Phi_1,\dots,\Phi_{p-1}\}$ actÃºa discontinuamente. Llamamos *espacio 
lente* al cociente:

\[ L_p^{2n+1} = \mathbb{S}^{2n+1}/G_p\]

**** 5. Existencia de espacios recubridores
***** Caso del recubridor universal
Si $X$ admite recubridor universal, toda clase de conjugaciÃ³n de subgrupos de
$\Pi(X,x)$ estÃ¡ asociada a un recubridor.

***** Espacios semilocalmente simplemente conexos
Todo $x$ posee un entorno abierto y arcoconexo $U_x$ tal que el homomorfismo 
inducido por la inclusiÃ³n $\Pi(U_x,x) \longrightarrow \Pi(X,x)$ es trivial.

****** Contraejemplo
El espacio formado por infinitos cÃ­rculos de radio cada vez menor y unidos 
en un punto:

\[ X = \bigcup_{n>0} \left\{(x,y) \in \mathbb{R}^2 \mid 
\left(x-\frac{1}{n}\right)^2 + y^2 = \frac{1}{n^2} \right\}\]

cumple que cualquier entorno del $(0,0)$ contiene un lazo no trivial.

***** Existencia del recubridor universal
Un espacio semilocalmente simplemente conexo tiene recubridor universal.

***** Teorema de existencia de recubridores
Sea $X$ semilocalmente simplemente conexo. Para toda clase de conjugaciÃ³n de 
subgrupos de $\Pi(X,x)$, existe un recubridor que la tiene asociada.

****** DemostraciÃ³n
Sea $C$ clase de conjugaciÃ³n de algÃºn $H < \Pi(X,x)$. Sea $Y$ recubridor universal 
de $X$, que existe por ser semilocalmente simplemente conexo, cumpliendo 
$Aut(Y,p) \cong \Pi(X,x)$; puedo tomar $G < Aut(Y,p)$ cumpliendo $G \cong H$.

Los automorfismos del recubridor actÃºan discontinuamente, asÃ­ que $Y/G$ es un 
espacio del que es $Y$ recubridor. Induciendo la siguiente aplicaciÃ³n:

\[ \begin{tikzcd}
Y \rar{p} \dar{q} & X \\
Y/G \urar{\hat{p}} &
\end{tikzcd} \]

Que estÃ¡ bien definida por respetar la relaciÃ³n y es continua. Veamos que es 
un recubridor. Sea $U^x$ entorno fundamental, las arcocomponentes de su 
preimagen cumplen:

\[ p^{-1}(U) = \bigcup_{\phi\in Aut(Y,p)} \phi(V) \]

Si tenemos en cuenta que $\forall \phi \in G: q(\phi(V)) = q(V)$, tendremos:

\[\hat{p}^{-1}(U)
= \bigcup_{\phi \in Aut(Y,p)/G} q(\phi(V)) \]

Donde hay $[G:H]$ componentes. Ahora intentamos ver que $\hat{p}_\ast(\Pi(Y/G,-))$ genera
la clase de conjugaciÃ³n de $C$. Recordando cÃ³mo actuaban los caminos en el 
espacio base sobre los puntos del recubridor, buscamos los $\alpha\in\Pi(X,x)$ que 
cumplan $q(y)\cdot\alpha = q(y)$, es decir $\exists \phi\in G: y\cdot\alpha = \phi(y)$; luego tenemos $\alpha\in H$. Si 
tengo otro $\beta \in H$, debe cumplir $\phi(y) = y\cdot\beta$ para algÃºn $\phi\in G$, y entonces
$q(y)\cdot\alpha = q(y)$, siendo de los buscados. Tenemos:

\[\hat{p}(\Pi(Y/G, q(y)) = \operatorname{Stab}(q(y)) = H\]

***** ClasificaciÃ³n de recubridores del cÃ­rculo
El cÃ­rculo $\mathbb{S}^1$ tiene como grupo fundamental a $\mathbb{Z}$, que es abeliano y tiene como 
subgrupos a $n\mathbb{Z}$.

- $0$ tiene a la *recta* como recubridor universal $\mathbb{R}$ con $p(t) = e^{2\pi it}$.
- $n\mathbb{Z}$ tiene al *cÃ­rculo* como recubridor $\mathbb{S}^1$ de $n$ hojas con $p(z) = z^n$.
- $\mathbb{Z}$ tiene al recubridor *identidad*.

***** ClasificaciÃ³n de recubridores del toro
El toro $\mathbb{S}^1\times\mathbb{S}^1$ tiene como grupo fundamental a $\mathbb{Z}\times\mathbb{Z}$, que tiene como subgrupos 
a los generados por un generador $<(a,b)>$ o a los generados por dos, de la 
forma $<(a,b),(0,d)>$.

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ con $p(x,y) = (e^{2\pi ix},e^{2\pi iy})$.
- $<(a,b)>$ tiene al *cilindro* como recubridor $\mathbb{S}\times\mathbb{R}$ con $p(z,y) = (az, bze^{2\pi iy})$.
- $<(a,b),(0,d)>$ tienen al *toro* como recubridor $\mathbb{S}\times\mathbb{S}$ con
  $p(z,w) = (z^a,z^bw^d)$.

***** ClasificaciÃ³n de recubridores del cilindro
El cilindro $\mathbb{S}\times\mathbb{R}$ tiene como grupo fundamental a $\mathbb{Z}$, que es abeliano y tiene 
como subgrupos a $n\mathbb{Z}$.

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ con $p(x,y) = (e^{2\pi ix},y)$.
- $n\mathbb{Z}$ tiene al *cilindro* como recubridor universal con $p(z,y) = (e^{2\pi inz},y)$.
- $\mathbb{Z}$ tiene al recubridor *identidad*.

***** ClasificaciÃ³n de recubridores de la cinta de MÃ¶bius.
La cinta de MÃ¶bius tiene tipo de homotopÃ­a del cÃ­rculo y por tanto grupo 
fundamental $\mathbb{Z}$. 

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ sobre el que actÃºa el grupo
  $\phi_n(x,y) = (x+n, (-1)^ny)$ discontinuamente.
- $n\mathbb{Z}$ con $n = 2m$ par tiene al *cilindro* recubriÃ©ndose a sÃ­ mismo $m$ veces y 
  un recubrimiento de dos hojas del *cilindro* a la banda de MÃ¶bius.
- $n\mathbb{Z}$ con $n$ impar tiene a la *banda de MÃ¶bius* como recubridor.
- $\mathbb{Z}$ tiene al recubridor *identidad*.
*** 3. Superficies compactas
**** 3.0. ClasificaciÃ³n de variedades 1-dimensionales
***** Variedad 1-dimensional
Una variedad topolÃ³gica 1-dimensional es un espacio topolÃ³gico que
sea:

  - Conexo.
  - $T_2$, [[https://en.wikipedia.org/wiki/Hausdorff_space][Hausdorff]].
  - 2AN, [[https://en.wikipedia.org/wiki/Second-countable_space][segundo axioma de numerabilidad]].

y tal que:

\[\forall x \in X : \exists x \in U_x \in \tau:\quad
U_x \cong\: ]a,b[\]

***** Ejemplos
****** Los reales y el cÃ­rculo
$\mathbb{R}$ es variedad trivialmente. El cÃ­rculo $\mathbb{S}^1$ es tambiÃ©n una variedad.

****** Contraejemplo: lemniscata
Un espacio que se cruza consigo mismo formando una lemniscata
es un contraejemplo. No hay abierto homeomorfo a un entorno del cruce.

[[file:./images/lemniscata.svg]]

****** Contraejemplo: folium
Una curva inyectiva y continua que se aproxima a un punto sin contenerlo.
Ninguno de los entornos del punto crÃ­tico tiene un entorno abierto
homeomorfo a un intervalo.

#+begin_center
#+attr_latex: :width 50px
[[./images//folium.png]]
#+end_center

****** Contraejemplo de 2AN
No existe una base numerable para el punto $(0,0)$ si usamos la
topologÃ­a que une la recta izquierda con cada una de las rectas
derechas de forma separada, no podemos encontrar una base
numerable.

\[
X
=
\{(x,0) \mid x<0\}
\cup
\left(
\bigcup_{y \in \mathbb{R}\setminus\{0\}}
\{(x,y) \mid x \geq 0\}
\right)\]

#+begin_center
#+attr_latex: :width 50px
[[./images//2an.png]]
#+end_center

***** ClasificaciÃ³n de variedades topolÃ³gicas en dimensiÃ³n 1
Toda variedad topolÃ³gica 1-dimensional es homeomorfa a $\mathbb{R}$ o a $\mathbb{S}^1$.

**** 3.1. Variedades topolÃ³gicas
***** Espacio localmente euclÃ­deo
Un espacio $X$ es *localmente euclÃ­deo* cuando cada punto admite un
entorno abierto homeomorfo a un abierto de $\mathbb{R}^n$.

****** DefiniciÃ³n equivalente
Cada punto admite un entorno abierto homeomorfo a una bola 
abierta de $\mathbb{R}^n$.

****** Carta
Cada entorno con su homeomorfismo forma una *carta local* $(U_x,\phi)$.

****** Bola euclÃ­dea
Carta homeomorfa a la bola unidad euclÃ­dea.

***** Variedad topolÃ³gica
Una variedad topolÃ³gica es un espacio topolÃ³gico cumpliendo:

  1. Localmente euclÃ­deo.
  2. Hausdorff $T_2$.
  3. Segundo axioma de numerabilidad 2AN.

A una variedad de dimensiÃ³n 2 la llamamos *superficie*.

****** Axioma de Hausdorff
Todo par de puntos distintos tienen entornos que los separan. Es decir,
para $x \neq y$, existen $U_x \cap V_y = \varnothing$.

****** Segundo axioma de numerabilidad
Un espacio es 2AN si tiene una base numerable. Es decir, existe un
conjunto $\{U_n\}_{n \in \mathbb{N}}$ tal que todo abierto es uniÃ³n de ellos.

***** Ejemplos de variedades topolÃ³gicas
****** Los espacios euclÃ­deos
****** Las esferas n-dimensionales
****** Recubridores de espacios localmente euclÃ­deos
****** Recubiertos por espacios localmente euclÃ­deos
***** Teorema de la invarianza de la dimensiÃ³n
Si $U \subseteq \mathbb{R}^n$ y $V \subseteq \mathbb{R}^m$ son abiertos homeomorfos, $n=m$.
Por tanto cada variedad tiene una dimensiÃ³n asignada.

****** DemostraciÃ³n
Puede demostrarse comprobando grupos de homologÃ­a distintos para una
bola a la que retiramos un punto.

***** Base de las cartas
Los dominios de las cartas forman una base de la topologÃ­a.

****** DemostraciÃ³n
Usamos simplemente que la intersecciÃ³n de un abierto con
una carta es una carta con el homeomorfismo restricciÃ³n.

Dado un abierto, cada punto suyo tiene un entorno que es
una carta. Al intersecarlo con el abierto da otra carta,
y finalmente el abierto inicial es uniÃ³n de todas las
cartas en cada punto.

***** Bola regular euclÃ­dea
Sea $B \subseteq X$ una bola euclÃ­dea, es regular cuando:

  1. Existe $B' \subseteq X$ bola euclÃ­dea con $\overline{B} \subseteq B'$.
  2. Existe $r > 0$ y una carta $\phi : B' \longrightarrow \mathbb{D}(0,2)$ tal que $\phi(\overline{B}) = \overline{\mathbb{D}(0,1)}$.

****** Contraejemplos
Una esfera sin un punto es una bola euclÃ­dea pero no una bola
regular euclÃ­dea.

***** Base de las bolas regulares euclÃ­deas
Las bolas regulares euclÃ­deas forman una base de la topologÃ­a de una
variedad topolÃ³gica.

****** Existencia de bola regular euclÃ­dea
En una variedad topolÃ³gica sea $p \in S$ y $U \subseteq S$ entorno de $p$ abierto.
Existe una bola regular euclÃ­dea con $p \in B \subseteq U$.

******* DemostraciÃ³n
Es localmente euclÃ­deo, luego localmente habrÃ¡ una bola centrada
en la imagen de $p$ y otra de mitad de radio. Su preimagen serÃ¡
regular euclÃ­dea.

****** DemostraciÃ³n
En cualquier entorno existe una bola regular euclÃ­dea entre
el punto y el entorno.

***** Autohomeomorfismo de superficies conexas
Sea $S$ superficie conexa con $p,q \in S$. Entonces existe $f : S \overset{\cong}\longrightarrow S$
con $f(p) = q$.

****** Lema al endomorfismo
Existe una constante $\varepsilon \in ]0,1]$ tal que dado $x \in B(0,\varepsilon)$, existe
un $F : \mathbb{R}^2 \overset{\cong}\longrightarrow \mathbb{R}^2$ con:

  1. $F(0) = x$.
  2. $F|_{\mathbb{R}^2\setminus D(0,2)}$ es la identidad.

******* DemostraciÃ³n
Tenemos funciones infinitamente diferenciables y no nulas que nos
permiten crear funciones que muevan un punto en otro dejando todo
el resto del plano igual.

******** TODO ConstrucciÃ³n explÃ­cita
****** DemostraciÃ³n
Consideremos la relaciÃ³n de equivalencia $pRq$ cuando $\exists f : S \cong S$
con $f(p) = q$. Veamos que $[p]$ es abierta por $p$ arbitrario y por
tanto, $[p] = S$.

******* La clase de equivalencia es abierta
Dado $p$, tenemos $O$ disco [[*Base de las bolas regulares euclÃ­deas][regular]] centrado en Ã©l y $O'$ disco 
euclÃ­deo cubriendo su clausura y cumpliendo:

\[
\exists \phi : O' \overset{\cong}\longrightarrow D(0,\varepsilon)
\]

con $\phi(O) = \overline{D(0,\varepsilon/2)}$. El lema nos da una $F_x(0) = x$.

Definimos $D_0 = \phi^{-1}(D(0,\varepsilon)) \subset D'$, abierto cubriendo a $p$. Y para
cualquier $y \in D_0$, definimos la funciÃ³n $G_y : S \cong S$ como:

\[
G_y(z) = \left\{\begin{array}{ll} 
z & \mbox{if } z \notin O'  \\
\phi^{-1} \circ F_{\phi(y)} \circ \phi& \mbox{if } z \in O'
\end{array} 
\right.
\]

Lo que nos da $D_0 \subseteq [p]$, haciÃ©ndolo abierto.

***** Recubridor de una superficie
Sea $\pi : \widetilde{X} \longrightarrow X$ recubridor:

  1. Si $\widetilde X$ es una superficie y $A_\pi = \{(x,y) \in \widetilde{X} \times \widetilde{X} \mid \pi(x) = \pi(y) \}$
     es cerrado, entonces $X$ es superficie.
  2. Si $X$ es superficie, $\widetilde{X}$ es superficie.

****** DemostraciÃ³n
******* Localmente euclÃ­dea
La aplicaciÃ³n recubridora es localmente homeomorfismo, por lo
que lleva en ambas direcciones el ser localmente euclÃ­deo.

******* TODO Hausdorff

******* TODO Axioma de numerabilidad

**** 3.2. Complejos simpliciales
***** P-sÃ­mplice
Sean $v_1,\dots,v_p \in \mathbb{R}^n$ afÃ­nmente independientes. Se define el *p-sÃ­mplice*
generado como:

\[
\langle v_0,\dots,v_p \rangle
=
\left\{\;
\sum_{j=0}^p \lambda_jv_j 
\;\middle|\;
0 \leq \lambda_j \leq 1, \sum_{j=0}^p \lambda_j = 1
\;\right\}
\]

Llamamos a los $v_i$ *vÃ©rtices* del p-sÃ­mplice y al entero $p$ se le llama
*dimensiÃ³n* del p-sÃ­mplice.

****** P-sÃ­mplice abierto
Se define el *p-sÃ­mplice abierto* como:

\[
{\cal O}(\langle v_0,\dots,v_p \rangle)
=
\left\{
\sum_{j=0}^p \lambda_jv_j 
\;\middle|\;
0 < \lambda_j \leq 1, \sum_{j=0}^p \lambda_j = 1
\right\}
\]

Observemos que en general no es igual al interior de un p-sÃ­mplice,
que puede ser vacÃ­o en una dimensiÃ³n alta.

****** Caras de un p-sÃ­mplice
Las *caras* de un p-sÃ­mplice $\sigma$ son los k-sÃ­mplices generados por $k+1$ 
de sus vÃ©rtices. Se notan por $\tau < \sigma$.

Llamamos *aristas* a las caras de dimensiÃ³n 1 y *triÃ¡ngulos* a las 
caras de dimensiÃ³n 2.

***** Complejo simplicial
ColecciÃ³n de sÃ­mplices $K$ en algÃºn $\mathbb{R}^n$ cumpliendo:

  1. $\tau < \sigma, \sigma \in K \implies \tau \in K$.
  2. $\sigma,\tau \in K,\; \sigma \cap \tau \neq \varnothing$ $\implies$ $\sigma \cap \tau < \sigma$ y $\sigma \cap \tau < \tau$.
  3. Todo punto en el sÃ­mplice tiene un entorno abierto que corta a
     una cantidad finita de sÃ­mplices en $\bigcup_{\tau \in K}\tau \subseteq \mathbb{R}^n$.

Llamamos *dimensiÃ³n* de $K$ a la dimensiÃ³n del mayor sÃ­mplice que 
contiene.

****** Subcomplejo simplicial
Llamamos subcomplejo simplicial a $K' \subseteq K$ complejo simplicial.

***** Poliedro
Dado $K$ complejo simplicial, llamamos *poliedro* de $K$ al espacio
topolÃ³gico formado por la uniÃ³n de todos los sÃ­mplices de $K$ con 
la topologÃ­a inducida:

\[|K| = \bigcup_{\tau \in K} \tau \subseteq \mathbb{R}^n\]

***** Un compacto corta una cantidad finita de sÃ­mplices
Sea $G \subseteq |K|$ compacto, entonces $G$ corta a una cantidad finita de
sÃ­mplices de $K$. En particular $|K|$ compacto tiene una cantidad 
finita de sÃ­mplices.

****** DemostraciÃ³n
Supongamos $\{\tau_n\}$, tomamos $\{p_n\} \in G \cap \tau_n$. Por compacidad habrÃ­a 
una parcial convergente $\{p_n\} \longrightarrow p_\infty \in G \subseteq |K|$.

Pero por definiciÃ³n de complejo simplicial, este $p_\infty$ tiene un
entorno que corta sÃ³lo a una cantidad finita de sÃ­mplices y por
tanto, de los $p_n$.

***** Aplicaciones simpliciales
Una $f : |K|\longrightarrow |L|$ entre complejos simpliciales es *aplicaciÃ³n simplicial* 
si:

  1. $f$ lleva sÃ­mplices de $K$ en sÃ­mplices de $L$.
  2. La restricciÃ³n de $f$ a cada sÃ­mplice es afÃ­n $Ax+b$.

****** Homeomorfismos simpliciales
Una aplicaciÃ³n simplicial homeomorfismo la llamamos 
*homeomorfismo simplicial*.

****** CategorÃ­a de las aplicaciones simpliciales
La inversa de un homeomorfismo simplicial es homeomorfismo simplicial;
asÃ­ como la identidad y la composiciÃ³n de funciones. Las aplicaciones
simpliciales forman una categorÃ­a.

******* TODO DemostraciÃ³n

***** Superficies triangulables
Una superficie topolÃ³gica $S$ se dice *triangulable* si existe algÃºn
complejo simplicial de dimensiÃ³n $2$ con $S \cong |K|$.

***** Teorema de RadÃ³
Toda superficie admite una triangulaciÃ³n por un complejo simplicial
euclÃ­deo de dimensiÃ³n 2 donde cada 1-sÃ­mplice es cara de exactamente
dos sÃ­mplices.

****** RecÃ­proco falso
No todo complejo simplicial es triangulaciÃ³n de una superficie.

***** CaracterizaciÃ³n de triangulaciones
Un complejo simplicial $K$ de dimensiÃ³n 2 es triangulaciÃ³n de una
superficie ssi:

  1. Todo sÃ­mplice es cara de un 2-sÃ­mplice.
  2. Todo 1-sÃ­mplice es cara de exactamente dos 2-sÃ­mplices.
  3. Si $\forall v \in K^{(0)}$ definimos $st(v) = \{ \tau \in K \mid v < \tau \}$ y

     \[ L(v) = \{ \tau \in K \mid \exists \sigma \in K : v < \sigma, \tau < \sigma, v \not< \tau
     \}
     \]

     y tenemos que $|st(v)| \cong \overline{D}$ y $|L(v)|$ es conexo.

****** DemostraciÃ³n
No se explica en la asignatura.

**** 3.3. Suma convexa
***** Curva de Jordan
Una curva de Jordan en $\mathbb{R}^n$ es una curva cerrada y simple.

****** DefiniciÃ³n equivalente
Una curva de Jordan es la imagen de $f : \mathbb{S}^1 \longrightarrow \mathbb{R}^n$ que sea
homeomorfismo sobre su imagen.

***** Homeomorfismo que preserva la orientaciÃ³n
Sean $C_1,C_2 \subseteq \mathbb{R}^2$ dos curvas de Jordan y $f : C_1 \overset{\cong}\longrightarrow C_2$ un homeomorfismo.
Decimos que $f$ *preserva la orientaciÃ³n* si $\exists \alpha : [0,1] \longrightarrow C_1$ parametrizaciÃ³n
tal que $\alpha$ recorre $C_1$ en el sentido de las agujas del reloj y $f \circ \alpha$
recorre $C_2$ en el sentido de las agujas del reloj.

***** Teorema de Jordan-SchÃ¶nflies
Sean $C_1,C_2$ curvas de Jordan con $f : C_1 \cong C_2$. Entonces existe una
$F : \mathbb{R}^2 \cong \mathbb{R}^2$ con $F|_{C_1} = f$. AdemÃ¡s, 

  1. Si $f$ preserva la orientaciÃ³n y $U \subseteq \mathbb{R}$ es un subconjunto 
     homeomorfo a $\mathbb{D}$ disco, con $U \supseteq C_1 \cup C_2$, entonces existe
     un $F : \mathbb{R}^2 \cong \mathbb{R}^2$ con $F|_{C_1} = f$ y $F|_{\mathbb{R}^2\setminus U} = Id$.
  2. Si $f$ revierte la orientaciÃ³n y $U \subseteq \mathbb{R}^2$ con $U \supseteq C_1 \cup C_2$
     con $U \cong \mathbb{D}$ disco, puedo tener $F$ con: $F|_{\mathbb{R}^2\setminus U}(x,y) = (x,-y)$.

****** DemostraciÃ³n
No trivial

******* Segundo punto desde el primero
Si tengo $f : C_1 \cong C_2$, revirtiendo la orientaciÃ³n, puedo tomar
la curva reflejada en el eje Y:

\[\widetilde{C_2} = \{(x,y) \mid (x,-y) \in C_2 \}\]

Y definir una $\widetilde{f} : C_1 \cong \widetilde{C_2}$ como $\widetilde{f} = (f_1,-f_2)$, que preserva la 
orientaciÃ³n. El teorema nos da entonces una $\widetilde{F}$ desde la que podemos
definir $F = (\tilde F_1, \tilde F_2)$, la buscada.

***** Teorema de la curva de Jordan
Sea $C$ curva de Jordan en $\mathbb{R}^2$. Entonces, $\mathbb{R}^2\setminus C$ tiene exactamente
dos componentes conexas, una de ellas acotada (*interior* de $C$) y
la otra es no acotada (*exterior* de $C$).

****** DemostraciÃ³n
Dado un $f : C \cong \mathbb{S}^1$ homeomorfismo, por Jordan-SchÃ¶nflies, tenemos
una $F : \mathbb{R}^2 \cong \mathbb{R}^2$ extendiÃ©ndola, lo que nos da $\mathbb{R}^2\setminus C_1 \cong \mathbb{R}^2 \setminus \mathbb{S}^1$.

****** Contraejemplo en dimensiones superiores: esfera de Alexander
La [[https://es.wikipedia.org/wiki/Esfera_cornuda_de_Alexander][esfera cornuda de Alexander]] es una 2-esfera embebida en $\mathbb{R}^3$
cuyo exterior no es homeomorfo al exterior de la 2-esfera en $\mathbb{R}^3$.

***** Teorema de Jordan-Schonflies para la esfera
Sean $C_1,C_2$ dos curvas de Jordan en $\mathbb{S}^2 \subseteq \mathbb{R}^3$ y $f : C_1 \overset{\cong}\longrightarrow C_2$.
Entonces $\exists F : \mathbb{S}^2 \cong \mathbb{S}^2$ con $F|_{C_1} = f$.

****** DemostraciÃ³n
Si tomamos $p \in \mathbb{S}^2 \setminus \{C_1 \cup C_2\}$, tenemos $\mathbb{S}^2 \setminus \{p\} \cong \mathbb{R}^2$ por la proyecciÃ³n
estereogrÃ¡fica. Tomamos el $F$ que da el teorema de Jordan-SchÃ¶nflies
y lo componemos con la inversa de la proyecciÃ³n para tener el $F$.

Dividimos en los dos casos para ver que el $F$ es continuo en el
punto impropio.

***** Suma conexa
Sean $S_1,S_2$ dos superficies convexas. Sean $D_1 \subseteq S_1$, $D_2 \subseteq S_2$ discos
regulares euclÃ­deos y sea $\varphi : \partial D_1 \longrightarrow \partial D_2$ homeomorfismo. 

Denotemos $S'_i = S_i \setminus D_i$. En $S'_1 \sqcup S_2'$ definimos $R_{\varphi}$ relaciÃ³n de equivalencia 
entre el borde y su imagen:

\[ x\; R_\varphi \; \varphi(x)\quad \forall x \in \partial D_1\]

Tenemos una superficie topolÃ³gica conexa que es independiente de los
discos y el homeomorfismo elegidos. La llamamos *suma conexa*:

\[
S_1 \# S_2
=
S_1' \sqcup S_2' / R_\varphi
\]

***** La suma es independiente de los discos
Para $D_1,D_3 \subseteq S'$ discos regulares euclÃ­deos centrados en $p_1$, tenemos
que:

\[S_1 \setminus D_1 \cong S_1 \setminus D_3\]

****** DemostraciÃ³n
Sin pÃ©rdida de generalidad, $\overline{D_1} \subset D_3$. Sea $\psi$ el que hace bola [[*Bola regular euclÃ­dea][regular]]
a $D_3$. Definimos:

\[ C_3 = \psi(\partial D_3)\]
\[C_1 = \psi(\partial D_1)\]

Tomamos un $f : C_1 \cong C_3$ y Jordan-SchÃ¶nflies nos da un $F$.

Definimos ahora por partes $\widetilde F$ como:

\[\widetilde{F}|_{D_3'} = \psi^{-1} \circ F \circ \psi\]
\[\widetilde{F}|_{S_1 \setminus D_3'} = Id\]

Comprobamos que estÃ¡ bien definida y es homeomorfismo, por lo que
tenemos $\widetilde F : S_1\setminus D_1 \cong S_3 \setminus D_3$.

***** La suma es independiente de los centros
Los puntos en los que centramos la suma conexa. Tomando $p_1 \neq p_3 \in S_1$,
con $D_1,D_3$ discos regulares euclÃ­deos centrados en ellos:

\[S_1 \setminus D_1 \cong S_3 \setminus D_3\]

****** DemostraciÃ³n
Ya tenemos que hay un [[*Autohomeomorfismo de superficies conexas][automorfismo]] que centra un punto en otro.
Como los discos regulares son base de la topologÃ­a, existirÃ¡ un
$\overline{D_3''} \subseteq D_3$ y suficientemente pequeÃ±o para que $F(D_3'') \subseteq D_1$.

Como $F(D_3'')$ es disco regular euclÃ­deo:

\[
S_1/D_3 \cong 
S_1/D_3'' \cong
S_1/F(D_3'') \cong
S_1/D_1
\]

***** La suma es independiente del homeomorfismo
El homeomorfismo no influye. Dados $\varphi, \xi : bD_1 \longrightarrow bD_2$ homeomorfismos,
queremos ver:

\[
S_1 \sqcup S_2 / R_\varphi \cong S_1 \sqcup S_2 / R_\xi
\]

****** DemostraciÃ³n
Suponemos s.p.g. que $\xi \circ \varphi^{-1}$ preserva la orientaciÃ³n. Como $D_2$ es disco
regular euclÃ­deo, hay un $D_2' \subset S_2$ euclÃ­deo con $\psi_2 : D_2' \longrightarrow \mathbb{D}(0,2)$:

\[
\psi_2(D_2') = \overline{\mathbb{D}(0,1)}
\]

Llamamos:

  1. $C = \psi_2(bD_2) = \mathbb{S}^1$, curva de Jordan.
  2. $f = \psi_2 \circ \xi \circ \varphi^{-1} \circ \psi_2^{-1} : C \cong C$.
  3. $U = \mathbb{D}(0,3/2)$.

Por [[*Teorema de Jordan-SchÃ¶nflies][Jordan-SchÃ¶nflies]], tenemos un $F : \mathbb{R}^2 \cong \mathbb{R}^2$ con $F|_G = f$, $F|_{\mathbb{R}^2-U} = Id$.

Definimos ahora $\widetilde F : S_2 \cong S_2$ cumpliendo $\widetilde F|_{D_2'} = \psi_2^{-1} \circ F \circ \psi$ y teniendo
tambiÃ©n $\widetilde F|_{S_2-D_2'} = Id$.

Finalmente definimos $\widehat F : S_1 \sqcup S_2 \cong S_1' \sqcup S_2'$ con $\widehat F|_{S_1'} = Id, \widehat F|_{S_2'} = \widetilde F$.
El siguiente diagrama nos da el homeomorfismo:

\[\begin{tikzcd}
S_1' \sqcup S_2'           \rar{\widehat F}\dar{p_\xi}& 
S_1' \sqcup S_2'           \dar{p_\varphi} \\
S_1' \sqcup S_2'/R_\varphi \rar[dashed] & 
S_1' \sqcup S_2'/R_\xi
\end{tikzcd}\]

Ya que dados dos puntos $x,y$, si estÃ¡n fuera del borde su relaciÃ³n es
trivial, y si estÃ¡n en el borde cumplen $y = \varphi(x)$:

  - $\widehat F(x) = x \in bD_1 \subseteq S_1'$
  - $\widehat F(y) = \widetilde F(y) \in bD_2 \subseteq S_2'$

Luego basta comprobar que:

\[\widetilde F(y) = 
\widetilde F(\varphi(x)) =
\psi_2^{-1} (F(\psi_2(\varphi(x)))) =
\psi_2^{-1} (f(\psi_2(\varphi(x)))) =
\xi (x)
\]

***** La suma conexa de superficies conexas es superficie conexa
La suma conexa de dos superficies topolÃ³gicas conexas es una
superficie topolÃ³gica conexa.

****** DemostraciÃ³n
Sean $S_1,S_2,D_1,D_2,\varphi$ las superficies y los discos que definen
la suma conexa:

\[
S_1 \# S_2 =
S_1' \sqcup S_2' / R_\varphi
\]

******* Es localmente euclÃ­dea
******** Fuera de los discos
En los puntos fuera de los discos, es trivial por ser $S_1,S_2$
localmente euclÃ­deas. Sea $p : S_1' \sqcup S_2' \longrightarrow S_1 \# S_2$, como es un 
homeomorfismo fuera del borde, llamamos:

\[
V_i = p(S_i' - bD_i') \subseteq S_1 \# S_2
\]

Y como tenemos por otro lado:

\[
p^{-1}(V_i) = S_i'-bD_i = (S_i-\overline{D_i})\cap S_i'
\]

Llegamos a que $p^{-1}(V_i)$ es abierto en $S_i'$ y por tanto $V_i$ es abierto
en $S_1 \# S_2$. Tenemos $V_i \cong S_i' - bD_i$ localmente euclÃ­deo.

******** TODO En los discos
Sea $q \in p(bD_1) = p(bD_2)$. Como son discos regulares euclÃ­deos, se
tienen $D_1',D_2'$ con cartas $\psi_i(D_1') = \mathbb{D}(0,2)$, $\psi(D_1) = \overline{\mathbb{D}(0,1)}$.

Pegamos ambos discos y comprobamos que son homeomorfos a un abierto
del plano.

******* Cumple Hausdorff
El Ãºnico caso no trivial es el de $x \in V_0$, $y \in V_1 - V_0$. Como podemos tomar
$V_0$ abitrariamente pequeÃ±o, tenemos s.p.g. $y \notin \overline{V_0}$. AsÃ­, hay un abierto de $y$
que no corta a $\overline{V_0}$, y como $V_0$ era un abierto, hay otro abierto de $x$ que
queda dentro de Ã©l.

******* Cumple axioma de numerabilidad
El que exista una proyecciÃ³n abierta, continua y sobreyectiva desde un
espacio 2AN hace al espacio 2AN.

**** 3.4. PresentaciÃ³n poligonal de superficies
***** PolÃ­gono homeomorfo
Toda superficie topolÃ³gica compacta es homeomorfa a un polÃ­gono con lados
identificados 2 a 2.

****** Idea de demostraciÃ³n
Dada $S$ compacta, por un teorema que no demostramos, $S \cong |K|$.
Por [[*Teorema de RadÃ³][RadÃ³]], podemos triangularlo y por compacidad, tiene un nÃºmero
finito de caras. Ese polÃ­gono puede desplegarse.

***** RegiÃ³n poligonal
Un $P \subseteq \mathbb{R}^2$ es regiÃ³n poligonal si:

  1. Es compacta.
  2. Tiene por borde una curva poligonal (complejo de dimensiÃ³n 1).
  3. Cada $v \in \operatorname{b}(P)$ vÃ©rtice admite entorno $U \subseteq \mathbb{R}^2$ tal que:

     \[U \cap P = U \cap H_1 \cap H_2\]

     donde $H_1,H_2$ son semiplanos cerrados con $H_1 = H_2$ o $\operatorname{b}H_1 \cap \operatorname{b}H_2 = \{v\}$.

****** Ejemplos
******* PolÃ­gonos regulares
Toda regiÃ³n compacta, convexa y bordeada por una curva poligonal
es una regiÃ³n poligonal.

******* Contraejemplo: polÃ­gono no convexo
Un polÃ­gono no convexo no cumple la propiedad 3 en cualquier
vÃ©rtice que no estÃ© en la envolvente.

***** Superficie de una regiÃ³n poligonal
Sea $P \subseteq \mathbb{R}^2$ regiÃ³n poligonal con un nÃºmero par de aristas. Si $R$ es relaciÃ³n 
de equivalencia, identificando cada arista con exactamente otra arista
mediante homeomorfismo simplicial, $P/R$ es una superficie topolÃ³gica 
compacta.

****** DemostraciÃ³n
******* Compacta
Sabemos que $P$ es compacta, como $p$ es continua y sobreyectiva,
tenemos $P/R$ compacto.

******* Localmente euclÃ­dea
Consideramos tres casos:

******** Interior del polÃ­gono
Tenemos que la proyecciÃ³n es homeomorfismo local.

******** Interior de las aristas
Tenemos $q_i \in a_i$ identificadas por un homeomorfismo simplicial.
Podemos crear un disco centrado en $\mathbb{R}^2$ uniendo dos discos y que
la relaciÃ³n que define esa proyecciÃ³n sea la misma que la que
define la equivalencia de aristas.

******** VÃ©rtices
Trabajamos dependiendo de con cuÃ¡ntos puntos estÃ© identificado
$p^{-1}(x) = \{v_1,\dots,v_k\}$. Sean $D_i$ discos abiertos en $\mathbb{R}^2$ para cada 
vÃ©rtice cumpliendo $\overline{D_i} \cap \overline{D_j} = \varnothing$ y llamamos $U_i = D_i$.

Creamos un disco uniendo todos las partes de disco y comprobamos
que la relaciÃ³n que defina sea la misma que la de la proyecciÃ³n.

******* TODO Segundo axioma de numerabilidad

******* TODO Espacio de Hausdorff

***** PresentaciÃ³n poligonal
Llamamos presentaciÃ³n a una expresiÃ³n de la forma:

\[\langle A;\; W_1,\dots,W_n \rangle\]

donde $W_i$ son palabras con todos los sÃ­mbolos $\langle A \rangle$ de longitud mayor 
que 3. Permitimos ademÃ¡s los casos especiales:

  - $\langle \{a\}, aa \rangle$
  - $\langle \{a\}, aa^{-1} \rangle$
  - $\langle \{a\}, a^{-1}a^{-1} \rangle$
  - $\langle \{a\}, a^{-1}a \rangle$

***** PresentaciÃ³n de un complejo simplicial
Dado un complejo simplicial $K$ donde cada sÃ­mplice es cara de un
2-sÃ­mplice, tenemos una presentaciÃ³n poligonal asignando a cada
2-sÃ­mplice una palabra de longitud 3.

***** RealizaciÃ³n geomÃ©trica de una presentaciÃ³n
Cada $P = \langle A; W_1,\dots,W_n \rangle$ determina $|P|$, una topologÃ­a de la forma:

  1. Para cada $W_i$ tomamos un polÃ­gono $P_i$ de n-lados.
  2. BiyecciÃ³n entre lados y aristas de $P_i$ en sentido antihorario.
  3. Identificamos cada arista con la que tiene el mismo nombre en
     el sentido marcado por la inversa.

Tomamos la uniÃ³n por esta relaciÃ³n $|P| = \bigsqcup P_i / R$.

***** ExtensiÃ³n a homeomorfismo
Para $P_1,P_2$ polÃ­gonos convexos con el mismo nÃºmero de aristas.
El $f : \operatorname{b}P_1 \longrightarrow \operatorname{b}P_2$ homeomorfismo simplicial se extiende a un 
homeomorfismo $F : P_1 \longrightarrow P_2$.

****** TODO DemostraciÃ³n

***** PresentaciÃ³n de superficie
Una presentaciÃ³n $\langle A; W_1,\dots,W_n \rangle$ es de superficie si 
cada sÃ­mbolo de $A$ aparece exactamente dos veces en los $W_i$.

***** PresentaciÃ³n asociada a una superficie
Si $S$ es superficie compacta y $|P| \cong S$, llamamos a $P$ una
*presentaciÃ³n* de $S$.

***** Presentaciones topolÃ³gicamente equivalentes
Dos presentaciones $P_1,P_2$ son topolÃ³gicamente equivalentes si:

\[ |P_1| \cong |P_2|\]

***** Transformaciones elementales de presentaciones poligonales
Dado $\langle A; W_1,\dots,W_n \rangle$, podemos definir las siguientes transformaciones
elementales.

****** 1. Renombrar
Cambiar un sÃ­mbolo $a \in A$ por $e \notin A$ para cada palabra.

****** 2. Subdividir
Sustituir $a \mapsto ae$ y $a^{-1} \mapsto e^{-1}a^{-1}$.

****** 3. Consolidar
Inversa de subdividir.

****** 4. Reflejar
Cambiar orden de una palabra $a_1\dots a_m \mapsto a_m^{-1}\dots a_1^{-1}$.

****** 5. Rotar
Empezar una palabra en otro vÃ©rtice $a_1\dots a_m \mapsto a_2\dots a_ma_1$.

****** 6. Cortar
Partir dos palabras $W_1W_2 \mapsto W_1e,e^{-1}W_2$.

****** 7. Pegar
Inversa de cortar.

****** 8. Doblar
Anular dos lados adyacentes $W_1ee^{-1} \mapsto W_1$.

****** 9. Desdoblar
Inversa de doblar.

***** Transformaciones elementales preservan la realizaciÃ³n
Las transformaciones elementales de una presentaciÃ³n poligonal
producen una presentaciÃ³n poligonal equivalente.

****** DemostraciÃ³n
******* Renombrar
Trivialmente.

******* TODO Subdividir/Consolidar
******* Reflejar
Una aplicaciÃ³n que lleve cada arista en la inversa del mismo nombre
respetarÃ¡ vÃ©rtices.

******* Cortar/Pegar
Probaremos que $\langle A; W_1W_2 \rangle \cong \langle A\cup \{e\} ; W_1e,e^{-1}W_2 \rangle$. Llamamos $P'$ al 
sÃ­mplice generado por la primera y $P_1,P_2$ a los generados por las
segundas. Sea $f$ una aplicaciÃ³n simplicial que une la arista de corte:

\[\begin{tikzcd}
P_1 \sqcup P_2 \dar{p} \rar{f} & 
P' \dar{p'}
\\
\lvert P\rvert \rar{\widetilde f} &
\lvert P' \rvert
\end{tikzcd}\]

Sabemos que $f$ baja al cociente de manera continua y $\widetilde f$ es cerrada
por ir de un compacto a un cerrado. AdemÃ¡s es biyectiva y por
tanto homeomorfismo.

En caso de que hubiera mÃ¡s de dos palabras podrÃ­amos extenderla
por la identidad y seguir el mismo razonamiento.

******* Doblar/Desdoblar
******** Caso triangular
Definimos una aplicaciÃ³n simplicial respetando el nombre de las
aristas:

#+begin_center
#+attr_latex: :width 50px
[[./images//doblartransformacion.png]]
#+end_center

Esa aplicaciÃ³n simplicial baja de forma continua y genera una
aplicaciÃ³n biyectiva entre compacto y cerrado.

******** Caso general
Podemos cortar o subdividir para llegar al caso triangular.

***** PresentaciÃ³n poligonal de la suma
Dadas $S_1,S_2$ con presentaciones $P_1 = \langle A_1; W_1 \rangle$ y $P_2 = \langle A_2;W_2 \rangle$ con
$A_1 \cap A_2 = \varnothing$. Entonces una presentaciÃ³n de $S_1 \# S_2$ es:

\[
\langle A_1 \cup A_2 ; W_1 W_2 \rangle
\]

****** DemostraciÃ³n
Desdoblamos y cortamos para llegar a la presentaciÃ³n:

\[
\langle W_1c^{-1}b^{-1}a^{-1}, abc \rangle
\]

De donde salen dos complejos simpliciales $P$ y $Q$. El segundo de
ellos se proyecta a un disco regular euclÃ­deo.

\[ D_1 = p'(\mathring Q) \subseteq |P'| \]

Mientras que el primero se proyecta al polÃ­gono sin un disco cuyo
borde es $bD_1 = p(c^{-1}b^{-1}a^{-1})$.

\[
p'(P) \cong S_1 \setminus D_1
\]

Aplicando el mismo argumento a $W_2$, llegamos a una presentaciÃ³n de
$S_2\setminus D_2$, que al identificar ambos bordes permite unirlos pegando
y doblando:

\[
\langle W_1c^{-1}b^{-1}a^{-1}, abc W_2 \rangle \cong
\langle W_1W_2 \rangle
\]

***** Presentaciones modelo
****** Esfera
La presentaciÃ³n de la esfera $\mathbb{S}^2$ es:

\[P_0 = \langle a \mid aa^{-1} \rangle\]

****** Suma de toros
La presentaciÃ³n de la suma de toros $\mathbb{T}^{\#n} = \mathbb{T} \# \overset{n}\dots \#\mathbb{T}$ es:

\[
P_n = \langle
a_1,b_1,\dots,a_n,b_n 
\mid 
a_1b_1a_1^{-1}b_1^{-1}\dots a_nb_na_n^{-1}b_n^{-1} 
\rangle
\]

****** Suma de planos proyectivos
La presentaciÃ³n de la suma de planos proyectivos $\mathbb{RP}^{2\#n}$ es:

\[ Q_n =
\langle a_1,\dots,a_n \mid a_1a_1\dots a_na_n \rangle
\]
**** 3.5. ClasificaciÃ³n de superficies compactas I
***** Aristas retorcidas y complementarias
Un par de aristas en una presentaciÃ³n se dicen retorcidas si
aparecen como $a$ y $a$; y complementarias si aparecen como $a$ y $a^{-1}$.

***** Lema: botella de Klein
La botella de Klein $K$ es homeomorfa a la suma conexa de dos planos
proyectivos:

\[
K \cong \mathbb{RP}^2 \# \mathbb{RP}^2
\]

****** DemostraciÃ³n
Simplemente comprobando que sus presentaciones son equivalentes:

\[
\langle abab^{-1} \rangle \cong \langle ccbb \rangle
\]

***** Lema: suma de toro y plano proyectivo
La suma conexa de un toro y un plano proyectivo es homeomorfa a la 
suma conexa de 3 planos proyectivos:

\[
\mathbb{T} \cong \mathbb{RP}^2 \# \mathbb{RP}^2 \# \mathbb{RP}^2
\]

****** DemostraciÃ³n
Comprobando que sus presentaciones son equivalentes:

\[
\langle aba^{-1}b^{-1}cc \rangle \cong \langle aabbcc \rangle
\]

Podemos usar el [[*Lema: botella de Klein][lema]] anterior y ver que:

\[\begin{aligned}
\langle abab^{-1}cc \rangle 
&=
\langle cabd^{-1},dab^{-1}c \rangle 
\\&=
\langle abd^{-1}ba^{-1}d^{-1} \rangle
\\&=
\langle a^{-1}d^{-1}abe,e^{-1}d^{-1}b \rangle
\\&=
\langle a^{-1}d^{-1}abe,e^{-1}d^{-1}b \rangle
\\&=
\langle a^{-1}d^{-1}abe,b^{-1}de \rangle
\\&=
\langle ea^{-1}d^{-1}ade \rangle
\end{aligned}\]

***** Teorema de clasificaciÃ³n de presentaciones de superficies compactas
Cualquier presentaciÃ³n poligonal de una superficie compacta y conexa
es equivalente a una de las siguientes:

  1. $P_0$, presentaciÃ³n de $\mathbb{S}^2$.
  2. $P_n$, presentaciÃ³n de $\mathbb{T}^{\#n}$.
  3. $Q_n$, presentaciÃ³n de $\mathbb{RP}^{2\#n}$.

****** DemostraciÃ³n
Sea $S$ superficie con presentaciÃ³n $P$.

******* Paso 1: Reducir a una cara
Como el cociente es arcoconexo, si hay mÃ¡s de una palabra, comparten
entre sÃ­ alguna letra. Rotamos, reflejamos si es necesario y pegamos
para tener una sola cara.

******* Paso 2: Retirar complementarias adyacentes
Dos lados adyacentes pueden retirarse.

******* Paso 3: Colocar aristas retorcidas adyacentes
Dada una palabra de la forma $WaVa$:

  1. Cortamos: $Wab^{-1},bVa$.
  2. Reflejamos y rotamos: $b^{-1}Wa, a^{-1}V^{-1}b^{-1}$.
  3. Pegamos y rotamos: $b^{-1}b^{-1}WV^{-1}$.

******* Paso 4: Identificar todos los vÃ©rtices
Fijado un vÃ©rtice, puedo cortar un triÃ¡ngulo que lo contenga y
volver a pegar por uno de sus lados para identificar otro vÃ©rtice
con Ã©l.

******* Paso 5: Las complementarias tienen complementarias intercaladas
Para un par de complementarias $a,a^{-1}$, hay otro par de complementarias
intercalado como: $a \dots b \dots a^{-1} \dots b^{-1}$.

Si no fuera asÃ­, tendrÃ­amos $aXa^{-1}Y$ sin forma de relacionar ningÃºn
vÃ©rtice de $X$ con un vÃ©rtice de $Y$.

******* Paso 6: Las intercaladas pueden presentarse en bloques
Si tenemos una palabra de la forma $WaXbYa^{-1}Zb^{-1}$:

  1. Cortando antes de $b$ y pegando por $a$: $XcWZb^{-1}c^{-1}bY$.
  2. Cortando antes de $c$ y pegando por $b$: $d^{-1}WZYXcdc^{-1}$.

Luego tenemos: $WaXbYa^{-1}Zb^{-1} \longrightarrow WZYXcdc^{-1}d^{-1}$.

******* Paso 7: Nos queda suma de planos proyectivos y toros
Nos acaba quedando una palabra de la forma: 

\[aabb\dots cdc^{-1}d^{-1}\dots\]

Suma conexa de toros y planos proyectivos. Por el [[*Lema: suma de toro y plano proyectivo][lema]],
sabemos que serÃ¡ suma de toros o suma de planos proyectivos.

***** PreclasificaciÃ³n de superficies compactas
Toda superficie compacta y conexa es homeomorfa a al menos una de las 
siguientes:

  1. $\mathbb{S}^2$
  2. $\mathbb{T}^{\#n}$
  3. $\mathbb{RP}^{2\#n}$

****** DemostraciÃ³n
Cada superficie compacta es triangulable por [[*Teorema de RadÃ³][RadÃ³]], con cada 1-sÃ­mplice
siendo cara de dos 2-sÃ­mplices. Todo complejo simplicial tiene despuÃ©s
una [[*PresentaciÃ³n de un complejo simplicial][presentaciÃ³n poligonal]]. La compacidad da la finitud.

Podemos reducir su [[*Teorema de clasificaciÃ³n de presentaciones de superficies compactas][presentaciÃ³n]] a una de las de estas superficies
mediante transformaciones equivalentes. 
**** 3.6. ClasificaciÃ³n de superficies compactas II
***** Grupo fundamental de la presentaciÃ³n
Sea $S$ compacta y convexa determinada por la presentaciÃ³n $P$, entonces
su grupo fundamental tiene la misma presentaciÃ³n de $P$.

Es decir cociente del libre por la clausura normal de las $W$:

\[\Pi(S) = \frac{F(A)}{N_W}\]

****** DemostraciÃ³n
Aplicando Seifert-Van Kampen sobre una cara tenemos las aristas
como generadores y las palabras como relaciones entre ellas.

***** Conmutador
Dado $G$ se define su conmutador $[G,G]$ como el subgrupo normal de $G$ que
contiene las clases de conjugaciÃ³n, de la forma:

\[
[G,G] 
= 
\langle aba^{-1}b^{-1} \mid a,b \in G \rangle\]

***** Abelianizado
El conmutador cumple:

  1. $[G,G] \cong \{e\}$ $\iff$ $G$ abeliano.
  2. $Ab(G) = G/[G,G]$ es abeliano, llamado el *abelianizado* de $G$.

****** DemostraciÃ³n
Primer punto trivial. Para el segundo, comprobamos $ab[G,G] = ba[G,G]$.

***** Abelianizados de los grupos fundamentales de los modelos
Los abelianizados de los grupos de los modelos son:

  1. $Ab(\pi_1(\mathbb{S}^2)) = \{1\}$.
  2. $Ab(\pi_1(\mathbb{T}^{\#n}) \cong \mathbb{Z}^{2n}$.
  3. $Ab(\pi_1(\mathbb{RP}^{2\#n})) \cong \mathbb{Z}^{n-1} \times \mathbb{Z}_2$.

Distintos entre sÃ­.

****** DemostraciÃ³n
******* Grupo abelianizado del toro
Podemos definir una funciÃ³n sobre el grupo libre:

  - \[\varphi(a_i) = (0,\dots,\overset{i}1,\dots,0)\]
  - \[\varphi(b_i) = (0,\dots,\overset{n-i}1,\dots,0)\]

Y comprobar que respeta las relaciones de abelianidad y de la
palabra. AdemÃ¡s de dar un homomorfismo invertible.

******* Grupo abelianizado del plano proyectivo
Volvemos a definir sobre el grupo libre:

  - \[\varphi(a_i) = (0,\dots,\overset{i}1,\dots,0,0)\]
  - \[\varphi(a_n) = (-1,\dots,-1,1)\]

Volvemos a comprobar que tiene una inversa despuÃ©s de bajarla al
cociente.

****** Los productos de enteros son distintos
Trivial. Puede comprobarse dividiendo y por inducciÃ³n.

****** El producto por el cÃ­clico de orden 2 los hace distintos
Claramente, el producto por el cÃ­clico de orden 2 aÃ±ade un elemento de
orden 2 al grupo que antes no estaba.

***** ClasificaciÃ³n de superficies compactas
Una superficie topolÃ³gica compacta $S$ es homeomorfa a una sola
de las siguientes:

  1. $\mathbb{S}^2$.
  2. $\mathbb{T}^{\#n}$.
  3. $\mathbb{RP}^{2\#n}$.

Dos superficies compactas son homeomorfas ssi sus grupos 
fundamentales son isomorfos.

**** 3.7. CaracterÃ­stica de Euler y orientabilidad
***** GÃ©nero
Se define el gÃ©nero de $S$ superficie compacta conexa:

\[ g(S) =
\left\{\begin{array}{ll} 
0 & \mbox{if } S \cong \mathbb{S}^2 \\
n & \mbox{if } S \cong \mathbb{T}^{\#n} \mbox{ Ã³ } S \cong \mathbb{RP}^{2\#n}
\end{array} 
\right.
\]

***** CaracterÃ­stica de Euler
Se define la caracterÃ­stica de Euler de $S$ superficie compacta conexa:

\[\chi(P) = C-A+V\]

donde $C,A,V$ son los nÃºmeros de caras, aristas y vÃ©rtices.

***** La caracterÃ­stica de Euler es invariante a transformaciones
La caracterÃ­stica de Euler es invariante a transformaciones
elementales.

****** DemostraciÃ³n
Podemos comprobar que cada transformaciÃ³n elemental preserva la
caracterÃ­stica aunque aÃ±ada caras, aristas o vÃ©rtices.

***** Orientabilidad
Una superficie es orientable si admite una presentaciÃ³n orientada.

****** PresentaciÃ³n orientable
Una presentaciÃ³n poligonal es orientable si no tiene ningÃºn par
de aristas retorcidas.

***** La orientabilidad de superficies compactas es un invariante
Una superficie compacta conexa es orientable ssi es homeomorfa a
la esfera o a la suma de planos proyectivos.

***** Bicolorabilidad                                                                                       :extra:
Una presentaciÃ³n es *bicoloreable* si podemos colorear sus palabras
con dos colores de forma que las dos ocurrencias de una arista:

  - tengan distinto color y mismo signo
  - o tengan el mismo color y distinto signo

***** Bicolorabilidad invariante a transformaciones elementales                                             :extra:
La bicolorabilidad es invariante a transformaciones elementales.

****** DemostraciÃ³n
******* Renombrar
Trivialmente manteniendo la coloraciÃ³n.

******* Subdividir
Trivial por la misma coloraciÃ³n por cumplirlo la subdividida.

******* Consolidar
Trivial por cumplirlo ambas aristas involucradas.

******* Reflejar
Cambiando la coloraciÃ³n de la palabra.

******* Rotar
Trivial, mantiene coloraciÃ³n.

******* Cortar
Manteniendo el mismo color en las dos palabras.

******* Pegar
Ambas palabras deben tener el mismo color.

******* Doblar
Anular dos lados adyacentes no cambia nada.

******* Desdoblar
Las dos partes estÃ¡n en la misma palabra y tienen el mismo color.

***** Bicolorabilidad coincide con orientabilidad                                                           :extra:
La bicolorabilidad coincide con la orientabilidad.

****** DemostraciÃ³n
Coincide en los modelos y por ende en todas las demÃ¡s superficies.

***** Algoritmo de bicolorabilidad                                                                          :extra:
Podemos calcular la bicolorabilidad de una presentaciÃ³n simplemente
eligiendo un color para la primera cara y coloreando a partir de
ella.

****** DemostraciÃ³n
Asignado el color de la primera cara, queda determinado el color
de todas las caras que contengan una arista de ella. Como toda
cara estÃ¡ conectada a la inicial por conexiÃ³n, toda cara queda
determinada.

Si esta coloraciÃ³n cumple las condiciones de bicolorabilidad,
hemos encontrado una forma de bicolorear. Si no lo cumple, ninguna
bicoloraciÃ³n es posible.

***** ClasificaciÃ³n por invariantes                                                                         :extra:
Tabla de clasificaciÃ³n de superficies compactas por invariantes:

|------------+--------+----------+----------------|
| Superficie | GÃ©nero | C. Euler | Orientabilidad |
|------------+--------+----------+----------------|
| S          | 0      | 2        | Orientable     |
| T^n        | n      | 2-2n     | Orientable     |
| RP^n       | n      | 2-n      | No orientable  |
|------------+--------+----------+----------------|

Y podemos calcular el gÃ©nero desde la orientabilidad y la caracterÃ­stica:

  - $S$ orientable: $g(S) = \frac{1}{2}(2-\chi(S))$
  - $S$ no orientable: $g(S) = 2-\chi(S)$

*** Ejercicios
**** RelaciÃ³n de problemas 1
***** Ejercicio 1
Trabajando en el grupo fundamental $\Pi(X,x)$.

***** Ejercicio 2
****** Punto 1
Trivial trabajando en el grupo fundamental.
****** Punto 2
La condiciÃ³n es $\gamma\beta \in {\cal Z}(\Pi(X,x))$.

***** Ejercicio 3
Calculamos usando Seifert-Van Kampen.

***** Ejercicio 6
Por Seifert-Van Kampen haciendo tres grupos, uno con equivalencia homotÃ³pica al
cÃ­rculo y otro trivial. Sale $\mathbb{Z}$.

***** Ejercicio 7
Equivalencia homotÃ³pica a un cÃ­rculo en el que se identifican antÃ­podas, que es
homeomorfo a un cÃ­rculo. Sale $\mathbb{Z}$.

***** Ejercicio 8
Por Van Kampen, teniendo un trozo con equivalencia homotÃ³pica a la esfera y otro
que es un simple disco abierto. Grupo fundamental trivial.

***** Ejercicio 9
Por el Van Kampen que aplicamos en estos casos, es $\mathbb{Z}_3$.

***** Ejercicio 10
Homeomorfo al anterior.
**** RelaciÃ³n de problemas 2
***** Ejercicio 1
****** Punto 1
     Trivial
****** Punto 2
     Subiendo la intersecciÃ³n del entorno abierto que es isomorfo a $\mathbb{R}^n$ con el 
     entorno abierto que nos da el recubridor.
****** Punto 3
     Probamos la caracterizaciÃ³n, que es ser semilocalmente simplemente conexo. Cada
     punto tiene un entorno homeomorfo a un abierto de $\mathbb{R}^n$, asÃ­ que puedo tomar una bola
     abierta en el punto y que sea homeomorfa a un abierto en el punto en el que el
     grupo fundamental sea trivial.
****** Punto 4
     Las esferas.

***** Ejercicio 2
    #+begin_statement
    Sea $\{a,b\}$ una base de $\mathbb{R}^2$ y $R$ la relaciÃ³n de equivalencia en $\mathbb{R}^2$ dada por:

    \[ qRq' \text{ si }\ q'-q = ma+nb,\quad m,n\in\mathbb{Z}\]

    Sea $T_{a,b}$ el espacio topolÃ³gico cociente.
    #+end_statement

****** Punto 1
Usamos el recubrimiento de cocientes que surgen de acciones discontinuas para
el grupo de acciones de los $\phi_{n,m}(z) = z + (n,0) + (0,m)$ con $n,m\in\mathbb{Z}$.

**** Ejercicios de clase
***** CÃ¡lculo del espacio proyectivo con Van-Kampen
Para calcular el grupo de $\mathbb{R}\mathbb{P}^2$, lo definimos como una proyecciÃ³n desde la bola 
cerrada $C$ y tomamos abiertos en Ã©l.

 #+begin_center
 #+attr_latex: :width 50px
 [[./img/rpvankampen.png]]
 #+end_center

Aplicaremos Van Kampen sobre los siguientes abiertos:

 \[ U = \pi(C-\{p\})\]
 \[ V = \pi(B(p,\epsilon))\]
 \[ U \cap V = \pi((C-\{p\}) \cap B(p,\epsilon)) = \pi (B(p,\epsilon)) - \{p\}\]

Y tenemos que el grupo de $V$ es trivial. Para calcular el grupo de $U$ usarÃ© que
tiene el mismo tipo de homotopÃ­a que su borde, que es isomorfo a un cÃ­rculo y
por tanto tiene grupo fundamental $\mathbb{Z}$.

De la intersecciÃ³n tomaremos un generador $f$ y lo llevaremos al borde para tener:

 \[ f \simeq \alpha \ast c \ast a \ast \tilde\alpha\]

Que proyectando mientras sabemos que el generador de $U$ es 
$g = [\pi(\alpha \ast c \ast \tilde\alpha)]$ nos da finalmente que:

\[ [\pi(f)] \simeq 
 [\pi(\alpha\ast c \ast\tilde\alpha \ast \alpha \ast a \ast \tilde\alpha)] \simeq
 [g] \ast [g] \simeq 2[g]\]

Y el cÃ¡lculo del grupo nos da:

\[\frac{<g>}{<2g>} \cong \mathbb{Z}_2\]

**** Examen 13 enero 2016
****** Ejercicio 3
******* Punto 1     
      Sabemos que todos los $\Phi_n$ son homeomorfismos. Dado un punto $(a,b)$ tomamos un
      disco abierto de radio $1/4$ alrededor de Ã©l, y comprobamos que $(a+n,(-1)^nb)$
      estÃ¡ siempre a distancia mayor a $n$ y mayor a $1/2$. Llamando a la bola $V$, 
      tenemos:
      
      \[\phi(V) \cap V = \varnothing\]

******* Punto 2
      Como $G$ actÃºa propia y discontinuamente sobre $\mathbb{R}^2$, tenemos que es un 
      recubridor regular.

******* Punto 3
      Como $G$ es ahora el grupo de automorfismos de un recubridor regular sobre $M$,
      tenemos que:
      
      \[ G \cong \Pi(M,x)\]

      Con lo que su grupo fundamental es $\mathbb{Z}$.

******* Punto 4
      Trivial desde lo anterior.

******* Punto 5
      Podemos definir una funciÃ³n de $C$ a $M$ desde las representaciones en $\mathbb{R}^2$,
      vemos luego que es identificaciÃ³n y que por tanto hay homeomorfismo entre
      la imagen y el cociente por la relaciÃ³n que define. Comprobamos que ese
      cociente es igual al que define el grupo.

******* Punto 6
      Tengo ya al cilindro como recubridor y al plano. Faltan las cintas de MÃ¶bius
      en el caso impar que se realizarÃ¡n como en el ejercicio 5.
      
**** RelaciÃ³n de problemas 3
***** Ejercicio 1
#+begin_statement
Prueba que un espacio topolÃ³gico conexo es arco-conexo si y sÃ³lo si
cada punto tiene un entorno arco-conexo. Demuestra que todo espacio
topolÃ³gico localmente euclÃ­deo es conexo si y sÃ³lo si es arco-conexo.
#+end_statement

Si es arcoconexo, claramente se tiene localmente arcoconexo.  Si es
localmente arcoconexo, una componente arcoconexa debe ser abierta y
cerrada, porque cada punto al que se pueda llegar tiene un entorno al
que se puede llegar. Y cada punto al que no se pueda llegar tiene un
entorno al que no se pueda llegar.

***** Ejercicio 3
#+begin_statement
Estudia si el subespacio topolÃ³gico de $\mathbb{R}^3$ dado por:

\[ X = \{(x,y,z) \in \mathbb{R}^3 \mid z^2 = x^2+y^2\}\]

es una superficie topolÃ³gica.
#+end_statement

Si $z=0$ se tiene $x,y = 0$, asÃ­, el Ãºnico punto en ese plano es 
el $0$. Cualquier entorno suyo, si quitamos ese punto, tiene dos
componentes conexas y no puede ser homeomorfo a un disco.

El espacio no es localmente euclÃ­deo.

***** Ejercicio 4
Ambos son localmente euclÃ­deos.

***** Ejercicio 5
#+begin_statement
Prueba que el p-sÃ­mplex generado por $\{a_1,\dots,a_n\}$ es la envolvente
convexa del conjunto $\{a_1,\dots,a_n\}$.
#+end_statement

La envolvente convexa debe contener a todos los puntos y a sus
combinaciones convexas, luego debe contener a todo el simplex.

El simplex es convexo trivialmente:

\[
t \sum \lambda_i v_i + (1-t) \sum \lambda_i' v_i =
\sum t \lambda_i v_i + \sum (1-t) \lambda_i' v_i
\]

Y entonces la suma de los coeficientes suma la unidad, usando
que lo cumplen ambos elementos del sÃ­mplice:

\[
\sum t\lambda_i + \sum (1-t)\lambda_i' = t + (1-t) = 1
\]

***** Ejercicio 6
#+begin_statement
Dado un vÃ©rtice de un complejo simplicial $K$ se define la estrella 
abierta de $v$ como $ost(v) = \{o(\sigma) \mid \sigma \in st(v)\}$. Prueba que $ost(v)$ es un
entorno abierto de $v$ en $|K|$ y la colecciÃ³n de todas las estrellas 
abiertas es un recubrimiento abierto de $|K|$.
#+end_statement

Todo punto aquÃ­ pertenece a algÃºn $x \in o(\sigma), \sigma \in st(v)$. Una bola cerrada
alrededor suya es compacta y sÃ³lo contiene a una cantidad finita de
sÃ­mplices. Un sÃ­mplice $\tau$ que contenga a $x$ se corta con $\sigma$, nos da
$\sigma \cap \tau < \sigma$. Tenemos dos casos:

  - $\sigma \cap \tau = \sigma$, entonces $v \in \tau$.
  - $\sigma \cap \tau < \sigma$, que darÃ­a que $x$ estÃ© en una cara de $\sigma$ y no pueda estar
    en $o(\sigma)$.

AsÃ­, no hay ningÃºn vÃ©rtice que no contenga a $v$ tocando a $x$. Todos estÃ¡n
a distancia mayor que $0$, y puedo tomar el mÃ­nimo de las distancias (que
son un conjunto finito) y tener una bola cerrada que sÃ³lo corta sÃ­mplices
de la estrella. La bola abierta sÃ³lo cortarÃ¡ a los sÃ­mplices abiertos.

[[./images/starcomplex.png]]

Imagen de [[http://math.stackexchange.com/questions/633307/definition-of-star-in-a-simplicial-complex][Math.SE: Definition of star in a simplicial complex]].

***** Ejercicio 7
#+begin_statement
Describe una triangulaciÃ³n del toro, el plano proyectivo y la botella
de Klein.
#+end_statement

NÃ³tese que la triangulaciÃ³n debe cumplir los requisitos de los complejos
simpliciales, especialmente, que cada intersecciÃ³n debe ser vacÃ­a o cara
de ambos factores.

***** Ejercicio 8
#+begin_statement
Describe una triangulaciÃ³n del espacio topolÃ³gico cociente que se
obtiene cuando en un toro identificamos un meridiano a un punto.
#+end_statement

NÃ³tese primero que esto no es una superficie compacta porque el punto
en el que hemos identificado el meridiano no es localmente euclÃ­deo.

***** Ejercicio 9
#+begin_statement
Demuestra que la suma conexa de uno o mÃ¡s planos proyectivos contiene un
subespacio que es homeomorfo a una banda de MÃ¶bius.
#+end_statement

Podemos obtener en la presentaciÃ³n poligonal, uniendo con una banda
dos aristas identificadas, una banda de MÃ¶bius.

***** Ejercicio 10
#+begin_statement
Para cada una de las siguientes presentaciones de superficies calcula
la caracterÃ­stica de Euler y determina a cuÃ¡l de las superficies modelo
es homeomorfa:

  1. $\langle a,b,c \mid abacb^{-1}c^{-1} \rangle$.
  2. $\langle a,b,c \mid abca^{-1}b^{-1}c^{-1} \rangle$.
  3. $\langle a,b,c,d,e,f \mid abc,bde,c^{-1}df,e^{-1}fa \rangle$.
#+end_statement

****** Superficie 1
Calculamos Euler:

\[\chi(S) = 1 - 3 + 1\]

Y vemos que no es orientable. Es $\mathbb{RP}^{2\#3}$.

****** Superficie 2
Es un toro.

****** Superficie 3
Calculamos Euler:

\[\chi(S) = 4 - 5 + 3 = 2\]

Y a la vez que sacamos los vÃ©rtices comprobamos que es orientable.

***** Ejercicio 12
#+begin_statement
Prueba que la caracterÃ­stica de Euler de la suma conexa de dos
superficies compactas es igual a la suma de sus caracterÃ­sticas de
Euler menos dos.
#+end_statement

Si unimos las dos mediante cualquier triangulaciÃ³n razonable:

****** TriangulaciÃ³n 1
Se pierden 2 caras para cortar, se pegan con 3 caras, y 3 aristas.

#+begin_center
#+attr_latex: :width 50px
[[./images//sumatriangulacion1.png]]
#+end_center

****** TriangulaciÃ³n 2
Perdemos dos caras e identificamos 3 aristas y 3 vÃ©rtices.

#+begin_center
#+attr_latex: :width 50px
[[./images//sumatriangulacion2.png]]
#+end_center
**** ClasificaciÃ³n de superficies compactas
***** Calcular la orientabilidad
Un algoritmo que usa la caracterizaciÃ³n de bicolorabilidad para
determinar si una superficie es orientable o no:

#+BEGIN_SRC haskell
  import Text.ParserCombinators.Parsec
  type Letra = (Char,Bool)
  type Palabra = [Letra]
  type Color = Bool
  type Presentacion = [Palabra]
  type Coloracion = [(Palabra,Color)]

  orientable :: Presentacion -> Bool
  orientable presentacion = any esBuenaColoracion (posiblesColoraciones presentacion)

  posiblesColoraciones :: Presentacion -> [Coloracion]
  posiblesColoraciones [] = [[]]
  posiblesColoraciones (w:presentacion) =
    map ([(w,False)] ++) (posiblesColoraciones presentacion) ++
    map ([(w,True)] ++) (posiblesColoraciones presentacion)
  
  esBuenaColoracion :: Coloracion -> Bool
  esBuenaColoracion c = compruebaColores $ concat $ map (\(w,l) -> map (\x -> (x,l)) w) c

  compruebaColores :: [(Letra,Color)] -> Bool
  compruebaColores [] = True
  compruebaColores (vt : xs) = (all (\ut -> buenaArista ut vt) xs) && compruebaColores xs

  buenaArista :: (Letra,Color) -> (Letra,Color) -> Bool
  buenaArista ((x,u),v) ((y,w),z) = (x /= y) || (u == w && v /= z) || (u /= w && v == z)

  main :: IO ()
  main = return ()


  esOrientable :: String -> Bool
  esOrientable s = either undefined orientable (parse pres "" s) 

  pres :: Parser Presentacion
  pres = word `sepBy` (char ',')

  word :: Parser Palabra
  word = many letra

  letra :: Parser Letra
  letra = try letrainvertida <|> do
    l <- letter
    return (l,True)
  
  letrainvertida :: Parser Letra
  letrainvertida = do
    l <- letter
    _ <- char '*'
    return (l,False)
#+END_SRC
** Ãlgebra moderna
*** 1. ConstrucciÃ³n de anillos
**** 1.1. Anillos
***** Anillos
Un *anillo* es $(R,+,\times,1)$ siendo:

 1) $(R,+)$ un grupo aditivo abeliano.
 2) $(R,\times,1)$ monoide multiplicativo.
 3) $\times$ distributivo con $+$.

Llamamos $0$ al elemento neutro de la suma.

****** Anillos conmutativos
Llamamos *anillo conmutativo* a un anillo con $\times$ conmutativo.

****** Propiedades en anillos
Sea $r_1,r_2 \in R$ anillo:

 - $0r=0=r0$
 - $r_1(-r_2) = -r_1r_2 = (-r_1)r_2$
 - $r(r_1-r_2) = rr_1-rr_2$
 - $r_1+r_2=r_2+r_1$

******* DemostraciÃ³n
Usando distributividad se prueban trivialmente.

***** Morfismos de anillos
Un $f : R \to S$ es homomorfismo de anillos cuando:

  - $f(r_1+r_2) = f(r_1)+f(r_2)$
  - $f(r_1r_2) = f(r_1)f(r_2)$
  - $f(1) = 1$

****** CategorÃ­a de los anillos
La composiciÃ³n de dos morfismos de anillos es morfismo de anillos y
la identidad es morfismo de anillos. Los anillos unitales forman asÃ­
una categorÃ­a $\mathtt{Ring}$.

****** Isomorfismos de anillos
***** Subanillos
***** RetÃ­culo de subanillos
***** Ideales
***** Ideales extendidos y contraidos
***** RetÃ­culo de ideales
***** Ejemplo: Matrices infinitas
***** Ejemplo: Ãlgebra de Weyl
Se llama *Ã¡lgebra de Weyl* al anillo de operadores en los polinomios
generado por $X$ (multiplicaciÃ³n por la indeterminada) y $\frac{\partial}{\partial x}$ (diferenciaciÃ³n);
con la composiciÃ³n como producto.

****** CaracterizaciÃ³n
El Ã¡lgebra de Weyl es isomorfa a:

\[
\frac{K[X,Y]}{(YX-XY-1)}Ã±
\]

***** Ejemplo: Anillo de un monoide
Dado un monoide multiplicativo $M$, definimos $R[M]$ como los polinomios que
usan como exponentes los elementos de $M$. Es decir,

\[
\sum_i r_i[m_i]
\]

Y forma un Ã¡lgebra definiendo:

  - Suma: $\sum_i r_i[m_i] + \sum_i r_i'[m_i] = \sum_i (r_i+r_i')[m_i]$
  - Producto: $\left(\sum_i r_i[m_i]\right)\left(\sum_i r_i'[m_i]\right) = \sum_k \left(\sum_{m_im_j=m_k} (r_ir_j')[m_k]\right)$

****** Generaliza al anillo de polinomios
NÃ³tese que generaliza al anillo de polinomios en una variable cuando 
el monoide es $\mathbb{N}$, y que generaliza al anillo de polinomios en varias 
variables cuando el monoide es $\mathbb{N}^n$.

***** TODO Monoide libre
**** 1.2. ConstrucciÃ³n de anillos
***** Anillo cociente
****** ProyecciÃ³n
***** Propiedad universal del anillo cociente
***** Primer teorema de isomorfÃ­a
***** Segundo teorema de isomorfÃ­a
***** Tercer teorema de isomorfÃ­a
***** Producto directo
***** CaracterizaciÃ³n del producto por ortogonales centrales idempotentes
***** Anillo opuesto
***** Centro
***** Propiedad universal del anillo de un monoide
***** Anillo de polinomios
***** Propiedad universal del anillo de polinomios
**** 1.3. MÃ³dulos
***** R-mÃ³dulos
****** CaracterizaciÃ³n por anillo opuesto
***** Morfismo de R-mÃ³dulos
***** SubmÃ³dulos
****** Ideales como submÃ³dulos 
***** MÃ³dulo cociente
***** Propiedad universal del mÃ³dulo cociente
***** RetÃ­culo de submÃ³dulos
****** IntersecciÃ³n de submÃ³dulos
****** Suma de submÃ³dulos
***** SubmÃ³dulos maximales
**** TODO 1.4. CategorÃ­as y funtores
**** 1.5. La categorÃ­a Mod-R
***** CaracterizaciÃ³n de monomorfismos y epimorfismos
***** Primer teorema de isomorfÃ­a
***** Segundo teorema de isomorfÃ­a
***** Tercer teorema de isomorfÃ­a
***** Producto directo
***** Suma directa
***** LÃ­mites
***** ColÃ­mites
***** Ejemplos de lÃ­mite
***** Cambios de anillo
https://en.wikipedia.org/wiki/Change_of_rings
*** 2. ConstrucciÃ³n de mÃ³dulos
**** 2.1. Producto tensor
***** Aplicaciones bilineales
Sean $M$ un R-mÃ³dulo derecho y $N$ un R-mÃ³dulo izquierdo. Un homomorfismo de
grupos es R-bilineal si:

  - $\varphi(m_1+m_2,n) = \varphi(m_1,n) + \varphi(m_2,n)$
  - $\varphi(m,n_1+n_2) = \varphi(m,n_1) + \varphi(m,n_2)$
  - $\varphi(mr,n) = \varphi(m,rn)$

***** Producto tensor
Construimos el grupo producto tensor como el grupo libre generado por
los elementos del producto cartesiano, dividido por el grupo generado 
por las relaciones de bilinealidad:

\[
M \otimes_R N = \frac{\langle
(m,n) \mid m \in M, n \in N
\rangle}{B}
\]

Donde $B$ estÃ¡ generado por:

  - $(m_1+m_2,n) - (m_1,n) - (m_2,n)$
  - $(m,n_1+n_2) - (m,n_1) - (m,n_2)$
  - $(mr,n)-(m,rn)$

NÃ³tese que ademÃ¡s tenemos la proyecciÃ³n $b : M \times N \to M \otimes N$.

***** Propiedad universal del producto tensor
Sean $M_R, _RN$ mÃ³dulos con $f : M \times N \to X$ bilineal, existe un
Ãºnico homomorfismo de grupos $\overline{f} : M \oplus_R N \to X$ tal que conmuta:

\[\begin{tikzcd}
M \times N \rar{b}\drar[swap]{f} & 
M \otimes_R N \dar[dashed]{\exists! \overline{f}} \\
& X
\end{tikzcd}\]

****** TODO DemostraciÃ³n

***** Neutro del producto tensor
Se cumple $M \otimes R \cong M$ y $R \otimes N \cong N$.

***** TODO Producto tensor en anillos conmutativos
***** Producto tensor de Ã¡lgebras
Si $R,S$ son dos A-Ã¡lgebras, su producto tensor lo es con el producto
dado por:

\[
(r_1 \otimes s_1)(r_2 \otimes s_2) = (r_1r_2)\otimes(s_1s_2)
\]

****** TODO Inclusiones en el producto tensor de Ã¡lgebras

***** TODO Producto tensor de Ã¡lgebras como coproducto
**** 2.2. MÃ³dulos a dos lados
***** MÃ³dulos a dos lados
Un $M$ R-mÃ³dulo izquierda y S-mÃ³dulo derecha se llama *(R;S)-mÃ³dulo a dos lados*
si cumple:

\[
r(ms)=(rm)s
\]

***** TODO CaracterizaciÃ³n de mÃ³dulos a dos lados
***** TODO MÃ³dulos a dos lados balanceados y fieles
***** TODO Propiedad universal del producto tensor como mÃ³dulo a dos lados
**** 2.3. El retÃ­culo de submÃ³dulos
***** CategorÃ­a de los conjuntos parcialmente ordenados
Tomamos la *categorÃ­a de los conjuntos parcialmente ordenados* ${\cal P}$, siendo 
sus morfismos las aplicaciones crecientes:

\[
x \leq y \implies f(x) \leq f(y)
\]

****** SubmÃ³dulos como conjuntos parcialmente ordenados
Existe el funtor covariante retÃ­culo ${\cal L} : \mathtt{Mod-}R \to {\cal P}$ que lleva cada mÃ³dulo 
en su retÃ­culo de submÃ³dulos y cada homomorfismo de mÃ³dulos lo aplica sobre
cada submÃ³dulo del retÃ­culo:

\[
{\cal L}(f)(N) = f(N)
\]

Y existe el funtor contravariante retÃ­culo ${\cal L} : \mathtt{Mod-}R \to {\cal P}$ que lleva cada
mÃ³dulo en su retÃ­culo y cada homomorfismo de mÃ³dulos lo aplica de manera
inversa sobre cada submÃ³dulo del retÃ­culo:

\[
{\cal L}(f)(N) = f^{-1}(N)
\]

***** RetÃ­culos
Un *retÃ­culo* es un conjunto parcialmente ordenado donde todo par de
elementos tiene supremo e Ã­nfimo, llamados $a \vee b$ y $a \wedge b$.

***** Propiedades del retÃ­culo
En todo retÃ­culo $({\cal L}, \leq)$ se verifican:

 1) Idempotencia, $a \vee a = a$
 2) Conmutatividad, $a \vee b = b \vee a$
 3) Asociatividad, $a \vee (b \vee c) = (a \vee b) \vee c$
 4) AbsorciÃ³n, $a \vee (a \wedge c) = a$

Y sus duales:

 1) Idempotencia, $a \wedge a = a$
 2) Conmutatividad, $a \wedge b = b \wedge a$
 3) Asociatividad, $a \wedge (b \wedge c) = (a \wedge b) \wedge c$
 4) AbsorciÃ³n, $a \wedge (a \vee c) = a$

***** RetÃ­culo abstracto
Llamamos *retÃ­culo abstracto* a un conjunto $L$ con operaciones $\vee,\wedge$ que
cumplen las propiedades de retÃ­culo.

****** Orden en un retÃ­culo abstracto
Un retÃ­culo abstracto determina una relaciÃ³n de orden, y ademÃ¡s
se cumple en Ã©l:

\[
a \vee b = b \iff a \wedge b = a
\]

******* TODO DemostraciÃ³n
******** RelaciÃ³n de orden
******** Propiedad
****** Homomorfismo de retÃ­culos abstractos
Un homomorfismo de retÃ­culos abstractos es una aplicaciÃ³n preservando
supremos e Ã­nfimos.

\[
f(a\wedge b) = f(a) \wedge f(b) \qquad f(a\vee b) = f(a) \vee f(b)
\]

****** CategorÃ­a de retÃ­culos abstractos
La categorÃ­a de retÃ­culos abstractos contiene a los retÃ­culos y los
homomorfismos de retÃ­culos entre ellos. Es una categorÃ­a isomorfa
a la categorÃ­a de conjuntos parcialmente ordenados.

***** RetÃ­culo de submÃ³dulos de un mÃ³dulo
Los submÃ³dulos forman un retÃ­culo con:

  - $N \vee N' = N + N'$
  - $N \wedge N' = N \cap N'$

***** RetÃ­culos acotados
Un retÃ­culo con cero y uno se llama acotado, donde:

  - El elemento cero cumple: $a \wedge 0 = 0$
  - El elemento uno cumple:  $a \vee 1 = 1$

***** RetÃ­culos modulares
Llamamos retÃ­culo modular al que cumple la *ley modular*:

\[
N_1 \vee (N_2 \wedge N_3) = (N_1 \vee N_2) \wedge N_3
\]

****** El retÃ­culo de submÃ³dulos es modular
Los submÃ³dulos forman retÃ­culos modulares.

******* TODO DemostraciÃ³n

***** RetÃ­culos completos
Un retÃ­culo en el que existe el supremo e Ã­nfimo de cualquier familia de
submÃ³dulos se dice *completo*.

****** El retÃ­culo de submÃ³dulos es completo
El retÃ­culo de submÃ³dulos es completo, siendo el supremo e Ã­nfimo de
cada familia $\{ N_i \mid i \in I\}$:

  - $\bigvee N_i = \sum N_i$

  - $\bigwedge N_i = \bigcap N_i$

***** TODO RetÃ­culos superiormente continuos y compactamente generados
**** 2.5. MÃ³dulos finitamente generados
***** TODO MÃ³dulos finitamente generados
***** TODO ConstrucciÃ³n de finitamente generados
***** TODO SubmÃ³dulo maximal
***** TODO CaracterizaciÃ³n de finitamente generados
***** TODO Homomorfismos a la suma directa
***** TODO ConmutaciÃ³n con sumas directas
***** TODO Compacidad
***** TODO MÃ³dulos noetherianos
***** TODO MÃ³dulos noetherianos: propiedades
**** TODO 2.6. MÃ³dulos noetherianos
*** 3. Sumas directas y productos directos de mÃ³dulos
**** 3.1. Biproducto de mÃ³dulos
***** Biproducto de mÃ³dulos
Se llama *biproducto de mÃ³dulos* a la terna $(M_1\oplus M_2, \{p_1,p_2\}, \{q_1,q_2\})$,
con las proyecciones e inclusiones de producto y coproducto.

****** Propiedades del biproducto
Las composiciones de proyecciones e inyecciones cumplen:

 - $p_1q_1 = id_1$
 - $p_2q_2 = id_2$
 - $p_1q_2 = 0$
 - $p_2q_1 = 0$
 - $q_1p_1 + q_2p_2 = id$

**** TODO 3.2. Independencia y sumas directas
**** TODO 3.3. MÃ³dulos libres
**** TODO 3.4. DescomposiciÃ³n de anillos
*** Ejercicios
**** Semana 1
#+begin_statement
Prueba que los ideales (bilÃ¡teros) del anillo $M_n(R)$ son de la forma
$M_R(\mathfrak{a})$, para un ideal (bilÃ¡tero) $\mathfrak{a} \subseteq R$.
#+end_statement

Llamamos $E_{ij}$ a la matriz que tiene todas sus entradas nulas excepto
la entrada $i,j$. Dada $N$, una matriz con elementos $N = (n_{ij})_{i,j}$, el
producto de matrices es:

\[
E_{ia}NE_{bj} = n_{ab}E_{ij}
\]

Es decir, si una matriz $N$ estÃ¡ en un ideal bilÃ¡tero $J$, todas las 
matrices de la forma $n_{ab}E_{ij}$ estarÃ¡n tambiÃ©n en el ideal.

Esto nos da por un lado que los elementos que aparecen en matrices 
del ideal forman un ideal $I$, si $a,b$ estÃ¡n en el ideal, $(a+\alpha b)E_{ij}$ 
estarÃ¡ en el ideal. Y ademÃ¡s, cualquier matriz de la forma $xE_{ij}$ para
$x \in I$ estÃ¡ en el ideal. AsÃ­, el ideal es de la forma:

\[
J = M_R(\mathfrak{a})
\]

**** Semana 2
#+begin_statement
Prueba que $A[|X|]$, el anillo de las series formales de potencias en una
indeterminada $X$, es isomorfo al lÃ­mite inverso del sistema de anillos
dirigido inferiormente $(\{\frac{A[X]}{(X^n)}\}, \{f_{n,m}\}_{n \geq m})$, donde $f_{n,m} : \frac{A[X]}{(X^n)} \to \frac{A[X]}{(X^m)}$ es
el homomorfismo de anillos definido por $f_{n,m}(\overline{X}) = \overline{X}$, si $n \geq m$.
#+end_statement

***** Subanillo isomorfo
Empezamos definiendo un subanillo del producto de los anillos $\frac{A[X]}{(X^n)}$.
NÃ³tese que es subanillo por ser las funciones $f_{i,j}$ morfismos de anillos:

\[
H = \left\{
(p_i)_i \in \prod_i \frac{A[X]}{(X^i)}
\;\middle|\;
f_{i,j}(m_i) = m_j
\right\}
\]

Comprobaremos que es isomorfo a $A[|X|]$; para ello definimos la funciÃ³n
siguiente:

\[
g(a_0+a_1X+a_2X^2+\dots) = (a_0,a_0+a_1X,a_0+a_1X+a_2X^2,\dots)
\]

Es trivialmente inyectiva porque si $p \neq q$, se diferenciarÃ¡n en el primer
polinomio en el que tengan un coeficiente distinto. Es trivialmente
sobreyectiva porque si tengo un elemento de la forma $(u_0,u_1,\dots) \in H$,
se debe tener $u_n = u_{n-1} + a_n X_n$, para $u_{n-1}$ de grado $n-1$. Esto asegura
que el elemento serÃ¡ de la forma:

\[
(a_0,a_0+a_1X, a_0+a_1X+a_2X^2,\dots) = g(a_0+a_1X+a_2X^2+\dots)
\]

***** LÃ­mite inverso
Para probar que es lÃ­mite inverso, probaremos que si existiera un $Z$ con
morfismos $\phi_n : Z \to \frac{A[X]}{(X^n)}$ cumpliendo $\phi_m = f_{n,m} \circ \phi_n$, existirÃ­a un Ãºnico 
morfismo $Z \to H$ haciendo conmutar el diagrama:

\[\begin{tikzcd}
\frac{A[X]}{(X^0)} \rar &
\frac{A[X]}{(X^1)} \arrow{rr} &&
\frac{A[X]}{(X^2)} \rar &
\dots \\
&&
H \arrow{ull} \ular \urar \arrow{urr}
& & \\
& &
Z
\arrow[bend left]{uull} \arrow[bend left]{uul}
\arrow[bend right]{uurr} \arrow[bend right]{uur}
\uar[dashed]{!\exists}
&&
\end{tikzcd}\]

Ahora bien, dado $Z$ y los morfismos $\phi_n$, por propiedad universal del
producto directo, tenemos que existe un Ãºnico morfismo $h: Z \to \prod_i \frac{A[X]}{(X^i)}$,
cumpliendo ademÃ¡s que $\phi_n = \pi_n \circ h$. Aplicando $f_{n,m}$ tenemos:

\[
f_{n,m} \circ \pi_n \circ h = f_{n,m} \circ \phi_n =
\phi_m = \pi_m\circ h
\]

Lo que nos da que $im(h) \subseteq H$ y por tanto el morfismo buscado, que hereda
la unicidad.

**** Semana 3
***** Ejercicio 1.5.
#+begin_statement
Sea $R$ un anillo y $0 \neq e \in R$ un elemento idempotente; llamamos $f = 1-e$.

 1. Prueba que $eRe$ y $fRf$ son anillos.
 2. Prueba que $eRf$ es un $(eRe;fRf)$ mÃ³dulo, y $fRe$ es un $(fRf,eRe)$ mÃ³dulo.
 3. Prueba que existe un homomorfismo inyectivo de anillos

    $\lambda : R \longrightarrow 
    \begin{pmatrix}
    eRe & eRf \\
    fRe & fRf 
    \end{pmatrix}$, definido: $\lambda(r) = \begin{pmatrix} ere&erf\\fre&frf \end{pmatrix}$ para cada $r \in R$.

 4. Prueba que existe un isomorfismo de grupos abelianos
    $Hom_R(eR_R,fR_R) \cong fRe$, y un isomorfismo de anillos $End_R(eR_R) \cong eRe$.
 5. Prueba que:

    \[
    R \overset{\beta}\cong End_R(R_R) = 
    End_R((eR\oplus fR)_R) \cong \begin{pmatrix} eRe&eRf\\fRe&fRf \end{pmatrix}
    \]

    siendo $\beta(r)(x) = rx$. Como consecuencia $\lambda$ es un isomorfismo de anillos.

$\quad$
#+end_statement

****** Punto 1
Por propiedad distributiva, son cerrados con la misma suma que $R$. Son
trivialmente cerrados con el producto y nos falta comprobar que contiene
un elemento unidad, que es $e$ y es neutro gracias a ser idempotente:

\[
e(ere) = ere = (ere)e
\]

NÃ³tese que $f$ es tambiÃ©n idempotente y se repite el razonamiento.

****** Punto 2
Comprobamos que $eRf$ es cerrado para la suma, y ademÃ¡s:

 - $(ese)(erf) = e(ser)f$
 - $(erf)(ftf) = e(rft)f$

Por lo que es un mÃ³dulo a izquierda para $eRe$ y a derecha para $fRf$.

NÃ³tese que el caso de $fRe$ es simÃ©trico.

****** Punto 3
NÃ³tese que $\lambda$ preserva sumas trivialmente. Debemos comprobar que respeta
la unidad y el producto. Notamos primero gracias a que $ef=0$ tenemos:

\[
\lambda(1) = \begin{pmatrix}e&0\\0&f\end{pmatrix}
\]

Que se comprueba trivialmente que es el uno de su anillo, ya que es neutro
respecto al producto:

\[\begin{pmatrix}e&0\\0&f\end{pmatrix}\begin{pmatrix}er_1e&er_2f\\fr_3e&fr_4f\end{pmatrix} =\begin{pmatrix}er_1e&er_2f\\fr_3e&fr_4f\end{pmatrix}\]

Por Ãºltimo comprobamos que el producto se preserva:

\[\begin{pmatrix}ere&erf\\fre&frf\end{pmatrix}\begin{pmatrix} ese & esf \\ fse & fsf \end{pmatrix}
= \begin{pmatrix}erse&ersf\\frse&frsf\end{pmatrix}\]

Donde usamos crucialmente que $erese+erfse=er(e+f)se=erse$.

****** Punto 4
******* Isomorfismo de grupos abelianos
Suponiendo que se consideran los homomorfismos como mÃ³dulos a derecha
de $R$, podemos llevar cada homormofismo $\lambda$ a $\lambda(e)e$ y cada elemento $fre$
al homomorfismo $\psi(x) = (fre)x$.

Comprobamos que esto da una biyecciÃ³n para elemento cualquiera $fre \in fRe$
y $\lambda \in Hom(eR,fR)$ comprobando que la composiciÃ³n es la identidad:

\[
fre \mapsto \psi_{fre} \mapsto \psi_{fre}(e)e = freee = fre
\]
\[
\lambda(ex) \mapsto \lambda(e)e \mapsto \lambda(e)e(ex) = \lambda(ex)
\]

Donde hemos usado en el Ãºltimo paso que $\lambda$ es homomorfismo de R-mÃ³dulos
a derecha. Que esto preserva la suma es trivial.

******* Isomorfismo de anillos
En este caso tenemos un isomorfismo de grupos abelianos dado por el
caso anterior. AdemÃ¡s, es operaciÃ³n multiplicativa al tenerse:

\[
(\psi\circ\varphi)(e)e = \psi(e)\varphi(e)e
\]

Por ser homomorfismos de mÃ³dulos a derecha. Y es unital por tenerse:

\[
id(e)e = e
\]

****** Punto 5
******* Primer isomorfismo
Trivialmente $\beta$ es inyectivo porque $\beta(r)$ aplica la unidad en $r$.
Que es sobreyectivo es trivial porque cada funciÃ³n estÃ¡ determinada
por dÃ³nde lleva la unidad. Por ser homomorfismo de R-mÃ³dulos:

\[
\varphi(r) = \varphi(1)r
\]

******* Segundo isomorfismo
NÃ³tese que dada una $\varphi \in End_R(eR\oplus fR)$, podemos descomponer su aplicaciÃ³n
a cualquier elemento como:

\[
\varphi(er+fr) = \varphi(er)+\varphi(fr) = e\varphi(er)+f\varphi(er)+
e\varphi(fr)+f\varphi(fr)
\]

Por lo que queda determinada por dos endomorfismos entre $eR$ y $fR$ y
dos homomorfismos de $eR$ a $fR$ y de $fR$ a $eR$; y se puede escribir como:

\[
\varphi(ex+fy) = \begin{pmatrix}f_1&f_2\\f_3&f_4\end{pmatrix}\begin{pmatrix}ex\\fy\end{pmatrix}
\]

Con los isomorfismos anteriores tenemos lo buscado.

******* Isomorfismo de anillos
Notamos trivialmente que el isomorfismo asÃ­ determinado es $\lambda$.
Dado $r$, podemos ver que se divide como:

\[
rx = erex + erfx + frex + frfx
\]

Donde cada elemento pertenece al buscado.

***** Ejercicio 1.6.
#+begin_statement
Sea $R$ un anillo, $0\neq e \in R$ un elemento idempotente, y $f = 1 - e$. Para cada
R-mÃ³dulo derecha $M$ se define $Me = \{me \mid m \in M\}$, y $Mf = \{mf \mid m \in M\}$.

 1. Prueba que $Me$ es un $eRe$ mÃ³dulo derecha y $Mf$ un $fRf$ mÃ³dulo derecha.
 2. Prueba que $Me \times Mf$ es un $\begin{pmatrix}eRe&eRf\\fRe&fRf\end{pmatrix}$ mÃ³dulo derecha con estructura
    dada por,

    \[
    (m_1e, m_2f)
    \begin{pmatrix}em_{11}e&em_{12}f\\fm_{21}e&fm_{22}ff\end{pmatrix} =
    (m_1em_{11}e + m_2fm_{21}e, m_1em_{12}f + m_2fm_{22}f)
    \]

 3. Prueba que $h : M \longrightarrow Me \times Mf$, definido $h(m) = (me,mf)$, es un isomorfismo
    de R-mÃ³dulos derecha, donde la estructura de $Me \times Mf$ estÃ¡ dada vÃ­a $\lambda$.
    Observa que $Me$ y $Mf$ son subgrupos de $M$, pero no necesariamente submÃ³dulos.

$\quad$
#+end_statement

****** Punto 1
Siendo $me \in Me$, tenemos que $me(ere) = (mer)e \in Me$, donde usamos que
$M$ es mÃ³dulo a derecha. De la misma forma se cumple para $f$, que es
idempotente.

****** Punto 2
Simplemente tenemos que comprobar que la aplicaciÃ³n de multiplicar por
la matriz es lineal en $(m_1,m_2)$, y ademÃ¡s, que los elementos vuelven
a estar en $Me \times Mf$ por escribirse como:

 - $(m_1em_{11})e + (m_2fm_{21})e$
 - $(m_1em_{12})f + (m_2fm_{22})f$

Usando de nuevo que $M$ es mÃ³dulo a derecha.

****** Punto 3
Tenemos que cada elemento se escribe de forma Ãºnica como $m = me+mf$.
Si tuviÃ©ramos otra suma $m = ae + bf$, se tendrÃ­a $me=ae$ y $mf=bf$ al
multiplicar por cada uno de los idempotentes.

Tenemos por tanto una biyecciÃ³n, que ademÃ¡s es lineal y preserva la
multiplicaciÃ³n por la derecha:

\[
(mre,mrf) = (me,mf)\begin{pmatrix}ere&erf\\fre&frf\end{pmatrix}
\]

Observamos que $Me$ y $Mf$ son cerrados para la suma. Pero no tienen por
quÃ© ser cerrados como mÃ³dulo. NÃ³tese que puede darse el caso de que
$mer \notin Me$, como ocurre en las matrices, donde hay idempotentes no
centrales:

\[ e\begin{pmatrix}
a & b \\ c & d
\end{pmatrix} = \begin{pmatrix}
1 & 0 \\ 0 & 0
\end{pmatrix} \begin{pmatrix}
a & b \\ c & d
\end{pmatrix} = \begin{pmatrix}
a & b \\ 0 & 0
\end{pmatrix}\]

Que no puede pertenecer al $Me$ porque cambia al multiplicarla a la derecha
por $e$.
**** Semana 4
***** Ejercicio 1.7.
#+begin_statement
Si $K$ es un cuerpo, se considera el anillo:

\[
R = \begin{pmatrix}
K & K \\ 0 & K
\end{pmatrix}
\]

 1) Estudia los ideales derecha de $R$.
 2) Estudia los ideales izquierda de $R$.
 3) Estudia los ideales bilÃ¡teros de $R$.

Ver tambiÃ©n ejercicios anteriores.
#+end_statement

****** Ideales derecha
La multiplicaciÃ³n por un elemento del ideal serÃ­a:

\[\begin{pmatrix}
a & b \\ 0 & d
\end{pmatrix}\begin{pmatrix}
k_1 & k_2 \\ 0 & k_3
\end{pmatrix}  = \begin{pmatrix}
k_1a & k_2a+k_3b \\ 0 & k_3d
\end{pmatrix}\]

Estudiando cada combinaciÃ³n de $a,b,d$ nulos o no nulos, se obtienen
los ideales siguientes:

 - El ideal total, $\begin{pmatrix}K & K \\ 0 & K\end{pmatrix}$.

 - Suponiendo $a=0$, $\langle\begin{pmatrix}0 & k \\ 0 & 1\end{pmatrix}\rangle$.

 - Suponiendo $a=0,d=0$, $\begin{pmatrix}0 & K \\ 0 & 0\end{pmatrix}$.

 - Suponiendo $a=0,b=0$, $\begin{pmatrix}0 & 0 \\ 0 & K\end{pmatrix}$.

 - Suponiendo $d=0$, $\begin{pmatrix}K & K \\ 0 & 0\end{pmatrix}$.

 - El ideal trivial, $\begin{pmatrix}0 & 0 \\ 0 & 0\end{pmatrix}$.

****** Ideales izquierda
La multiplicaciÃ³n por un elemento del ideal es:

\[\begin{pmatrix}
k_1 & k_2 \\ 0 & k_3
\end{pmatrix} \begin{pmatrix}
a & b \\ 0 & d
\end{pmatrix} = \begin{pmatrix}
k_1a & k_1b+k_2d \\ 0 & k_3d
\end{pmatrix}\]

Estudiando cada combinaciÃ³n de $a,b,d$ nulos o no nulos, se obtienen
los ideales siguientes:

 - El ideal total, $\begin{pmatrix}K & K \\ 0 & K\end{pmatrix}$.

 - Suponiendo $d=0$, $\begin{pmatrix}K & K \\ 0 & 0\end{pmatrix}$.

 - Suponiendo $a=0$, $\begin{pmatrix}0 & K \\ 0 & K\end{pmatrix}$.

 - Suponiendo $a=0,d=0$, $\begin{pmatrix}0 & K \\ 0 & 0\end{pmatrix}$.

 - Suponiendo $a=0,b=0$ $\begin{pmatrix}K & 0 \\ 0 & 0\end{pmatrix}$.

 - El ideal trivial, $\begin{pmatrix}0 & 0 \\ 0 & 0\end{pmatrix}$.

****** Ideales bilÃ¡teros
Buscamos los ideales que lo son a izquierda y derecha:

$\begin{pmatrix}K & K \\ 0 & K\end{pmatrix}$ $\begin{pmatrix}K & K \\ 0 & 0\end{pmatrix}$ $\begin{pmatrix}0 & K \\ 0 & K\end{pmatrix}$ $\begin{pmatrix}0 & K \\ 0 & 0\end{pmatrix}$ $\begin{pmatrix}0 & 0 \\ 0 & 0\end{pmatrix}$

***** Ejercicio 1.8.
#+begin_statement
Estudia los ideales derecha e izquierda del anillo:

\[
R = \begin{pmatrix}\mathbb{Q}&\mathbb{R}\\0&\mathbb{R}\end{pmatrix}
\]

 1) Prueba que $R$ es un anillo artiniano derecha y noetheriano derecha.
 2) Prueba que $R$ no es un anillo artiniano izqierda ni noetheriano izquierda.

$\quad$
#+end_statement

****** Ideales derecha
Los ideales no triviales a la derecha son los siguientes:

- $\begin{pmatrix} 0 & 0 \\ 0 & \mathbb{R} \end{pmatrix}$

- $\langle\begin{pmatrix} 0 & k \\ 0 & 1 \end{pmatrix}\rangle$

- $\begin{pmatrix} \mathbb{Q} & \mathbb{R} \\ 0 & 0 \end{pmatrix}$

- $\begin{pmatrix} \mathbb{Q} & \mathbb{R} \\ 0 & \mathbb{R} \end{pmatrix}$

Habiendo sÃ³lo una cantidad finita de ideales, el anillo serÃ¡ artiniano
y noetheriano.

****** Ideales izquierda
Considerando de nuevo los casos y teniendo esta vez en cuenta que el
primer coeficiente estÃ¡ en $\mathbb{Q}$.

- $\begin{pmatrix} \mathbb{Q} & \mathbb{K} \\ 0 & 0 \end{pmatrix}$, para cualquier $\mathbb{K}$ extensiÃ³n de cuerpos $\mathbb{Q}\subset \mathbb{K}\subset\mathbb{R}$.

- $\begin{pmatrix} 0 & \mathbb{R} \\ 0 & \mathbb{R} \end{pmatrix}$

- $\begin{pmatrix} \mathbb{Q} & \mathbb{R} \\ 0 & \mathbb{R} \end{pmatrix}$

Comprobamos que no es artiniano ni noetheriano porque podemos crear
cadenas que rompen la condiciÃ³n de cadena ascendente y descendente.
Sabiendo que los reales tienen dimensiÃ³n infinita sobre los racionales
como espacio vectorial, creamos ambas cadenas aÃ±adiendo y retirando
progresivamente vectores de la base.

**** Semana 6
#+begin_statement
Sea $M$ un grupo abeliano finitamente generado y libre de torsiÃ³n.
Prueba que $M$ es un grupo libre.
#+end_statement

Si no fuera libre, cada conjunto de generadores
$\left\{ m_1,\dots,m_{n} \right\}$ deberÃ­a cumplir ecuaciones de la forma

\[n_1m_1+ \dots + n_tm_t = 0,\]

y entre todos los posibles conjuntos de generadores de cardinalidad
mÃ­nima y combinaciones, podemos elegir una que minimice $|n_1|+\dots + |n_t|$.
Ahora, si hay una combinaciÃ³n en la que $|n_i|<|n_j|$ para dos $i\neq j$,
podemos usar que

\[
n_im_i + n_jm_j = (n_i-n_j)m_i + n_{j}(m_j-m_i)
\]

para reescribir la relaciÃ³n, teniendo otro sistema de generadores
equivalente $\left\langle m_1,\dots,m_i,m_j,\dots,m_{t} \right\rangle = \left\langle m_1,\dots,m_i,m_j-m_i,\dots,m_t \right\rangle$ que
tiene un $|n_1|+\dots + |n_t|$ menor. AsÃ­, en el mÃ­nimo, $|n_i| = d$ para cualquier
Ã­ndice. Pero este $d$ no puede ser mayor que $1$ porque si no se tendrÃ­a un
elemento de torsiÃ³n

\[
d \left( \frac{n_1}{d}m_1 + \dots + \frac{n_t}{d}m_t \right) = 0.
\]

AsÃ­, hemos llegado a una relaciÃ³n en la que un generador puede ponerse
como suma y diferencia de los otros, 

\[
m_1 = \pm m_2 \pm m_3 \pm \dots \pm m_{t},
\]

contraviniendo la minimalidad del
sistema de generadores

**** Semana 7
#+begin_statement
Sea $R$ un anillo con un Ãºnico ideal izquierda maximal $\mathfrak{a}$.

 1) Prueba que $\mathfrak{a}$ es un ideal bilÃ¡tero.
 2) Prueba que $\mathfrak{a}$ es el Ãºnico ideal derecha maximal.
 3) Prueba que $R/\mathfrak{a} = {\cal U}(R) = R^{\times}$, el conjunto de los elementos 
    invertibles de $R$.

Un anillo con un Ãºnico ideal derecha maximal se llama *anillo local*.
#+end_statement

NÃ³tese que por Teorema de Krull, todo ideal propio (izquierda,
derecha, bilÃ¡tero) estÃ¡ contenido en un ideal maximal (izquierda,
derecha, bilÃ¡tero). Todo elemento no unidad estÃ¡ contenido en un
ideal maximal (izquierda, derecha, bilÃ¡tero).

***** Primer punto
Si $x \in \mathfrak{a}$, sabemos que no puede ser unidad; asÃ­, $xy$ tampoco puede serlo 
para ningÃºn $y \in R$, y como no puede serlo, debe estar contenido en algÃºn
ideal maximal izquierda, que debe ser $\mathfrak{a}$.

***** Segundo punto
Usando el tercer punto, cualquier elemento que estuviera en un ideal
derecha maximal que no estuviera en el Ãºnico ideal bilÃ¡tero que existe
deberÃ­a ser una unidad.

***** Tercer punto
Si hubiera algÃºn $x + \mathfrak{a}$ no invertible, se tendrÃ­a que $\left\langle x \right\rangle$ generarÃ­a un
ideal propio que deberÃ­a estar contenido en un maximal. Este maximal
deberÃ­a ser $\mathfrak{a}$, y por tanto $x = 0$.

**** Semana 8
#+begin_statement
Sea $R$ un anillo y $e \in R$ un elemento idempotente

 1) para cada ideal derecha $\mathfrak{a} \subseteq R$ prueba que $\mathfrak{a} \cap Re = \mathfrak{a}e$.
 2) para cada ideal $\mathfrak{A} \subseteq R$ prueba que $\mathfrak{A} \cap Re = \mathfrak{A}e$.
 3) $eRe$ es un anillo y $\mathfrak{A} \mapsto e\mathfrak{A}e$ define una aplicaciÃ³n sobreyectiva que respeta
    el orden del retÃ­culo de los ideales de $R$ en el retÃ­culo de los ideales de
    $eRe$.
 4) prueba que tenemos un funtor $\text{Mod-}R \to \text{Mod-}eRe$, definido $M \mapsto Me$.
 5) si $M$ es un $R\text{-mÃ³dulo}$ derecha simple, prueba que $Me = 0$ Ã³ $Me$ es un
    $eRe\text{-mÃ³dulo}$ derecha simple.
 6) Â¿se conservan los $R\text{-mÃ³dulos}$ derecha proyectivos?
 7) Â¿se conservan los $R\text{-mÃ³dulos}$ izquierda inyectivos?
#+end_statement

***** Punto 1
Sea $x \in \mathfrak{a} \cap Re$, entonces $x = xe \in \mathfrak{a}e$. Sea $ae \in \mathfrak{a}e$, es trivial que $ae \in \mathfrak{a} \cap Re$.

***** Punto 2
Un ideal es un ideal derecha.

***** Punto 3
Se comprueba que $eRe$ es anillo con unidad $e$. El producto de dos elementos
sigue siendo bilineal con $ere \cdot ese = erese$. Si $S \subseteq R$, es claro que $eSe \subseteq eRe$.
Es sobreyectiva porque si $I$ es $eRe\text{-ideal}$, podemos comprobar que $RIR$ es un
$R\text{-ideal}$ y que $eRIRe = eReIeRe = I$.

***** Punto 4
El funtor llevarÃ¡ $f \colon M \to N$ a $\widetilde f \colon Me \to Ne$ definida como

\[\widetilde f(me) = f(m)e.\]

Es funtor por cumplir $\widetilde g \circ \widetilde f (me) = (g \circ f(m))e$.

***** Punto 5
$Me$ serÃ­a un submÃ³dulo, asÃ­ que podrÃ­a ser $Me = 0$ o $Me = M$.
En el segundo caso serÃ­a un $eRe\text{-mÃ³dulo}$ por ser un $R\text{-mÃ³dulo}$,
y en ese caso, como $M = Me$, se tendrÃ­a que si hubiera un
submÃ³dulo $A$ de $M$ como $eRe\text{-mÃ³dulo}$, serÃ­a de $M$ como $R\text{-mÃ³dulo}$
por tenerse $AR = ((Ae)R)e = A(eRe) = A$.

***** Punto 6
Si tenemos $P \oplus H \cong R^{(I)}$, multiplicando, $Pe \oplus He \cong (Re)^{(I)}$. Pero
sabemos que $Re \cong eRe$ como $eRe\text{-mÃ³dulo}$.

***** Punto 7
Si $Q$ es un submÃ³dulo izquierda inyectivo, para cualquier $R\text{-mÃ³dulo}$ $M$
con $Q \leq M$ existe un $Q \leq K$ tal que $K \oplus Q = M$, como producto directo
interno.

Sea ahora un $eRe\text{-mÃ³dulo}$ $N$ tal que $Qe \leq N$. Tenemos que $Ne = N$ por
ser $e$ unidad del anillo. Como $Q$ es inyectivo, existe un $K$ tal que
$Q \cap K = \left\{ 0 \right\}$ y $Q + K = Q + N$. Si multiplicamos por $e$ tenemos

\[
Qe + Ke = Qe + N = N.
\]

De aquÃ­ se tiene que $Ke \cap N = Ke$. Entonces, $Ke \subset K \cap N$ y dado $l \in K \cap N$,
se tiene que como $l \in N$, $le=l$, luego $l \in Ke$. AsÃ­, $Ke = K \cap N$, tenemos

\[
Qe + K \cap N = N,
\]

y como $Q \cap K = \left\{ 0 \right\}$, se tiene $Qe \cap K = \left\{ 0 \right\}$ y por tanto $Qe \cap (K\cap N) = \left\{ 0 \right\}$.

**** Semana 9
#+begin_statement
Sea $R$ un anillo y $M$ un $R\text{-mÃ³dulo}$ derecha. Se considera el anillo $S=M_n(R)$ y
el grupo abeliano $M^n$.

 1) Prueba que $M^n$ es un $S\text{-mÃ³dulo}$ derecha con acciÃ³n dada por
    $(m_i)_i(a_{ij})_{ij} = \left( \sum_i m_ia_{ij} \right)_j$.
 2) Prueba que $M \mapsto M^n$, extendiendo para homomorfismos en la forma obvia,
    define un functor $F \colon \text{Mod-}R \to \text{Mod-}S$.
 3) Se considera el idempotente $e_{11} \in M_n(R)$, la matriz que tiene $1$ en el
    lugar $(1,1)$, y $0$ en el resto. Observa que $e_{11}Se_{11} \cong R$. Tenemos entonces un
    funtor $G \colon \text{Mod-}S \to \text{Mod-}e_{11}Se_{11} = \text{Mod-}R$.
 4) Prueba que para cada $R\text{-mÃ³dulo}$ derecha $M$ se tiene un isomorfismo
    $\theta_M\colon M \cong GF(M)$, y que si $f\colon M_1\to M_2$ es un homomorfismo de $R\text{-mÃ³dulos}$,
    entonces tenemos un cuadrado conmutativo de homomorfismos de $R\text{-mÃ³dulos}$.

    \[\begin{tikzcd}
    M_1 \rar{f} \dar[swap]{\theta_{M_1}} & M_2 \dar{\theta_{M_2}} \\
    GF(M_1) \rar{GF(f)} & GF(M_2) 
     \end{tikzcd}\]

 5) Prueba que para cada $S\text{-mÃ³dulo}$ derecha $N$ se tiene un isomorfismo
    $\nu_N\colon N \cong FG(N)$, y que si $g\colon N_1 \to N_2$ es un homomorfismo de $S\text{-mÃ³dulos}$,
    entonces tenemos un cuadrado conmutativo de homomorfismos de $S\text{-mÃ³dulos}$.

    \[\begin{tikzcd}
    N_{1} \rar{g} \dar[swap]{\nu_{N_1}} & N_{2} \dar{\nu_{N_2}} \\
    FG(N_{1}) \rar{FG(g)} & FG(N_{2})
    \end{tikzcd}\]

 6) Prueba que si $M$ es un $R\text{-mÃ³dulo}$ derecha simple (resp. proyectivo,
    inyectivo), tambiÃ©n $F(M)$ lo es.
#+end_statement

***** Punto 1
Comprobaremos que cumple la definiciÃ³n de mÃ³dulo, es decir,

 * hay una *identidad* dada por $(m_i)_i(\delta_{ij})_{ij} = (m_{j})_{j}$.
 * el *producto* es bilineal
   $(m_i+n_i)_i(a_{ij})_{ij} = (\sum_i (m_i+n_i)a_{ij})_j = (\sum m_ia_{ij} + \sum n_ia_{ij})_j$ y
   $(m_i)_i(a_{ij}+b_{ij})_{ij} = (\sum m_i(a_{ij}+b_{ij}))_j = \sum m_ia_{ij} + \sum m_ib_{ij}$.
 * y es *asociativa* con el producto de matrices usual
   $((m_i)_ia_{ij})(b_{ij}) = (\sum_j (\sum_i m_ia_{ij})b_{jk})_{k} = (\sum_i m_i\sum_{j}a_{ij}b_{jk})_k$.

***** Punto 2
Si los extendemos de forma obvia aplicando el homomorfismo a cada una de las
entradas de la matriz, es obvio que se conserva la composiciÃ³n de funciones
como

\[
g(f((m_{i})_i)) = (gf(m_i))_i,
\]

y que la indentidad se preserva por la extensiÃ³n.

***** Punto 3
Notamos que podemos llevar cada matriz a su Ãºnica entrada $e_{11}(r_{ij})e_{11} \mapsto r_{11}$.
La suma es por componentes y por tanto se respeta por la aplicaciÃ³n; el
producto de matrices de una entrada coincide con el producto del anillo.

***** Punto 4
NÃ³tese que $F(M) = M^n$ y que $GF(M) = (M\ 0\ 0\ \dots )$, donde ademÃ¡s hay un
isomorfismo $e_{ii}Se_{11} \cong R$. El isomorfismo de mÃ³dulos lleva $m$ en $(m\ 0\ 0\ \dots)$,
y se comprueba trivialmente que la multiplicaciÃ³n funciona de la misma
manera.

Dado un homomorfismo de mÃ³dulos, tenemos que $GF(f)$ aplicarÃ¡ el homomorfismo
sobre el Ãºnico elemento llevando $GF(f)(m\ 0\ 0\ \dots) = (f(m)\ 0\ 0\ \dots)$.

***** Punto 5
Tenemos por ser idempotente que $G(Ne_{11}) = G(N)$, pero 

\[FG(N) \cong FG(Ne_{11}) \cong Ne_{11} \oplus Ne_{22} \oplus \dots \cong N\]

por ser $Ne_{11}\cong Ne_{22}$ como $R\text{-mÃ³dulos}$ y ser $\left\{ e_{1},\dots,e_{n} \right\}$ un conjunto de
idempotentes centrales.

Una funciÃ³n $g\colon N_1 \to N_2$ estÃ¡ unÃ­vocamente determinada por cÃ³mo actÃºa
sobre cada sumando directo, por lo que conmuta su actuaciÃ³n antes y
despuÃ©s de aplicarla explÃ­citamente sobre cada sumando directo.

***** Punto 6
Los dos puntos anteriores han definido dos isomorfismos naturales
que constituyen una equivalencia de categorÃ­as.

**** Semana 10
#+begin_statement
Se considera la categorÃ­a de grupos abelianos; en este caso $R = \mathbb{Z}$.

 1) Prueba que $\mathbb{Z}$ es un grupo abeliano uniforme. Determina todos los grupos
    cÃ­clicos uniformes.
 2) Prueba que el grupo $\mathbb{Z}_{p^{\infty}}$ es un grupo uniforme y no es un grupo cÃ­clico.
    Se consideran $\mathbb{Q}$ y $\mathbb{R}$; Â¿es alguno uniforme?
 3) Determina todos los grupos abelianos inyectivos indescomponibles.
 4) Si $M$ es un grupo abeliano finitamente generado sabemos que 
    $M \cong \left( \bigoplus^t_{i=1} \mathbb{Z}_{p^{n_i}} \right) \oplus \mathbb{Z}^n$, para $n,n_1,\dots,n_t \in \mathbb{N}$. Â¿CuÃ¡l es la descomposiciÃ³n
    de $E(M)$ como suma de inyectivos indescomponibles?
#+end_statement

***** Punto 1
Dados dos submÃ³dulos de $\mathbb{Z}$, que estarÃ¡n generados por dos enteros, podemos
comprobar que se intersecarÃ¡n en su mÃ­nimo comÃºn mÃºltiplo.

***** Punto 2
Para comprobar que el grupo $\mathbb{Z}_{p^{\infty}}$ es uniforme tomamos un mÃ³dulo no 
nulo que tenga al menos un elemento $a/p^n$ y otro con $b/p^m$; 
para $a,b$ coprimos con $p$.  ExistirÃ¡n $x,y,p$ tales que
$xp a/p^n = 1/p^{n+p} = y b/p^{m}$, por lo que serÃ¡ uniforme.

Comprobamos que $\mathbb{R}$ no es uniforme porque tiene, por ejemplo $(\pi) \cap (1) = 0$.
Sin embargo $\mathbb{Q}$ sÃ­ lo es porque si tenemos dos mÃ³dulos y cada uno contiene
al menos un elemento no nulo $a/b$ y $c/d$; tenemos que $cb \cdot a/b = ad \cdot c/d$.

***** Punto 3
Los inyectivos indescomponibles estÃ¡n en correspondencia con los ideales
primos. Tenemos para $p$ primo que $E(\mathbb{Z}_p) = \mathbb{Z}_{p^{\infty}}$, ya que es extensiÃ³n esencial
y es inyectivo. Para el ideal primo $\{0\}$ tenemos a su vez que $E(\mathbb{Z}) = \mathbb{Q}$, ya
que es inyectivo y uniforme.

***** Punto 4
Como $\mathbb{Z}$ es noetheriano, tenemos $\bigoplus E(M_i) = E \left( \bigoplus M_i \right)$, asÃ­ que la suma
debe ser

\[
E(M) = \left( \bigoplus_{i=1}^t \mathbb{Z}_{p^{\infty}} \right) \oplus \mathbb{Q}^n
\]

considerando los sumandos con exponente no nulo.

*** Trabajos
**** Funtores adjuntos
***** Transformaciones naturales
#+begin_definition
Dados dos funtores $S,T : A \to B$, una *transformaciÃ³n natural* $\tau : S \Longrightarrow T$ 
es una funciÃ³n asignando a cada objeto $a \in A$ un morfismo $Sa \to Ta$ y 
cumpliendo el siguiente diagrama conmutativo:

\[\begin{tikzcd}
a \dar{f} & & Sa \rar{\tau_a}\dar{Sf} & Ta \dar{Tf} \\
a' & & Sa' \rar{\tau_{a'}} & Ta'
\end{tikzcd}\]

En este caso, decimos que $\tau_a$ es /natural en/ $a$.
#+end_definition

#+begin_definition
Llamamos *isomorfismo natural* a la transformaciÃ³n natural en la que
cada componente $\tau_a$ tiene una inversa. Podemos definir una transformaciÃ³n
natural inversa $\tau^{-1}$ que tiene por componentes a cada una de las inversas.
#+end_definition

****** ComposiciÃ³n vertical de transformaciones naturales
#+begin_definition
Dados funtores $R,S,T : {\cal A} \to {\cal B}$ y transformaciones naturales $\tau : S \Longrightarrow T$
y $\sigma : R \Longrightarrow S$, podemos componerlas componente a componente para formar
una *transformaciÃ³n naturalcomposiciÃ³n vertical* $\tau \circ \sigma$.

\[\begin{tikzcd}
Rc \rar{Rf}\dar{\sigma_c}\arrow[dd,bend right=90] &
Rc' \dar{\sigma_{c'}} \arrow[dd,bend left=90] \\
Sc \rar{Sf} \dar{\tau_c} & Sc' \dar{\tau_{c'}} \\
Tc \rar{Tf} & Tc' 
\end{tikzcd}
\]

#+end_definition

NÃ³tese que la naturalidad se preserva, ya que si los dos cuadrados
pequeÃ±os son conmutativos, conmuta todo el diagrama.

****** ComposiciÃ³n horizontal de transformaciones naturales
#+begin_definition
Dados funtores $S,T : {\cal A} \longrightarrow {\cal B}$ y $S',T' : {\cal B} \longrightarrow {\cal C}$ y dadas transformaciones
naturales $\tau : S \Longrightarrow T$ y $\tau' : S' \Longrightarrow T'$, podemos crear una transformaciÃ³n
natural entre los funtores compuestos, $\tau' \ast \tau$:

\[\begin{tikzcd}
S'Sx \arrow{rr}{(\tau' \ast \tau)_x} \dar &&
T'Tx \dar \\
S'Sy \arrow{rr}{(\tau' \ast \tau)_y} &&
T'Ty
\end{tikzcd}\]

Cada componente se crea aprovechando la siguiente igualdad:

\[
(\tau' \ast \tau) = T'\tau \circ \tau' = \tau' \circ S'\tau
\]
#+end_definition

Y puede comprobarse que constituye una transformaciÃ³n natural.

****** CategorÃ­a de los funtores
#+begin_definition
Dadas ${\cal A},{\cal B}$ categorÃ­as, los funtores entre ellas forman una *categorÃ­a de funtores* 
que llamaremos $Funct({\cal A},{\cal B})$ y que tiene como morfismos a las transformaciones 
naturales con la composiciÃ³n vertical:

\[
Nat(S,T) = \{ \tau \mid \tau : S \Longrightarrow T \}
\]
#+end_definition

NÃ³tese que la composiciÃ³n es asociativa y que consta de una identidad
en la transformaciÃ³n natural de cada funtor consigo mismo que tiene
como componentes identidades en cada objeto.

***** DefiniciÃ³n de funtores adjuntos por naturalidad
#+begin_definition
Una *adjunciÃ³n* entre categorÃ­as ${\cal A}$ y ${\cal B}$ es un par de funtores $F:{\cal A} \to {\cal B}$ y
$G: {\cal B} \to {\cal A}$ con una familia de isomorfismos $\varphi_{a,b} : Hom(Fa,b) \cong Hom(a,Gb)$
que determinan transformaciones naturales en ambas componentes.
#+end_definition

Notamos al par de funtores adjuntos como $F \dashv G$. Llamamos a $F$ adjunto
izquierdo y a $G$ adjunto derecho.

****** Condiciones de naturalidad
Las condiciones de naturalidad de esa familia de isomorfismos equivalen
a que los siguientes diagramas conmuten:

\[\begin{tabular}{cc} \begin{tikzcd}
Hom(Fa,b) \rar{\varphi_{a,b}} \dar[swap]{f_\ast} & 
Hom(a,Gb) \dar{(Gf)_\ast} \\
Hom(Fa,b') \rar{\varphi_{a,b'}} &
Hom(a,Gb')
\end{tikzcd} &
\begin{tikzcd}
Hom(Fa,b) \rar{\varphi_{a,b}} \dar[swap]{(Fg)^\ast} & 
Hom(a,Gb) \dar{g^\ast} \\
Hom(Fa',b) \rar{\varphi_{a,b'}} &
Hom(a',Gb)
\end{tikzcd} \end{tabular}\]

NÃ³tese que cada uno de ellos expresa la naturalidad entre los dos bifuntores
cuando se fija un argumento. Es decir, hay dos isomorfismos naturales

 1) $Hom(F-,b) \Longrightarrow Hom(-,Gb)$.
 2) $Hom(Fa,-) \Longrightarrow Hom(a,G-)$.

***** DefiniciÃ³n por unidad y counidad
#+begin_definition 
Una *adjunciÃ³n* entre categorÃ­as ${\cal A}$ y ${\cal B}$ es un par de funtores $F:{\cal A} \to {\cal B}$ y
$G: {\cal B} \to {\cal A}$ con dos transformaciones naturales:

  - La *unidad*:   $\eta : 1_{\cal A} \Longrightarrow GF$
  - La *counidad*: $\epsilon: FG \Longrightarrow 1_{\cal B}$

Cumpliendo que las composiciones siguientes dan la identidad:

 - $F \overset{F \eta} \Longrightarrow FGF \overset{\varepsilon F}\Longrightarrow F$
 - $G \overset{\eta G} \Longrightarrow GFG \overset{G \varepsilon}\Longrightarrow G$
#+end_definition

Demostraremos que esta definiciÃ³n es equivalente a la anterior.

****** Equivalencia de definiciones: desde familia de isomorfismos a unidades
#+begin_theorem
Dada una adjunciÃ³n en tÃ©rminos de una familia de isomorfismos, podemos
construir una adjunciÃ³n en tÃ©rminos de unidad y counidad.
#+end_theorem

#+begin_proof
/Paso 1: ConstrucciÃ³n de la unidad y la counidad./

Supongamos que tenemos la familia de transformaciones naturales
$\varphi_{a,b} : Hom(Fa,b) \to Hom(a,Gb)$. Particularizaremos los cuadrados de
naturalidad en los dos casos $b = Fa$ y $a = Gb$ para crear la unidad y
la counidad.

\[\begin{tikzcd}
Hom(Fa,Fa) \arrow{d}[swap]{(Ff)^\ast} \arrow{r}{\varphi} & 
Hom(a,GFa) \arrow{d}{(f)^\ast} \\
Hom(Fa', Fa) \arrow{r}{\varphi} &
Hom(a',GFa)
\end{tikzcd}\]

Si tomamos la identidad $1_{Fa}$ y llamamos $\eta_a = \varphi(1_{Fa})$, tenemos que
$\eta \circ f = \varphi(Ff)$ por conmutatividad.

Si damos la vuelta al isomorfismo $\varphi$ para tomar $\varphi^{-1}$, llamarlo de la
misma forma y repetir el mismo proceso:

\[\begin{tikzcd}
Hom(FGb,b) \arrow{d}[swap]{(Ff)^\ast} & 
Hom(Gb,Gb) \arrow{d}{(f)^\ast} \lar[swap]{\varphi} \\
Hom(FGb', b) &
Hom(Gb',Gb) \lar{\varphi}
\end{tikzcd}\]

NÃ³tese que aquÃ­ usamos $\varphi$ para notar un isomorfismo y su inversa;
dependerÃ¡ sÃ³lo del contexto determinar cuÃ¡l estamos usando.
Si tomamos la identidad $1_{Gb}$ y llamamos $\varepsilon_b = \varphi(1_{Gb})$, tenemos que
$\varepsilon \circ Ff = \varphi(f)$.

Aplicamos el mismo proceso al segundo cuadrado natural.

\[\begin{tikzcd}
Hom(Fa,Fa) \arrow{d}[swap]{g_\ast} \arrow{r}{\varphi} & 
Hom(a,GFa) \arrow{d}{(Gg)_\ast} \\
Hom(Fa, Fa') \arrow{r}{\varphi} &
Hom(a,GFa')
\end{tikzcd}\]

Y volvemos a tomar la identidad para tener $\varphi(g) = Gg \circ \eta$. Volviendo a
dar la vuelta a los isomorfismos llegamos a:

\[\begin{tikzcd}
Hom(FGb,b) \arrow{d}[swap]{(g)_\ast} & 
Hom(Gb,Gb) \arrow{d}{(Gg)_\ast} \lar[swap]{\varphi} \\
Hom(FGb,b') &
Hom(Gb,Gb') \lar{\varphi}
\end{tikzcd}\]

Que nos da, tomando la identidad, $\varphi(Gg) = g \circ \varepsilon$.

/Paso 2: Naturalidad de la unidad y la counidad./

Una vez tenemos definidas la unidad y la counidad, podemos comprobar
su naturalidad desde las ecuaciones que hemos obtenido:

\[\begin{aligned}
\eta     \circ f        &= \varphi(Ff) \\
g        \circ \epsilon &= \varphi(Gg) \\
\epsilon \circ Ff       &= \varphi(f) \\
Gg       \circ \eta     &= \varphi(g) \\
\end{aligned}\]

Y la naturalidad de $\eta$ y $\varepsilon$ se deduce desde ahÃ­ por la conmutatividad de los
siguientes diagramas, con $\eta \circ f = GFf \circ \eta$ y $g \circ\varepsilon = \varepsilon\circ FGg$:

\[\begin{tabular}{cc}\begin{tikzcd}
GFa  \arrow{r}{GFf} & 
GFb \\
a \arrow{u}[swap]{\eta_X} \arrow{r}[swap]{f} & 
b \arrow{u}{\eta_Y}
\end{tikzcd} & \begin{tikzcd}
FGX \arrow{d}[swap]{\epsilon_X} \arrow{r}{FGg} & FGY \arrow{d}{\epsilon_Y}\\
X \arrow{r}[swap]{g} & Y
\end{tikzcd}\end{tabular}\]

/Paso 3: Comprobar la condiciÃ³n de composiciÃ³n./

Por Ãºltimo tenemos los dos triÃ¡ngulos siguientes, cuya conmutatividad
equivale a la condiciÃ³n de que la composiciÃ³n debÃ­a ser la identidad.

\[\begin{tabular}{cc} \begin{tikzcd}
F \arrow{r}{F \eta_X} \arrow{dr}{id} & FGF \arrow{d}{\epsilon_{FX}} \\
 & F
\end{tikzcd} & \begin{tikzcd}
G \arrow{r}{\eta_{GX}} \arrow{dr}{id} & GFG \arrow{d}{G\epsilon_X} \\
 & G
\end{tikzcd}\end{tabular}
\]

Para ello usamos las identidades anteriores comprobando que:

\[\begin{aligned}
\epsilon \circ F\eta &= \varphi(\eta) = 1 \\
G\epsilon \circ \eta &= \varphi(\epsilon) = 1
\end{aligned}\]

$\quad$
#+end_proof

****** Equivalencia de definiciones: desde unidades a familia de isomorfismos
#+begin_theorem
Dada una adjunciÃ³n en tÃ©rminos de unidad y counidad, podemos construir
una adjunciÃ³n en tÃ©rminos de familia de isomorfismos.
#+end_theorem

#+begin_proof
/Paso 1: DefiniciÃ³n de los isomorfismos./

Por las condiciones sobre la composiciÃ³n de unidad y counidad,
tenemos:

\[\begin{aligned}
\varepsilon \circ F\eta &= 1 \\
G\varepsilon \circ \eta &= 1
\end{aligned}\]

Y por las condiciones de naturalidad de ambas transformaciones, se
tiene:

\[\begin{aligned}
\eta \circ f &= GFf \circ \eta \\
g \circ \varepsilon &= \varepsilon \circ FGg
\end{aligned}\]

Definimos el isomorfismo y su inversa, que seguimos notando igual,
como:

\[\begin{aligned}
\varphi(f) &= \varepsilon \circ Ff \\
\varphi(g) &= Gg \circ \eta
\end{aligned}\]

Se comprueba trivialmente que es isomorfismo por las condiciones
anteriores. Tenemos asÃ­ las igualdades:

\[\begin{aligned}
\eta     \circ f        &= \varphi(Ff) \\
g        \circ \epsilon &= \varphi(Gg) \\
\epsilon \circ Ff       &= \varphi(f) \\
Gg       \circ \eta     &= \varphi(g) \\
\end{aligned}\]

/Paso 2: Naturalidad de los isomorfismos./

Demostraremos que el isomorfismo es natural en cada una de sus
componentes. La naturalidad aquÃ­ se deduce de que la definiciÃ³n
de $\varphi$ nos da las siguientes ecuaciones para cualesquiera $f,g,h$:

\[\begin{aligned}
\varphi(f \circ h)   &= Gf \circ \varphi(h) \\
\varphi(h \circ Fg) &= g \circ \varphi(h)
\end{aligned}\]

Que nos dan la naturalidad de $\varphi$ en ambas componentes.
#+end_proof

***** Unicidad del adjunto
#+begin_theorem
El adjunto es esencialmente Ãºnico, es decir,
si tenemos funtores $F : {\cal A} \to {\cal B}$ y $G,G' : {\cal B} \to {\cal A}$ y son ambos adjuntos por la
derecha al primero, $F \dashv G, F \dashv G'$; entonces existe un isomorfismo natural
$\tau : G \cong G'$.
#+end_theorem

#+begin_proof
/Paso 1: Definiendo el isomorfismo natural./

Por ser ambas adjunciones, tenemos un isomorfismo natural en ambas
variables $X,Y$ dado por $\varphi : Hom(X,GY) \cong Hom(X,G'Y)$, ya que
ambos eran isomorfos a $Hom(FX,Y)$.

Tomamos para cada $A$, la componente de nuestro isomorfismo natural
en $A$ como $\tau_A = \varphi_A(id_{GA})$.

/Paso 2: Probando la naturalidad./

Aplicamos dos veces la naturalidad de $\varphi$ para tener, dado un
$f : A \to B$:

\[\begin{tabular}{cc}
\begin{tikzcd}
Hom(GA,GA)\rar{\varphi} \dar[swap]{f^\ast} & Hom(GA,G'A) \dar{(Gf)^\ast} \\
Hom(GA,GB)\rar{\varphi} & Hom(GA,G'B)
\end{tikzcd} & \begin{tikzcd}
Hom(GB,GB)\rar{\varphi} \dar[swap]{(Gf)_\ast} & Hom(GB,GB') \dar{(Gf)_\ast} \\
Hom(GA,GB)\rar{\varphi} & Hom(GA,G'B)
\end{tikzcd}\end{tabular}\]

Obtenemos, tomando de la identidad en ambos diagramas, que $\tau \circ Gf = \varphi(Gf)$
y que $G'f \circ \tau = \varphi(Gf)$. Y uniendo ambas igualdades tenemos la condiciÃ³n de
naturalidad de la transformaciÃ³n $\tau$. Por ser la imagen por un isomorfismo 
natural del isomorfismo identidad, todas sus componentes son isomorfismos.
#+end_proof
***** Continuidad
#+begin_theorem
Todo funtor que es un adjunto derecho (equivalentemente, que tiene un
adjunto izquierdo) es *continuo*; es decir, preserva lÃ­mites
categÃ³ricos. Por otro lado, todo functor que es un adjunto izquierdo
es *cocontinuo* y preserva colÃ­mites categÃ³ricos.
#+end_theorem

#+begin_proof
Sea $a$ el lÃ­mite de un funtor en la categorÃ­a $A$ y sea $G : A \to B$ un
funtor con adjunto a la izquierda $F \dashv G$. Comprobaremos que si existiera
otro cono desde $x$, descompondrÃ­a de forma Ãºnica por $Ga$, haciÃ©ndolo lÃ­mite.

\[\begin{tabular}{ccc}\begin{tikzcd}
a \dar[d]\dar[d,shift left=1, bend left]\dar[d,shift right=1, bend right] \\
\dots
\end{tikzcd} &
$\Longrightarrow$
&
\begin{tikzcd}
x
\arrow[in=70, out=290]{dd}
\arrow[bend left, shift left=1]{dd}
\arrow[bend right, shift right=1]{dd} \\
Ga 
\dar[d]\dar[d,shift left=1, bend left]
\dar[d,shift right=1, bend right] \\
G\dots
\end{tikzcd}\end{tabular}\]

Pero entonces, por la adjunciÃ³n, por cada $x \to Gi$ tenemos un $Fx \to i$, y estas
aplicaciones generan un cono que conmuta con el diagrama por tenerse

\[\begin{tabular}{ccc}\begin{tikzcd}[column sep=0.5em]
& x \dlar[swap]{\alpha}\drar{\beta} & \\
Gi \arrow{rr}{Gf} & & Gj
\end{tikzcd} &
$\Longrightarrow$
&
\begin{tikzcd}[column sep=0.5em]
& Fx \dlar[swap]{\overline{\alpha}}\drar{\overline{\beta}} & \\
i \arrow{rr}{f} & & j
\end{tikzcd}\end{tabular}\]

y por las condiciones de naturalidad de la transformaciÃ³n
$Hom(F-,-) \cong Hom(-,G-)$ tenemos que

\[
\beta = Gf \circ \varphi(\overline{\alpha}) = 
\varphi(f \circ \overline{\alpha}) = \varphi(\overline{\beta})
.\]

AsÃ­, como $a$ es lÃ­mite, tenemos un Ãºnico $Hom(Fx,a)$ que hace conmutar a los
diagramas. Como sÃ³lo existe uno, sÃ³lo existe un $Hom(x,Ga)$, lo que conlleva
que sea $Ga$ efectivamente el lÃ­mite.

El caso de cocontinuidad se obtiene aplicÃ¡ndolo a la categorÃ­a dual.
cite:lane78categories
#+end_proof

***** Ejemplos
****** MÃ³dulos libres
Un *funtor de olvido* es aquel que proyecta estructuras en una
categorÃ­a de estructuras mÃ¡s generales, "olvidando" en el proceso
parte de su estructura. En nuestro caso particular de R-mÃ³dulos,
tenemos el funtor de olvido que lleva cada mÃ³dulo a su conjunto
subyacente y cada homomorfismo a su aplicaciÃ³n de conjuntos:

\[
U : R\mathtt{-Mod} \longrightarrow \mathtt{Set}
\]

Sobre cada conjunto puede generarse un R-mÃ³dulo libre, y cada
aplicaciÃ³n de conjuntos puede extenderse directamente por linealidad
a todo el mÃ³dulo libre. Esto nos da el *funtor de mÃ³dulo libre*:

\[
F : \mathtt{Set} \longrightarrow R\mathtt{-mod}
\]

Definido como, 

\[F(S) = <S> \qquad F(f)\left(\sum rx\right) = \sum rf(x)\]

Hay una *adjunciÃ³n* entre el funtor libre y el funtor de olvido
$F \dashv U$, ya que tenemos la correspondencia natural entre homomorfismos
dada por, para un conjunto $X$ y un R-mÃ³dulo $M$:

\[
Hom(FX,M) \cong Hom(X,UM)
\]

Que hace corresponder a cada aplicaciÃ³n entre conjuntos su extensiÃ³n
lineal, que estÃ¡ biunÃ­vocamente determinada.

La naturalidad se tiene por tenerse para cada $x \in X$:

\[\begin{aligned}
\varphi(g\circ f)(x) = g(f(x)) =& Ug \circ f(x) \\
\varphi(Ff \circ g)(x) = Ff(g(x)) =& f(\varphi(g)(x))
\end{aligned}\]

****** Otros funtores libres y de olvido
De la misma forma que funciona el funtor de olvido entre
mÃ³dulos y conjuntos, funciona con otras estructuras algebraicas,
como por ejemplo:

 - Grupos a conjuntos.
 - Grupos abelianos a grupos.
 - K-Ã¡lgebras a K-mÃ³dulos.

****** Funtor diagonal
******* CategorÃ­a producto
#+begin_definition
Dada una categorÃ­a ${\cal C}$ con productos y coproductos ($\mathtt{Set}$, por ejemplo) 
definimos ${\cal C}\times{\cal C}$ como la categorÃ­a que tiene como objetos a pares de 
objetos de ${\cal C}$ y morfismos a pares de morfismos que se componen componente
a componente:

\[
(f,g)\circ(h,i) = (f\circ h, g\circ i)
\]
#+end_definition

En la categorÃ­a producto, tenemos un *funtor diagonal* $\Delta : {\cal C} \to {\cal C}\times{\cal C}$, que 
lleva cada objeto $A$ a $A\times A$ y cada morfismo $f$ a $(f,f)$.

******* Producto como adjunto derecho
Si definimos el *funtor producto*, $\times : {\cal C}\times{\cal C} \to {\cal C}$, lleva $(A,B)$ en $A\times B$
y cada par de morfismos $f : A \to C$ y $g : B \to D$, en el morfismo producto
x$f \times g : A\times B \to C \times D$, dado por el Ãºnico que hace conmutar:

\[\begin{tikzcd}
& A\times B \dlar[swap]{\pi}\drar{\pi}\dar[dashed] & \\
A\dar[swap]{f} & C \times D \dlar[swap]{\pi}\drar{\pi} & B\dar{g} \\
C & & D \\
\end{tikzcd}\]

Este funtor es adjunto derecho al funtor diagonal. NÃ³tese que se
tiene:

\[
Hom(\Delta A, (B,C)) \cong Hom(A, B \times C)
\]

Y utilizamos la propiedad universal del producto para llevar dos
morfismos $A \to B$ y $A \to C$ a un morfismo al producto $A \to B \times C$.
Y puede comprobarse la naturalidad.

******* Coproducto como adjunto izquierdo
Si definimos el *funtor coproducto* $\coprod : {\cal C}\times{\cal C} \to {\cal C}$, lleva $(A,B)$ en
$A \coprod B$ y cada par de morfismos $f : A \to C$ y $g : B \to D$, en el morfismo
coproducto, dado por el Ãºnico que hace conmutar:

\[\begin{tikzcd}
A\dar{f}\drar{i} & & B\dar{g}\dlar[swap]{i} \\
C\drar{i} & A \coprod B \dar[dashed] &D\dlar[swap]{i} \\
& C \coprod D &
\end{tikzcd}\]

Y este funtor es adjunto izquierdo al funtor diagonal. TeniÃ©ndose
el isomorfismo siguiente y la naturalidad por la propiedad universal
del coproducto:

\[
Hom\left(A \coprod B, C\right) \cong Hom((A, B), \Delta C)
\]

****** Tensor-Hom
Existe una adjunciÃ³n entre los funtores *tensor y Hom*. cite:kan58adjoint
Si $R,S$ son dos anillos y fijamos un (R;S)-mÃ³dulo $X$, tenemos los dos funtores

\[\begin{aligned}
F : \mathtt{Mod-}R \longrightarrow \mathtt{Mod-}S
&\qquad&
F(Y) = Y \otimes_R X\\
G : \mathtt{Mod-}S \longrightarrow \mathtt{Mod-}R
&\qquad&
G(Z) = \mathrm{Hom}(X,Z)
\end{aligned}\]

y tenemos el isomorfismo natural

\[
\mathrm{Hom}_{S}(Y \otimes_{R} X, Z) \cong
\mathrm{Hom}_{R}(Y, \mathrm{Hom}_{S}(X,Z))
\]

dado por $\widehat{f}(y)(x) = f(y \otimes x)$.

bibliographystyle:unsrt
bibliography:math.bib
**** RetÃ­culos
**** MÃ³dulos libres
# PÃ¡gs 167 Aluffi -> DefiniciÃ³n de mÃ³dulos libres
# PÃ¡gs 349 Aluffi -> ClasificaciÃ³n de mÃ³dulos libres sobre PIDs
# PÃ¡gs 359 Aluffi -> Endomorfismos de mÃ³dulos libres

***** DefiniciÃ³n de mÃ³dulo libre
#+begin_definition 
Definimos el *R-mÃ³dulo libre* cite:aluffi09_rings sobre $A$ como un mÃ³dulo $F^R(A)$ 
con una inclusiÃ³n $j : A \to F^R(A)$ como aquel que cumple que para cualquier 
aplicaciÃ³n $f : A \to M$ a un R-mÃ³dulo, existe un Ãºnico homomorfismo de 
R-mÃ³dulos $\varphi:F^R(A) \to M$ que hace conmutar el diagrama

\[\begin{tikzcd}
F^R(A) \rar[dashed]{\exists!\varphi} & M \\
A \uar{j}\urar[swap]{f} &
\end{tikzcd}\]
#+end_definition

Sabemos por ser una propiedad universal que si existe, serÃ¡ Ãºnico salvo
isomorfÃ­as y que $j\colon A \to F^R(A)$ serÃ¡ inyectivo.

****** ConstrucciÃ³n
#+begin_definition
Dado un conjunto $A$, definimos la *suma directa indexada* sobre Ã©l como
las aplicaciones de soporte finito

\[
N^{\oplus A}
=
\left\{ \alpha\colon A \to N 
\mid 
\alpha(a) \neq 0 \text{ sÃ³lo para un nÃºmero finito de elementos} \right\}
\]

a las que les damos estructura de R-mÃ³dulo con $(r\alpha)a = r\alpha(a)$.
#+end_definition

AdemÃ¡s, existe una inclusiÃ³n $j\colon A \to R^{\oplus A}$ definida como

\[
j(a)(x) = \left\{\begin{array}{ll} 
1 & \mbox{if } x = a  \\
0 & \mbox{if } x \neq a 
\end{array} 
\right. .
\]

#+begin_theorem
El asÃ­ definido es el mÃ³dulo libre sobre $A$. Es decir, $F^R(A) \cong R^{\oplus A}$.
#+end_theorem

#+begin_proof
NÃ³tese que podemos escribir realmente los elementos de esta suma directa
indexada como

\[
\sum_{a \in A} r_aa,
\]

ademÃ¡s de forma Ãºnica y en un nÃºmero finito de sumandos, uno para cada
elemento en el que la aplicaciÃ³n sea no nula. Esto que nos lleva a que,
una vez definida la imagen de cada elemento $a$, queda definida la imagen
que debe tener $\varphi$ sobre toda el anillo de forma Ãºnica.
#+end_proof

***** Independencia lineal y bases
#+begin_definition
Decimos que un conjunto indexado $i \colon I \to M$ es *linealmente independiente*
(respectivamente *sistema generador*) si el homomorfismo natural desde su
mÃ³dulo libre, $\varphi\colon F^{R}(I) \to M$, haciendo conmutar

\[\begin{tikzcd}
F^{R}(I) \rar{\varphi} & M \\
I \uar{j}\urar[swap]{i} &
\end{tikzcd}\]

es inyectivo (respectivamente sobreyectivo). cite:aluffi09_linear
#+end_definition

#+begin_definition
Un conjunto indexado $I \to M$ es una base cuando es linealmente
independiente y genera $M$.
#+end_definition

#+begin_lemma
Un conjunto indexado $B \to M$ es una base si y sÃ³lo si el homomorfismo
natural desde su mÃ³dulo libre es un isomorfismo $R^{\oplus B} \cong M$. AsÃ­, un
$R\text{-mÃ³dulo}$ es libre si y sÃ³lo si admite una base.
#+end_lemma

#+begin_proof
Trivial si combinamos las definiciones de linealmente independiente y
sistema generador.
#+end_proof

***** Caso de los espacios vectoriales
#+begin_theorem
Los mÃ³dulos sobre un cuerpo son necesariamente libres. Podemos
probarlo usando la caracterizaciÃ³n anterior por bases. De hecho, dado un
subconjunto de vectores linealmente independientes en un espacio
vectorial, existe una base del espacio conteniÃ©ndolos.
#+end_theorem

La nociÃ³n de dimensiÃ³n de un espacio vectorial nos permite recuperar
la cardinalidad de la base sobre la que es mÃ³dulo libre.

***** ClasificaciÃ³n de mÃ³dulos libres en dominios de integridad
#+begin_theorem
Sea $M$ un $R\text{-mÃ³dulo}$ libre para $R$ dominio de integridad con
$B$ un conjunto linealmente independiente maximal. Para cualquier $S$
linealmente independiente,

\[ \# S \leq \# B.
\]

En particular, cualesquiera dos conjuntos linealmente independientes
maximales tienen la misma cardinalidad.
#+end_theorem

# Completar la parte de cuerpos de fracciones.
#+begin_proof
Empezamos tomando cuerpos de fracciones y pasamos a considrear el caso
de $R$ un cuerpo y $M$ un espacio vectorial.

Comprobaremos que podemos ir reemplazando elementos de $B$ por
elementos de $S$ sucesivamente para ir creando sucesivos $B'$ y
seguir manteniendo independencia lineal y la maximalidad. Si tomamos
$B' \cup \{v\}$ para algÃºn $v \in S$, por maximalidad tenemos una
dependencia lineal

\[
c_0v + c_1b_1 + \dots + c_tb_t = 0
\]

con $c_0 \neq 0$ para no contravenir la independencia de $B$; ademÃ¡s, no sÃ³lo
pueden existir elementos no nulos de $S$, porque contravendrÃ­a su independencia.
Debe existir un $c_1 \neq 0$ con $b_1 \in B' \setminus S$ y podemos intercambiar $b_1$ por $v$
teniendo de nuevo un conjunto linealmente independiente maximal, ya que

\[
v = -c_0^{-1}c_1b_1 - \dots - c_0^{-1}c_tb_t.
\]

Si aplicamos inducciÃ³n transfinita bajo una buena ordenaciÃ³n de $S$, podemos
asegurar que se llega a un conjunto de cardinalidad $\#B$ que contiene a
los elementos de $S$.
#+end_proof

#+begin_corollary
Para $R$ un dominio de integridad y dos conjuntos $A,B$,

\[
F^R(A) \cong F^R(B) \iff A \cong B.
\]
#+end_corollary

#+begin_corollary
Para $R$ un dominio de integridad se satisface la propiedad IBN

\[
R^m \cong R^n \iff m = n.
\]
#+end_corollary

***** ClasificaciÃ³n de mÃ³dulos libres en dominios de ideales principales
#+begin_lemma
Sea $R$ un dominio de ideales principales y $F$ un mÃ³dulo libre finitamente
generado sobre Ã©l. Entonces existen $a\in R, x\in F, y\in M$ con $y = ax$ y
$M' \subseteq M,F' \subset F$ con $M' = F' \cap M$ submÃ³dulos cumpliendo

\[
F = \left\langle x \right\rangle \oplus F',
\qquad
M = \left\langle y \right\rangle \oplus M'.
\]
#+end_lemma

#+begin_proof
La familia de ideales $\left\{ \varphi \in \mathrm{Hom}(F,R) \mid \varphi(M) \right\}$ es no vacÃ­a. Como los PID son
noetherianos, tiene un elemento maximal $\alpha(M) = (a)$, para algÃºn $\alpha(y) = a$.

Dado cualquier $\varphi(y)$, si tomamos el generador $(b) = (a,\varphi(y))$ tenemos que

\[
b = ra + s\varphi(y)
\]

y que si definimos $\psi = r\alpha + s\varphi$, tenemos $b = \psi(y) \in \psi(M)$, luego
$(a) \subseteq (b) \subseteq \psi(M)$, y por maximalidad, $a \mid \varphi(y)$.

Si vemos $y = \left( s_1,\dots,s_n \right)$ como elemento de $F\cong R^{\oplus n}$, tenemos $a \mid \pi_i(y) = s_i$,
asÃ­ que sabemos $s_i = ar_i$, y definimos

\[
x = \left( r_1,\dots,r_n \right).
\]

Ahora tomamos $F' = \mathrm{ker}(\alpha)$ y comprobamos las sumas directas.
#+end_proof

#+begin_proposition
Sea $R$ un dominio de ideales principales y $F$ un mÃ³dulo libre finitamente
generado sobre Ã©l. Todo submÃ³dulo $M \subset F$ serÃ¡ libre.
#+end_proposition

#+begin_proof
Aplicamos el lema anterior a los sucesivos $M^{(i)}$ que genere. Tendremos
que eventualmente $M^{(i)} = 0$, ya que los $y^{(i)}$ son independientes y $F$ es
finitamente generado.
#+end_proof

****** Resoluciones en PIDs
#+begin_proposition
Sea $R$ dominio de integridad. SerÃ¡ dominio de ideales principales si y sÃ³lo
si para cualquier epimorfismo a un mÃ³dulo finitamente generado

\[
R^{m_0} \overset{\pi_0} \longrightarrow M \longrightarrow 0,
\]

existe un mÃ³dulo libre haciendo exacta la secuencia

\[
0 \longrightarrow 
R^{m_1} \overset{\pi_1} \longrightarrow
R^{m_0} \overset{\pi_0} \longrightarrow
M \longrightarrow
0.\]
#+end_proposition

***** Anillo de endomorfismos
#+begin_proposition
Los endomorfismos de un $R\text{-mÃ³dulo}$ $F$, $\mathrm{End}(F)$ forman un Ã¡lgebra con la
composiciÃ³n.
#+end_proposition

****** Semejanza
#+begin_definition
Dos matrices $A,B \in {\cal M}_n(R)$ son *semejantes* si representan el mismo
endomorfismo $F \to F$, diferenciÃ¡ndose en la elecciÃ³n de la base.
#+end_definition

#+begin_proposition
Dos matrices $A,B$ son semejantes si y sÃ³lo si

\[
B = PAP^{-1}.
\]
#+end_proposition
# Sacar demostraciÃ³n de pÃ¡gina 360.

#+begin_definition
Dos endomorfismos $\alpha,\beta \colon F \to F$ son *semejantes* si existe un
automorfismo $\pi \colon F \to F$ cumpliendo

\[
\beta = \pi \circ \alpha \circ \pi^{-1}.
\] cite:aluffi09_linear
#+end_definition

****** Semejanza y acciones de anillos de polinomios
#+begin_proposition
Una transformaciÃ³n lineal de $F$ es exactamente lo mismo que una estructura
como $R[X]\text{-mÃ³dulo}$ compatible con la estructura de $R\text{-mÃ³dulo}$.
#+end_proposition

Si tenemos una transformaciÃ³n lineal $\alpha$, podemos definir la acciÃ³n de
un polinomio como

\[
\left( r_mt^m + \dots + r_1t + r_0 \right)(v) =
r_{m}\alpha^m(v) + \dots r_1\alpha(v) + r_0v.
\]

Y por la propiedad universal del anillo de polinomios, toda estructura
de $R[t]\text{-mÃ³dulo}$ quedarÃ¡ determinada por el endomorfismo que asignemos a $t$.

#+begin_lemma
Dadas transformaciones lineales $\alpha,\beta$ de $F$; las estructuras como $R[t]\text{-mÃ³dulo}$
son isomorfas si y sÃ³lo si $\alpha$ y $\beta$ son semejantes.
#+end_lemma
#+begin_proof
Si llamamos $F_{\alpha}$, $F_{\beta}$ a las dos estructuras como $R[t]\text{-mÃ³dulo}$, tendremos que
un isomorfismo $\pi\colon F_{\alpha}\to F_{\beta}$ serÃ¡ lo mismo que una transformaciÃ³n invertible
$\pi\colon F \to F$ cumpliendo $\beta = \pi\circ\alpha\circ\pi^{-1}$.

NÃ³tese de hecho que un isomorfismo entre mÃ³dulos debe comportarse como

\[
\pi\circ\alpha (v) = \pi(tv) = t\pi(v) = \beta\circ\pi(v),
\]

por lo que $\pi\circ\alpha = \beta\circ\pi$ es la condiciÃ³n que lo distingue de cualquier
otra transformaciÃ³n lineal.
#+end_proof

#+begin_corollary
Hay una correspondencia biyectiva entre clases de semejanza de transformaciones
lineales de un $R\text{-mÃ³dulo}$ libre $F$ y clases de isomorfÃ­a de estructuras de
$R[t]\text{-mÃ³dulo}$ en $F$.
#+end_corollary

NÃ³tese que esto se expande a las matrices en el caso finito-dimensional.

***** Proyectividad
#+begin_theorem
Todo mÃ³dulo libre es proyectivo.
#+end_theorem
#+begin_proof
Supongamos que tenemos un mÃ³dulo libre $F$ sobre el conjunto $A$. Dado
un epimorfismo $\varphi\colon M \to N$, tendremos la situaciÃ³n siguiente, donde
podemos definir una aplicaciÃ³n de $A$ a $M$ por ser $\varphi$ epimorfismo.
Dado cualquier $\alpha\colon F \to N$ se tiene

\[\begin{tikzcd}
& A\dar{i}\ar{ddl} \\
& F\dar{\alpha}\dlar[dashed] \\
M\rar{\varphi} & N \\
\end{tikzcd}\]

tales que conmutan el triÃ¡ngulo exterior y el superior. AsÃ­ tenemos
que ambas funciones coinciden sobre la base y por tanto coinciden
para todo el mÃ³dulo libre.
#+end_proof
***** Referencias
bibliographystyle:unsrt
bibliography:math.bib
**** CategorÃ­as abelianas
***** Objeto nulo
#+begin_definition
En una categorÃ­a, un *objeto nulo* es aquel que es a la vez inicial y final.
#+end_definition

NÃ³tese que no todas las categorÃ­as tienen por quÃ© tener un objeto nulo.
La categorÃ­a $\mathtt{Set}$, por ejemplo, tiene objetos inicial y final no isomorfos.

#+begin_definition
En una categorÃ­a con objeto nulo llamamos *morfismo cero* entre dos
objetos, $0_{a,b}\colon a \to b$, al que resulta de componer el Ãºnico morfismo $a \to 0$ con 
el Ãºnico morfismo $0 \to b$.
#+end_definition

***** NÃºcleos y conÃºcleos
#+begin_definition
En una categorÃ­a con objeto nulo, el *nÃºcleo* de un morfismo
$f \colon a \to b$ es un morfismo $k \colon \mathrm{ker}(f) \to a$ tal que $f\circ k = 0$ y que es universal 
respecto a esa propiedad; es decir, para cualquier otro $h$ cumpliendo 
que $f \circ h = 0$, se tiene el diagrama

\[\begin{tikzcd}
c \dar[dashed]{\exists! h'}\ar[bend right=90,swap]{dd}{h}\arrow[bend left=45]{ddr}{0} &   \\
\mathrm{ker}(f) \dar{k}\drar{0} &   \\
a\rar{f} & b & .\\
\end{tikzcd}\]
#+end_definition

De otra forma, podrÃ­amos definirlo como el *ecualizador* del morfismo $f$ con
el morfismo cero, es decir, como el universal respecto al diagrama

\[\begin{tikzcd} \mathrm{ker}(f) \rar{k} & 
a \rar[bend left]{f}\rar[bend right,swap]{0} & b
\end{tikzcd},\]

y por tanto, es un lÃ­mite finito y es Ãºnico salvo isomorfismo. cite:aluffi09_linear

#+begin_definition
En una categorÃ­a con objeto nulo, se define el *conÃºcleo*, $c \colon b \to \mathrm{coker}(f)$
de manera dual al nÃºcleo, como universal segÃºn el siguiente diagrama 
conmutativo

\[\begin{tikzcd}
c  &   \\
\mathrm{coker}(f)  \uar[dashed]{\exists! h'} &   \\
a \ar[bend left=90]{uu}{0}\uar{0} \rar{f} & b \ular[swap]{c} \arrow[bend right=45,swap]{uul}{h} \\
\end{tikzcd}\]
#+end_definition

****** Propiedades del nÃºcleo
#+begin_proposition
Cualquier nÃºcleo es un monomorfismo. Dualmente, cualquier conÃºcleo es
un epimorfismo.
#+end_proposition
#+begin_proof
Si se tienen dos $m,n\colon d \to \mathrm{coker}(f)$, entonces sabemos que $m \circ k$ y $n \circ k$,
por propiedad universal, hacen que exista un Ãºnico $h \circ k = m\circ k=n\circ k$.
Debe tenerse por tanto $h=m=n$.
#+end_proof

NÃ³tese que el converso no tiene por quÃ© ser cierto. En general, no todo
monomorfismo es nÃºcleo ni todo epimorfismo es conÃºcleo.

****** Ejemplo: grupos
En la categorÃ­a $\mathtt{Grp}$, el objeto cero es el grupo trivial. El nÃºcleo de
cualquier morfismo es lo que llamamos usualmente nÃºcleo, como se puede
comprobar trivialmente. NÃ³tese que todos los nÃºcleos son normales en un 
grupo pero que no todas las inclusiones lo son como subgrupo normal, por 
lo que no todos los monomorfismos serÃ¡n aquÃ­ nÃºcleos.

***** CategorÃ­as preaditivas
#+begin_definition
Una *categorÃ­a preaditiva* es aquella en la que cada conjunto de morfismos
$\mathrm{hom}(a,b)$ es un grupo abeliano y la composiciÃ³n es bilinear respecto a la
operaciÃ³n de grupo.
#+end_definition

#+begin_proposition
Para un objeto en una categorÃ­a preaditiva, $z \in {\cal A}$, equivalen:

  1) $z$ es inicial.
  2) $z$ es final.
  3) $\mathrm{id}_z$ es el elemento neutro de $\mathrm{hom}(z,z)$.
  4) $\mathrm{hom}(z,z)$ es el grupo trivial.
#+end_proposition
#+begin_proof
Si $z$ es inicial o final, se tiene un Ãºnico $\mathrm{id}_z = 0$, que da el grupo trivial.
Si se tiene $\mathrm{id}_z=0$, entonces para cualquier morfismo $f\colon a \to z$, se tendrÃ¡

\[
f = \mathrm{id}_z \circ f = 0\circ f = 0
\]

por la bilinealidad de la composiciÃ³n. Dualmente se verÃ¡ que es inicial.
#+end_proof

****** Biproductos
#+begin_definition
Un *biproducto* para dos objetos en una categorÃ­a preaditiva $a,b \in A$ es un
$c$ con morfismos

\[\begin{tikzcd}
a \rar[bend right,swap]{i_{1}} &
c \rar[bend left]{p_2} \lar[bend right,swap]{p_1} &
b \lar[bend left]{i_2}
\end{tikzcd}\]

cumpliendo las identidades $p_1i_1 = \mathrm{id}_a$, $p_2i_2 = \mathrm{id}_b$, y $i_1p_1 + i_2p_2 = \mathrm{id}_{c}$.
#+end_definition

#+begin_theorem
Dos objetos en una categorÃ­a preaditiva $a,b \in A$ tienen producto (o coproducto) 
si y sÃ³lo si tienen un biproducto, que serÃ¡ a su vez producto y coproducto.
#+end_theorem

***** CategorÃ­as abelianas
#+begin_definition
Una *categorÃ­a abeliana* es una categorÃ­a preaditiva cumpliendo que

 1) tiene un objeto nulo.
 2) tiene biproductos finitos.
 3) todo morfismo tiene nÃºcleo y conÃºcleo.
 4) todo monomorfismo es nÃºcleo y todo epimorfismo es conÃºcleo.
#+end_definition

****** FactorizaciÃ³n de un morfismo
#+begin_proposition
En una categorÃ­a abeliana, cada morfismo se factoriza como $f = m\circ e$,
donde $m = \mathrm{ker}(\mathrm{coker}(f))$ y $e = \mathrm{coker}(\mathrm{ker}(f))$. 
AdemÃ¡s, esta factorizaciÃ³n cumple que, dada cualquier otra factorizaciÃ³n
de la forma $f' = m'e'$ con $m'$ monomorfismo, $e'$ epimorfismo y con morfismos
de la forma

\[\begin{tikzcd}
\cdot \rar{f}\dar[swap]{g} & \cdot \dar{h} \\
\cdot \rar[swap]{f'} & \cdot &,
\end{tikzcd}\]

existe un Ãºnico $k$ cumpliendo

\[\begin{tikzcd}
\cdot \arrow[bend left]{rr}{f}\dar[swap]{g} \rar{e}& 
\cdot\rar{m}\dar{k} & \cdot \dar{h} \\
\cdot \arrow[bend right,swap]{rr}{f'} \rar{e'} & \cdot\rar{m'} & \cdot
\end{tikzcd}\]
#+end_proposition
#+begin_proof
Tomamos $m = \ker(\operatorname{coker} f)$. Como $(\operatorname{coker} f)\circ f = 0$, por propiedad universal
del nÃºcleo sabemos que $f$ se escribe como $f = me$ para algÃºn $e$. Como puede
demostrarse que $e$ serÃ¡ epimorfismo, luego $e = \operatorname{coker}(\ker f)$.

Dadas $f=me$ y $f'=m'e'$ con $g,h$ del diagrama, consideramos
$u = \ker f = \ker e$ y entonces tenemos que $0 = hfu = m'e'gu$, luego $e'gu = 0$.
Por ser $u$ nÃºcleo, $e'g$ factoriza en $e = \operatorname{coker}(u)$ como $e'g = ke$ para algÃºn
$k$ que ademÃ¡s debe ser Ãºnico. AsÃ­, $m'ke = hme$ y $m'k = hm$, dando la
conmutatividad del diagrama.
#+end_proof

#+begin_definition
La *imagen* y *coimagen* de un morfismo $f = me \colon a \to b$ se definen como

 * $\operatorname{im} f = m$
 * $\operatorname{coim} f = e$
#+end_definition

La proposiciÃ³n anterior se usa para comprobar que son Ãºnicas salvo
isomorfismo.

***** Secuencias exactas
****** Exactitud
#+begin_definition
Un par de morfismos componibles es *exacto* en el objeto que comparten
cuando $\operatorname{im} f = \operatorname{ker} g$. Equivalentemente, cuando $\operatorname{coker} f = \operatorname{coim} g$.
#+end_definition

****** Complejos de cadenas
#+begin_definition
En una categorÃ­a abeliana, un *complejo de cadenas* es una secuencia

\[\begin{tikzcd}
\dots \rar &
c_{n+1} \rar{\partial_{n+1}} &
c_n \rar{\partial_n} &
c_{n-1} \rar &
\dots
\end{tikzcd}\]

cumpliendo que $\partial_n\partial_{n+1} = 0$.
#+end_definition

****** Secuencias exactas cortas
#+begin_definition
Una *secuencia exacta corta* es un diagrama

\[
0 \longrightarrow
a \overset{f}\longrightarrow
b \overset{g}\longrightarrow
c \longrightarrow
0
\]

que es exacto en $a$,$b$ y $c$.
#+end_definition

#+begin_definition
Un *morfismo de secuencias exactas cortas* estÃ¡ formado por tres morfismos
$f,g,h$ que hacen conmutar el diagrama

\[\begin{tikzcd}
0 \rar& 
\cdot \rar{m}\dar{f}& 
\cdot \rar{e}\dar{g}& 
\cdot \rar\dar{h}& 
0 \\
0 \rar& 
\cdot \rar{m'}& 
\cdot \rar{e'}& 
\cdot \rar& 
0 & .\\
\end{tikzcd}\]

Las secuencias exactas cortas de una categorÃ­a abeliana $A$ con estos 
morfismos forman la categorÃ­a $\mathtt{Ses}(A)$, que se hace preaditiva sumando
las tres componentes de cada morfismo.
#+end_definition

***** Resultados en categorÃ­as abelianas
****** ManipulaciÃ³n elemental en categorÃ­as abelianas
#+begin_proposition
Si dados dos morfismos $f,g$ hacia $c$ calculamos su producto fibrado
(/pullback/) tendremos que $f$ epimorfismo nos da $f'$ epimorfismo en

\[\begin{tikzcd}
s\rar{f'} \dar[swap]{g'} & d \dar{g} \\
b\rar{f} & c
\end{tikzcd}\]

donde ademÃ¡s, el nÃºcleo de $f$ factoriza como $\mathrm{ker}(f) = g'\circ \mathrm{ker}(f')$.
#+end_proposition
#+begin_proof
El producto fibrado se construye formando la secuencia exacta

\[\begin{tikzcd}
0\rar&s\rar{m}&b\oplus d\rar{fp_1-gp_2}& c
\end{tikzcd}\]

y tomando $g' = p_1m$ y $f'=p_2m$. Probaremos que $fp_1-gp_2$ es un
epimorfismo, para lo que basta comprobar que si $h(fp_1-gp_2) = 0$
entonces

\[
0 = h(fp_1-gp_2)i_1 = hfp_1i_1 = hf,
\]

y por ser epimorfismo $f$, $h = 0$. Ahora probaremos que $f'$ es
epimorfismo; si $uf'=up_2m=0$, por exactitud, es de la forma
$up_2 = u'(fp_1-gp_2)$. Ahora tenemos

\[
0 = up_2i_1 = u'f
\]

llegÃ¡ndose a $u'=0$ por ser $f$ epimorfismo.
#+end_proof

#+begin_definition
Definimos un *miembro* de $a$ como un morfismo con codominio $a$. Existe
una equivalencia $x \equiv y$ entre dos miembros cuando existen epimorfismos
$u,v$ tales que $xu=yv$.
#+end_definition
#+begin_proof
Para demostrar la transitividad de esta relaciÃ³n de equivalencia,
debemos aplicar la proposiciÃ³n anterior al diagrama siguiente,

\[\begin{tikzcd}
\cdot \rar\dar & 
\cdot \rar\dar& 
\cdot \dar{x}\\
\cdot \rar\dar& 
\cdot \rar{y}\dar{y}&
a \\
\cdot \rar{z} &
a &&,\\
\end{tikzcd}\]

donde probamos que si $x \equiv y$ y $y \equiv z$, entonces $x \equiv z$.
#+end_proof

Dado un morfismo $f \colon a \to b$, cada $x \in a$ da lugar a $f \circ x \in b$; y ademÃ¡s,
$x \equiv y$ implica $f x \equiv f y$. Gracias a esto, podemos tratar a los miembros
de un objeto en una categorÃ­a abeliana de la misma manera de la que
tratamos a los elementos de un conjunto. La aplicaciÃ³n de funciones
se comporta de la misma manera y preserva la relaciÃ³n de equivalencia
de los miembros.

#+begin_proposition
En cualquier categorÃ­a abeliana cite:lane78categories

 1) $f \colon a \to b$ es /monomorfismo/ ssi para $x \in a$, $f(x) \equiv 0 \implies x \equiv 0$.
 2) $f \colon a \to b$ es /monomorfismo/ ssi para $x,y \in a$, $f(x) \equiv f(y) \implies x\equiv y$.
 3) $g\colon b \to c$ es /epimorfismo/ ssi para $z\in c$, existe $y \in b$ con $g(y) \equiv z$.
 4) $h\colon r \to s$ es /nulo/ ssi para $x \in r$, $hx \equiv 0$.
 5) $a \overset{f}\to b\overset{g}\to c$ es /exacta/ ssi $gf = 0$ y para cada $g(y)\equiv 0$ existe un
    $x \in a$ tal que $f(x) \equiv v$.
 6) Si existen $g(x) = g(y)$, existe $g(z) = 0$; ademÃ¡s cualquier $f(x) \equiv 0$
    implica $f(y) \equiv f(z)$ y cualquier $h(y)\equiv 0$ implica $h(x) \equiv -h(z)$.
#+end_proposition
#+begin_proof
Se tienen (1) y (2) por definiciÃ³n de monomorfismo. Se tiene ademÃ¡s
(3) por construcciÃ³n del producto fibrado y (4) por definiciÃ³n.

Si factorizamos $f = me$, por exactitud se tendrÃ¡ $\operatorname{ker} g = m$. Si $g y \equiv 0$,
$y \equiv my'$, y si construimos el producto fibrado

\[\begin{tikzcd}
\cdot\dar[dashed]{y''}\rar[dashed]{e'} & \cdot\dar{y'}\drar[bend left=45]{y} \\
\cdot\rar{e} & \cdot\rar{m} & \cdot &,
\end{tikzcd}\]

como $e'$ es epimorfismo, $y \equiv fy''$.

A la inversa, si para $y \in b$ existe $k = \ker g$, entonces $k \in b$ y $gk \equiv 0$.
Existe entonces $x \in a$ con $fx \equiv k$, es decir, $ku \equiv mexv$. Esto lleva
a $\operatorname{im} f \geq \ker g$ y a $gf = 0$, la exactitud.
#+end_proof

****** Lema de los cinco
#+begin_theorem
En un diagrama conmutativo con filas exactas

\[\begin{tikzcd}
a_1 \rar{g_1} \dar{f_1} & 
a_2 \rar{g_2} \dar{f_2} &
a_3 \rar{g_3} \dar{f_3} & 
a_4 \rar{g_4} \dar{f_4} & 
a_5 \dar{f_5} \\
b_1 \rar{h_1} &
b_2 \rar{h_2} &
b_3 \rar{h_3} &
b_4 \rar{h_4} &
b_5 & ,
\end{tikzcd}\]

si $f_2,f_4$ son isomorfismos, $f_1$ es epimorfismo y $f_5$ es monomorfismo, $f_3$ es isomorfismo.
#+end_theorem
#+begin_proof
Usando la manipulaciÃ³n de diagramas cuyas reglas hemos escrito en
la proposiciÃ³n anterior, demostraremos que $f_3$ es monomorfismo.
La dualidad servirÃ¡ para demostrar a su vez que es epimorfismo.

ExplÃ­citamente, si hubiera un elemento en $a_3$ que diera un cero en
$b_3$, habrÃ­a un cero en $b_4$, por ser isomorfismo, habrÃ­a un cero en
$a_4$, y entonces existirÃ­a, por exactitud, un elemento en $a_2$ cuya
imagen serÃ­a el elemento original en $a_3$. Por isomorfismo, este
deberÃ­a dar un elemento en $b_2$ cuya imagen serÃ­a cero, asÃ­ que
por exactitud existirÃ­a un elemento en $b_1$ del que serÃ­a imagen.
Como $f_1$ es epimorfismo, existirÃ­a un elemento en $a_1$ del que
serÃ­a imagen, y entonces el elemento original serÃ­a la imagen
por la composiciÃ³n de dos morfismos en secuencia exacta del
primer elemento. DeberÃ­a ser cero, quedando probado $f_3$ como
monomorfismo.
#+end_proof

****** Lema de la serpiente
#+begin_theorem
Dado un morfismo de secuencias exactas cortas $f,g,h$; existe un morfismo
$\delta \colon \operatorname{ker} h \to \operatorname{coker} f$ tal que la secuencia siguiente es exacta

\[\begin{tikzcd}
0 \rar &
\mathrm{ker}(f) \rar{m} &
\mathrm{ker}(g) \rar{e} &
\mathrm{ker}(h) \arrow[out = 0,in =180,swap]{dll}{\delta} \\&
\mathrm{coker}(f) \rar{m'} &
\mathrm{coker}(g) \rar{e'} &
\mathrm{coker}(h) \rar &
0
\end{tikzcd}\]
#+end_theorem
#+begin_proof
El diagrama extendido que tenemos es

 \[ \begin{tikzcd}
	& 0 \dar              & 0 \dar            & 0 \dar           &   \\
 0 \rar & ker(f) \rar \dar  & ker(g) \rar \dar    & ker(h) \dar \ar[out=355, in=175,looseness=1, overlay, swap]{dddll}{\delta}       &   \\
 0 \rar & a \rar{m} \dar{f}  & b \rar{e} \dar{g} & c \rar \dar{h}        & 0 \\
 0 \rar & a' \rar{m'} \dar & b' \rar{e'} \dar & c' \rar \dar        & 0 \\
	& coker(f) \rar \dar & coker(g) \rar \dar  & coker(h) \rar \dar & 0 \\
	& 0                   & 0                 & 0                &
 \end{tikzcd} \]

y desde Ã©l, manipulando de nuevo el diagrama podemos construir primero
el morfismo $\delta$ y demostrar despuÃ©s que efectivamente es exacto.

ExplÃ­citamente, lo que harÃ­amos serÃ­a tomar un elemento en $\mathrm{ker}(h)$.
Este elemento pasarÃ­a a $c$ y luego, como un cero a $c'$. Como $e$ es
sobreyectiva, existirÃ­a un elemento en $b$, y luego uno en $b'$ que
harÃ­a conmutar el diagrama. Pero como este elemento irÃ­a hacia
un cero al aplicar $e'$, deberÃ­a estar en la imagen de $m'$, asÃ­ sÃ³lo
deberÃ­amos pasar de $a$ a $\mathrm{coker}(f)$ para terminar la construcciÃ³n de
$\delta$.

NÃ³tese que si el elemento procede de $\mathrm{ker}(g)$, entonces serÃ­a nulo en $b'$,
y de ahÃ­ serÃ­a nulo en $a'$ y en $\mathrm{coker(f)}$; y que la imagen de un
elemento que hubiera llegado desde $\delta$, al pasar a $\mathrm{coker}(g)$ deberÃ­a ser
$0$ por provenir desde $b$. Esto demuestra la exactitud del diagrama.
#+end_proof

***** Referencias
bibliographystyle:unsrt
bibliography:math.bib

** AnÃ¡lisis funcional
# Exportaba con config.setup

*** 1. Espacios normados
**** Espacios normados
***** Norma y seminorma
*Norma*. FunciÃ³n $\|\cdot\| : X \longrightarrow \mathbb{R}$ verficando:

  - $\|x\| = 0 \Leftrightarrow x = 0$
  - $\|ax\| = |a| \|x\|$
  - $\|x+y\| \leq \|x\|+\|y\|$

****** Seminorma
Cuando $\|x\| = 0$ no implica $x = 0$, se llama *seminorma*. Define
una norma en un espacio cociente.

  - $\|ax\| = |a| \|x\|$
  - $\|x+y\| \leq \|x\|+\|y\|$

****** Sublineal
Cuando ademÃ¡s $\|\alpha x\| = \alpha\|x\|$ sÃ³lo se cumple para reales positivos, 
se llama funcional *sublineal*.

  - $\|ax\| =  a \|x\|$ para $a \in \mathbb{R}^+$
  - $\|x+y\| \leq \|x\|+\|y\|$

****** Distancia de la norma
Desde la norma se puede definir una *distancia* asociada 
$d(x,y) = \|x-y\|$, que hace a $X$ un *espacio mÃ©trico*. La distancia 
cumple:

  - $d(x+a,y+a) = d(x,y)$
  - $d(\lambda a) = |\lambda| d(a)$

****** TopologÃ­a de la norma
La distancia hace a un espacio normado un *espacio topolÃ³gico*
con abiertos:

\[\tau = \{G \subset X \;|\; \forall a \in G: \exists r > : B(a,r) \subset G\}\]

Dos normas que generan el mismo espacio topolÃ³gico son 
*equivalentes*.

****** Equivalencia proporcional
Dos normas $\|.\|$ y $\|.\|_\ast$ generan topologÃ­as equivalentes cuando:

\[\exists m,M \in \mathbb{R^+}:\; m \|x\| \leq \|x\|_\ast \leq M \|x\|\]

******* DemostraciÃ³n
Se demuestra por mutua inclusiÃ³n de bolas.

***** Espacios vectoriales topolÃ³gicos
Cualquier espacio vectorial sobre $\mathbb{R}$ o $\mathbb{C}$ es normado.

****** DemostraciÃ³n
Dada una base \(\{e_i\}\) del espacio, podemos escribir $x = \sum \alpha_i e_i$ 
y definirla como:

\[ \|x\| = \sum |\alpha_i|\]

La suma es finita por definiciÃ³n de base.

***** Continuidad de la norma, suma y producto
La norma es continua en su espacio por ser lipschitziana:

\[ |\|x\| -  \|y\|| \leq \|x-y\| \]

****** Continuidad de suma y producto
La suma y el producto por escalares son continuos, usando que la
convergencia en el producto equivale a la convergencia por
coordenadas.

****** Continuidad de homotecias y translaciones
Como corolario, lo son las *homotecias* y *translaciones*, todas las
bolas cerradas son homeomorfas a la bola unidad.

***** Operaciones sobre conjuntos
Para $X$ espacio normado:

  1. $A$ abierto $\Rightarrow$ $A+B$ abierto.
  2. $A$ cerrrado, $B$ compacto $\Rightarrow$ $A+B$ cerrado.
  3. $A,B$ compactos $\Rightarrow$ $A+B$ compacto.
  4. $M$ subespacio $\Rightarrow$ $\overline{M}$ subespacio.

****** DemostraciÃ³n de 1
Es la uniÃ³n de abiertos, $A + B = \bigcup_{b \in B} (b + A)$.

****** DemostraciÃ³n de 2
Sea $x \in \overline{A+B}$, $\exists \{a_n,b_n\} : \{a_n,b_n\} \longrightarrow x$, por compacidad
tenemos $\{b_{\sigma_n}\} \longrightarrow b \in B$ y por tanto $\{a_{\sigma_n}\} \longrightarrow x-b \in A$.

****** DemostraciÃ³n de 3
Se tiene $A\times B$ compacto, y la suma es continua, luego
$A+B$ es compacto.

****** DemostraciÃ³n de 4
Usando la continuidad de la suma y del producto por 
escalares:

\[(+)(\overline{M}\times\overline{M}) 
= (+)\overline{(M\times M)}
\subset \overline{(+)(M \times M)}\]
\[(*)(\mathbb{K}\times\overline{M})
= (*)\overline{(\mathbb{K}\times M)}
\subset \overline{(*)(\mathbb{K} \times M)}\]
      
****** Contraejemplo de suma de cerrados
La suma de dos cerrados puede no ser cerrado:

\[\left\{n + \frac{1}{n} \mid n \in \mathbb{N}\right\} + 
\left\{-n \mid n \in \mathbb{N}\right\} = 
\left\{\frac{1}{n} \mid n \in \mathbb{N}\right\}\]

***** ConexiÃ³n de espacios normados
Todo espacio normado es *conexo* y *localmente arcoconexo*;
por tanto *arcoconexo*. De hecho, la bola unidad es *convexa*.

***** Espacios de Banach
Un *espacio de Banach* es un espacio normado completo.

**** Desigualdades bÃ¡sicas
***** Desigualdad de Young
Para $a,b\in\mathbb{R}^+$ y $p>1$ con $\frac{1}{p}+\frac{1}{q} = 1$ se tiene:

\[ab \leq \frac{a^p}{p}+\frac{b^q}{q}\]

****** DemostraciÃ³n
Se demuestra aplicando desigualdad de Taylor al logaritmo con 
pesos $1/p$ y $1/q$.

\[\log(ab) = \frac{1}{p}\log(a^p) + \frac{1}{q}\log(b^q) \leq 
\log\left(\frac{a^p}{p} + \frac{b^q}{q}\right)\]

***** Desigualdad de HÃ¶lder
Para $a_1\dots a_nb_1\dots b_n \in \mathbb{R}^+_0$ con $\frac{1}{p} +\frac{1}{q} = 1$ se verifica:

\[\sum a_kb_k \leq \left(\sum a_k^p\right)^{1/p}\left(\sum b_k^q\right)^{1/q}\]

****** DemostraciÃ³n
Se demuestra aplicando Young a la divisiÃ³n de ambos lados y
cuidando el caso $0$. Llamamos $\alpha = \left(\sum_k a^p_k\right)^{1/p}$, $\beta = \left(\sum_k \beta^q_k\right)^{1/q}$:

\[\frac{a_kb_k}{\alpha\beta}
\leq \frac{a_k^p}{p\alpha^p} + \frac{b_k^q}{q\beta^q}\]

Sumando cada desigualdad tenemos:

\[\frac{1}{\alpha\beta} \sum a_kb_k \leq 
\frac{1}{p\alpha^p}\sum a_k^p +
\frac{1}{q\beta^q}\sum b_k^q = 1\]

***** Desigualdad de Minkowski
Para $a_1\dots a_nb_1\dots b_n \in \mathbb{R}^+_0$, $p>1$, se verifica:

\[\left(\sum_{k=1}^n (a_k+b_k)^p \right)^{1/p} \leq 
\left(\sum_{k=1}^n a_k^p \right)^{1/p} + 
\left(\sum_{k=1}^n b_k^p \right)^{1/p} \]

Dicho de otra forma:

\[\|a+b\|_p \leq \|a\|_p + \|b\|_p\]

****** DemostraciÃ³n
Aplicando HÃ¶lder con $p$, $1 - 1/p$ para tener:

\[\begin{aligned}
\left( \sum_{k=1}^n (a_k+b_k)^p \right)^{1/p} 
&=
\left( \sum_{k=1}^n (a_k+b_k)(a_k+b_k)^{p-1} \right)^{1/p}
\\&=
\left( \left( \sum_{k=1}^n a_k^p \right)^{1/p} + \left( \sum_{k=1}^n b_k^p \right)^{1/p} \right)
\left( \sum_{k=1}^n (a_k+b_k)^{\frac{p(p-1)}{p-1}} \right)^{1-1/p}
\end{aligned}\]

**** Ejemplos de espacios normados
***** Espacios de dimensiÃ³n finita
Solemos notar por ${l}_p^n = (\mathbb{K}^n,\|.\|_p)$ al espacio de Banach sobre $\mathbb{R}^n$ o $\mathbb{C}^n$ 
que da la norma:

\[\|x\|_p = \left(\sum |x_{(k)}|^p \right)^{1/p}\]

NÃ³tese el caso especial $l^n_\infty$ que da la norma del mÃ¡ximo.

****** Normas
Todas estas normas lo son gracias a la [[*Desigualdad de Minkowski][desigualdad de Minkowski]].

****** Equivalencia
Todas las normas son equivalentes y generan el mismo espacio de
Banach:

\[ \|x\|_\infty \leq \|x\|_p \leq \|x\|_1 \leq N \|x\|_\infty\]

******* DemostraciÃ³n
Aplicamos la desigualdad de las medias.

***** Espacios de sucesiones
Las *sucesiones* tales que su p-suma es convergente,
con las normas $\|.\|_p$, dan los siguientes espacios de Banach:

\[\ell_p = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K} \,\middle|\,
\sum_{n=1}^\infty |x(n)|^p < \infty \right\}\]

siendo un caso particular el de la norma del supremo
sobre *sucesiones acotadas*:

\[{\cal \ell}_\infty = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ \left|\ x(n) \text{ acotada} \right\}\]

****** Forman un subespacio vectorial
Pasando la desigualdad de Minkowski al lÃ­mite, tenemos:

\[\left(\sum^\infty_{k=1} (a_k+b_k)^p \right)^{1/p} \leq 
\left(\sum^\infty_{k=1} a_k^p \right)^{1/p} + 
\left(\sum^\infty_{k=1} b_k^p \right)^{1/p} \]

Por tanto, es un subespacio vectorial y se obtiene una norma
como:

\[\|x\|_p = \left(\sum_{n=1}^\infty |x(n)|^p \right)^{1/p}\]

****** Son espacios de Banach
Como tenemos $|x_n(k)-x_m(k)| \leq \|x_n-x_m\|$, cuando $\{x_n\}$ es 
Cauchy en $\ell_p$, tambiÃ©n es Cauchy $\{x_n(k)\}$. Y por tanto, es
convergente por componentes $x(k) = \lim_{n\to\infty}x_n(k)$.

Usamos $\{x_n\}$ de Cauchy para tener:

\[\exists n_0 : \forall m,n\geq n_0 :
\|x_n-x_m\| < \varepsilon\]

Es decir,

\[\sum_{k=1}^N |x_n(k)-x_m(k)|^p \leq (\|x_n-x_m\|_p)^p < \varepsilon^p\]

Tomando $m \to \infty$, y luego tomando $N \to \infty$:

\[\sum_{k=1}^\infty |x_n(k)-x(k)|^p \leq \varepsilon^p\]

AsÃ­, tenemos que $x = x_n - (x_n-x) \in \ell_p$, y como $\|x_n-x\|_p \leq \varepsilon$,
tenemos $\{x_n\} \to x$.

***** Subespacios del espacio de sucesiones
El espacio de sucesiones cuenta con subespacios usando la misma 
norma:

****** Sucesiones convergentes
Es un subespacio de $\ell_\infty$ cerrado, y por tanto, de Banach.

\[c = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \{x(n)\} \text{ convergente } \right\}\]
 
****** Sucesiones nulas
Otro subespacio de $\ell_\infty$, tambiÃ©n cerrado y por tanto, de Banach.

\[c_0 = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \{x(n)\} \longrightarrow 0 \right\}\]

****** Sucesiones casi-nulas o de soporte finito
Es un subespacio para todo $\ell_p$.

\[c_{00} = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \exists m: \forall n \geq m:\  x(n) = 0 \right\}\]

Este es un subespacio denso ya que toda sucesiÃ³n es lÃ­mite de
sucesiones de soporte finito. Pero no es el total, asÃ­ que no
serÃ¡ completo.

***** Espacios de funciones continuas acotadas
Dado $T$ espacio topolÃ³gico, tomamos el espacio de funciones
continuas y acotadas:

\[{\cal C}_b(T) = \left\{f : T \longrightarrow \mathbb{K} \mid
f \text{ continua, acotada}\right\}\]

Y lo dotamos de la *norma del supremo*:

\[ \|f\|_\infty = \sup\{ |f(t)| \mid t \in T\}\]

Que da la *convergencia uniforme*.

****** Es espacio de Banach
Si tenemos $\{f_n\}$ de Cauchy, $\{f_n(x)\}$ es de Cauchy y converge a $f(x)$.
Como tenemos algÃºn $n$ para el que $\forall p\geq n: \|f_n-f_p\|_\infty \leq \varepsilon$, entonces:

\[|f_n(x)-f_p(x)| \leq \|f_n-f_p\|_\infty \leq \varepsilon\]

Y tomando lÃ­mite en $p$ se tiene $|f_n(x) - f(x)| \leq \varepsilon$, luego
$\|f_n-f\|_{\infty} \leq \varepsilon$.

****** Anuladas en infinito
Sea $L$ compacto y separado. Una funciÃ³n se *anula en el infinito*
cuando:

\[\forall \varepsilon: 
\{t\in L \mid |f(t)|\geq\varepsilon\} 
\text{ es compacto}\]

Las funciones continuas que se anulan en el infinito forman 
${\cal C}_0(L)$, subespacio de ${\cal C}_b(L)$. Es espacio cerrado 
y por tanto de Banach.

****** Funciones de soporte compacto
El soporte de $f : L \longrightarrow \mathbb{K}$ para $L$ localmente compacto 
separado es:

\[sop(f) = 
\overline{\{t \in L \mid f(t) \neq 0\}} \subset
L \]

Las funciones con soporte compacto forman ${\cal C}_{00}(L)$, subespacio
vectorial de ${\cal C}_0(L)$.

****** RelaciÃ³n entre ambas
Tenemos que ${\cal C}_{00}(L)$ no es completo en general y que:

\[\overline{{\cal C}_{00}(L)} = {\cal C}_0(L)\]

***** Espacios de funciones derivables
Consideraremos el espacio de funciones sobre un intervalo que sean $d$
veces derivables con derivadas continuas, ${\cal C}^n([a,b],\mathbb{K}^d)$. Escritas:

\[f^{k)} = \left(f^{k)}_1,f^{k)}_2,\dots,f^{k)}_d\right)\]

Sobre Ã©l defimos una norma del supremo sobre cada derivada
$\|f^{k)}\| = max \{\|f^{k)}_i\|_\infty\}$. La norma del espacio es la suma de la de 
cada una de las derivadas.

\[ \|f\|_\infty = \sum \|f^{k)}\|_\infty\]

****** Es espacio de Banach
Esto, por el *teorema de la convergencia uniforme* nos lleva a que una
sucesiÃ³n de Cauchy converja de manera que respete la derivada. Este
serÃ¡ un espacio de Banach.

***** Espacios de funciones integrables
Consideramos el *espacio de funciones p-integrables* como:

\[ L_p(\Omega) =
\left\{ f \in L(\Omega) \;\middle|\; \int_\Omega |f(t)|^p dt < \infty \right\}
\]

NormÃ¡ndolas con:

\[ \|f\|_p =
\left( \int_\Omega |f(t)|^p dt \right)^{1/p}\]

Debemos /identificar funciones que coinciden c.p.d./ para
deducir $\|f\|_p = 0 \Rightarrow f = 0$. Estos espacios son siempre completos.

****** Desigualdades integrales de HÃ¶lder y Minkowski
A partir de la desigualdad de Young llegamos
a la *desigualdad integral de HÃ¶lder* para funciones
tales que $|f|^p, |g|^p$ son Lebesgue-integrables:

\[\int_\Omega |f(t)g(t)| dt \leq
\left( \int_\Omega |f(t)|^p dt \right)^{1/p}
\left( \int_\Omega |g(t)|^q dt \right)^{1/q}\]

Y desde ella, la *desigualdad integral de Minkowski*:

\[\left( \int_\Omega |f(t) + g(t)|^p dt \right)^{1/p} \leq
\left( \int_\Omega |f(t)|^p dt \right)^{1/p} +
\left( \int_\Omega |g(t)|^p dt \right)^{1/p}\]

Esto nos da la naturaleza de norma.

****** Complitud en el caso real
Usando el Teorema de Riesz-Fisher.

***** Espacios de funciones esencialmente acotadas
Una funciÃ³n es *esencialmente acotada* cuando es $f : [0,1] \longrightarrow \mathbb{K}$:

\[\exists M: |f| \leq M \text{ c.p.d.}\]

Al espacio de funciones esencialmente acotadas lo llamamos ${\cal L}_\infty[0,1]$ y 
le damos una seminorma:

\[\phi_\infty(f) = \inf\{ M
 \mid |f|\leq M \text{ c.p.d.}\}\]

****** Naturaleza de seminorma
Se cumple por un lado que:

\[\phi_\infty(\alpha f) = |\alpha|\phi_\infty(f)\]

Y por otro lado que:

\[ \phi_\infty(f+g) \leq \phi_\infty(f) + \phi_\infty(g) \]

****** Estructura de espacio normado
Podemos convertirlo en espacio normado si tomamos cociente sobre:

\[ N = \{ f \in {\cal L}_\infty[0,1] \mid
\phi_\infty(f)=0 \} \]

Esto es espacio de Banach con la norma $\phi_\infty$.

**** CategorÃ­a de espacios normados
***** Homomorfismos topolÃ³gicos
Los morfismos de la categorÃ­a de espacios normados son los
*homomorfismos topolÃ³gicos*, operadores lineales y continuos que
ademÃ¡s son abiertos en su imagen. Hablamos igualmente de
*monomorfismos topolÃ³gicos*, *epimorfismos topolÃ³gicos* o de
*isomorfismos topolÃ³gicos*.

***** Producto de espacios normados
Dados $X_1,\dots,X_n$ espacios normados, podemos definir normas 
sobre su producto cartesiano $\prod X_i$:

 - *Norma del mÃ¡ximo*: $\|x\|_\infty = \max\{\|x_i\|\}$
 - *Norma p*: $\|x\|_p = (\sum \|x_i\|^p)^{1/p}$

****** TopologÃ­a del producto
La convergencia es trivialmente coordenada a coordenada, y
las topologÃ­as asociadas son equivalentes a la topologÃ­a
producto. El espacio es *Banach* ssi lo son las componentes.

***** Cociente de un espacio normado
Sea $M$ subespacio vectorial cerrado de $X$. Podemos hacer a $X/M$ 
normado con:

\[ \|x+M\| 
= \inf\left\{ \|x - m\| \mid m \in M \right\} 
= d(x,M)\]

****** TopologÃ­a del cociente
Este espacio es de Banach ssi $X$ es de Banach y $M$ completo. 
La topologÃ­a coincidirÃ¡ con la topologÃ­a cociente.

****** Proyecciones
La proyecciÃ³n $Q: X \longrightarrow X/M$ es lineal, sobreyectiva, abierta
y continua.

**** Operadores y funcionales lineales
***** Operadores lineales continuos
Para $T : X \longrightarrow Y$ lineal entre espacios normados, equivalen:

- $T$ lipschitziana
- $T$ uniformemente continua
- $T$ continua
- $T$ continua en $0$
- $\exists M: \|Tx\| \leq M\|x\|$
- $T$ preserva acotaciÃ³n, $A$ acotado da $TA$ acotado
- $TB_X$ acotado
- $TS_X$ acotado

Y lo llamamos *operador lineal continuo*, $T \in L(X,Y)$.

****** DemostraciÃ³n
******* Lipschitzianidad
La lipschitzianidad implica hasta la continuidad en $0$.

******* Continuidad en 0 implica acotaciÃ³n
Si hay continuidad en $0$ existe $\|x\| < \delta \implies \|Tx\| \leq 1$. Tenemos
entonces:

\[ \left\|\frac{\delta}{\|x\|}x\right\| = \delta \implies
T\left( \frac{\delta}{\|x\|}x \right) \leq 1\]

Luego $\|Tx\| \leq \|x\|/\delta$.

******* AcotaciÃ³n
La acotaciÃ³n implica todo lo demÃ¡s excepto lipschitzianidad.

******* AcotaciÃ³n de la esfera implica lipschitzianidad.
Como tenemos $\frac{x-y}{\|x-y\|} \in S_X$, lo tenemos acotado por algÃºn $\alpha$ y:

\[T(x-y) \leq \alpha\|x-y\|\]

Por lo tanto, es lispchiztiano.
****** Norma de operadores
Se define la *norma de operadores* como:

\[\begin{aligned}
\|T\| =& \sup_{x \in B_X}\left\{\|Tx\|\right\} \\
=& \sup_{x \in S_X}\left\{\|Tx\|\right\} \\
=& \min\{k \mid \|Tx\| \leq k\|x\| \}
\end{aligned}\]

Y cumple que $\|Tx\| \leq \|T\|\|x\|$.

******* TODO EstÃ¡ bien definida
***** Cuatro teoremas sobre la norma de operadores
Dados $X,Y$ espacios normados:

 1. $(L(X,Y),\|.\|)$ es espacio normado.
 2. $\{T_n\}\longrightarrow T$ ssi $\{T_n\}\longrightarrow T$ uniformemente en $B_X$.
 3. Si $Y$ es de Banach, $L(X,Y)$ es de Banach.
 4. Para $X \overset{T}\longrightarrow Y \overset{S}\longrightarrow Z$, se tiene $\|S \circ T\| \leq \|S\|\|T\|$.

****** DemostraciÃ³n
******* Primer punto
La norma de operadores es norma porque hereda la desigualdad
triangular y la linealidad de la norma del espacio. NÃ³tese
que si $\sup_{x \in B_X} \|Tx\|=0$, es porque $T = 0$.

******* Segundo punto
NÃ³tese que la norma de operadores mide el supremo en la bola
unidad y por homotecias se extiende al espacio.

\[\begin{aligned}
\{T_n\} \longrightarrow T 
&\iff \{\|T_n-T\|\} \longrightarrow 0 
\\&\iff 
\sup_{x \in B_X}\{\|T_n(x) - T(x)\|\} \longrightarrow 0 
\\&\iff 
\sup_{x \in X}\left\{ \left\|T_n\left(\frac{x}{\|x\|}\right) - T\left(\frac{x}{\|x\|}\right) \right\|\right\}
\longrightarrow 0
\end{aligned}\]

******* Tercer punto
Sea $\{T_n\}$ Cauchy. Tenemos $\|T_p(x) - T_q(x)\| \leq \|T_p - T_q\|\|x\|$, por lo que
sabemos $\{T_n(x)\}$ Cauchy; por complitud  $\{T_n(x)\} \longrightarrow T(x)$. Es lineal:

\[T(\alpha x + \beta y) = 
\lim \left( \alpha T_n(x) + \beta T_n(y) \right)
\longrightarrow
\alpha T(x) + \beta T(y)
\]

Tomando lÃ­mites en la condiciÃ³n de Cauchy, vemos que es acotada
la funciÃ³n $T_p - T$:

\[\|T_p(x) - T(x)\| \leq \varepsilon\]

Luego $T$ es lineal y continua. Y ademÃ¡s, $\{T_n\} \longrightarrow T$.

******* Cuarto punto
$\|S \circ T (x)\| \leq \|S\|\|T\|\|x\|$

***** Espacio dual topolÃ³gico
Sea $X$ normado, su *dual topolÃ³gico* es el espacio de funciones al
cuerpo con la norma de operadores:

\[ X^\ast = 
L(X,\mathbb{K}) 
= \left\{ f : X \longrightarrow \mathbb{K} \mid
f \text{ lineal y continua} \right\}
\]

***** ExtensiÃ³n desde un subespacio denso
Sea $M$ subespacio denso en $X$, para cada $T \in L(M,Y)$ existe
un $S \in L(X,Y)$ tal que $S|_M = T$ y $\|S\| = \|T\|$

****** DemostraciÃ³n
Podemos extenderla por continuidad, y comprobamos que es lineal:

\[\begin{aligned}
S(\alpha x_n + \beta y_n) = 
T(\alpha x_n + \beta y_n) = 
\alpha Tx_n + \beta Ty_n 
\longrightarrow \alpha Sx_n + \beta Ty_n
\end{aligned}\]

Como la norma es continua y el supremo invariante a clausuras,
hay igualdad entre las normas.

***** CaracterizaciÃ³n de operadores abiertos
Sea $T : X\longrightarrow Y$ lineal. Equivalen:

 - $T$ es abierta.
 - $T(B_X)$ es entorno de $0$.

****** Sobreyectividad
Una funciÃ³n lineal y abierta debe ser sobreyectiva ya que la imagen
es subespacio vectorial abierto, y por tanto el total.

****** DemostraciÃ³n
Si es abierta, trivialmente $T(B_X)$ es abierto.

Sea $O \subseteq X$ abierto. Sea $x \in O$, con $B(x,r) \subseteq O$. Como $T(B_X)$ es entorno 
de $0$, tenemos $T(x) + T(B_X)r = T(B(x,r)) \subseteq T(O)$ entorno de $x$, por ser
la translaciÃ³n y homotecia [[*Continuidad de homotecias y translaciones][homeomorfismos]].

***** CaracterizaciÃ³n de operadores abiertos sobre la imagen
Sea $T : X \longrightarrow Y$ lineal. Equivalen:

 - $T$ abierta sobre $TX$.
 - $\exists r>0:\quad TX \cap rB_Y \subset TB_X$.
 - $\exists\alpha>0: \forall y\in TX: \exists x\in X: 
  \quad \|x\| \leq \alpha\|y\|$, cumpliendo $Tx = y$.

****** DemostraciÃ³n
Aplicando la caracterizaciÃ³n anterior a $TX$ tenemos que
equivalen el primer y segundo apartado.

Por el apartado 2, tengo que dado un $y$, $y\frac{\delta}{\|y\|} \in \delta B_Y$.
Por tanto, $\exists x: Tx = y \frac{\delta}{\|y\|}$. Tomamos $x' = x \frac{\|y\|}{\delta}$, y tenemos
que $T(x') = y$, y ademÃ¡s que $\|x'\| = \|x\|\frac{\|y\|}{\delta} \leq \frac{\|y\|}{\delta}$.

***** DescomposiciÃ³n canÃ³nica: proyecciÃ³n al cociente
Sea $X$ un espacio normado, $M$ subespacio cerrado y $\pi$ la proyecciÃ³n.
Entonces $\pi \in L(X,X/M)$ es sobreyectiva y abierta, con $\|\pi\| = 1$ cuando
$M \neq X$.

****** DemostraciÃ³n
******* Es continua
Por caracterizaciÃ³n de [[*Operadores lineales continuos][operadores lineales continuos]]:

\[ \|\pi(x)\| = \|x+M\| \leq \|x\|\]

******* Tiene norma unidad
Para ver que tiene norma 1, tomamos $x_0 \notin M$.

\[\forall m \in M: \|x+M\| = \|\pi(x+m)\| \leq \|\pi\|\|x+m\|\]

En particular,

\[ \|x+M\| \leq \|\pi\| \inf\{\|x+m\|\} = \|\pi\|\|x+M\| \]

******* Sobreyectiva y abierta
Trivialmente es sobreyectiva. Usamos la caracterizaciÃ³n de 
[[*CaracterizaciÃ³n de operadores abiertos][operadores abiertos]], comprobando que:

\[\pi^{-1}(B(r,0)) = \{x \in X \mid \|x+m\| < r\} = \bigcup_{m \in M} B(r,m)\]

Por tanto, $B(r,0) \subseteq \pi(B(1,0))$ es un entorno de $0$ por contener un 
abierto.

***** DescomposiciÃ³n canÃ³nica: isomorfismo
Sea $T: X\longrightarrow Y$ lineal con $\ker(T)$ cerrado. Se define:

\[\begin{tikzcd}
X \rar[two heads]{\pi} & X/\ker T \rar{\widehat T}[swap]{\cong} & \im T \rar[hook]{i} & Y
\end{tikzcd}\]

Y se tiene:

  - $\hat{T}$ continua ssi $T$ continua. En cuyo caso $\|T\| = \|\hat{T}\|$.
  - $T$ abierta en $TX$ ssi $\hat{T}$ abierta en $TX$.

****** TODO DemostraciÃ³n
**** Teorema de Tychonoff. DimensiÃ³n finita
***** Lema a Tychonoff
Toda aplicaciÃ³n lineal desde $\ell_2^n$ es continua.

****** DemostraciÃ³n
Comprobamos que estÃ¡ acotada. Dada una base finita y las coordenadas
sobre ella:

\[
\|Tx\|
=
\left\|\sum_{k=1}^n T(e_k)x_k \right\| 
\leq 
\sum_{k=1}^n |x_k| \|T(e_k)\|
\leq
\|x\| \sum_{k=1}^n \|T(e_k)\|
\]

***** Teorema de Tychonoff
Sea $X$ espacio normado. Toda biyecciÃ³n lineal de $\ell^n_2$ sobre $X$ es 
isomorfismo topolÃ³gico.

****** DemostraciÃ³n
Por el lema es continua. La esfera de $\ell^n_2$ es compacta; y podemos 
aplicar la caracterizaciÃ³n de aplicaciones abiertas anterior.

***** Corolarios al teorema de Tychonoff
Se cumple que:

  1. $T:X\longrightarrow Y$ lineal con $dim(X) < \infty$ nos da $T$ lipschiztiana.
  2. Dos espacios de dimensiÃ³n finita son isomorfos ssi tienen igual
     dimensiÃ³n.
  3. En un espacio de dimensiÃ³n finita, todas las normas son
     equivalentes.
  4. Todo espacio de dimensiÃ³n finita es Banach.
  5. Todo subespacio de dimensiÃ³n finita de espacio normado es cerrado.
  6. Un subconjunto de un espacio normado de dimensiÃ³n finita es
     compacto ssi es cerrado y acotado.

****** DemostraciÃ³n
******* Punto 1
Tenemos $X \cong \ell^n_2 \longrightarrow Y$, [[*Lema a Tychonoff][continua]].

******* TODO Punto 2

***** Dual topolÃ³gico en dimensiÃ³n finita
Sea $X$ normado de dimensiÃ³n finita, su *dual topolÃ³gico* es:

\[X^\ast =
\left\{ f:X \longrightarrow \mathbb{K} \mid
f \text{ lineal } \right\}\]

****** DemostraciÃ³n
Toda lineal es continua si sale de dimensiÃ³n finita.

***** Compacidad relativa y precompacidad
Llamamos $A$ *relativamente compacto* cuando $\overline{A}$ es compacto.
Llamamos $A$ *precompacto* cuando, dado un $\varepsilon$, existen $x_1,\dots,x_n$:

\[ A \subset \bigcup_{k=1}^n B(x_n,\varepsilon) \]

****** Cadena de implicaciones
Compacidad implica compacidad relativa, que a su vez implica
precompacidad, que implica acotaciÃ³n.

****** Corolario de Tychonoff de compacidad
En un espacio normado de dimensiÃ³n finita, un subconjuto es
relativamente compacto ssi es acotado y ssi es precompacto.

***** Corolario de caracterizaciÃ³n de continuas
Sea $T : X \longrightarrow Y$ lineal con $TX$ de dimensiÃ³n finita.
Equivalen:

  1. $T$ es continua.
  2. $\ker T$ cerrado en $X$.

****** DemostraciÃ³n
Cuando $\ker T$ es [[*DescomposiciÃ³n canÃ³nica: isomorfismo][cerrado]], $X/\ker T \cong TX$ dimensiÃ³n finita. $\widehat T$ serÃ¡
continua.

***** Corolario de caracterizaciÃ³n de abiertas
Sea $T : X \longrightarrow Y$ lineal con $X$ de dimensiÃ³n finita.
Equivalen:

  1. $T$ es abierta.
  2. $T$ es sobreyectiva.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
$TX$ serÃ­a abierto y [[*Corolarios al teorema de Tychonoff][cerrado]] a la vez.

******* Segunda implicaciÃ³n
$T$ es continua, luego $\ker T$ [[*Corolario de caracterizaciÃ³n de continuas][cerrado]]. $\widehat T : X/\ker T \cong Y$ biyecciÃ³n lineal,
que es por tanto isomorfismo topolÃ³gico y abierta.

***** Corolario de caracterizaciÃ³n de la dimensiÃ³n finita.
Equivalen:

  1. Todo cerrado y acotado es compacto.
  2. Bola unidad compacta.

**** Teorema de Riesz
***** Lema al teorema de Riesz
Sea $X$ espacio normado con $M$ subespacio propio cerrado. Si
$\varepsilon \in (0,1)$, existe $x \in \mathbb{S}_X$ tal que:

\[ \|x+M\| = d(x,M) > 1-\varepsilon \]

****** DemostraciÃ³n
Sea $x_0 \notin X-M$. Por ser $M$ cerrado $d(x_0,M)>0$. Por ser
el Ã­nfimo, tengo que existe $m_0$ tal que:

\[\frac{1}{1-\varepsilon}\| x_0 - M\| > \| x_0-m_0 \| \]

Por tanto, tomando $x = \frac{x_0-m_0}{\|x_0-m_0\|} \in \mathbb{S}_X$, tenemos:

\[ \| x + M \| 
=  \left\| \frac{x_0-m_0}{\|x_0-m_0\|} + M \right\|
= \frac{\|x_0-M\|}{\|x_0-m_0\|} > 0\]

***** Teorema de Riesz
Son equivalentes:

1. $X$ de dimensiÃ³n finita.
2. $X$ es localmente compacto.
3. $B_X$ es compacta.
4. $B_X$ es [[*Compacidad relativa y precompacidad][precompacta]].

****** DemostraciÃ³n
1. Cuando la dimensiÃ³n es finita, $X \cong \mathbb{K}^n$, que es localmente
   compacto.
2. Sea $U$ entorno compacto de $0$, $\exists r>0: \overline{B}(0,r) \subset U$. Por ser un cerrado
   en compacto, es compacto. Por homeomorfismo, lo es $B_X$.
3. Compacidad implica precompacidad.
4. Por ser $B_X$ precompacta,

   \[B_X \subseteq \bigcup B\left(x_i,\frac{1}{2}\right)\]
   
   Como $M = \langle x_1,x_2,\dots,x_k \rangle$ es de dimensiÃ³n finita, es un subespacio 
   cerrado y propio en $X$. Por el lema de Riesz, existe $x_0 \in \mathbb{S}_X$ 
   tal que:

   \[ \|x_0 - x_i\| \leq d(x_0,M) > \frac{1}{2} \]
   
   Teniendo entonces $x \notin \bigcup B(x_i,\frac{1}{2})$, que nos lleva a 
   contradicciÃ³n.

*** 2. Principios fundamentales del anÃ¡lisis funcional
**** Teorema de Hahn-Banach
***** VersiÃ³n analÃ­tica de Hahn-Banach
Sea $M \subseteq X$ subespacio con $p$ sublineal y $g : M \longrightarrow \mathbb{K}$ lineal 
verificando:

\[Re(g(m)) \leq p(m)\]

Entonces existe $f : X \longrightarrow \mathbb{K}$ lineal extendiÃ©ndolo y verificando:

\[Re(f(x)) \leq p(x)\]

Cuando $p$ es [[*Seminorma][seminorma]], se tiene ademÃ¡s que $|f(x)|\leq p(x)$.

****** DemostraciÃ³n
******* Primera extensiÃ³n en los reales
En un primer caso, sea $\mathbb{K} = \mathbb{R}$. Podemos tomar $x_0 \notin X-M$, 
crear $Y = M\oplus x_0\mathbb{R}$ y extender como:

\[ f(m + \lambda x_0) = g(m) + \lambda\alpha\]

******* Elegir el coeficiente de la extensiÃ³n
Nos falta elegir el $\alpha$. Sabemos que debe cumplir:

\[\alpha \leq \frac{1}{\lambda}\left( p(m+\lambda x_0) - g(m) \right)\]

Tomando un $\lambda$ positivo y negativo llegamos a dos 
condiciones:

\[ g(v) - p(v-x_0) 
\leq \alpha 
\leq p(u+x_0) - g(u)\]

Y tenemos un $\alpha$ cumpliendo esta condiciÃ³n por ser 
equivalente a:

\[ g(u+v) \leq p(u+v) \leq p(u+x_0) + p(v-x_0)\]

Y lo sacamos partiendo la desigualdad, minimizando
y maximizando cada lado de la desigualdad, y dando
la vuelta a todas las desigualdades:

\[ g(u) - p(v-x_0) \leq \alpha \leq p(u+x_0) - g(u)\]

******* Lema de Zorn
Puedo ordenar las extensiones por inclusiÃ³n, teniendo
ademÃ¡s que una cadena de extensiones tiene por maximal a
la uniÃ³n de todos los espacios, con la funciÃ³n definida
por el primer conjunto en el que aparece el primer elemento.

Si $Y \neq X$ fuera el maximal, podrÃ­a aÃ±adir $x_0 \in X-Y$ y
contravenir la maximalidad de $Y$ extendiendo una dimensiÃ³n.

******* Caso complejo
Como todo espacio sobre los complejos lo es sobre los reales,
aplicamos el caso real a $g_0 = Re(g)$, y obtenemos $f_0$ cumpliÃ©ndolo
y siendo lineal en los reales.

Creo $f$ siendo lineal en los complejos como:

\[f(x) = f_0(x) - if_0(xi)\]

Que cumple las condiciones

******* Caso de la seminorma
Cuando $p$ es seminorma tengo, para $|\alpha|=1$ dando el giro
apropiado:

\[f(\alpha x) = |f(x)| = Re(f(\alpha x)) \leq p(\alpha x) = p(x)\]

***** ExtensiÃ³n equinÃ³rmica de Hahn-Banach
Sea $M \subseteq X$ subespacio vectorial, con $g \in M^\ast$. 
Existe $f\in X^\ast$ tal que $f|_M = g$ y $\|f\| = \|g\|$.

****** DemostraciÃ³n
Sea $p(x) = \|g\|\|x\|$, es una seminorma y cumple $Re(g(m)) \leq p(x)$.
Por *Hahn-Banach*, tenemos una extensiÃ³n $f$, cumpliendo $\|f\| \geq \|g\|$
por ser extensiÃ³n y:

\[ \|f\| 
= \sup\left\{ \frac{|f(x)|}{\|x\|} \mid x\in X-\{0\}\right\} 
\leq \|g\|\]

***** SeparaciÃ³n en el dual topolÃ³gico
Sea $x_0 \in X$, entonces existe $f \in \mathbb{S}_{X^\ast}$, tal que $f(x_0) = \|x_0\|$.
En consecuencia:

 1. Si $x \neq y$, existe $f \in X^\ast: f(x) \neq f(y)$.
 2. $\forall x \in X: \|x\| = max_{f \in B_{X^\ast}} \{ |f(x)| \}$

****** DemostraciÃ³n
Podemos aplicar Hahn-Banach a $g : x_0\mathbb{K} \longrightarrow \mathbb{K}$ con $g(\lambda x_0) = \lambda \|x_0\|$,
para obtener una extensiÃ³n de norma $1$.

****** Corolario 1
Aplicamos el resultado para $f(x-y) = \|x-y\| = 0$.

****** Corolario 2
Tenemos $|f(x)| \leq \|x\|$. El mÃ­nimo se alcanza en un $f$ dado por
la proposiciÃ³n.

***** Corolario para subespacios finitos
Sea $X$ un espacio normado $\{x_1,\dots,x_n\} \subseteq X$ linealmente independientes
y $\alpha_1,\dots,\alpha_n \in \mathbb{K}$, entonces existe $f \in X^\ast$ tal que $f(x_i) = \alpha_i$.

****** DemostraciÃ³n
Puedo crear una funciÃ³n lineal, sobre $\langle x_1,\dots,x_n \rangle$ que lo cumpla.
Por venir de dimensiÃ³n finita serÃ¡ continua, asÃ­ que podemos
aplicar Hahn-Banach para obtener una extensiÃ³n.

***** Corolario: L(X,Y) es Banach ssi Y es Banach
El espacio $L(X,Y)$ con la norma de operadores es Banach ssi
el espacio $Y$ es Banach.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
[[*Cuatro teoremas sobre la norma de operadores][Cuatro teoremas sobre la norma de operadores]]

******* Segunda implicaciÃ³n
[[http://math.stackexchange.com/questions/1023681/y-is-a-banach-space-if-bx-y-is-a-banach-space][Y is a Banach space if B(X,Y) is a Banach spacea]]

**** InyecciÃ³n canÃ³nica e isometrÃ­as
***** InyecciÃ³n canÃ³nica
Se define $J_X(x_0) : X^\ast \longrightarrow \mathbb{K}$ como:

\[J_X(x_0)(f) = f(x_0)\]

****** La inyecciÃ³n es isomÃ©trica
Se verifica:

  1. $\forall x \in S_X: J_X(x)$ es lineal y de mÃ³dulo $1$.
  2. $J_X : X \longrightarrow X^{\ast\ast}$ es isomÃ©trica y lineal.

******* TODO DemostraciÃ³n
***** CompletaciÃ³n de un espacio
$X^\ast$ es completo para cualquier espacio normado $X$. Cuando $X$ no es 
completo, $J_X$ no es sobreyectivo y podemos completarlo como:

\[ X \subset \overline{J_X(X)} \]

Que lo serÃ¡ por ser cerrado en $X^{\ast\ast}$, que sÃ­ es completo.

***** Polar de un subespacio
Dado $M \subset X$, el *polar* de $M$ se define como:

\[M^0 = \left\{ f\in X^\ast \mid f(M) = \{0\} \right\}\]

****** DefiniciÃ³n equivalente
Para la restricciÃ³n $S : X^\ast \longrightarrow M^\ast$, $M^0 = \ker S$.

***** Primer teorema de isometrÃ­a
Sea $M \subseteq X$ subespacio, \[\cdot|_M : {X^\ast}/{M^0} \longrightarrow M^\ast \] es biyecciÃ³n lineal 
isomÃ©trica.

****** DemostraciÃ³n
Tenemos $\|f|_M\| \leq \|f+M^0\|$, y hay igualdad por extensiÃ³n 
equinÃ³rmica.

***** Segundo teorema de isometrÃ­a
Sea $M\subseteq X$ subespacio vectorial cerrado. Para $\pi_{X/M}$ proyecciÃ³n, 
$\_ \circ \pi : (X/M)^\ast \longrightarrow M^0$ es una biyecciÃ³n lineal isomÃ©trica.

****** DemostraciÃ³n
Claramente es lineal. Tenemos que es lipschitziana por:

\[ \|T(f)\| = \| f \circ \pi \| \leq \|f\|\|\pi\| = \|f\| \]

Y por otro lado,

\[ \|T(f)\|\|x+m\| \geq 
\|T(f)(x+m)\| = 
\|f(x+M)\|\]

Por tanto, $\|T(f)\| \geq \|f\|$.

***** FunciÃ³n de aproximaciÃ³n
Sea $M \subseteq X$ subespacio con $x_0 \in X-\overline{M}$. Existe $f\in X^\ast$ con $\|f\|=1$,
tal que $f \in M^0$ y $f(x_0) = d(x_0,\overline{M}) = d(x_0,M)$.

****** DemostraciÃ³n
Como $\overline{M}$ es subespacio cerrado, tenemos $g \in (X/M)^\ast$ con $\|g\|=1$ y
$g(x_0+\overline{M}) = \|x_0+\overline{M}\|$. Entonces $T(g) \in \overline{M}^0$ y $T(g)(x) = g(x+\overline{M})$.
Tenemos $\|T(g)\|= \|g\|=1$ y $f(x_0) = g(x_0+\overline{M}) = \|x_0+\overline{M}\|$.

***** Clausura desde el polar
Sea $M \subseteq X$ subespacio. Entonces:

\[ \overline{M} = \bigcap_{f \in M^0} \ker(f)\]

***** Distancia a un kernel
Sea $X$ espacio normado, $f \in X^\ast - \{0\}$ y $x_0 \in X$. Entonces,

\[d(x_0,\ker(f)) = \frac{|f(x_0)|}{\|f\|}\]

****** DemostraciÃ³n
Por el [[*FunciÃ³n de aproximaciÃ³n][teorema de aproximaciÃ³n]] con $M = \ker(f)$, tenemos una $g$ con
$\|g\| = 1$ tal que $g(x_0) = d(x_0,M)$; tenemos $\ker(f) \subseteq \ker(g)$.

\[\exists u\in X: f(u)=1\]. Para $x\in X$, tenemos $x -f(x)u \in \ker(f)$ y entonces:

\[ 0 = g(x-f(x)u) = g(x) - f(x)g(u)\]

Aplicando esto en $x_0$ tenemos $g(x_0) = g(u)f(x_0)$, que tomando mÃ³dulos,
nos da $\|g\| = \|f\| |g(u)|$.

**** Funcional de Minkowski
***** Funcional de Minkowski
Se define $p_U : X \longrightarrow \mathbb{R}^+$ para $U$ entorno de $0$ como:

\[ p_U(x) = \inf\{ \lambda \in \mathbb{R}^+_0 \mid x \in \lambda U\} \]

****** Sublinealidad
Para $U$ convexo, $p_U$ es sublineal.

***** SeparaciÃ³n de un punto
Sea $U$ entorno de $0$ convexo con $x_0 \notin U$, existe $f \in X^\ast$ tal que 
$Re(f(x)) \leq 1$ para todo $x \in U$; mientras $Re(f(x_0)) \geq 1$.

Se cumple ademÃ¡s:

\[ \{x \in X \mid p_U(x)<1\} \subset
U \subset
\{ x \in X \mid p_U(x) \leq 1\}\]

****** DemostraciÃ³n
Tomamos $g : x_0\mathbb{R} \longrightarrow \mathbb{R}$ definido por $g(\alpha x_0) = \alpha p_U(x_0)$.
Aplicamos [[*VersiÃ³n analÃ­tica de Hahn-Banach][Hahn-Banach]] sobre $x_0\mathbb{R}$ y tenemos un $f$ extensiÃ³n de $g$
verificando que $f(x_0) = p_U(x_0) \geq 1$, y que para $x \in U$ se tiene 
$f(x) \leq p_U(x) \leq 1$.

Ahora para el caso complejo, tenemos $f_0$ lineal y continuo
cumpliendo que $f_0(x_0)\geq 1$, pero $f_0(x) \leq 1$ para todo $x \in U$.
Definimos $f(x) = f_0(x) - if_0(ix)$, y entonces es lineal en los
complejos cumpliendo lo pedido.

***** SeparaciÃ³n de convexos (para un abierto)
Sean $A,B \subset X$ convexos con $A \cap B = \varnothing$ con $A$ abierto. Existe $f \in X^\ast$ 
con $\alpha \in \mathbb{R}$ tal que:

\[ Re(f(a)) < \alpha \leq Re(f(b)) \quad \forall a \in A, b \in B\]

***** Existencia de funcionales de soporte
Sea $X$ normado, $A \subset X$ convexo cerrado con $\mathring{A} \neq \varnothing$. Para cada 
$x_0 \in Fr(A)$; existe $f \in X^\ast$ tal que:

  - $\|f\| = 1$
  - $Re(f(x_0)) = \max\{Re(f(x)) \mid x \in A\}$

****** TODO DemostraciÃ³n

***** SeparaciÃ³n de convexos (para un compacto)
Sean $A,B \subset X$ convexos con $A \cap B = \varnothing$ con $A$ compacto y 
$B$ cerrado. Existe $f \in X^\ast$ con $\alpha \in \mathbb{R}$ tal que:

\[ Re(f(a)) < \alpha < Re(f(b)) \quad \forall a \in A, b \in B\]

****** TODO DemostraciÃ³n

**** Lema de categorÃ­a de Baire
***** Teorema de Baire
Sea $E$ espacio mÃ©trico completo y $G_n \subset E$ abiertos densos.
$\bigcap_{n \in \mathbb{N}} G_n$ es denso.

****** DemostraciÃ³n
Empezando con un abierto $G$ y con $G_1$, puedo a cada paso tomar el 
abierto anterior, tomar un abierto dentro de Ã©l como 
$\overline{B}(a_i,r_i) \subset G_i \cap B(a_{i-1},r_{i-1})$, y construir una sucesiÃ³n $\overline{B}(a_n,r_n)$.

Esta sucesiÃ³n podemos tomarla para que cumpla $\{r_n\} \to 0$. Desde
aquÃ­ tenemos que converge $\{a_n\} \to a \in \bigcap \overline{B}(a_n,r_n) \cap G$ por ser de 
Cauchy.

***** Corolario al teorema de Baire
Sea $E$ espacio mÃ©trico completo y $F_n \subset E$ cerrados con:

\[E = \bigcup_{n \in \mathbb{N}} F_n\]

Entonces, $\exists n \in \mathbb{N}$ tal que $\mathring{F}_N \neq \varnothing$.

****** DemostraciÃ³n
Aplicando el teorema de Baire en sus complementos.

***** DimensiÃ³n en espacios de Banach
Todo espacio de Banach tiene dimensiÃ³n finita o no numerable.

****** DemostraciÃ³n
Si fuera $X$ espacio de Banach con base numerable, cualquier
subespacio de dimensiÃ³n finita serÃ­a cerrado, pero entonces,
tomando $F_n = \langle e_1,\dots,e_n \rangle$:

\[ X = \bigcup F_n \]

Luego para algÃºn $n$, se tiene $\mathring{F_n} \neq \varnothing$; asÃ­ que debe ser 
$X=F_n$.

**** Teorema de la aplicaciÃ³n abierta
***** Teorema de la aplicaciÃ³n abierta
Sean $X,Y$ Banach con $T \in L(X,Y)$ sobreyectiva. 
Entonces $T$ epimorfismo topolÃ³gico (abierta).

****** DemostraciÃ³n
******* La imagen de bola tiene interior no vacÃ­o
Como $X = \bigcup_{n \in \mathbb{N}} n B_X$, tenemos que:

\[ Y 
= T\left(\bigcup_{n \in \mathbb{N}} n B_X \right)
= \bigcup_{n \in \mathbb{N}} n T(B_X) 
\subseteq \bigcup_{n \in \mathbb{N}} \overline{n T(B_X)}
= Y
\]

Aplicando corolario a Baire, $\exists N: \mathring{\overline{NT(B_X)}} \neq \varnothing$, luego
$\mathring{\overline{T(B_X)}} \neq \varnothing$. 

******* La imagen de la bola es entorno de 0
Sea ahora, $y_0 \in \mathring{\overline{T(B_X)}}$, se tendrÃ¡ que:

\[ 0 \in \mathring{\overline{T(B_X)}} - y_0
\subset \overline{T(B_X)} - \overline{T(B_X)}
\subset 2\overline{T(B_X)}\]

Siendo por tanto $\overline{T(B_X)}$ un entorno de 0.

******* AcotaciÃ³n de la bola
Tenemos $\exists\delta > 0: \delta B_Y \subset \overline{T(B_X)}$, y en general:

\[ \forall n \in \mathbb{N}: \frac{\delta}{2^n}B_Y \subseteq \overline{T\left(\frac{1}{2^n}B_X\right)}\]

******* SucesiÃ³n
Tomamos $x_0 = 0$, $y \in \overline{T(\frac{1}{2}B_X)}$, y construimos sabiendo:

\[ y - T(x_i) \in
\frac{\delta}{2^i} B_Y \subseteq
T\left(\frac{1}{2^i} B_X\right)\]

Luego existe un $x_{i+1}$ verificando $\|x_{i+1}\|\leq \frac{1}{2^{i+1}}$ y que:

\[ \left\| y - \sum_{k=1}^{i+1} T(x_k) \right\| < \frac{\delta}{2^{i+2}} \]

******* La suma converge
Definimos la suma de la sucesiÃ³n:

\[ S_n = \sum_{k=1}^n x_k \in X\]

Es de Cauchy por ser convergente $\sum_{n=1}^\infty \frac{1}{2^n}$. Por complitud,
converge, $S_n \to x$, con:

\[ \|S_n\| \leq \sum \|x_k\| \leq \sum \frac{1}{2^n} = 1 \]

Luego $\|x\| \leq 1$, $x \in B_X$. Como ademÃ¡s se tiene:

\[\| y - T(S_n) \| \leq  
\|y - \sum T(x_k) \| < 
\frac{\delta}{2^{n+1}} \]

Tenemos $\lim\{T(S_n)\} = T(x)$ y concluimos $y \in T(B_X)$.

******* ConclusiÃ³n
Hemos probado $\overline{T(\frac{1}{2} B_X)} \subseteq T(B_X)$, y finalmente,

\[ \frac{\delta}{2} B_Y \subset \overline{T\left(\frac{1}{2} B_X\right)} \subset T(B_X)\]

AsÃ­ que $T(B_X)$ es un entorno de $0$ y $T$ es abierta.

***** Teorema de los isomorfismos de Banach
Sean $X,Y$ Banach con $T \in L(X,Y)$ biyectiva.
Entonces $T$ es isomorfismo topolÃ³gico.

****** DemostraciÃ³n
Por [[*Teorema de la aplicaciÃ³n abierta][teorema de la aplicaciÃ³n abierta]] $T$ y $T^{-1}$ son abiertas;
luego $T^{-1}$ y $T$ son continuas.

***** Teorema del homomorfismo de Banach
Sean $X,Y$ espacios de Banach con $T \in L(X,Y)$. SerÃ¡ homomorfismo 
topolÃ³gico ssi $TX$ es cerrado en $Y$.

****** TODO DemostraciÃ³n
***** Equivalencia de normas en espacios de Banach
Sean $\|\cdot\|_1, \|\cdot\|_2$ dos normas en un espacio de Banach. Si cumplen que:

\[\exists M>0 : \|x\|_1 \leq M \|x\|_2\]

Entonces son equivalentes.

****** DemostraciÃ³n
Sea $T(x)=x$ biyecciÃ³n lineal entre las dos normas. Es lipschitziana
por la condiciÃ³n. Por el [[*Teorema de los isomorfismos de Banach][teorema de los isomorfismos de Banach]], es 
isomorfismo topolÃ³gico.

**** Teorema de la grÃ¡fica cerrada
***** GrÃ¡fica de una funciÃ³n
La *grÃ¡fica* de una funciÃ³n $f$ se define como:

\[ Graf(f) = 
\{(x,f(x)) \mid x \in A\} \subseteq
A \times B\]

Una $T$ es lineal ssi $Graf(T)$ es subespacio vectorial, y
cuando $f$ es continua en Hausdorff, $Graf(f)$ es cerrada.

***** Teorema de la grÃ¡fica cerrada
Sean $X,Y$ Banach con $T : X \longrightarrow Y$ lineal. Si la grÃ¡fica 
de $T$ es cerrada, $T$ es continua.

****** DemostraciÃ³n
Si tomamos $\|x+y\| = \|x\|+\|y\|$, que genera la topologÃ­a
producto en $X \times Y$, tenemos un Banach. Como $T$ es lineal,
$G(T)$ es subespacio lineal de $X \times Y$, y como $G(T)$ es cerrado,
es de Banach con la norma inducida.

Sea $\Phi : GT \longrightarrow X$ definida por:

\[ \Phi(x,Tx) = x\]

Como $\Phi$ es lineal, su restricciÃ³n es continua, lineal y biyectiva.
Por [[*Teorema de los isomorfismos de Banach][teorema de isomorfismo de Banach]], $\Phi^{-1}$ es continua; y entonces
$T = \pi_2 \circ \Phi^{-1}$ es continua.

***** CaracterizaciÃ³n de la grÃ¡fica cerrada en espacios normados
Sean $X,Y$ normados con $T: X \longrightarrow Y$ lineal. Equivalen:

- $T$ con grÃ¡fica cerrada.
- Si $\{x_n\} \longrightarrow 0$ y $\{Tx_n\}\longrightarrow y$; entonces $y=0$. Cuasicontinuidad en 0.

****** TODO DemostraciÃ³n

**** Teorema de Banach-Steinhaus
***** Teorema de Banach-Steinhaus para funcionales
Sea ${\cal A} \subset L(X,Y)$ para $X$ de Banach e $Y$ normado; una familia de 
operadores acotada puntualmente:

\[ \forall x : \exists M(x): \forall T \in A: \quad \|T(x)\| \leq M(x)\]

Entonces estÃ¡ acotada:

\[\exists M : \forall T \in A: \quad \|T\| \leq M \]

****** DemostraciÃ³n
Tomamos $F_n$ como intersecciÃ³n de conjuntos que son cerrados
por la continuidad de $T$:

\[F_n = \bigcap_{T \in {\cal A}} \{ x \in X \mid \|Tx\| \leq n\}\]

Como estÃ¡ acotada puntualmente, $\bigcup F_n = X$; asÃ­ que aplicamos
el [[*Corolario al teorema de Baire][corolario a Baire]] para tener un $\mathring{F_N} \neq \varnothing$. Eso quiere decir
que $\exists a: \exists r: a + rB_X \subseteq F_n$. Para $T \in {\cal A}$ se tiene:

\[\begin{aligned}
\|Tx\| =& \frac{1}{r} \| T(a+rx) - Ta\| \\
\leq& \frac{1}{r} (\| T(a+rx) \| + \|Ta\|) \\
\leq& \frac{N}{r} + \|Ta\|\frac{1}{r} \\ 
\leq& \frac{N}{r} + M(a)\frac{1}{r}
\end{aligned}\]

AcotaciÃ³n independiente de $X$.

***** Teorema del cierre de Steinhaus
Sea $X$ de Banach, $Y$ normado, y una sucesiÃ³n $\{T_n\} \in L(X,Y)$ 
convergiendo puntualmente. La convergencia puntual da un operador 
lineal y continuo:

\[ T(x) = \lim_{n \longrightarrow \infty} T_n(x) \in L(X,Y)\]

****** DemostraciÃ³n
La linealidad se tiene trivialmente:

\[ T(\alpha x_1 + \beta x_2) 
= \lim T_n (\alpha x_1 + \beta x_2)
= \alpha T(x_1) + \beta T(x_2) \]

Como $\{T_n\}$ es una familia acotada puntualmente por converger
puntualmente, se tiene por [[*Teorema de Banach-Steinhaus para funcionales][Banach-Steinhaus]] que estÃ¡ acotada.

Entonces para $x \in B_X$, tenemos $\|T_n(x)\| \leq M$, asÃ­ que:

\[ \|\lim T_n(x)\| =
\lim \|T_n(x)\| \leq M\]

Luego $\|T(x)\| \leq M$, y por estar acotada en la bola unidad
y ser lineal, $T$ es continua.

***** Corolario a Banach-Steinhaus para el dual
Sea $X$ espacio de Banach y $A \subseteq X^\ast$, equivalen:

1. $A$ acotado, $\exists M>0: \forall f \in A: \|f\| \leq M$
2. $A$ puntualmente acotado, $\forall x\in X: \{f(x) \mid f \in A\}$ acotado.

****** DemostraciÃ³n
Por [[*Teorema de Banach-Steinhaus para funcionales][teorema Banach-Steinhaus]] con $X^\ast = L(X,\mathbb{K})$ se tiene
la segunda implicaciÃ³n. La primera se tiene simplemente por
tenerse:

\[ \|f(x)\| \leq \|f\|\|x\| \leq M \|x\| \]

***** Corolario a Banach-Steinhaus para el doble dual
Sea $X$ espacio normado con $A \subseteq X$. Equivalen:

1. $A$ acotado.
2. $\{ f(x) \mid x \in A\}$ acotado para cualquier $f \in X^\ast$.

****** DemostraciÃ³n
Sabemos $J_X$ isometrÃ­a, luego $J_X(A)$ estÃ¡ acotado. Como 
la acotaciÃ³n equivale a la acotaciÃ³n puntual, para cualquier
punto del espacio $f \in X^\ast$ se tiene acotado:

\[ \{J_X(x)(f) \mid x \in A \} \]

*** 3. Espacios de Hilbert I
**** Espacios prehilbertianos
***** Producto escalar
Sea $H$ un K-espacio vectorial. Un producto escalar es: $\langle \cdot,\cdot\rangle : H \times H \longrightarrow \mathbb{K}$
cumpliendo:

  1. $\langle \alpha u + \beta v, w\rangle = \alpha\langle u,w \rangle + \beta\langle v,w \rangle$, lineal en la primera variable.
  2. $\langle u,\alpha v + \beta w\rangle = \overline{\alpha}\langle u,v\rangle + \overline{\beta}\langle u,w\rangle$, conjugadalineal en la segunda variable.
  3. $\langle u,v \rangle = \overline{\langle v,u \rangle}$, hermÃ­tica.
  4. $\langle u,u \rangle \geq 0$, definida positiva.
  5. $\langle u,u \rangle = 0$ ssi $u=0$, no nula.

****** Observaciones
Cuando $\mathbb{K}=\mathbb{R}$, 3 es conmutatividad; cuando $\mathbb{K}=\mathbb{C}$, implica que 4 estÃ¡ 
bien definido por ser $\langle u,u \rangle$ real.

***** Espacio prehilbertiano
Llamamos espacio prehilbertiano a un espacio vectorial dotado de un 
producto escalar.

***** Ejemplos de espacios prehilbertianos
****** Espacio euclÃ­deo complejo
Para el espacio $\mathbb{C}^n$:

\[\langle (x_1,\dots,x_n), (y_1,\dots,y_n) \rangle =
\sum_{i=0}^n x_i\overline{y_i} \]

****** Espacio de sucesiones
Para el espacio $\ell^2$, de sucesiones de cuadrado sumable:

\[\langle \{x_i\},\{y_i\} \rangle = \sum^\infty_{n=0} x_n\overline{y_n}\]

NÃ³tese que es sumable por tenerse $2|x_n||y_n| \leq |x_n|^2+|y_n|^2$.

****** Espacio de funciones continuas
Para ${\cal C}([0,1],\mathbb{K})$ funciones continuas:

\[
\langle f,g \rangle = \int_{[0,1]} f\overline{g}
\]

****** Espacio de funciones de cuadrado integrable
Para funciones de cuadrado integrable $L^2(\mu)$, tenemos:

\[\langle f,g \rangle = \int_\Omega f\overline{g} \;d\mu\]

Donde por HÃ¶lder tenemos la integrabilidad:

\[
\int_\Omega |f\overline{g}| \;d\mu \leq
\sqrt{
\int_\Omega |f|^2 d\mu
}
\sqrt{
\int_\Omega |g|^2 d\mu
}
\]

***** Desigualdad de Cauchy-Schwarz
Para $H$ prehilbertiano, si notamos $\|u\| = \sqrt{\langle u,u \rangle}$,

\[ |\langle u,v \rangle| \leq \|u\|\|v\|
\]

Con caso de igualdad $u = v$.

****** DemostraciÃ³n
Elevando al cuadrado los dos nÃºmeros positivos:

\[\begin{aligned}
0 
&\leq 
\|u\|^2\|v\|^2 - 2|\langle u,v \rangle|^2  + |\langle u,v \rangle|^2
\\ 0 &\leq
\|v\|^2 - \frac{2|\langle u,v \rangle|^2}{\|u\|^2}  + \frac{|\langle u,v \rangle|^2}{\|u\|^2}
\\ 0 &\leq
\left\langle 
\frac{\overline{\langle u,v \rangle}}{\|u\|^2}u + v,
\frac{\overline{\langle u,v \rangle}}{\|u\|^2}u + v
\right\rangle
\end{aligned}\]

***** Desigualdad de Minkowski
Para $H$ prehilbertiano, si notamos $\|u\| = \sqrt{\langle u,u \rangle}$,

\[
\| u + v \| \leq \|u\| + \|v\|
\]

****** DemostraciÃ³n
Desarrollando llegamos a $\langle u,v \rangle + \langle v,u \rangle \leq 2\|u\|\|v\|$, que es cierto por:

\[
\langle u,v \rangle + \langle v,u \rangle \leq
2Re(\langle u,v \rangle) \leq 
2|\langle u,v \rangle| \leq
2 \|u\|\|v\|
\]

Aplicando Cauchy-Schwarz en la Ãºltima desigualdad.

***** Espacio prehilbertiano es normado
Todo espacio prehilbertiano es normado con norma:

\[
\| u \| = \sqrt{\langle u,u \rangle}
\]

****** Cumple propiedades de la norma
Tenemos trivialmente:

  1. $\|u\| = 0 \iff u =0$
  2. $\|\alpha u\| = |\alpha| \|u\|$
  3. $\|u+v\| \leq \|u\|+\|v\|$

Donde la Ãºltima se deduce de la desigualdad de Minkowski.

***** El producto escalar es continuo
En un espacio prehilbertiano:

\[\{u_n\} \longrightarrow u, \{v_n\} \longrightarrow v 
\implies
\{ \langle u_n, v_n \rangle\} \longrightarrow \langle u,v \rangle\]

****** DemostraciÃ³n
Se tiene:

\[\begin{aligned} |\langle u_n,v_n \rangle - \langle u,v \rangle| 
&\leq |\langle u_n-u,v \rangle| + |\langle u,v_n-v \rangle| \\
&\leq \|u_n-u\|\|v\| + \|u\|\|v_n-v\| \longrightarrow 0
\end{aligned}\]

**** Identidades de polarizaciÃ³n
***** Identidades de polarizaciÃ³n
Sea $H$ prehilbertiano:

  1. $\langle u,v \rangle = \frac{1}{4}\left( \|u+v\|^2 - \|u-v\|^2 \right)$, cuando $\mathbb{K} = \mathbb{R}$.
  2. $\langle u,v \rangle = \frac{1}{4}\left( \|u+v\|^2 - \|u-v\|^2 \right) + \frac{i}{4}\left( \|u+iv\|^2 - \|u-iv\|^2 \right)$, cuando $\mathbb{K} = \mathbb{C}$.

****** DemostraciÃ³n
La segunda es trivial calculando y la primera es un caso particular.

***** Identidad del paralelogramo
Sea $H$ normado, es prehilbertiano ssi se verifica:

\[
\|u+v\|^2 + \|u-v\|^2 = 2\left( \|u\|^2 + \|v\|^2\right)
\]

Con el producto escalar dado por la [[*Identidades de polarizaciÃ³n][identidad de polarizaciÃ³n]]:

\[ \langle u,v\rangle = 
\frac{1}{4}\left(\|u+v\|^2-\|u-v\|^2 \right) +
\frac{i}{4}\left(\|u+iv\|^2-\|u-iv\|^2 \right)
\]

****** DemostraciÃ³n
Cuando es prehilbertiano, se verifica la ecuaciÃ³n trivialmente.
Cuando se verifica la ecuaciÃ³n, podemos ver que la identidad de 
polarizaciÃ³n nos da un producto escalar que es conjugadolineal,
hermÃ­tico, definido positivo y no nulo.

# Â¿En cuÃ¡l de estas comprobaciones se usa paralelogramo?
# Parecen largas de comprobar.

***** Ejemplos de normados prehilbertianos
****** Contraejemplo: funciones continuas con el mÃ¡ximo
El espacio ${\cal C}[0,1]$ con la norma del mÃ¡ximo no es prehilbertiano.
Hay un contraejemplo a la identidad del paralelogramo en $f(t) = 1$ y
$g(t) = t$.

****** Espacio de sucesiones de cuadrado sumable
El espacio $\ell_p$ es prehilbertiano ssi $p=2$, con:

\[\|x\| = \left(\sum_{i=1}^\infty |x_n|^2 \right)^{1/2}\]

******* DemostraciÃ³n
Se cumple que:

\[
\sum_{n=1}^k |a_n + b_n|^2 + \sum_{n=1}^k |a_n-b_n|^2
= 2\left(\sum_{n=1}^k |a_n|^2+|b_n|^2\right)
\]

Y tomando lÃ­mites tenemos lo pedido.

**** Espacios de Hilbert
***** Espacios de Hilbert
Un espacio prehibertiano completo es un espacio de Hilbert.
Equivalentemente, un espacio de Banach con norma asociada a un producto
escalar.

***** Hilbert de dimensiÃ³n finita
Todo prehilbertiano de dimensiÃ³n finita es Hilbert.

****** DemostraciÃ³n
Todo espacio normado de dimensiÃ³n finita es de Banach.

***** CompleciÃ³n de prehilbertianos
La completaciÃ³n de un espacio prehilbertiano es espacio de Hilbert.

****** DemostraciÃ³n
La [[*CompletaciÃ³n de un espacio][completaciÃ³n]] restringida al espacio orginal tiene su norma. Y la
norma es [[*Continuidad de la norma, suma y producto][continua]]. Por tanto, serÃ¡ prehilbertiano al cumplir la
[[*Identidad del paralelogramo][identidad del paralelogramo]]:

\[
\lim_{n \to \infty} \|u_n+v_n\|^2 + \|u_n-v_n\|^2
=
2\left(\|\lim_{n \to \infty} u_n\|^2+\|\lim_{n \to \infty} v_n\|^2\right)
\]

**** Ortogonalidad
***** Ley de los cosenos
La ley de los cosenos puede reinterpretarse como una definiciÃ³n del
Ã¡ngulo para espacios distintos de $\mathbb{R}^2$.

\[
cos(\theta) = \frac{\langle u,v \rangle}{\|u\|\|v\|}
\]

***** Ortogonalidad
Dos vectores se dicen *ortogonales* $u \perp v$ cuando su producto escalar 
es nulo:

\[\langle u,v \rangle = 0\]

***** Espacio ortogonal
Para $H$ hilbertiano, $S \subseteq H$; definimos el ortogonal de $S$ como:

\[
S^\perp = \left\{
u \in H \mid \forall s \in S: u \perp s
\right\}
\]

***** Propiedades del espacio ortogonal
Para $0 \subset S \subset H$, tenemos:

  1. $0 \in S^\perp$.
  2. $S \cap S^\perp \subseteq \{0\}$.
  3. $\{0\}^\perp = H$, $H^\perp = \{0\}$.
  4. $S_1 \subseteq S_2 \implies S_1^\perp \supseteq S_2^\perp$.
  5. $S^\perp$ es subespacio vectorial cerrado.
  6. $S \subseteq S^{\perp\perp}$.
 
****** DemostraciÃ³n
Triviales. La quinta se tiene por nÃºcleo de una funciÃ³n lineal y
continua.

***** Suma directa
Sea $X$ normado con $M,N$ subespacios. Se dice que hay suma directa
$X = M \oplus N$, cuando:

  1. $X = M + N$
  2. $M \cap N = \{0\}$

***** Suma directa topolÃ³gica
Se dice que hay suma directa topolÃ³gica cuando $M \oplus N$ cumplen que
$x_n = m_n + n_n$ respeta la convergencia $x = m + n$ con $m \in M, n \in N$.

****** Suma directa topolÃ³gica en Banach
En un espacio de Banach, la suma directa es topolÃ³gica cuando
ambos espacios $M,N$ son cerrados.

***** Lema de aproximaciÃ³n Ã³ptima
Sea $S$ un cerrado y convexo de $H$ prehilbertiano. Hay un sÃ³lo elemento
en el conjunto que realiza la mÃ­nima norma.

\[\exists! s_0 \in S:\quad \|s_0\| = \min\{\|s\| \mid s\in S\}\]

****** DemostraciÃ³n
******* Existencia
Sea $t$ el Ã­nfimo. Por convexidad tenemos: $\|\frac{1}{2}(u+v)\| \geq t$. Dada una
sucesiÃ³n $\{\|s_n\|\} \longrightarrow t$; vemos que es de Cauchy:

\[\begin{aligned}
\|s_n-s_m\|^2 &\leq \|s_n+s_m\|^2 + \|s_n-s_m\|^2 - 4t^2 \\&=
2(\|s_n\|^2 -t^2) + 2(\|s_m\|^2 - t^2) \longrightarrow 0
\]

Luego converge en el cerrado.

******* Unicidad
Si hubiese dos mÃ­nimos, se tendrÃ­a:

\[
\|s+s'\|^2 + \|s-s'\|^2 = 2\left(\|s\|^2 + \|s'\|^2 \right) 
= 4t^2 \leq \|s+s'\|^2
\]

Por lo que $\|s-s'\| = 0$.

***** Teorema de la aproximaciÃ³n Ã³ptima
Sea $H$ Hilbert, $M \subseteq H$ subespacio cerrado y $u \in H$. Entonces, existe 
una Ãºnica mejor aproximaciÃ³n a $M$, esto es:

\[\exists! \pi_M(u) \in M:\quad d(u, \pi_M(u)) = d(u,M) \]

****** DemostraciÃ³n
Aplicaremos el [[*Lema de aproximaciÃ³n Ã³ptima][lema de aproximaciÃ³n Ã³ptima]] a $u+M$. Tenemos que probar
que es convexo, y para ello:

\[\lambda (u+m) + (1-\lambda)(u+m') = u + m\lambda + m'(1-\lambda) \in u+M\]

Sea $s_0 \in u+M$ la mÃ­nima norma en $u+M$:

\[ \|s_0\| = \min\{\| u + m \| \mid m \in M\}\]

Tomamos $\pi_M(u) = u - s$, y tenemos:

\[ \| u - \pi_M(u)\| = \|s \| \leq \|u + m\|\]

La unicidad la da la unicidad en el lema de aproximaciÃ³n Ã³ptima.

**** Proyecciones y proyecciÃ³n ortogonal
***** Proyecciones
Sea $H$ Hilbert y $p : H \longrightarrow H$ lineal. Se llama proyecciÃ³n cuando $p \circ p = p$.

***** Suma directa de una proyecciÃ³n
Sea $p:H \longrightarrow H$ proyecciÃ³n, entonces $H = \ker(p) \oplus \im(p)$.

****** DemostraciÃ³n
Para $h \in H$ tenemos la descomposiciÃ³n $p(h) + (h-p(h))$. Dado $p(g) \in \ker(p)$,
se tiene $p(g) = p(p(g)) = 0$.

***** Proyecciones ortogonales
Se dice proyecciÃ³n ortogonal a una proyecciÃ³n $p$ en la que $\ker(p) \perp \im(p)$.

***** Lemas al teorema de la proyecciÃ³n ortogonal
Sea $H$ Hilbert y $M$ subespacio cerrado. Entonces:

  1. $u-\pi_M(u) \in M^\perp$.
  2. $\pi_M(\alpha u) = \alpha \pi_M(u)$.
  3. $\pi_M(u + v) = \pi_M(u)+\pi_M(v)$.
  4. $\pi_M(\pi_M(u)) = \pi_M(u)$.

****** DemostraciÃ³n
******* Punto 1
Para cualquier $t \in \mathbb{R}$ y $m \in M$ tenemos:

\[
0 \leq 
\| u - \pi u + tm \| - \| u - \pi u \| \leq
2t\; Re \langle u-\pi u, m\rangle + t^2 \|m\|^2
\]

Pero para que esto sea cierto, debe ser $Re \langle u-\pi u, m \rangle = 0$.
Tomando $im$ se tiene la parte imaginaria tambiÃ©n nula, luego
debe ser $\langle u-\pi u, m \rangle = 0$.

******* Punto 2
Tenemos:

\[
\| \alpha u - m \| 
= |\alpha| \left\|u - \frac{m}{\alpha}\right\| \geq |\alpha| \|u- \pi u\|
\]

DÃ¡ndose la igualdad con $m = \alpha \pi u$.

******* Punto 3
La ortogonalidad del primer punto:

\[
\|u + v - m\| = 
\|(u -\pi u) + (v - \pi v) - \widetilde m\| =
\|u - \pi u+  v - \pi v \| + \|\widetilde m\|
\]

Para el mÃ­nimo debe tenerse $\widetilde m = 0$.

******* Punto 4
Usando de nuevo la ortogonalidad del primer punto:

\[
\pi_M(u) - \pi_M\pi_M(u) \in M^\perp \cap M = \{0\}
\]

***** Teorema de la proyecciÃ³n ortogonal
Para $H$ Hilbert y $M$ subespacio cerrado:

  1. $H = M \oplus M^\perp$.
  2. La proyecciÃ³n a $M$ es la aproximaciÃ³n Ã³ptima.
  3. $\|u\|^2 = \|\pi_M(u)\|^2 + \| u - \pi_M(u) \|^2$.

AnÃ¡logamente, $\pi_{M^\perp}$ da la aproximaciÃ³n Ã³ptima a $M^\perp$.

****** DemostraciÃ³n
Por el [[*Lemas al teorema de la proyecciÃ³n ortogonal][lema]] sabemos que $\pi_M$ es una proyecciÃ³n. Como ademÃ¡s tiene 
$\ker(\pi_M) = M^\perp$ e $\im(\pi_M) = M$, se tiene que es la buscada.

Por ser ortogonales:

\[
\|u\|^2 = 
\|u - \pi_M(u) + \pi_M(u)\|^2 =
\|u-\pi_M(u)\|^2 + \|\pi_M(u)\|^2
\]

***** Nota: Unicidad de la descomposiciÃ³n ortogonal
Sea $H = M \oplus N$ con $\langle m, n \rangle = 0$ para $m \in M,\; n \in N$. Se tiene que $N = M^\perp$.

****** DemostraciÃ³n
Tenemos $N \subseteq M^\perp$ y ademÃ¡s, para $m+n \in M^\perp$, se tiene 
$0 = \langle m, m+n \rangle = \|m\|^2$, luego $m = 0$.

***** Corolario: clausura del doble ortogonal
Para $H$ Hilbert, $M$ subespacio, $M^{\perp\perp} = \overline{M}$.

****** DemostraciÃ³n
El espacio puede partirse de dos formas distintas como suma ortogonal 
de cerrados:

\[ H = \overline{M} \oplus \overline{M}^\perp \]
\[ H = M^{\perp\perp} \oplus M^{\perp}\]

Como $M^\perp = \overline{M}^\perp$, debe ser $\overline{M} = M^{\perp\perp}$.

***** Corolario: caracterizaciÃ³n de la densidad
Para $H$ Hilbert, $M$ subespacio, $M$ es denso ssi $M^\perp = \{0\}$.

****** DemostraciÃ³n
Si es denso, $M^\perp = \overline{M}^\perp = \{0\}$. 
Si $M^\perp = \{0\}$, tenemos $H = \overline{M} \oplus \{0\}$.

***** Teorema de Lindestrauss-Tzafriri
Un espacio de Banach es Hilbert ssi todo subespacio cerrado suyo admite
un complemento topolÃ³gico. Es decir, para cada $M$ cerrado hay un $N$ cerrado
tal que:

\[
X = M \overset{t}{\oplus} N
\]

****** TODO DemostraciÃ³n

**** Teorema de Riesz-Frechet
***** Recordatorio: por Teorema de Hahn-Banach
Sea $X$ espacio normado sobre $\mathbb{K}$. Entonces $\exists f: X \longrightarrow \mathbb{K}$ lineal y continua
no nula. AdemÃ¡s, dado $x \in X\setminus\{0\}$, tenemos $\exists f \in X^\ast: f(x) \neq 0$. De hecho,

\[
\|x\| = \sup\{ |f(x)| \mid f \in X^\ast \}
\]

****** DemostraciÃ³n
Hemos reenunciado la [[*SeparaciÃ³n en el dual topolÃ³gico][separaciÃ³n en el dual topolÃ³gico]], que era 
consecuencia de la [[*VersiÃ³n analÃ­tica de Hahn-Banach][versiÃ³n analÃ­tica de Hahn-Banach]].

****** RelaciÃ³n en el caso prehilbertiano
Cada vector tiene asociada una funciÃ³n lineal y continua en el dual
dada por su producto escalar: $v \mapsto \langle \cdot,v \rangle$.

***** Teorema de Riesz-FrÃ©chet
Sea $H$ Hilbert y $f : H \longrightarrow \mathbb{K}$ lineal y continuo. Existe un Ãºnico $v \in H$ 
tal que:

\[f(u) = \langle u,v \rangle\]

De otra forma, $v \mapsto \langle \cdot,v \rangle$ es una biyecciÃ³n conjugada-lineal de $H$ en $H^\ast$.

****** DemostraciÃ³n
******* Existencia: caso nulo
En el caso $f=0$, simplemente tomamos $v=0$.

******* Existencia: caso general
Dado $f$, $\ker(f)$ serÃ¡ propio, luego necesitamos $\ker(f)^\perp$ espacio propio
para que la suma directa sea el total. Sea $w \in \ker(f)^\perp$ no nulo, tenemos:

\[
0 = \langle f(u)w - f(w)u , w \rangle = \|w\|^2f(u) - f(w)\langle u,w \rangle
\]

Por lo que tenemos:

\[
f(u) = \left\langle u, \frac{f(w)}{\|w\|^2} w \right\rangle
\]

******* Unicidad: caso nulo
Debe ser un vector en $H^\perp = \{0\}$.

******* Unicidad: caso general
Simplemente notando que $f(u) = \langle u,v \rangle = \langle u,w \rangle$ nos darÃ­a $\langle u,v-w \rangle = 0$.
Un vector perpendicular a todo el espacio es nulo.

***** Corolario: el dual es de Hilbert
Si $H$ es Hilbert, $H^\ast$ con la norma de operadores es de Hilbert.

****** DemostraciÃ³n
Tomamos como producto escalar:

\[
\langle f_v,f_w \rangle = \langle w,v \rangle
\]

Y comprobamos que cumple los axiomas. NÃ³tese que es necesario invertir
el orden para que sea hermÃ­tico. Por otro lado, la norma es la misma
que la norma de operadores:

\[ \sqrt{\langle f_v,f_v \rangle} = \|v\|
\]

mientras que si $\|u\| = 1$, por Cauchy-Schwarz hay caso de igualdad en:

\[ |f_v(u)| = |\langle u,v \rangle| \leq \|v\|\]

***** Corolario: el doble dual es Hilbert
Si $H$ es Hilbert, $H^{\ast\ast}$ es Hilbert.

****** DemostraciÃ³n
Componemos dos veces lo que hemos hecho con el dual. Tenemos una
biyecciÃ³n lineal en este caso.

***** Corolario: completaciÃ³n de prehilbertianos
Si $H$ es prehilbertiano, $H^{\ast\ast}$ es su completaciÃ³n.

****** DemostraciÃ³n
Veremos que si $\widehat H$ es su completaciÃ³n, $H^\ast \cong \widehat{H}^\ast$, por lo que tendrÃ¡ que
tenerse $H^{\ast\ast} \cong \widehat{H}^{\ast\ast} \cong \widehat{H}$.

Pero $H^\ast \cong \widehat{H}^\ast$ es cierto simplemente porque la Ãºnica extensiÃ³n continua y
la restricciÃ³n serÃ¡n inversas.

***** Corolario: extensiÃ³n Ãºnica
Sea $H$ Hilbert con $M \subset H$ subespacio y $f \in M^\ast$. Existe una Ãºnica extensiÃ³n
lineal y continua cumpliendo:

\[ \|f_H\| = \|f\| \]

****** DemostraciÃ³n
******* Existencia
Podemos extender la funciÃ³n de $M$ a $\overline{M}$ por continuidad. Como es cerrado
y subespacio de Hilbert, serÃ¡ Hilbert. Entonces aplicamos Riesz-Frechet
para tener que la funciÃ³n serÃ¡ de la forma $f(x) = \langle x,m \rangle$ para algÃºn
$m \in M$. Ahora, $f_m = \langle \cdot,m \rangle$ es extensiÃ³n y cumple:

\[
\|f_m\| = \|m\| = \|f\|
\]

******* Unicidad
Si hubiera otra extensiÃ³n $\langle \cdot,u \rangle$, se tendrÃ­a $\langle \cdot,m \rangle - \langle \cdot,u \rangle = 0$ en $M$. 
Y entonces, $m - u \in M^\perp$, lo que implicarÃ­a:

\[ \|u\|^2 = \|m\|^2 + \|m-u\|^2 \geq \|m\|^2\]

Con caso de igualdad sÃ³lo si $\|m-u\| = 0$.

 - [[http://math.stackexchange.com/questions/332350/hilbert-spaces-and-unique-extensions-of-linear-functions][functional analysis - Hilbert spaces and unique extensions of
   linear functions. - Mathematics Stack Exchange]]

*** 4. Espacios de Hilbert II
**** Convergencia dÃ©bil
***** Convergencia dÃ©bil
En $H$ Hilbert, se dice que $\{u_n\}$ converge dÃ©bilmente a $u$ cuando:

\[\{u_n\} \overset{w}\longrightarrow u 
\iff \forall v \in H: \{\langle u_n,v \rangle\} \longrightarrow \langle u,v \rangle \]

De otra forma, $\forall f \in H^\ast: \{f(u_n)\} \longrightarrow f(u)$.

****** Unicidad del lÃ­mite dÃ©bil
La unicidad se tiene porque si $\forall v: \langle u,v \rangle = \langle u',v \rangle$, entonces $f_u = f_{u'}$,
que por [[*Teorema de Riesz-Frechet][Riesz-Frechet]] nos da $u = u'$.

***** Convergencia implica convergencia dÃ©bil
En $H$ Hilbert, la convergencia implica la convergencia dÃ©bil: 

\[\{u_n\} \longrightarrow u \implies \{u_n\} \overset{w}\longrightarrow u\]

****** DemostraciÃ³n
Trivial por continuidad del producto escalar.

****** Contraejemplo del recÃ­proco
En el espacio de sucesiones cuadrado sumables $\ell_2$, sabemos que los
tÃ©rminos de toda sucesiÃ³n tienden a $0$, por eso:

\[ \{e_n\} \overset{w}\longrightarrow 0\]

Pero no se tiene $\{e_n\} \longrightarrow 0$.

***** Compacidad dÃ©bil
Un conjunto es dÃ©bilmente compacto si toda sucesiÃ³n suya tiene una
parcial dÃ©bilmente convergente.

***** Compacidad de la bola unidad
Sea $H$ Hilbert, entonces la bola $\overline{B(0,1)}$ es dÃ©bilmente compacta.

****** TODO DemostraciÃ³n

***** Espacio vectorial topolÃ³gico
Un espacio vectorial topolÃ³gico es un espacio vectorial con una topologÃ­a
que hace continuos a la norma y el producto por escalares.

**** Ortonormalidad
***** Ortogonalidad y ortonormalidad
Sea dice $S \subset H$ ortogonal cuando $\forall u,v \in S: \; u \perp v$. Se dice que es ademÃ¡s
ortonormal cuando $\forall u \in S : \|u\| = 1$.

***** Independencia de ortogonales
Si hay un conjunto ortogonal $S$, es linealmente independiente.

****** DemostraciÃ³n
Supongamos que tenemos $e_1,\dots,e_n \in S$ y una combinaciÃ³n lineal suya.
Entonces para cada $e_k$:

\[
0 = \left\langle \sum \alpha_i e_i, e_k \right\rangle
= \alpha_k \|e_k\| = \alpha_k
\]

***** Gram-Schmidt
Sea $H$ prehilbertiano de dimensiÃ³n $n$, finita:

  1. $\{e_1,\dots,e_n\}$ ortogonal $\implies$ $\{e_1,\dots,e_n\}$ base
  2. Existe una base ortonormal.

****** DemostraciÃ³n
******* Punto 1
Son linealmente independientes y generan un espacio de dimensiÃ³n $n$,
que debe ser el total.

******* Punto 2
Sabemos que existe una base $u_1,\dots,u_n$, podemos generar una base
ortonormal tomando a cada paso:

\[ e_i = u_i - \sum_{j < i} \langle u_i,e_j \rangle e_j\]

Para tener una base ortogonal. Dividiendo por la norma para tener una
base ortonormal.

***** Base ortonormal finita
Sea $e_1,\dots,e_n$ una base ortonormal de un Hilbert. Cada elemento puede
escribirse en coordenadas de sus productos escalares:

\[
u = \sum_{i=1}^n \langle u,e_i \rangle e_i
\]

Y su norma serÃ¡:

\[
\|u\| = \sqrt{\sum^n_{i=1} |\langle u,e_i \rangle|^2}
\]

****** DemostraciÃ³n
Si escribimos las coordenadas $u = \sum \alpha_ie_i$ tenemos que $\alpha_i = \langle u,e_i \rangle$.
La norma se obtiene desde la descripciÃ³n de coordenadas por 
ortonormalidad.

**** Familias sumables
***** Familia sumable
Sea $X$ normado y $\{x_i\}_{i\in I}$ familia; se dice sumable si:

\[\exists x \in X: 
\forall \varepsilon > 0:
\exists J_\varepsilon:
\forall J \text{ finito} \supseteq J_\varepsilon: 
\quad
\left\|\; \sum_{i \in J} x_i - x \;\right\| < \varepsilon
\]

Llamamos suma de la familia a $\sum_{i \in I} x_i = x$.

****** Redes
NÃ³tese que esto es una generalizaciÃ³n del concepto de sucesiÃ³n.
Las [[https://es.wikipedia.org/wiki/Red_(matem%25C3%25A1tica)][redes]] son una generalizaciÃ³n de las secuencias para una
cantidad no numerable de elementos.

# QuizÃ¡ podrÃ­amos ver que toda red es Cauchy ssi es convergente.
# Eso nos ahorrarÃ­a las demostraciones posteriores de familias sumables.

***** Unicidad de la suma
La suma de una familia sumable es Ãºnica.

****** DemostraciÃ³n
Si tuviera dos sumas $x$ y $x'$, tomarÃ­amos los $J_\varepsilon, J_\varepsilon'$ para tener:

\[
\| x - x' \| \leq
\left\| x - \sum_{x_i \in J_\varepsilon \cup J_\varepsilon'} x_i \right\| +
\left\| \sum_{x_i \in J_\varepsilon \cup J_\varepsilon'} x_i - x' \right\|
\leq 2\varepsilon
\]

***** Propiedades de las familias sumables
Sea $X$ normado con $\{x_i\}_{i \in I}$ familia con suma $x$:

  1. $\sum_{i \in I} x_{\sigma(i)} = x$, para cualquier permutaciÃ³n $\sigma$.
  2. $\sum_{i \in I} \alpha x_i + \beta y_i = \alpha x + \beta y$, siendo la suma lineal.
  3. $\sum_{i \in I} Tx_i = Tx$, para $T$ lineal continua.

****** DemostraciÃ³n
NÃ³tese que cuando coinciden en subsumas finitas, deben coincidir
en la suma total, por definiciÃ³n.

******* Punto 1
Trivial por la definiciÃ³n.

******* Punto 2
Tomando el conjunto $J_{\frac{\varepsilon}{2\alpha}}^x \cup J_{\frac{\varepsilon}{2\beta}}^y = K$, tenemos que:

\[ \left\| \alpha x + \beta y - \left(\sum_{i \in K} \alpha x_i + \beta y_i \right)\right\| 
\leq |\alpha| \left\| x - \sum_{i \in K} x_i \right\| + |\beta| \left\|y - \sum_{i \in K} y_i \right\| 
\leq \varepsilon\]

******* Punto 3
Si tomo unos $\varepsilon \longrightarrow 0$ tendrÃ©:

\[
\left\| Tx - \sum_{i \in J_\varepsilon} Tx_i \right\| = 
\left\| T\left( x - \sum_{i \in J_\varepsilon} x_i \right) \right\| \leq
\|T\|\varepsilon \longrightarrow 0
\]

***** CaracterizaciÃ³n de familia sumable real
En los reales positivos, una familia $\{r_i\}_{i \in I} \in \mathbb{R}^+$ es sumable ssi:

\[\sup\left\{\;
\sum_{i \in J} r_i \;\middle|\; J \;\mtext{ finito } \subset I
\;\right\} < \infty\]

donde ademÃ¡s, $\sum_{i \in I} r_i$ es el supremo.

****** DemostraciÃ³n
Por la definiciÃ³n de supremo, dado cualquier $\varepsilon$ podemos encontrar:

\[s \geq \sum_{J} r_i \geq \sum_{J_\varepsilon} r_i\]

Cumpliendo por tanto para $J_\varepsilon \subset J$:

\[ 
\left|s - \sum_J r_i\right| \leq 
\left|s - \sum_{J_\varepsilon} r_i \right| \leq
\varepsilon\]

***** CondiciÃ³n de Cauchy en familias sumables
Una familia $\{x_i\}_{i \in I}$ verifica la condiciÃ³n de Cauchy cuando:

\[\forall \varepsilon > 0:
\exists J_\varepsilon \text{ finito}:
\forall J \text{ finito}: J \cap J_\varepsilon = \varnothing \implies
\left\|\; \sum_{i \in J} x_i \;\right\| < \varepsilon\]

***** Toda sumable es Cauchy
Si $\{x_i\}_{i \in I}$ es sumable, verifica la condiciÃ³n de suma de Cauchy.

****** DemostraciÃ³n
Suponiendo que suman $s$, podemos tomar $J_\varepsilon$ cumpliendo que, dado $J \cap J_\varepsilon = \varnothing$:

\[
\left\|\; s - \sum_{J_\varepsilon \cup J} x_i \;\right\| < \varepsilon
\]

Y por tanto, aplicando Minkowski:

\[
\left\|\; \sum_{J} x_i \;\right\|
\leq
\left\|\; \left(s - \sum_{J_\varepsilon} x_i \right) - \sum_J x_i \;\right\| +
\left\|\; s - \sum_{J_\varepsilon} x_i \; \right\|
\leq
2\varepsilon
\]

***** Toda Cauchy en un Hilbert es sumable
Si $\{x_i\}_{i \in I} \in H$ Hilbert verifica la condiciÃ³n de suma de Cauchy, 
es sumable.

****** DemostraciÃ³n
Primero tomamos la sucesiÃ³n de Cauchy siguiente, que por complitud
del espacio es convergente:

\[
\left\{
\sum_{i \in J_{\frac{1}{n}}} x_i
\right\}
\longrightarrow
s
\]

Tenemos, para $J \supseteq J_{\frac{1}{n}}$, para $n$ suficientemente grande, que:

\[
\left\|\sum_{i \in J} x_i - s \right\| \leq
\left\|\sum_{i \in J} x_i - \sum_{i \in J_{\frac{1}{n}}} x_i \right\| +
\left\|\sum_{i \in J_{\frac{1}{n}}} x_i - s \right\| \leq 
\frac{1}{n} + \varepsilon
\]

Siendo por tanto sumable.

***** Numerabilidad de familias de Cauchy
Toda familia de Cauchy tiene $\{ i \in I \mid x_i \neq 0\}$ numerable.

****** DemostraciÃ³n
Si verifica Cauchy, para cada $n$ podemos tomar, $J_n$ tal que 
para $J \cap J_n =\varnothing$:

\[\left\| \sum_{J} x_i \right\| \leq \frac{1}{n}\]

Si tenemos un $x \notin J_n$ para todo $n$, debe cumplir $\|x\| < \frac{1}{n}$, luego $x = 0$.
Como $\bigcup_{n \in \mathbb{N}} J_n$ es numerable. El conjunto de elementos no nulos es 
numerable.

***** Sumable es esencialmente numerable
En un espacio normado cualquiera equivalen:

  1. $\{x_i\}_{i \in I}$ sumable.
  2. $I_0 = \{ i \in I \mid x_i \neq 0\}$ numerable y para toda biyecciÃ³n $G : \mathbb{N} \longrightarrow I_0$:

     \[\sum_{i \in I_0} x_i = \sum_{n \in \mathbb{N}} x_{G(n)}\]

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Si es sumable cumple la condiciÃ³n de Cauchy y por tanto,
su conjunto de elementos no nulos es numerable.

AdemÃ¡s, si suma $s$, fijado $\varepsilon$, tengo un conjunto finito $J$.
Como $\{0,1,\dots,\max_{j \in J}\{G(j)\},\dots,m\} \supseteq G(J)$, se tiene:

\[
\left\| s - \sum_{i=0}^m x_{G(i)} \right\| \leq \varepsilon
\]

Por lo que la suma converge a $s$ para cualquier biyecciÃ³n.

******* TODO Segunda implicaciÃ³n
# Me gustarÃ­a probar que la convergencia incondicional da la convergencia
# absoluta, y entonces usar directamente la convergencia absoluta para
# probar que es sumable.

***** Corolario: convergencia conmutativa
Cuando $I = \mathbb{N}$, ser sumable equivale a converger conmutativamente,
esto es:

\[
\sum_{k \in \mathbb{N}} x_k = \sum_{k \in \mathbb{N}} x_{\sigma k}
\]

****** DemostraciÃ³n
Por el [[*Sumable es esencialmente numerable][teorema]] anterior.

***** Corolario: suma de particiones
En $X$ Banach, $I = \bigcup_{\lambda \in \Lambda} I_\lambda$ particiÃ³n arbitraria nos da que si $\{x_i\}_{i \in I}$
es sumable, $\{x_i\}_{i \in I_\lambda}$ es sumable; ademÃ¡s:

\[ \sum_{i\in I} x_i = \sum_{\lambda \in \Lambda} \left(\sum_{i \in I_\lambda} x_i \right)\]

****** TODO DemostraciÃ³n

**** Familias absolutamente sumables
***** Familia absolutamente sumable
Una familia $\{x_i\}_{i \in I}$ es absolutamente sumable cuando $\{\|x_i\|\}_{i \in I}$ es sumable
en $\mathbb{R}^+$:

\[
\sup_{J \subseteq I} 
\left\{ 
\sum_J \|x_i\| \;\middle|\; J \text{ finito}
\right\} < \infty
\]

***** Criterio de Abel
Sea $X$ Banach, $\{x_i\}_{i \in I} \in X$ y $\|x_i\| < |\alpha_i|$ cumpliendo $\sum |\alpha_i| < \infty$. Entonces
$\{x_i\}$ es sumable con:

\[
\left\|\sum_{i \in I} x_i\right\| \leq \sum_{i \in I} |\alpha_i|
\]

****** TODO DemostraciÃ³n
***** Absolutamente sumable implica sumable
En particular, absolutamente sumable implica sumabilidad en Banach, con:

\[
\left\|\sum x_i\right\| \leq \sum \|x_i\|
\]

****** DemostraciÃ³n
Trivialmente desde el [[*Criterio de Abel][criterio de Abel]].

**** Familias ortonormales
***** Suma ortogonal
Sea $\{e_i\}_{i\in I}$ familia ortogonal en un espacio de Hilbert $H$. Entonces
$\{e_i\}_{i \in I}$ es sumable ssi $\{\|e_i\|^2\}_{i \in I}$ es sumable en $\mathbb{R}^+$, en cuyo caso:

\[
\left\| \sum_{i \in I} e_i \right\| = \sqrt{\sum_{i \in I} \|e_i\|^2}
\]

****** DemostraciÃ³n
Coinciden en cualquier suma finita:

\[
\left\|\; \sum_{i \in I} e_i \;\right\|^2 = \sum_{i \in I} \|e_i\|^2
\]

Por lo tanto, coinciden sobre la condiciÃ³n de Cauchy.

***** Corolario: suma ortogonal con coeficientes
Sea $\{e_i\}_{i \in I}$ familia ortonormal con $f : H \longrightarrow \mathbb{K}$ aplicaciÃ³n. Entonces
$\{f(e_i)e_i\}$ es sumable ssi $\{|f(e_i)|^2\}$ es sumable en $\mathbb{R}^+$; en cuyo caso:

\[
\left\| \sum_{i \in I} f(e_i) e_i \right\| =
\sqrt{\sum_{i \in I} |f(e_i)|^2}
\]

****** DemostraciÃ³n
Trivial desde lo anterior viendo $\{f(e_i)e_i\}$ como familia ortogonal.

***** Corolario: suma ortogonal con productos escalares
Sea $H$ Hilbert, $\{e_i\}_{i \in I}$ familia ortonormal. Para $u \in H$ se tiene que
$\{\langle u,e_i \rangle e_i\}$ es sumable ssi $\{|\langle u,e_i \rangle|^2\}$ es sumable en $\mathbb{R}^+$.

****** DemostraciÃ³n
Caso particular de lo [[*Suma ortogonal][anterior]] con $f(x) = \langle u,x \rangle$.

***** Desigualdad de Bessel
Sea $\{e_i\}_{i \in I}$ ortonormal en $H$ Hilbert, entonces existe la suma siguiente
y estÃ¡ acotada:

\[
\sum_{i \in I} |\langle u,e_i \rangle|^2 \leq
\|u\|^2
\]

****** DemostraciÃ³n
En el caso finito:

\[
0 \leq
\left\| u - \sum_{J} \langle u,e_i \rangle e_i \right\|^2 =
\|u\|^2 - \sum_J \langle u,e_i \rangle^2 - \sum_J |\langle u,e_i \rangle|^2  + \sum_{J} \langle u,e_i \rangle^2
\]

Por tanto:

\[\sum_J |\langle u,e_i \rangle|^2 \leq \|u\|^2\]

Pero como estamos estudiando sumabilidad en los reales, basta haber
encontrado una cota sobre el [[*CaracterizaciÃ³n de familia sumable real][supremo]] para acotar la suma.

***** Corolario de Bessel
Sea $\{e_i\}_{i \in I}$ ortonormal en $H$ Hilbert, y sea $M$ el subespacio cerrado 
generado: $M = \overline{lin\{e_i \mid i \in I\}}$. Dado $u \in H$, la mejor aproximaciÃ³n de $u$ a $M$ 
es:

\[ \pi_M(u) = \sum_{i \in I} \langle u,e_i \rangle e_i
\]

En consecuencia se tiene:

\[
\|u\| 
= 
\sqrt{\;\sum_{i \in I} |\langle u,e_i \rangle|^2 + 
\left\|u - \sum_{i \in I} \langle u,e_i \rangle e_i\right\|^2\;}
\]

Y ademÃ¡s, equivalen:

  1. \[u \in M\]
     
  2. \[u = \sum \langle u,e_i \rangle e_i\]
     
  3. \[\|u\|^2 = \sum_{i \in I} |\langle u,e_i \rangle|^2\]

****** DemostraciÃ³n
******* Existe la suma
Por desigualdad de Bessel, comprobando la igualdad de ambas sumas
en los casos finitos, tenemos que existe la suma:

\[
\left\|\; \sum_{i \in I} \langle u,e_i \rangle e_i \;\right\|= 
\sqrt{\sum_{i \in I} |\langle u,e_i \rangle|^2} \leq \|u\|
\]

Ya que es una suma de positivos acotada. Ambas sumas cumplen
la misma condiciÃ³n de [[*Corolario: suma ortogonal con productos escalares][Cauchy]].

******* Hay ortogonalidad
Tomamos $m = \sum_{i \in I} \langle u,e_i \rangle e_i$, y comprobamos que $u - m \in M^\perp$. Como la
familia ortonormal genera el espacio, basta comprobar que es 
ortonormal a ella:

\[
\langle u-m,e_k \rangle = 0
\]

Por tanto, tenemos una descomposiciÃ³n $u = m + (u-m)$ y por teorema
de la [[*Teorema de la proyecciÃ³n ortogonal][proyecciÃ³n ortogonal]], $m$ es la mejor aproximaciÃ³n, y se tiene
la igualdad dada.

******* Equivalencias
Cuando $u \in M$, Ã©l mismo es su mejor aproximaciÃ³n y su norma puede
calcularse directamente. La igualdad de normas implica que la
norma de $u-m$ sea $0$, haciendo $u \in M$.
***** Existencia de familias ortonormales maximales
Existen sistemas ortonormales maximales.

****** DemostraciÃ³n
Lema de Zorn.

***** CaracterizaciÃ³n de bases ortonormales
En $H$ Hilbert equivalen:

  1. $u = \sum \langle u,e_i \rangle e_i$ para cualquier $u$.
  2. $\langle u,v \rangle = \sum \langle u,e_i \rangle \langle e_i,v \rangle$, identidad de Parseval.
  3. $\|u\|^2 = \sum_{i \in I} |\langle u,e_i \rangle|^2$.
  4. $\{e_i\}$ sistema ortonormal maximal.
  5. $\forall e_i : v \perp e_i \implies v =0$.
  6. $H = \overline{lin\{e_i\}}$

****** DemostraciÃ³n
Directamente desde el corolario de Bessel en el caso de $M$ denso,
donde no hay ningÃºn ortonormal a Ã©l. Podemos comprobar la implicaciÃ³n
en cada uno de los puntos.

**** Bases de Hilbert
***** Base de Hilbert
Se llama base de Hilbert a una familia ortonormal maximal.

****** Desarrollo en serie de Fourier
Una familia ortonormal maximal debe tener un subespacio cerrado
generado que sea [[*Corolario: caracterizaciÃ³n de la densidad][denso]], ya que si no fuera asÃ­, tendrÃ­a un $M^\perp$ no 
nulo. Por tanto cumple el corolario a Bessel y se tiene:

\[ u = \sum_{i \in I} \langle u,e_i \rangle e_i\]

llamada *Serie de Fourier*.

***** DimensiÃ³n Hilbertiana
El cardinal de las bases de Hilbert de un espacio es inveriante y
se llama *dimensiÃ³n Hilbertiana*.

****** DemostraciÃ³n
[[http://math.stackexchange.com/questions/232166/showing-the-basis-of-a-hilbert-space-have-the-same-cardinality][Showing the basis of a Hilbert Space have the same cardinality]].

***** IsomorfÃ­a entre espacios de igual dimensiÃ³n
Dos espacios de Hilbert con la misma dimensiÃ³n Hilbertiana son
topolÃ³gicamente isomorfos.

****** DemostraciÃ³n
Dado un isomorfismo entre las bases, definimos la aplicaciÃ³n lineal
que extiende el isomorfismo. Por el desarrollo en serie de Fourier,
sabemos que es equinÃ³rmica y por tanto continua.

Por el Teorema de los [[*Teorema de los isomorfismos de Banach][isomorfismos de Banach]], son isomorfos 
topolÃ³gicamente.

**** Operadores adjuntos
***** Operadores adjuntos
Dado $T \in L(H_1,H_2)$ entre espacios de Hilbert, existe:

\[\exists! T^\ast \in L(H_2,H_1):  \langle Tx,y \rangle = \langle x,T^\ast y\rangle\]

Llamado el *operador adjunto*.

****** DemostraciÃ³n
Por Riesz-FrÃ©chet, tenemos una aplicaciÃ³n $T^\ast y$ Ãºnica cumpliendo:

\[\langle T \cdot, y \rangle = \langle \cdot , T^\ast y \rangle\]

******* Es lineal
La funciÃ³n es lineal ya que, aplicando unicidad de Riesz-FrÃ©chet:

\[
\langle \cdot, T^\ast( \alpha y + y') \rangle =
\langle T \cdot, \alpha y + y' \rangle =
\overline{\alpha} \langle T \cdot, y \rangle + \langle T \cdot, y' \rangle =
\langle \cdot, \alpha T^\ast y + T^\ast y' \rangle
\]

******* Es continuo
La continuidad se tiene por acotaciÃ³n en la bola unidad:

\[
\| T^\ast y \|^2 \leq \|y\| \|TT^\ast y\| \leq \|T\| \|T^\ast y\|
\]

***** Propiedades de operadores adjuntos
Los adjuntos cumplen:

  1. $T^{\ast\ast} = T$.
  2. $\|T\| = \|T^\ast\| = \|TT^\ast\|^{1/2} = \|T^\ast T\|^{1/2}$.
  3. $(T_1+T_2)^\ast = T_1^\ast+T_2^\ast$
  4. $(\alpha T)^\ast = \overline{\alpha}T^\ast$.
  5. $(RT)^\ast = T^\ast R^\ast$.

****** DemostraciÃ³n
******* Punto 1
Aplicando unicidad de Riesz-Frechet a:

\[\overline{\langle y, T\cdot \rangle} = \overline{\langle y, T^{\ast\ast} \cdot \rangle}\]

******* Punto 2
Tenemos $\|T\| = \|T^{\ast\ast}\| \leq \|T^\ast\| \leq \|T\|$, como acotamos anteriormente.

AdemÃ¡s, tenemos doble acotaciÃ³n para los dos operadores:

\[\|TT^\ast\| \leq \|T\|\|T^\ast\| = \|T\|^2\]
\[\|Tx\|^2 \leq \|x\|^2 \|TT^\ast\|\]

******* Puntos 3, 4 y 5
Trivialmente por linealidad, usando la unicidad de Riesz-Frechet.

***** RelaciÃ³n con el adjunto
Sea $T \in L(H,H)$; se cumple:

  1. $\ker T^\ast = T(H)^\perp$.
  2. $\ker T = T^\ast(H)^\perp$.
  3. $T^\ast$ inyectivo $\iff$ $T(H)$ denso.

****** DemostraciÃ³n
******* Punto 1 y 2
Trivial por doble inclusiÃ³n y por reflexividad del adjunto.

******* Punto 3
Uniendo las condiciones de densidad y ortogonalidad con la 
caracterizaciÃ³n de inyectividad por nÃºcleo nulo.

***** Operador autoadjunto
Un operador $T \in L(H,H)$ es autoadjunto si $T^\ast = T$.

***** Propiedades de los autoadjuntos
Para $T \in L(H)$:

  1. $TT^\ast,T^\ast T, T+T^\ast$ son autoadjuntos.
  2. $T$ autoadjunto da $\alpha T$ autoadjunto.
  3. $\{ T = T^\ast\}$ es cerrado.
  4. Todo operador se divide en dos partes real e imaginaria:

     \[R = \frac{T+T^\ast}{2}\qquad S = \frac{T-T^\ast}{2}\]

**** Espectro y operadores compactos
***** Espectro
El espectro de un operador es el conjunto de valores propios,
llamamos:

****** Espectro

\[G(T) = \{\lambda \in \mathbb{C} \mid T - \lambda I \mbox{ invertible}\}\]

****** Espectro puntual

\[
G_p(T) =
\{
\lambda \in \mathbb{C} 
\mid
T - \lambda I \text{ no inyectivo}
\}
\]

****** Espectro comprimido

\[G_{com}(T) 
= 
\{\lambda \in \mathbb{C} \mid T - \lambda I \mbox{ con imagen no densa}\}
\]

****** Espectro aproximado

\[G_{ap}(T) =
\{\lambda \in \mathbb{C} \mid T-\lambda I \mbox{ no acotado por debajo}\}
\]

****** RelaciÃ³n
Equivalen ser invertible a tener imagen densa y estar acotado por 
debajo. AsÃ­,

\[G(T) = G_{com}(T) \cup G_{ap}(T)\]

AdemÃ¡s, en *dimensiÃ³n finita* y en compactos, equivalen inyectividad, 
sobreyectividad y biyectividad, luego:

\[G(T) = G_p(T)\]

******* TODO DemostraciÃ³n

***** Rango de un operador
El rango de $T$ es $n$ cuando puede escribirse con $u_i,w_i \in H$:

\[T = \sum u_i \otimes w_i\]

Es decir, como una matriz finita de ese rango.

****** Operadores de rango finito son compactos
Los operadores de rango finito son compactos.

******* DemostraciÃ³n
En dimensiÃ³n finita las bolas son compactas. Al ser continuo,
la sucesiÃ³n imagen es siempre acotada y con parcial convergente.

***** Operador compacto
Si $T \in L(H)$ es compacto si para cualquier acotada, la $\{T(u_n)\}$ tiene
una parcial convergente.

****** CaracterizaciÃ³n de compactos
Un $T$ es compacto ssi $T(\overline{B(0,1)})$ es compacto.

****** Compactos como lÃ­mite de los de rango finito
Los operadores de rango finito son compactos. De hecho, son su clausura.
Si tenemos:

 - \[KL(H) = \{T: H\longrightarrow H \mid \mbox{ compacto}\}\]
 - \[FL(H) = \{T : H\longrightarrow H \mid \mbox{ rango finito}\}\]

Se cumple $\overline{FL(H)} = KL(H)$.

******* TODO DemostraciÃ³n

****** El adjunto de un compacto es compacto
Si tenemos $T = \lim T_n$, con $T_n = \sum u_i \otimes w_i$. Podemos comprobar que
el adjunto es lÃ­mite de $T_n^\ast = \overline{\sum w_i \otimes u_i}$, de rango finito.

***** Teorema de la aplicaciÃ³n espectral
Para $H$ Hilbert, un polinomio no constante puede aplicarse a operadores
para tener:

\[G(p(T)) = \{p(\lambda) \mid \lambda \in G(T)\}\]

***** Teorema del nÃºcleo
Si $T \in L(H)$ compacto, $\ker(T - \lambda I)$ tiene dimensiÃ³n finita.

****** TODO DemostraciÃ³n
***** Teorema del rango
Si $T \in L(H)$ compacto, $\im(T-\lambda I)$ es cerrado.

****** TODO DemostraciÃ³n
***** Biyectividad en compactos
Si $T \in L(H)$ compacto, para $\lambda \neq 0$, $T-\lambda I$ es inyectivo ssi es 
sobreyectivo ssi es biyectivo.

\[G(T) \setminus \{0\} 
=
G_p(T) \setminus \{0\}
= 
G_{ap}(T) \setminus \{0\}
\]

***** Alternativa de Fredholm
Sea $T \in L(H)$ compacto y $\lambda \neq 0$. Consideramos las ecuaciones:

  1. $(T - \lambda I)x = 0$.
  2. $(T^\ast - \overline{\lambda} I)z = 0$.
  3. $(T-\lambda I)x = y$.
  4. $(T^\ast - \overline{\lambda} I)z = w$.

Y sabemos que se cumple una de estas dos alternativas:

  - O bien $x=0,z=0$ son las Ãºnicas soluciones de 1 y 2; en
    cuyo caso 3 y 4 tienen soluciÃ³n Ãºnica, que ademÃ¡s depende 
    continuamente.
  - O bien hay soluciones no nulas de 1 y 2 y entonces 3 tiene
    soluciÃ³n si $y \perp \ker(T^\ast - \overline{\lambda} I)$ y 4 tiene soluciÃ³n si $w \perp \ker(T-\lambda I)$.

****** Alternativamente
En resumen, para $T$ compacto:

  - $img(T - \lambda I) = \ker(T^\ast - \lambda I)^\perp$.
  - $img(T^\ast - \lambda I) = \ker(T-\lambda I)^\perp$.

Y si uno es nulo ambos lo son.

****** DemostraciÃ³n
******* Caso sin valor propio
Si $\lambda \notin G(T)$, entonces es invertible, asÃ­ como su adjunta.

******* Caso con valor propio
Si $\lambda \in G(T)$, entonces $\ker(T-\lambda I) \neq 0$ y $\ker(T^\ast-\lambda I) \neq 0$, porque
para compactos coinciden los espectros. Se tiene ademÃ¡s:

  - $y \in img(T - \lambda I) = (\ker(T^\ast-\lambda I)^\perp)$
  - $w \in img(T^\ast - \lambda I) = (\ker(T-\lambda I)^\perp)$

***** DiagonalizaciÃ³n de autoadjuntos compactos
Sea $T \in L(H)$, compacto y autoadjunto, entonces es *diagonalizable*.
Hay una base ortonormal con \[\lambda_n \longrightarrow 0\] de vectores propios, teniendo
convergencia uniforme sobre los compactos:

\[Tu = \sum \lambda_i \langle u,e_i \rangle e_i\]

Teniendo $G_p(T) \setminus \{0\} = \{\lambda_1,\dots,\lambda_n\}$, y coincide la dimensiÃ³n del 
espacio propio y el nÃºmero de veces que aparece $\lambda_i$.

**** Extra
***** Extra: El espectro de un autoadjunto es real
***** Extra: El espectro del adjunto es el conjugado en caso finito
****** Contraejemplo caso infinito
Pero en el caso general no. Un ejemplo es el siguiente:

\[
T(a_1,a_2,\dots) = (a_2,a_3,\dots)
\]

Que tiene cualquier valor en el espectro mientras su adjunto
no tiene ningÃºn valor propio.

***** Extra: En el caso finito, el adjunto es la conjugada de la traspuesta
***** Extra: [[https://en.wikipedia.org/wiki/Spectral_theorem][Teorema espectral]]
*** Ejercicios
**** 1. Espacios normados
***** Ejercicio 3
La sucesiÃ³n no puede tener ninguna parcial convergente a $0$, porque si no, 
al ser de Cauchy, convergerÃ­a a $0$. Por tanto, a partir de un cierto $n$, 
todos los tÃ©rminos deben alejarse de $0$ mÃ¡s de un determinado $\alpha$.

Sean ahora $\|x - y\| \leq \epsilon$, por desigualdad triangular inversa tenemos:

\[ \bigg|\|x\|-\|y\|\bigg| \leq \|x-y\| \leq \epsilon\]

Y por tanto:

\[1 - \frac{\epsilon}{\|x\|} \leq \frac{\|x\|}{\|y\|} \leq 1 + \frac{\epsilon}{\|x\|}\]

Ahora, por otro lado, comprobaremos que podemos demostrar a la funciÃ³n 
$f(x) = \frac{x}{\|x\|}$ uniformemente continua:

\[ 
\bigg| \frac{x}{\|x\|} - \frac{y}{\|y\|} \bigg| =
\frac{1}{\|x\|} \|(x-y) + \left(1 - \frac{\|x\|}{\|y\|}y\right) \leq
\frac{\epsilon}{\|x\|} + \frac{\|y\|}{\|x\|} \left|1-\frac{\|x\|}{\|y\|}\right|
\]

Pero como tenemos acotaciones uniformes de $\|x\|^{-1}$ y de $\frac{\|y\|}{\|x\|}$, hemos 
terminado.
***** Ejercicio 4
#+begin_statement
Sea $X$ espacio normado. Probar que equivalen:

  1. $X$ completo.
  2. $B_X$ completo.
  3. $S_X$ completo.
#+end_statement

****** Primera y segunda implicaciones
Cerrados dentro de un completo.

****** Tercera implicaciÃ³n
Sea $\{x_n\}$ una sucesiÃ³n de Cauchy. Descartamos el caso $\{\|x_n\|\} \longrightarrow 0$, 
que lleva a la convergencia a $0$. Podemos asumir $\|x_n\| \geq 0$ para
alguna cola de la sucesiÃ³n.

La sucesiÃ³n $\left\{\frac{x_n}{\|x_n\|}\right\}$ es de Cauchy (puede comprobarse acotando en el
caso en el que hemos descartado el $0$). Por tanto converge. NÃ³tese
que las normas tambiÃ©n son de Cauchy y tambiÃ©n convergen. AsÃ­,
podemos escribir un $x/\|x\|$ al que converja la sucesiÃ³n sobre $S_X$.

La distancia de cada elemento queda acotada por la distancia sobre
la bola unidad y la distancia en norma:

\[
\|x_n-x\| \leq
\|x_n\| 
\left( 
\left\| \frac{x_n}{\|x_n\|} - \frac{x}{\|x\|} \right\| +
\left\| \frac{x}{\|x\|} - \frac{x}{\|x_n\|} \right\|
\right)
\leq
\varepsilon
\]

El teorema es que si algo converge en la bola unidad y converge en
norma a distinto de $0$, converge a eso.

***** Ejercicio 7
#+begin_statement
Probar que, en un espacio normado, el interior de un subespacio
vectorial propio es vacÃ­o.
#+end_statement

Simplemente notando que si $B(m,r) \subseteq M$, entonces $B(0,1) \subseteq M$, y eso
lleva a $X = M$.

***** Ejercicio 16
****** Punto a
     Es de hecho una isometrÃ­a por tenerse: 

     \[\|Tx\| = \left(\sum_{n=0} |Tx_n|^p\right)^{1/p} = 
     0 + \left(\sum_{n=1} |x_n|^p\right)^{1/p} =
     \|x\|\]

     AsÃ­ que es continua y su norma es $1$.

****** Punto b
     Vemos que es lipschitziana trivialmente con $\|Tx\| \leq \|x\|$. Como ademÃ¡s realiza la cota
     sobre la bola unidad al tener: $\|T(0,1,0,\dots)\| = \|(1,0,\dots)\| = 1$.

****** Punto c
     Vemos que es isometrÃ­a $\|Tx\| = \|x\|$, por lo que es continua y de norma $1$.
**** 2. Hahn-Banach
***** Ejercicio 1
#+begin_statement
Sea $X$ espacio vectorial sobre $\mathbb{K}$, y sean $p_1,p_2 : X \longrightarrow \mathbb{R}$ seminormas.
Probar que si $f : X \longrightarrow \mathbb{K}$ es un funcional lineal verificando que 
$|f(x)|\leq p_1(x)+p_2(x)$ para todo $x\in X$, entonces existen $f_1,f_2 : X\longrightarrow\mathbb{K}$
funcionales lineales tales que $f = f_1+f_2$, y $|f_1(x)|\leq p_1(x)$, $|f_2(x)|\leq p_2(x)$
para todo $x\in X$.
#+end_statement

Sobre el espacio $X \times X$ definimos una seminorma desde las
dos seminormas anteriores:

\[ p(x,y) = p_1(x) + p_2(x)\]

Por otro lado, consideramos el subespacio diagonal:

\[ \Delta = \{(x,x) \mid x \in X\} \]

Y definimos sobre Ã©l un funcional lineal:

\[ h(x,x) = f(x) \leq p(x,y)\]


Ahora, podemos aplicar Hahn-Banach para obtener una extensiÃ³n
de $h$ definida para todo el espacio cumpliendo:

\[ |h(x,y)| \leq p(x,y)\]

Ahora, si definimos $f_1(x) = h(x,0)$ y $f_2(x) = h(0,x)$, las 
desigualdades se obtienen trivialmente desde la anterior.

***** Ejercicio 2
#+begin_statement
Sean $X$ un espacio normado, $M$ un subespacio vectorial de $X$, y
$u \in X$. Probar que existe $f \in X^\ast$ tal que $|f(x)| \leq dist(x,M)$ para todo
$x \in X$ y $f(u) = dist(u,M)$.
#+end_statement

Sobre el espacio $\langle u \rangle$ definimos el funcional $g(x) = dist(x,M)$, que es
lineal y continuo. Y por otro lado, definimos la seminorma $p(x) = dist(x,M)$
en todo el espacio. Por Hahn-Banach, existe un funcional que extiende
a $g$ y que cumple ademÃ¡s:

\[ |f(x)| \leq dist(x,M) \]

***** Ejercicio 3
#+begin_statement
Para cada $n \in \mathbb{N}$, sea $T_n : \ell_\infty \longrightarrow \mathbb{K}$ definido por $T_n(x) = \frac{1}{n}(x_1+\dots+x_n)$.
Sea $M = \{x \in \ell_\infty : \{T_n(x)\}\text{ converge} \}$ y definamos $T(x) = \lim\{T_n(x)\}$ sobre Ã©l.

 1. Probar que $T_n \in (l_\infty)^\ast$ y que $\|T_n\| = 1$ para todo $n \in \mathbb{N}$.
 2. Probar que $M$ es un subespacio vectorial de $\ell_\infty$ que contiene al espacio
    $c$ de las sucesiones convergentes.
 3. Probar que $T \in M^\ast$ con $\|T\| = 1$ y que $T(x) = \lim\{x_n\}$ para todo $x \in c$.
 4. Sea $\tau(x) = (x_2,x_3,\dots)$ para todo $x \in l_\infty$. Probar que 
    $x - \tau(x) \in \ker(T) \subseteq M$ para todo $x \in l_\infty$.
 5. Deducir que existe $S \in (l_\infty)^\ast$, extensiÃ³n de $T$ tal que $\|S\| = 1$ y
    $S(x) = S(\tau^n(x))$ para todo $x \in l_\infty$ y para todo $n \in \mathbb{N}$.
 6. Probar que $S(0,\frac{1}{2},0,\frac{1}{2},\dots) = \frac{1}{4}$.
#+end_statement

****** Punto 1
Tenemos que demostrar que es lineal y continua. Pero sabemos
que es suma y multiplicaciÃ³n por escalar de las proyecciones, que
lo son. Por otro lado, por desigualdad de las medias sabemos:

\[ \frac{1}{n}(x_1+\dots+x_n)
\leq \max\{x_1,\dots,x_n\}
\leq \|x_n\|_\infty\]

Por tanto $x \in B_{\ell_\infty}$ implica $T(x) \leq 1$. Y la desigualdad se realiza
en el caso $(1,1,1,\dots)$.

****** Punto 2
Tenemos que es subespacio vectorial porque si $T_n(x)$ y $T_n(y)$ convergen,
tambiÃ©n lo hace $T_n(x+\alpha y) = T_n(x) + \alpha T_n(y)$ por ser lineal.

****** Punto 3
El $T$ es el lÃ­mite puntual de los $T_n$. Por teorema del cierre de
Steinhaus, $T$ es lineal y continuo. Veamos que tiene norma unidad.
Por desigualdad de las medias la norma no puede ser mayor que $1$.
Tenemos para $x \in S_M$:

\[ T_n(x) \leq 1 \Rightarrow T(x) \leq 1\]

Y ademÃ¡s, para $u = (1,1,1,\dots)$ se tiene que $T_n(u) \to 1 = T(u)$.

****** Punto 4
Se ve que estÃ¡ en el nÃºcleo.

****** Punto 5
Por extensiÃ³n equinÃ³rmica de Hahn-Banach.

****** Punto 6
Sale desde la $T$.

***** Ejercicio 4
#+begin_statement
Fijado $n \in \mathbb{N}$, probar que existe un funcional lineal y continuo $f$ en
${\cal C}[0,1]$ tal que $f(p) = p'(0)$ para todo polinomio $p$ de grado menor o igual
que $n$. Probar que no existe un funcional lineal y continuo $f$ en ${\cal C}[0,1]$
tal que $f(p) = p'(0)$ para todo polinomio $p$.
#+end_statement

Fijado $n \in \mathbb{N}$ podemos crear una base del subespacio de polinomios
de grado menor o igual que $n$ y definir un $f$ sobre ella que cumple
lo pedido. Por Hahn-Banach, lo extenderemos a todo ${\cal C}[0,1]$. Es decir,

\[ \{1,x,x+x^2,x+x^3,\dots,x+x^n\} \overset{f}\longrightarrow \{0,1,1,\dots,1\}\]

Supongamos que existiera el funcional que lo cumple para todo polinomio.
Comprobamos que no es continuo, ya que si lo fuera deberÃ­a dejar acotada
la bola unidad. Sin embargo tenemos,

\[ \frac{d}{dx}(x-1)^n|_{x=0} = n\]

Mientras que $(x-1)^n \leq 1$ para $x \in [0,1]$; lo que nos da $\|(x-1)^n\| \leq 1$.

***** Ejercicio 5
#+begin_statement
Sean $X$ un espacio normado y $M$ un subespacio vectorial de $X$. Probar que 
para cada $T \in L(M,l_\infty)$, existe $S \in L(M,l_\infty)$ tal que $S|_M = T$ y $\|S\| = \|T\|$.
#+end_statement

Si aplico la extensiÃ³n equinÃ³rmica a cada una de las $\pi_i \circ T$, obtenemos
funciones $S_i \in X^\ast$ que extienden $T$ y tienen su misma norma. Ahora, la
funciÃ³n $S(x_1,x_2,\dots) = (S_1(x_1),S_2(x_2),\dots)$ es continua y lineal por serlo por
componentes; su restricciÃ³n a $M$ es trivialmente $T$, y ademÃ¡s, su
norma debe ser:

\[ \|S\| = \max\{ (S_1(x_1),S_2(x_2),\dots) \mid x \in B_X\} 
         = \max\{ \|S_1\|, \|S_2\|, \dots\} = \|T\| \]

***** Ejercicio 6
#+begin_statement
Sean $A$ y $B$ subconjuntos no vacÃ­os, abiertos, convexos y disjuntos de un 
espacio normado $X$. Probar que existen $f \in X^\ast$ con $\|f\| = 1$ y $\alpha \in \mathbb{R}$ tales
que $Re(f(a)) < \alpha < Re(f(b))$ para todo $a \in A$ y $b \in B$. Mostrar con un ejemplo
que, en general, no es posible encontrar $f$ tal que 
$\sup Re(f(A)) < \inf Re(f(B))$.
#+end_statement

Por el lema de separaciÃ³n de convexos tenemos que existen con
$\|f\|$ no necesariamente $1$. Dividiendo por la norma obtenemos el $f$
buscado.

Podemos tomar $A$ y $B$ como los dos semiplanos de $\mathbb{R}^2$ dados por
$\{x > 0\}$ y por $\{x < 0\}$. Por continuidad de $f$ se tendrÃ­a que:

$\sup Re(f(A)) \geq f(0) \leq \inf Re(f(B))$

Por lo que se tendrÃ­a la igualdad.
**** 3. Teoremas fundamentales
***** Ejercicio 2
#+begin_statement

#+end_statement
***** Ejercicio 3
#+begin_statement
Sean $X$ e $Y$ espacios de Banach y $T : X\longrightarrow Y$ una aplicaciÃ³n lineal y 
continua. Probar que $T$ es inyectiva y $T(X)$ es cerrado en $Y$ si y sÃ³lo
si, existe $m > 0$ tal que $\|T(x)\| \geq m \|x\|$ para todo $x \in X$.
#+end_statement

****** TODO Primera implicaciÃ³n
Si $T$ es inyectiva y ademÃ¡s $TX$ es cerrado en $Y$, entonces tenemos
que es un monomorfismo topolÃ³gico.

Si tomamos $\| x \|_2 = \|T x\|$, podemos observar que es norma porque cumple
la desigualdad triangular y la inyectividad nos da la condiciÃ³n en
$0$.

****** Segunda implicaciÃ³n 
Si tenemos que se cumple lo segundo, el nÃºcleo es trivial porque
para todo $x \neq 0$, se tiene $\|T(x)\| \geq m \|x\| > 0$. Es inyectiva.

Ahora, como $T$ es inyectiva la aplicaciÃ³n $T : X \longrightarrow TX$ es
biyectiva, luego es isomorfismo topolÃ³gico por el teorema de los
isomorfismos de Banach. Por el teorema del homomorfismo de
Banach, $TX$ es cerrado.

***** Ejercicio 4
#+begin_statement
Sea $M$ un subespacio cerrado de $l_p$ y de $l_q$. Probar que las normas inducidas
en $M$ por $l_p$ y $l_q$ son equivalentes.
#+end_statement

Tenemos $M$ un espacio normado y cerrado dentro de Banach, luego Banach.
Dentro de este espacio podemos usar equivalencia de normas en espacios
de Banach para tener que ambas son equivalentes a la norma inducida por
la norma del mÃ¡ximo.

\[ \| x\|_p 
= \sqrt[p]{\sum^\infty x_i^p} 
\geq \sqrt[p]{\max\{x_i\}^p}
= \| x\|_\infty \]

***** Ejercicio 5
#+begin_statement
Sean $X,Y$ espacios de Banach, y $A \subset Y^\ast$ tal que $A$ separa los puntos de $Y$.
Probar que si $T : X \longrightarrow Y$ es una aplicaciÃ³n lineal tal que $f \circ T \in X^\ast$ para
todo $f \in A$, entonces $T$ es continua.
#+end_statement

Comprobaremos que la grÃ¡fica de $T$ es cerrada, y por teorema de la grÃ¡fica
cerrada, tendremos que es continua. Sean $(x_i,Tx_i) \to (x,y)$; si fueran
distintos tendrÃ­amos que existe alguna funciÃ³n en $A$ que separe $f(y) \neq f(Tx)$.

Pero como $f \circ T$ es continua y $f$ es continua, tenemos:

\[ f(T(x_i)) \longrightarrow f(T(x))\]
\[f(Tx_i) \to f(y)\]

Por lo que deben ser iguales, contraviniendo separaciÃ³n.

***** Ejercicio 6
#+begin_theorem
Sea $X$ un espacio de Banach real. Dada una aplicaciÃ³n lineal 
$T: X \longrightarrow L_1([0,1])$ se considera, para cada $A \subset [0,1]$ medible, el funcional
lineal $T_A : X \longrightarrow \mathbb{R}$ definido por:

\[ T_A(x) = \int_A T(x)\]

Probar que si $T_A \in X^\ast$ para todo $A \subset [0,1]$ medible, entonces $T$ es continua.
#+end_theorem

****** Familia de funcionales que separan
Definimos $i_A \in L_1[0,1]^\ast$ para cualquier $A \subset [0,1]$ medible como:

\[ i_A(f) = \int_A f \]

Y comprobamos que separa las funciones de $L_1[0,1]$, ya que si dos funciones
integran igual en cualquier conjunto medible, su diferencia integra $0$ en
todo conjunto medible. Cuando esto ocurre, si fuera distinta de $0$ en un
conjunto de medida no nula, serÃ­a mayor que algÃºn $\varepsilon$ en un conjunto de medida
no nula, luego su integral no serÃ­a nula.

****** Desarrollo
Ahora aplicamos el ejercicio anterior, siendo $i_A$ la familia que separa
los puntos de $L_1[0,1]$. Como $i_A \circ T$ son todas lineales continuas, entonces
$T$ es continua.

***** Ejercicio 7
#+begin_statement
Sean $X$ un espacio de Banach sobre $\mathbb{K}$ e $I$ un conjunto no vacÃ­o. Dada una
aplicaciÃ³n lineal $T: X \longrightarrow l_\infty(I)$ se considera, para cada $i \in I$, el funcional
lineal $T_i : X \longrightarrow \mathbb{K}$ definido por:

\[ T_i(x) = T(x)(i)\]

Probar que si $T_i \in X^\ast$ para todo $i \in I$, entonces $T$ es continua.
#+end_statement

****** Familia de funcionales que separan
Llamamos $e_i \in l_\infty(I)^\ast$ a los funcionales lineales continuos siguientes,
definidos para cada $i \in I$:

\[ e_i(x) = x(i) \]

Trivialmente, si $x \neq y$, deben ser distintas en algÃºn $x(i) \neq y(i)$.

****** AplicaciÃ³n del ejercicio anterior
Aplicamos el [[*Ejercicio 5][ejercicio anterior]] sabiendo $l_\infty(I)$ de Banach. Como
$e_i \circ T$ es siempre continua, se tiene $T$ continua.

***** Ejercicio 8
#+begin_statement
Sean $X$ un espacio de Banach real y $T : X \longrightarrow C([0,1],\mathbb{R})$ una aplicaciÃ³n
lineal. Se considera, para cada $n \in \mathbb{N} \cup \{0\}$, el funcional lineal 
$T_n : X \longrightarrow \mathbb{R}$ definido por:

\[ T_n(x) = \int_0^1 t^n T(x)(t) dt\]

Probar que si $T_n \in X^\ast$ para todo $n \in \mathbb{N} \cup \{0\}$, entonces $T$ es continua.
#+end_statement

****** SeparaciÃ³n
Definimos los $e_n$ para cada natural como:

\[ e_n(f) = \int_0^1 t^n f(t) dt\]

Si una funciÃ³n fuera nula bajo todos los $e_n$ deberÃ­a ser ortogonal
a todos los polinomios, que son densos en $C([0,1],\mathbb{R})$; por lo que
deberÃ­a ser $0$ casi por doquier.

***** Ejercicio 9
#+begin_statement
Sean $X$ un espacio de Banach, $A \subset X$ tal que $X = \overline{Lin(A)}$, y $\{f_n\}$ una
sucesiÃ³n de elementos de $X^\ast$. Probar que equivalen:

 1. $\{f_n(x)\} \longrightarrow 0$ para todo $x \in X$
 2. $\sup \{ \|f_n\| \mid n \in \mathbb{N} \} < \infty$ y $\{f_n(a)\} \longrightarrow 0$ para todo $a\in A$.
#+end_statement

****** Primera implicaciÃ³n
Los $f_n$ forman una familia de operadores acotada puntualmente. Aplicando
el Teorema de Banach-Steinhaus a $X$ Banach, sabemos que debe estar 
acotada. Particulariza para los elementos de $a \in A$.

****** Segunda implicaciÃ³n
La convergencia a cero se mantiene por combinaciones lineales finitas,
asÃ­ que se tiene para cualquier $a \in Lin(A)$. Ahora, para $b \in \overline{Lin(A)}$, 
sea $a_n \longrightarrow b$; tenemos:

\[ \|f_n(b)\| 
\leq \|f_n(b - a_m)\| + \|f_n(a_m)\|
\leq M \|b - a_m\| + \|f_n(a_m)\| \to 0
\]

Puedo tomar un $m$ que haga suficientemente pequeÃ±o el primer sumando
y luego tomar un $n$ que haga suficientemente pequeÃ±o el segundo.

***** Ejercicio 10
#+begin_statement
Sea $\{x_n\}$ una sucesiÃ³n de escalares tal que la serie $\sum x_n y_n$ es convergente
para toda sucesiÃ³n $\{y_n\} \in c_0$. Probar que $\{ x_n \} \in l_1$.
#+end_statement

Definimos los funcionales $f_n \in c_0^\ast$ tales que:

\[ f_n(\{y_i\}) = \sum^n_{k=0} |x_k|y_k \leq \sum^\infty_{k=0} x_k \left(y_k \frac{x_k}{|x_k|} \right)\]

Donde usamos que si $y_k$ es convergente a $0$ tambiÃ©n lo serÃ¡ si la 
multiplicamos por escalares de valor absoluto $1$.

Como estÃ¡n acotados puntualmente, se tiene por Banach-Steinhaus que estÃ¡n
acotados. Esto es:

\[ \sup\left\{ \sum^n_{k=0} |x_k|y_k \middle| \|\{y_i\}\|_\infty = 1 \right\} 
= M < \infty\]

Y ahora, tenemos que las sucesiones con los $n$ primeros tÃ©rminos iguales
a $1$ y el resto nulos, estÃ¡n en la bola unidad y hacen que:

\[ \sum^n_{k=1} |x_k| \leq M < \infty\]

Dando asÃ­,

\[ \sum^\infty_{k=1} |x_k| < \infty\]

***** Ejercicio 11
#+begin_statement
Sea $\{x_n\}$ una sucesiÃ³n de escalares tal que la serie $\sum x_ny_n$ es convergente 
para toda sucesiÃ³n $\{y_n\} \in l_1$. Probar que $\{x_n\} \in l_\infty$.
#+end_statement

Tenemos funcionales $f_n \in l_1^\infty$ definidos como:

\[ f_n(\{ y_n \}) = \sum_{k=0}^n x_ky_k \leq \sum_{k=0}^\infty x_ky_k < \infty \]

Que por estar acotados puntualmente y ser $l_1$ un espacio de Banach, se
tiene por Banach-Steinhaus que estÃ¡n acotados. Esto es:

\[ \sup\left\{ \sum^n_{k=0} x_ky_k \middle| \|\{y_i\}\|_\infty = 1 \right\}
= M < \infty\]

En particular, si tomamos $g_i$ sucesiones con todos los elementos
nulos pero $g_{ii} = \frac{\overline{x_i}}{|x_i|}$, como $g_i \in \mathbb{S}_{l_\infty}$ tenemos que:

\[ |x_i| < \| f_n(g_n) \| \leq M \]

TeniÃ©ndose asÃ­ que $|x_i|$ estÃ¡ acotada.

***** Ejercicio 12
#+begin_statement
Sea $\{x_n\}$ una sucesiÃ³n de escalares tal que la serie $\sum x_ny_n$ es
convergente para toda sucesiÃ³n $\{y_n\} \in l_p$ $(1<p<+\infty)$. Probar
que $\{x_n\} \in l_q$, siendo $\frac{1}{p} + \frac{1}{q} = 1$.
#+end_statement

Tomamos los funcionales $f_n \in l_p^\ast$ definidos por:

\[f_n(\{y_k\}) = \sum_{k=0}^n x_ky_k < \sum_{k=0}^\infty x_ky_k \]

Que estÃ¡n acotados y vienen de espacio de Banach, por lo que, por
Banach-Steinhaus, se tiene que:

\[ \sup\left\{ \|f_n\|_p \right\}
= M < \infty\]

Por desigualdad de HÃ¶lder, tenemos una cota para la norma de los
operadores, sea $\{y_i\} \in S_{l_p}$:

\[ \|f_n\| \leq \sum^{n}_{k=0} |x_k||y_i| \leq \left(\sum^n_{k=0} |x_k|^q\right)^{1/q} \| \{y_i\}\|_p
 = \left(\sum^n_{k=0} |x_k|^q\right)^{1/q} \]

Y comprobamos que se realiza tomando el vector siguiente:

\[
\frac{1}{\left( \sum^n_{k=0} |x_k|^q \right)^{1/p}}
( |x_1|^{q-1}, |x_2|^{q-1}, \dots, |x_n|^{q-1}, 0,0,\dots )
\]

Que tiene imagen de norma:

\[ \left( \sum_{k=0}^n |x_k|^q  \right)^{1/q}\]

Mientras que Ã©l tiene norma $1$.

**** 4. Espacios de Hilbert I
***** Ejercicio 2
#+begin_statement
DemuÃ©strese que si $H$ es un espacio prehiilbertiano respecto de dos productos
escalares $\langle \cdot,\cdot\rangle_1$ y $\langle \cdot,\cdot\rangle_2$ entonces ambos productos coinciden salvo conjugaciÃ³n
ssi sus normas asociadas coinciden.
#+end_statement

****** Si coinciden, coinciden las normas
Si llamamos a las normas $\|\cdot\|_1$ y $\|\cdot\|_2$, tenemos:

\[\|u\|_1 = \sqrt{\langle u,u \rangle_1} 
= \sqrt{\overline{\langle u,u \rangle_2}} = \|u\|_2\]

Donde usamos que el producto escalar es hermÃ­tico.

****** Si coinciden las normas, coinciden
# Â¿Por quÃ© salvo conjugaciÃ³n?
Por la identidad de polarizaciÃ³n, tenemos:

\[
\langle u,v \rangle_1 = 
\frac{1}{4}\left( \|u+v\|^2-\|u-v\|^2+i\|u+iv\|^2-i\|u-iv\|^2 \right) =
\langle u,v \rangle_2
\]

***** Ejercicio 3
#+begin_statement
Sea $H$ un espacio prehilbertiano real. Probar que si $\|u+v\|^2 = \|u\|^2+\|v\|^2$,
entonces $u$ y $v$ son ortogonales. Â¿Se verifica esta propiedad en todo espacio
prehilbertiano complejo?
#+end_statement

En un espacio real, esto implica $2\langle u,v \rangle = 0$. En un espacio complejo,
sÃ³lo tenemos $\langle u,v \rangle + \langle v,u \rangle = 0$, asÃ­ que podemos buscar un ejemplo en el que
no se cumpla:

\[2 = \|1+i\|^2 = \|1\|^2 + \|i\|^2 \]

Pero $\langle 1,i \rangle = -i \neq 0$.

***** Ejercicio 4
#+begin_statement
Sea $H$ un espacio prehilbertiano sobre $\mathbb{{K}}$. Sean $u,v \in H$ y $\alpha \in \mathbb{K}$. Probar que
$u$ y $v$ son ortogonales ssi, $\|u+\alpha v\| = \|u-\alpha v\|$.
#+end_statement

Sea $\alpha \neq 0$, tenemos que $u \perp v \iff u \perp \alpha v$. AsÃ­, podemos demostrar que son
ortogonales ssi $\|u+v\| = \|u-v\|$. Pero esto sÃ³lo ocurre en $\mathbb{R}$, donde el ser
hermÃ­tico da simetrÃ­a al producto escalar, en $\mathbb{C}$ tenemos un contraejemplo
trivial en $u=1,v=i$, que no son ortogonales.

***** Ejercicio 15
#+begin_statement
Demostrar que el espacio ${\cal C}[-1,1]$ es suma directa del espacio de las funciones
pares y del espacio de las funciones impares. Â¿Son ortogonales dichos 
subespacios? Encontrar los complementos ortogonales de los siguientes 
conjuntos:

  - las funciones que se anulan en $[-1,0]$.
  - las funciones que se anulan en $x = 0$.
#+end_statement

****** Es suma directa
Sea $f \in {\cal C}[-1,1]$. Podemos escribirla como:

\[
f(x) = \frac{f(x) + f(-x)}{2} + \frac{f(x) - f(-x)}{2}
\]

Siendo cada sumando par e impar. Supongamos un $f$ par e impar, entonces se
tiene $f(x) = f(-x) = -f(-x) = 0$.

****** Es ortogonal
Usando que el producto de par e impar es impar:

\[
\int_{-1}^1 f(x)g(x) dx = \int_0^1 fg - \int_0^1 fg = 0x
\]

****** Complemento de las anuladas en [-1,0]
******* SÃ³lo son ortogonales si se anulan en [0,1]
Para cada intervalo definimos las funciones:

\[
u_{[a,b]}(x) = \left\{\begin{array}{ll} 
(x-a)\frac{2}{b-a}& \mbox{if } a \leq x \leq \frac{a+b}{2} \\
(b-x)\frac{2}{b-a}& \mbox{if } \frac{a+b}{2} \leq x \leq b \\
0 & \mbox{otherwise}
\end{array} 
\right.
\]

Una $f$ ortogonal a las que se anulan en $[-1,0]$ debe ser nula en $(0,1)$. Si
no lo fuera en $x \in (0,1)$, existirÃ­a un intervalo $[a,b] \subset (0,1)$ donde la funciÃ³n
preservarÃ­a el signo. Y entonces,

\[
\int f(x)u_{[a,b]}(x) \;dx \neq 0
\]

contraviniendo ortogonalidad.

******* Si se anulan en [0,1] son ortogonales
Por otro lado, una funciÃ³n que se anulara en $(0,1)$ serÃ­a ortogonal a
cualquiera que se anulara en $[-1,0]$.
****** Complemento de las anuladas en 0
Cualquier funciÃ³n ortogonal a las anuladas en $0$ debe anularse en todo $x \neq 0$.
Se tiene que si no se anulara en algÃºn punto distinto de $0$, existirÃ­a
un intervalo $[a,b]$ en el que preservarÃ­a el signo:

\[
\int^1_{-1} f(x)u_{[a,b]}(x) \;dx \neq 0
\]

contraviniendo ortogonalidad. Debe anularse en todo punto distinto de $0$,
y, por continuidad, tambiÃ©n en $0$.
**** 5. Espacios de Hilbert II
***** Ejercicio 1
#+begin_statement
Demostrar que un funcional lineal sobre un espacio de Hilbert es continuo si
y sÃ³lo si su nÃºcleo es cerrado. Â¿Es cierto esto para espacios de Banach en
general?
#+end_statement
****** Si es continua, tiene nÃºcleo cerrado
Un funcional lineal continuo debe tener siempre un nÃºcleo cerrado 
trivialmente.

****** TODO Si tiene nÃºcleo cerrado, es continua
En el caso de tener el nÃºcleo cerrado, [[*DescomposiciÃ³n canÃ³nica: isomorfismo][se tiene]] $\widehat{T} : H/\ker(T) \cong Im(T)$. 
Entonces $H/\ker(T)$ es de dimensiÃ³n finita y un isomorfismo entre espacios 
de dimensiÃ³n finita es continuo.

Por Ãºltimo $T = \widehat{T} \circ \pi$.

****** Otra soluciÃ³n
Viendo que hay suma topolÃ³gica $H = \ker(f) \oplus \mathbb{K}x_0$, ya que podemos escribir:

\[ u = (u - f(u)x_0) + f(u)x_0\]

exigiendo sÃ³lo $f(x_0) = 1$.

***** Ejercicio 2
#+begin_statement
Sean $f,g \in l_2 \longrightarrow \mathbb{K}$ los funcionales dados por $f(x) = \alpha_3+\alpha_4$ y $g(x) = 4\alpha_5$,
para cada $x = \{\alpha_n\}_{n \in \mathbb{N}} \in l_2$. Demostrar que son lineales y continuos. 
Â¿Son inyectivos? Calcular $\|f\|$ y $\|g\|$. Determinar $v,w \in l_2$ tales que
$f = \langle \cdot,g \rangle$ y $g = \langle \cdot,w \rangle$. Dado $n \in \mathbb{N}$, Â¿define $h_n(x) = \sum_{k=1}^n \alpha_k$ un funcional
lineal de $l_2$?; Â¿y $h(x) = \sum^\infty_{k=1} \alpha_k$?
#+end_statement

****** Continuas
Son trivialmente lineales. Son continuos porque estÃ¡n acotados ambos
en la bola unidad. Sabemos que debe tenerse $|\alpha_i| \leq 1$ para que estÃ© en la
bola unidad, porque en caso contrario, la suma de cuadrados serÃ­a mayor
que $1$:

\[ |f(x)| = |\alpha_3+\alpha_4| \leq 2\]
\[ |g(x)| = 4|\alpha_5| \leq 4\]

Son trivialmente no inyectivas.

De otra forma, las proyecciones, producto y suma son continuas.

****** Calcular la norma
******* Primer caso
Tenemos por desigualdad cuadrÃ¡tica-aritmÃ©tica que $|\alpha_3| + |\alpha_4| \leq 2\sqrt{\frac{1}{2}}$. 
Y se alcanza la cota con la sucesiÃ³n $f(0,0,0,\sqrt{1/2},\sqrt{1/2},0,\dots)$.

Podemos ver ademÃ¡s que $f = \langle \cdot,(0,0,0,1,1,0,\dots)\rangle$.

******* Segundo caso
Tenemos por $g(0,0,0,0,1,0,\dots) = 4$ que la cota anterior era correcta.
Es ademÃ¡s el vector por el que multiplicar para tener $g = \langle \cdot,w \rangle$.

****** Otros funcionales
******* Caso finito
Tenemos un funcional que es lineal trivialmente y que es continuo por
acotarse en la bola unidad por la desigualdad aritmÃ©tico-cuadrÃ¡tica:

\[
\left| \sum^n_{k=1} \alpha_k \right| \leq
\sum_{k=1}^n |\alpha_k| \leq
n \sqrt{\frac{1}{n}\sum^n_{k=1} |\alpha_n|^2} \leq \frac{n}{\sqrt{n}}
\]

******* Caso infinito
No tiene ni por quÃ© estar definido, la sucesiÃ³n $\{1/n\}$ es cuadrado 
sumable pero no es sumable.

***** Ejercicio 3
#+begin_statement
Sea $f : \mathbb{C}^3 \longrightarrow \mathbb{C}$ el funcional $f(\alpha_1,\alpha_2,\alpha_3) = 3i\alpha_1 + 2\alpha_2 - \alpha_3$. Demostrar que $f$
estÃ¡ en el dual topolÃ³gico de $\mathbb{C}^3$. Calcular $v \in \mathbb{C}^3$ tal que $f = \langle \cdot,v \rangle$ y
determinar $\|f\|$.
#+end_statement

Es suma y producto por escalares de las proyecciones, asÃ­ que es continua
y lineal. Comprobamos $f = \langle \cdot,(-3i,2,-1) \rangle$, y por Cauchy-Swarchz sabemos
la norma serÃ¡:

\[\|f\| = \|(3i,2,-1)\| = \sqrt{9+4+1} = \sqrt{14}\]

***** Ejercicio 4
#+begin_statement
#+end_statement

***** Ejercicio 5
#+begin_statement
Sea ${\cal P}(x)$ el espacio vectorial de los polinomios $p(x) :\mathbb{R} \longrightarrow \mathbb{R}$ con el 
producto interno dado por:

\[
\langle p,q \rangle = \int_0^1 p(t)q(t)\;dt
\]

Dar un ejemplo de un funcional lineal y continuo $\varphi : {\cal P}(x) \longrightarrow \mathbb{R}$ para el que
no exista $v(x) \in {\cal P}(x)$ tal que $\varphi = \langle \cdot,v \rangle$. Â¿Contradice este hecho el teorema
de Riesz-FrÃ©chet?
#+end_statement

Un ejemplo es:

\[
\varphi(p) = \int^1_0 e^tp(t) \;dx
\]

Es trivialmente lineal, estÃ¡ acotado en la bola unidad y por tanto es
continuo, de hecho, se tiene:

\[
\left| \int^1_0 e^tp(t) \;dx \right| \leq
\int^1_0 |e^tp(t)| \;dx \leq
e\int^1_0 |p(t)| \;dx = e \langle p,1 \rangle \leq e\|p\| \]

Sin embargo, no puede tenerse $\varphi = \langle \cdot,v \rangle$, porque si se tuviera, se tendrÃ­a
en el espacio de las funciones un polinomio que contravendrÃ­a Riesz-FrÃ©chet:

\[\int_0^1(e^t-v(t)) p(t) \;dt = 0
\]

Implicando que $e^t-v(t)$ es ortogonal a todos los polinomios. Como los
polinomios son densos en las continuas, esto implica que es ortogonal
a todas las funciones y por tanto $0$. $e^t = v(t)$, y no tenemos un polinomio
que cumpla eso.

No contradice Riesz-FrÃ©chet por no ser el espacio de los polinomios 
completo.

***** Ejercicio 6
#+begin_statement
ConsidÃ©rese el espacio complejo $C[0,1]$ dotado con la norma del mÃ¡ximo.
Sean $\varphi_1,\varphi_2,\varphi_3$ los funcionales dados por:

\[\varphi_1(f) = \int_0^1 x|f(x)|\;dx \]

\[\varphi_2(f) = \int^1_0 f(x)\;dx\]

\[ \varphi_3(f) = f\left(\frac{1}{2}\right)
\]

respectivamente, para $f \in C[0,1]$. Â¿CuÃ¡les de ellos son funcionales lineales
y continuos? Cuando lo sean, determinar su norma. Repetir el ejercicio
considerando la norma dada por $\|f\| = \int_0^1 |f(x)|^2\;dx$. Â¿En quÃ© casos existe
$g \in C[0,1]$ tal que el funcional dado es de la forma $\langle \cdot,g \rangle$?
#+end_statement

****** Con la norma del mÃ¡ximo
El primero no es lineal porque no cumple $\varphi_1(if) = i\varphi_1(f)$. El segundo y
el tercero son trivialmente lineales. Se comprueban continuos acotÃ¡ndolos
por la norma en la bola unidad. Calculamos su norma viendo que cumplen
esa acotaciÃ³n en la constante:

\[\varphi_2(1) = 1;\quad \varphi_3(1) =1;\]

****** Con la norma euclÃ­dea
Por HÃ¶lder tenemos un funcional acotado en la bola unidad:

\[
\left| \int^1_0 f(x)\;dx\right| \leq
\int^1_0 |f(x)|\;dx \leq
\sqrt{\int^1_0 |f(x)|^2\;dx}
\]

Pero el otro no estÃ¡ acotado; podemos tomar una familia $\psi_n$ de funciones
que sean nulas excepto en $[\frac{1}{2} - \frac{1}{2n^2}, \frac{1}{2} + \frac{1}{2n^2}]$, donde crecen hasta llegar
a $\psi_n\left(\frac{1}{2}\right) = n$ y decrecen. Las podemos construir con lÃ­neas considerando
su parte imaginaria nula. Tenemos a $\varphi_3(\psi_n) = n$, arbitrariamente grande, 
mientras:

\[
\int_0^1 |\psi_n(t)|^2 \;dt \leq \frac{1}{n^2}n^2 = 1
\]

****** Aplicamos FrÃ©chet en el resto de casos
Para tener que $\varphi_2 = \langle \cdot,1 \rangle$.

***** Ejercicio 7
#+begin_statement
Sea $\varphi : L^2(\mathbb{R}) \longrightarrow \mathbb{R}$ el funcional dado por $\varphi(g) = \int_0^1 3xg(x)\;dx$. Demostrar
que $\varphi$ es un funcional lineal y acotado. Determinar $f \in L^2(\mathbb{R})$ tal que
$\varphi = \langle \cdot,f \rangle$ y calcular $\|f\|$.
#+end_statement

Es lineal trivialmente. Podemos acotarlo en la bola unidad como:

\[
\left|
\int^1_0 3xg(x)\;dx
\right| \leq 
3 \int^1_0 |x||g(x)|\;dx \leq 
3\sqrt{\int_0^1 |g(x)|^2\;dx} \leq
3\sqrt{\int_{-\infty}^\infty |g(x)|^2\;dx} \leq 3
\]

Por otro lado, tenemos que $\varphi = \langle \cdot,\psi \rangle$, siendo:

\[\psi = \left\{\begin{array}{ll} 
3x& \mbox{if } 0 \leq x \leq 1  \\
0 & \mbox{otherwise }  
\end{array} 
\right.\]

La norma de $f$ serÃ¡ la de $\psi$, donde:

\[\|f\| = \int_{-\infty}^\infty \psi(x)\;dx = \frac{3}{2}\]

***** Ejercicio 10
#+begin_statement
Demostrar que los siguientes conjuntos forman una base ortonormal del 
espacio de Hilbert real $L^2[0,\pi]$.

  1. $B := \{ e_n \mid n \in \mathbb{N}_0\}$, donde $e_0 = \frac{1}{\sqrt{\pi}}$ y $e_n = \frac{\sqrt{2}}{\sqrt{\pi}} cos (nx)$.
  2. $B := \{ e_n \mid n \in \mathbb{N}\}$ donde $e_n = \frac{\sqrt{2}}{\sqrt{\pi}} sin(nx)$.

Obtener una base ortonormal del espacio de Hilbert complejo $L^2[0,\pi]$.
#+end_statement

****** Primer punto
******* Generan el espacio
Sabemos que los polinomios son densos en el espacio de funciones
continuas en $[0,1]$, y que la funciÃ³n arcocoseno es una biyecciÃ³n en
ese intervalo. Dado un $f$, tenemos un polinomio $p$ que aproxima a
la funciÃ³n $f \circ arccos$:

\[ 
\left\| f(x) - p(cos(x)) \right\|_2 \leq
\left\| f(x) - p(cos(x)) \right\|_\infty =
\left\| f(arccos(x)) - p(x) \right\|_\infty
\]

Ahora, un polinomio sobre $cos(x)$ puede reescribirse como una 
combinaciÃ³n lineal de los vectores de la base:

\[ cos(nx)cos(mx) = 
\frac{1}{2}\Big( cos((n+m)x) + cos((n-m)x)
\Big)\]

De esta forma, la clausura del espacio que generan los cosenos es
el espacio de las continuas. Si ademÃ¡s las continuas son densas
en $L_2[0,1]$, tenemos lo pedido.

******* Son una familia ortonormal
Como ademÃ¡s son familia ortonormal, forman una base ortonormal del 
espacio. Cuando $m \neq n$:

\[
\int_0^\pi cos(nx)cos(mx) \;dx = 
\frac{1}{n+m}[sen((n+m)x)]^\pi_0 +
\frac{1}{n-m}[sen((n-m)x)]^\pi_0 = 0
\]

Y cuando $m = n$, se tiene:

\[
\int_0^\pi cos(nx)^2 \;dx
=
\frac{1}{2}\int_0^\pi cos(2nx) - 1 \;dx
= \frac{\pi}{2}
\]

****** Segundo punto
******* Generan el espacio
Usaremos la derivaciÃ³n. Por el teorema anterior, tenemos un $p\circ cos$ que 
aproxima con precisiÃ³n $\varepsilon$ a la siguiente funciÃ³n:

\[
g(x) = \left\{\begin{array}{ll} 
\frac{f(x)}{sin(x)} & \mbox{if } x \in [0+\varepsilon,\pi+\varepsilon]  \\
\psi(x) & \mbox{otherwise } 
\end{array} 
\right.
\]

Donde $\psi(x)$ la podemos construir con rectas para que haga continua
a la funciÃ³n e integre menos de $\varepsilon$, algo como:

[[./images/epsilonint.png]]

Sea una primitiva suya el polinomio $P$:

\[\begin{aligned} 
\|f(x) - \partial (P\circ cos)(x))\|_2 
&\leq
K + \int_{0-\varepsilon}^{\pi-\varepsilon} 
\left|f(x) - \partial (P\circ cos)(x) \right|^2\;dx \\
&\leq 
K + \int_{0-\varepsilon}^{\pi-\varepsilon} 
\left|\frac{f(x)}{sin(x)} - \frac{\partial (P\circ cos)(x)}{sin(x)} \right|^2\;dx \\
&=
K + \int_{0-\varepsilon}^{\pi-\varepsilon} 
\left|\frac{f(x)}{sin(x)} - p(cos(x)) \right|^2\;dx < \varepsilon + K\\
\end{aligned}\]

Por otro lado, tenemos que:

\[\begin{aligned}
K &= 
\int_0^\varepsilon |f(x) - \partial(P \circ cos)(x)|^2 \;dx
\\&\leq
\int_0^\varepsilon |f(x)|^2\;dx + \int_0^\varepsilon |sin(x)p(cos(x))|^2\;dx
\\&\leq
\int_0^\varepsilon |f(x)|^2\;dx + \int_0^\varepsilon |p(cos(x))|^2\;dx
\\&\leq 
\varepsilon \|f\|_\infty + \int_0^\varepsilon |p(cos(x)) - \psi(x)|^2\;dx + \int_0^\varepsilon |\psi(x)|^2\;dx
\\&\leq
\varepsilon \|f^2\|_\infty + \varepsilon + \varepsilon
\end{aligned}\]

Como $\varepsilon$ es arbitrario, se tiene lo pedido. NÃ³tese que $\partial (P \circ cos)$ es la
derivada de una funciÃ³n polinÃ³mica en los cosenos que se puede escribir
como combinaciÃ³n lineal de la base anterior. Y nÃ³tese que la derivada
lleva cada elemento de la base anterior en uno de la nueva.

******* Son una familia ortonormal
Por Ãºltimo, como son ortonormales, forman una base ortonormal:

\[
\int^\pi_0 sin(nx)sin(mx) \;dx
= 
\int^\pi_0 cos((n-m)x) - cos((n+m)x)\;dx = 0
\]

****** Tercer punto
Toda funciÃ³n puede escribirse como:

\[
f(x) = \Re(f(x)) + i \Im(f(x))
\]

Y cada una de esas partes puede aproximarse como suma de cosenos o de
senos. AsÃ­, cualquier base ortonormal del espacio real que hemos 
construido anteriormente es tambiÃ©n base del espacio complejo.

***** Ejercicio 11
#+begin_statement
Demostrar que:

\[B:= \left\{
e_0(x) = \frac{1}{\sqrt{2\pi}},\;
e_n(x) = \frac{1}{\sqrt{\pi}} cos(nx),\;
e_{-n}(x) = \frac{1}{\sqrt{\pi}} sin(nx) 
\mid n \in \mathbb{N}
\right\}\]

define una base ortonormal en un espacio de Hilbert real $L^2[-\pi,\pi]$.
#+end_statement

****** Es ortogonal
Comprobamos integrando la ortonormalidad de la base.

****** Genera el espacio
Las funciones pares pueden aproximarse por la misma funciÃ³n que las
aproximaba en $[0,\pi]$ con los cosenos; las impares pueden aproximarse
por la funciÃ³n que las aproximaba en $[0,\pi]$ con senos.

Toda funciÃ³n es suma de par y de impar.

\[f(x) = \frac{f(x)-f(-x)}{2}+\frac{f(x)+f(-x)}{2}\]

AsÃ­ que toda funciÃ³n puede aproximarse por suma de senos y cosenos.

***** Ejercicio 12

***** Ejercicio 13
#+begin_statement
En el espacio $L^2[-\pi,\pi]$, en funciÃ³n de la base de Hilbert dada en el 
ejercicio 11, calcular el desarrollo en serie de Fourier de la funciÃ³n
$f(x) = |x|$ y deducir mediante la indentidad de Parseval que:

\[\sum^\infty_{n=1} \frac{1}{(2n-1)^4} = \frac{\pi^4}{96}\]
#+end_statement

****** Desarrollo en serie de Fourier
Usando que es funciÃ³n par

\[
\int_{-\pi}^\pi |x| \frac{1}{\sqrt{2\pi}} \;dx = \frac{\pi^2}{\sqrt{2\pi}}
\]

Usando que es una funciÃ³n par e integrando por partes:

\[
\int_{-\pi}^\pi |x| \frac{1}{\sqrt{\pi}} cos(nx) \;dx
=
\frac{2}{\sqrt{\pi}}\int_{0}^\pi x cos(nx) \;dx 
=
\frac{2}{\sqrt{\pi}n^2}((-1)^n-1)
\]

Usando que es una funciÃ³n impar:

\[
\int_{-\pi}^\pi |x| \frac{1}{\sqrt{\pi}} sin(nx) \;dx 
=
0
\]

****** Identidad de Parseval
Calculamos la norma de la funciÃ³n:

\[
\int_{-\pi}^\pi x^2 \;dx = \frac{2}{3} \pi^3
\]

Y tenemos finalmente, sumando cuadrados de los productos escalares
anteriores:

\[\begin{aligned}
\frac{\pi^3}{2} + \sum_{i=0}^\infty \frac{4}{\pi n^4}((-1)^n-1)^2
&=
\frac{2}{3} \pi^3
\\
\sum_{i=0}^\infty \frac{4}{n^4}((-1)^n-1)^2
&=
\frac{1}{6} \pi^4
\\
\sum_{i=1}^\infty \frac{1}{(2n-1)^4}
&=
\frac{1}{96} \pi^4
\end{aligned}\]

Considerando sÃ³lo los tÃ©rminos impares.

***** TODO Ejercicio 14
#+begin_statement
En el espacio $L^2[-\pi,\pi]$, en funciÃ³n de la base de Hilbert dada en el 
ejercicio 11, obtener el desarrollo en serie de Fourier de la funciÃ³n
$f(x) = x^2$. Usar dicho desarrollo para calcular:

\[\sum_{n=1}^\infty \frac{1}{n^2}\] y \[\sum_{n=1}^\infty \frac{1}{n^4}\]
#+end_statement

***** TODO Ejercicio 18
#+begin_statement
Encontrar $min_{\alpha,\beta,\gamma \in \mathbb{C}} \int_{-1}^1 |x^3-\alpha-\beta x -\gamma x^2|^2 \;dx$.
#+end_statement

Buscar la mejor aproximaciÃ³n en el espacio de los polinomios de grado
menor que 2. Hay que buscar primero una ortonormalizaciÃ³n de la base
con Gram-Schmitd

**** 6. Espacios de Hilbert III
***** Ejercicio 1
#+begin_statement
Sea $H$ un espacio de Hilbert y $P \in L(H)$. Demostrar que las siguientes
afirmaciones son equivalentes:

  1. $P$ es la proyecciÃ³n ortogonal de $H$ sobre un subespacio cerrado $M$.
  2. $P$ es idempotente y autoadjunto.
  3. $P$ es idempotente y $H = P(H) \oplus (I-P)(H)$ siendo los subespacios
     $P(H)$ y $(I-P)(H)$ ortogonales.
  4. $P$ es idempotente y $P(H)^\perp = \ker P$.
#+end_statement

****** Primera implicaciÃ³n
Trivialmente idempotente. Es autoadjunto por tenerse:

\[\begin{aligned}
\langle u,pv \rangle &= \langle pu,v \rangle \\
\langle u-pu,pv \rangle &= \langle pu,v - pv \rangle \\
0 &= 0
\end{aligned}\]

Donde usamos que $pu \in M$, pero $u - pu \in M^\perp$.

****** Segunda implicaciÃ³n
Tenemos $u = p(u) + (u - p(u))$ y son ortogonales por:

\[\langle p(u),v-p(v) \rangle = \langle u , p(v-p(v)) \rangle = 0\]

****** Tercera implicaciÃ³n
Comprobamos que son iguales $(I-P)(H) = \ker P$. Tenemos que $g = g - p(g)$
para un caso y $p(g -p(g)) = p(g)-p(g) = 0$ para el otro.

****** Cuarta implicaciÃ³n
Es una proyecciÃ³n sobre $P(H)$ y es ortogonal por definiciÃ³n.

***** Ejercicio 2
#+begin_statement
Sea $H$ un espacio de Hilbert y $\{e_n\}$ una base ortonormal de $H$. Sea
$\{\alpha_n\}$ una sucesiÃ³n acotada de nÃºmeros complejos y:

\[\alpha = \sup_{n \in \mathbb{N}} |\alpha_n|\]

Probar que existe un Ãºnico $T\in L(H)$ tal que $Te_n = \alpha_ne_n$, para cada $n \in \mathbb{N}$,
y que $\|T\| = \alpha$. Calcular $T^\ast$ y $\|T^\ast\|$. Demostrar que $T$ es normal. Â¿QuÃ©
condiciÃ³n ha de cumplir la sucesiÃ³n dada para que el operador $T$ sea
autoadjunto?Â¿y para que $T$ sea invertible?
#+end_statement

Expresando $u$ en serie de Fourier.

Autoadjunto cuando $\alpha_i = \overline{\alpha_i}$ e invertible cuando $\alpha_i \neq 0$.

***** Ejercicio 2.1
#+begin_statement
Sea $H = \mathbb{C}^3$ y sea $T \in L(H)$ el operador dado por:

\[T(\alpha_1,\alpha_2,\alpha_3) = (\alpha_1+\alpha_2,\alpha_1+\alpha_3,\alpha_3+2\alpha_1)\]

Â¿Es unitario?Â¿Es autoadjunto? Determinar el espectro de $T$.
#+end_statement

***** Ejercicio 3
#+begin_statement
Sea $H$ un espacio de Hilbert sobre $\mathbb{K}$ y $T \in L(H)$ un operador tal que
$\langle Tu,u \rangle = 0$ para cada $u \in H$. Demostrar que $T = 0$ si $\mathbb{K}=\mathbb{C}$, pero que no
puede decirse lo mismo si $\mathbb{K}=\mathbb{R}$, y buscar una condiciÃ³n suficiente para
que se verifique dicha propiedad en el caso real.
#+end_statement

****** Caso complejo
Desarrollando:

\[\begin{aligned}
0
&=& 
\langle T(u+v),u+v \rangle
&=&
\langle Tu,v \rangle + \langle Tv,u \rangle\\
0
&=& 
\langle T(u+iv),u+iv \rangle
&=&
(-i)\langle Tu,v \rangle + i\langle Tv,u \rangle
\end{aligned}\]

Por tanto, $\langle Tu,v \rangle = 0$ y debe tenerse $Tu = 0$.

****** Caso real
Hay contraejemplos en el caso real. En $\mathbb{R}^2$ se tiene:

\[T(x,y) = (-y,x)\]

cumpliendo lo pedido. En general, cuando se tiene $T^\ast = -T$, tenemos:

\[\langle Tu,u \rangle = -\langle u,Tu \rangle = -\langle Tu,u \rangle = 0\]

siendo una condiciÃ³n suficiente.

***** Ejercicio 4
#+begin_statement
Sea $H = \mathbb{C}^2$. Calcular el espectro y la norma del operador $T \in L(H)$ dado,
respectivamente, por cada una de las siguientes matrices:

  1. \[\begin{pmatrix} 0 & 1+i \\ 1 & 2+2i \end{pmatrix}\]

  2. \[\begin{pmatrix} 
     cos \theta & e^{i\phi} sen \theta \\
     e^{i\phi} sen \theta & -cos \theta
     \end{pmatrix}\]
#+end_statement

Usaremos que la [[http://math.stackexchange.com/a/586835/85067][norma de la matriz]] es la raÃ­z del mayor valor propio
de $M\overline{M^T}$.

****** Primera matriz
Reduciendo la matriz, tenemos como valores propios:

\[\begin{aligned}
\lambda_1 &= 1+i+\sqrt{1+3i}\\
\lambda_2 &= 1+i-\sqrt{1+3i}
\end{aligned}\]

Calculamos:

\[M\overline{M^T} = 
\begin{pmatrix} 
2 & 4 \\ 4 & 9 
\end{pmatrix}\]

A la que le podemos encontrar los valores propios:

\[\lambda_1 = \frac{1}{2}(11-\sqrt{113})\]

\[\lambda_2 = \frac{1}{2}(11+\sqrt{113})\]

#+BEGIN_SRC sage
M = Matrix([[0,1+i],[1,2+2*i]])
(M*(M.transpose().conjugate())).eigenvalues()
#+END_SRC

#+RESULTS:
: [-1/2*sqrt(113) + 11/2, 1/2*sqrt(113) + 11/2]
: a^2 - 11*a + 18

****** Segunda matriz
Reduciendo la matriz, llegamos a los valores propios
$\lambda_1=1,\lambda_2 = -1$.

Y podemos aplicar lo mismo que en la anterior.

#+BEGIN_SRC sage
t,f = var('t f')
assume(t,'real')
assume(f,'real')
M = Matrix([[cos(t), e^(i*f)*sin(t)],[e^(i*f)*sin(t),-cos(t)]])
expand((M*(M.transpose().conjugate())).eigenvalues())
#+END_SRC

#+RESULTS:
: 
: [(cos(t)^2*e^(I*f) + (-I*e^(2*I*f) + I)*cos(t)*sin(t) + e^(I*f)*sin(t)^2)*e^(-I*f),
:  (cos(t)^2*e^(I*f) + (I*e^(2*I*f) - I)*cos(t)*sin(t) + e^(I*f)*sin(t)^2)*e^(-I*f)]

***** Ejercicio 5
#+begin_statement
Sea $T \in L(H)$ donde $H$ es un espacio de Hilbert de dimensiÃ³n 3. 
CalcÃºlese el espectro de $T$ sabiendo que $T$ es autoadjunto,
que $\|T\| = 4$, que $T$ no tiene inverso, y que el rango de $T-2I$ es $2$.
#+end_statement

Sabemos que un valor propio es $2$, que otro valor propio es $0$ por no
tener inversa. Como la norma es la raÃ­z 

***** Ejercicio 6
#+begin_statement
Sea $H$ un espacio de Hilbert complejo de dimensiÃ³n $n+1$, y sea
$B = \{e_0,\dots,e_n\}$ una base ortonormal. Sea $T \in L(H)$ el operador determinado
por las igualdades:

   - \[T(e_0) = 0\]
   - \[T(e_j) = \sqrt{j} e_{j-1}\]
 
Obtener los operadores $T^\ast$ y $T^\ast T$. Probar que $T^{n+1} = 0$. Calcular el espectro
de los operadores $T$, $T^\ast$ y $T^\ast T$ asÃ­ como $\|T\|$ y $\|T^\ast T\|$.
#+end_statement

La soluciÃ³n es:

  - $T^\ast T(e_j) = je_j$.
  - $G_p(T^\ast T) = \{0,1,\dots,n\}$.
  - $G_p(T) = \{0\}$.
  - $\|T\| = \sqrt{n}$, $\|T^\ast T\| =n$.

***** Ejercicio 7
#+begin_statement
Diagonalizar (si es posible) los operadores dados (respectivamente) por:

  1. \[\begin{pmatrix} 
     7 & -2 & 1 \\
     -2 & 10 & 2 \\
     -1 & -2 & 7
     \end{pmatrix}\]

  2. \[\begin{pmatrix} 
     2 & -2 & 3 \\
     1 & 1 & 1 \\
     1 & 3 & 1 \\
     \end{pmatrix}\]

  3. \[\begin{pmatrix} 
     2 & 2 & 1 \\
     1 & 3 & 1 \\
     1 & 2 & 2 \\
     \end{pmatrix}\]
#+end_statement
** Grupos y representaciones
*** 1. Ãlgebras y mÃ³dulos
**** 1.1. NociÃ³n de Ã¡lgebra
***** 1.1. Ãlgebra
Un *Ã¡lgebra* sobre un cuerpo $K$ es un K-espacio vectorial dotado de una
aplicaciÃ³n bilineal $(\cdot) : K \times K \longrightarrow K$. La bilinealidad se expresa como:

  1. $(a+b)c = ac+bc$
  2. $a(b+c) = ab+ac$
  3. $(\alpha a)b = \alpha(ab) = a(\alpha b)$

****** Ãlgebra asociativa
Un Ã¡lgebra es *asociativa* si $(ab)c = a(bc)$.

***** 1.2. SubÃ¡lgebra
Una *subÃ¡lgebra* es un subespacio vectorial de un Ã¡lgebra cerrado para
el producto.

***** 1.3.a. Ãlgebra conmutativa
Un Ã¡lgebra es *conmutativa* si $ab = ba$.

***** 1.3.b. Centro de un Ã¡lgebra
Se define el *centro* de un Ã¡lgebra asociativa como:

\[
Z(A) = \{c \in A \mid ac = ca,\; \forall a \in A \}
\]

***** 1.4. Ideal de un Ã¡lgebra
Un subespacio vectorial de Ã¡lgebra, $I \subseteq A$ es *ideal* si el producto por
cualquier elemento estÃ¡ en el ideal $ai,ia \in I$.

***** 1.5. Cociente por un ideal
Sobre el K-espacio vectorial cociente por un ideal $A/I$, podemos definir
una K-Ã¡lgebra mediante $(a+I)(b+I) = ab+I$.

****** Buena definiciÃ³n
Supongamos que $a+I = a'+I$ y que $b+I = b'+I$, entonces tenemos 
que:

\[
ab- a'b' = a(b-b') - (a-a')b' \in I
\]

***** 1.6. Homomorfismos de K-Ã¡lgebras
Una aplicaciÃ³n lineal entre Ã¡lgebras $f : A \longrightarrow A'$ es homomorfismo de
K-Ã¡lgebras si respeta el producto:

\[
f(ab) = f(a)f(b)
\]

****** Isomorfismo de K-Ã¡lgebras
Cuando un homormofismo de K-Ã¡lgebras es biyectivo, se llama *isomorfismo*
y su inversa es tambiÃ©n homomorfismo de K-Ã¡lgebras.

***** 1.7. Primer teorema de isomorfÃ­a
Si $f : A \longrightarrow A'$ es homomorfismo de K-Ã¡lgebras, $\mathrm{Im} f$ es subÃ¡lgebra y $\ker f$
es ideal. AdemÃ¡s, tenemos un isomorfismo de K-Ã¡lgebras canÃ³nico:

\[
\widehat f : A/\ker f \longrightarrow \im f
\]

dado por $\widehat f(a+\ker f) = f(a)$.

****** DemostraciÃ³n
******* La imagen es subÃ¡lgebra
Trivialmente, la imagen es espacio vectorial y $f(a)f(b) = f(ab)$.

******* El nÃºcleo es un ideal
El nÃºcleo es subespacio vectorial y ademÃ¡s,

\[
f(ak) = f(a)f(k) = 0 = f(k)f(b) = f(kb)
\]

para cualquier $f(k) = 0$.

******* Isomorfismo de Ã¡lgebras
Comprobamos que estÃ¡ bien definido, ya que si $a + \ker f = b + \ker f$,
se tiene que $\widehat f(a+\ker f) - \widehat f(b + \ker f) = f(a-b) = 0$.

La funciÃ³n es lineal y preserva el producto por la definiciÃ³n de
producto con la que hemos dotado al ideal. Ahora, comprobamos que
es inyectiva por tenerse:

\[
\widehat f(a+\ker f) = 
\widehat f(b+\ker f) \implies f(a-b) =
0 \implies a-b \in I
\]

Es trivialmente sobreyectiva.

***** 1.8.a. Ãlgebras unitales
Una K-Ã¡lgebra asociativa es *unital* si existe un neutro para el
producto.

\[
1_Aa = a = a1_A; \quad 1_A \neq 0
\]

****** Homormofisos de Ã¡lgebras unitales
A los homomorfismos de Ã¡lgebras unitales se les pide respetar la
unidad. Para $f : A \to B$, $f(1_A) = 1_B$.

****** AsunciÃ³n posterior
En el curso trabajaremos siempre con Ã¡lgebras asociativas y unitales.

***** 1.9. InclusiÃ³n del cuerpo en el Ã¡lgebra
Sea $A$ una K-Ã¡lgebra, la inclusiÃ³n $u : K \longrightarrow A$ es homomorfismo inyectivo
de K-Ã¡lgebras. Como consecuencia $\im u \subseteq Z(A)$, y es una K-subÃ¡lgebra.

****** DemostraciÃ³n
******* Es homomorfismo
La inclusiÃ³n definida por $u(k) = k1_A$ es lineal por tenerse:

\[
u(\gamma\alpha+\beta) =
(\gamma\alpha+\beta)1 =
\gamma(\alpha 1) + \beta 1 =
\gamma u(\alpha) + u(\beta)
\]

Y ademÃ¡s es multiplicativa:

\[
u(\alpha)u(\beta) = (\alpha 1)(\beta 1) = \alpha\beta 11 = u(\alpha\beta)
\]

Y trivialmente unital por $u(1) = 1$.

******* Es inyectivo
Si $u(k) = u(k')$ entonces $(k-k')1_A = 0$.

******* Es subÃ¡lgebra del centro
Aplicando bilinealidad de la multiplicaciÃ³n:

\[u(\alpha) a = 
(\alpha 1)a = 
\alpha (1a) = 
\alpha (a1) = 
a(\alpha 1) =
au(\alpha)\]

**** 1.2. La representaciÃ³n regular. Unidades y divisores de cero
***** 1.11.a. Ãlgebra de endomorfismos
Los endomorfismos de un K-espacio vectorial forman una K-Ã¡lgebra con la
composiciÃ³n:

\[
End_K(V) = \{ f : V \longrightarrow V \mid f \text{ es lineal}\}
\]

****** DemostraciÃ³n
La suma se define por $(f+g)(v) = f(v)+g(v)$, lo que da un grupo abeliano
con el neutro $0(v) = 0$; ademÃ¡s, con $(\alpha f)(v) = \alpha f(v)$, nos da un espacio
vectorial.

Comprobamos ademÃ¡s que la composiciÃ³n es bilineal:

  1. $((f+g)\circ h)(v) = f(h(v)) + g(h(v)) = (f\circ h + g\circ h)(v)$
  2. $(f\circ (g+h))(v) = f(g(v)) + f(h(v)) = (f\circ g + f\circ h)(v)$
  3. $(\alpha f \circ g)(v) = \alpha (f\circ g)(v)) = f(\alpha g(v)) = (f \circ \alpha g)(v)$

Donde en el primer y segundo punto usamos la definiciÃ³n de suma; y
en el tercer punto usamos la linealidad de la funciÃ³n para conmutar
el elemento del cuerpo y la aplicaciÃ³n de la funciÃ³n.

***** 1.11.b. InclusiÃ³n en los endomorfismos
Sea $A$ cualquier K-Ã¡lgebra. La aplicaciÃ³n $\lambda : A \longrightarrow End_K(A)$ que asigna
a cada $a \in A$ la aplicaciÃ³n $\lambda_a : A \longrightarrow A$ definida por $\lambda_a(b) = ab$ para
todo $b \in A$ es un homomorfismo inyectivo de K-Ã¡lgebras.

****** Caso finito
Toda K-Ã¡lgebra de dimensiÃ³n finita es isomorfa a una subÃ¡lgebra de
matrices con coeficientes en $K$.

****** DemostraciÃ³n
******* Es homormorfismo
Por los axiomas de K-Ã¡lgebra, $\lambda_a$ es siempre lineal. AdemÃ¡s, la propia
$\lambda$ es lineal por tenerse:

\[
\lambda_{a+\alpha b}(c) = (a+\alpha b)c = ac + \alpha bc = 
\lambda_a(c) + \alpha\lambda_b(c) = (\lambda_a+\alpha\lambda_b)(c)
\]

AdemÃ¡s, es multiplicativa por asociatividad:

\[
\lambda_{ab}(c) = (ab)c = a(bc) = \lambda_a \circ \lambda_b (c)
\]

******* Es inyectiva
Si $\lambda_a = 0$, se tiene $a = \lambda_a(1) = 0$.

***** Ãlgebra opuesta
El Ã¡lgebra opuesta $A^{op}$ es la propia $A$ con el producto dado por:

\[
a \cdot b = ba
\]

***** Unidades y divisores de cero
Un $a \in A$ no nulo es *unidad* si existe $a^{-1} \in A$ tal que $aa^{-1} = 1 = a^{-1}a$.
El conjunto de las unidades, $U(A)$ forma un grupo con el producto.

****** Divisor de cero
Un $a \in A$ no nulo es *divisor de cero* si $\exists b: ab = 0$ Ã³ $ba = 0$.

****** ClasificaciÃ³n en unidades y divisores de cero en dimensiÃ³n finita
Sea $a \in A$ no nulo para $A$ k-Ã¡lgebra de /dimensiÃ³n finita/:

1. Equivalen:

   - $a \in U(A)$
   - $\exists b \in A : ab = 1$
   - $\exists c \in A : ca = 1$

2. Equivalen:

   - $a \notin U(A)$
   - $\exists b \in A: ab = 0$
   - $\exists c \in A : ca = 0$

Es decir, todo elemento no nulo es una unidad o un divisor de cero.

******* DemostraciÃ³n primer punto
Si $ab = 1$, $\lambda_a$ es sobreyectiva y $\lambda_b$ es inyectiva. Por ser de dimensiÃ³n
finita ambas son isomorfismos. Se aplica sobre el Ã¡lgebra opuesta para
llegar a la otra implicaciÃ³n.

******* DemostraciÃ³n segundo punto
Ssi $ab = 0$, $\lambda_a$ no es inyectiva, y por tanto no puede ser unidad.

****** ClasificaciÃ³n por el determinante
Si en un Ã¡lgebra de dimensiÃ³n finita tomamos la representaciÃ³n regular
$\lambda: A \to \mathrm{End}(A)$; podemos usar el determinante para decidir si $a \in A$ es
una unidad.

***** Ãlgebra de divisiÃ³n
Un Ã¡lgebra $A$ es un *Ã¡lgebra de divisiÃ³n* si $U(A) = A \setminus \{0\}$. Los cuerpos
son Ã¡lgebras de divisiÃ³n conmutativas.

**** 1.3. Representaciones y mÃ³dulos
***** RepresentaciÃ³n
Una *representaciÃ³n* de un Ã¡lgebra $A$ es un homomorfismo de k-Ã¡lgebras
$\mu : A \longrightarrow End_K(V)$, donde $V$ es el k-espacio vectorial de representaciÃ³n.

****** RepresentaciÃ³n fiel
Se llama representaciÃ³n *fiel* cuando $\mu$ es inyectiva.

****** RepresentaciÃ³n regular
La inclusiÃ³n en los endomorfismos $A \to \mathrm{End}(A)$ es una *representaciÃ³n fiel*
que llamamos representaciÃ³n regular.

***** MÃ³dulos de un Ã¡lgebra
Dada $A$ Ã¡lgebra, un A-mÃ³dulo por la izquierda es un espacio vectorial $V$
con un producto bilineal $A \times V \to V$ cumpliendo $(ab)v = a(bv)$ y $1v = v$.

****** SubmÃ³dulos
Llamamos *submÃ³dulo a izquierda* a un subconjunto $N$ de un mÃ³dulo que 
sea subgrupo aditivo y que cumpla $an \in N$ para $n \in N$. Los submÃ³dulos
de un Ã¡lgebra se llaman *ideales a izquierda*.

****** RetÃ­culo de submÃ³dulos
Llamamos ${\cal L}(M)$ a la familia de submÃ³dulos de $A$. Forman un retÃ­culo bajo
la suma y la intersecciÃ³n.

****** SubmÃ³dulo generado
El submÃ³dulo generado por un conjunto de elementos es el menor submÃ³dulo
que los contiene.

***** Equivalencia de mÃ³dulos y representaciones
Sean $A$ una k-Ã¡lgebra y $V$ un k-espacio vectorial. Hay una biyecciÃ³n
entre el conjunto de representaciones de $A$ sobre $V$ y los A-mÃ³dulos
por la izquierda sobre $V$.

****** DemostraciÃ³n
Si tenemos una representaciÃ³n $\mu\colon A \to \mathrm{End}(V)$, definimos el A-mÃ³dulo
dado por $av = \mu(a)(v)$. Si tenemos una estructura de A-mÃ³dulo podemos
construir la representaciÃ³n $\mu(a)(v) = av$.

***** Suma directa de mÃ³dulos
Llamamos *suma directa* de los mÃ³dulos $M_1,\dots,M_n$ al mÃ³dulo sobre su
producto cartesiano con las operaciones

 * $(m_1,\dots,m_n) + (m_1',\dots,m_n') = (m_1+m_1',\dots,m_n+m_n')$
 * $a(m_1,\dots,m_n) = (am_1,\dots,am_n)$

***** Teorema de Cayley-Hamilton
Todo endomorfismo de un espacio vectorial de dimensiÃ³n finita satisface
su ecuaciÃ³n caracterÃ­stica.

****** DemostraciÃ³n
Sea $T$ un endomorfismo en un espacio $V$ con base $\{v_1,\dots,v_n\}$, definido por
la matriz siguiente

\[C =\begin{pmatrix}
a_{11} & a_{12} & \dots \\
a_{21} & a_{22} & \dots \\
\vdots & \vdots & \ddots \\
\end{pmatrix}.
\]

Si consideramos la matriz $\Delta = (TI_n - C)^t$ en $K[X]$, tenemos que

\[\Delta\begin{pmatrix}
v_1 \\ v_2 \\ \vdots \\ v_n
\end{pmatrix} = \begin{pmatrix}
Tv_1 - \sum_{j=1}^n a_{j1}v_j \\
Tv_2 - \sum_{j=1}^n a_{j2}v_j \\
\vdots \\
Tv_n - \sum_{j=1}^n a_{jn}v_j  
\end{pmatrix} = \begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}.
\]

Multiplicando ahora por su matriz adjunta, se tiene que

\[
\widetilde \Delta\Delta\begin{pmatrix}
v_1 \\ v_2 \\ \vdots \\ v_n
\end{pmatrix} = \begin{pmatrix}
\mathrm{det}(\Delta)v_1 \\ \mathrm{det}(\Delta)v_2 \\ \vdots \\ \mathrm{det}(\Delta)v_n
\end{pmatrix} = \begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}.
\]

Luego, por ser una base, $\mathrm{det}(\Delta)v = 0$ para cualquier $v \in V$. Tenemos
entonces que $T$ satisface la ecuaciÃ³n polinÃ³mica

\[ \mathrm{det}(TI_n - C) = \mathrm{det}(\Delta^t) = \mathrm{det}(\Delta) = 0.
\]

***** SubmÃ³dulo finitamente generado
Un $A\text{-mÃ³dulo}$ $M$ es *finitamente generado* si existe $X \subseteq M$ subconjunto
finito que lo genera, $M = RX$.

****** SubmÃ³dulo cÃ­clico
Un mÃ³dulo generado por un elemento se llama *cÃ­clico*.

***** Suma de mÃ³dulos
Dados submÃ³dulos $N_1,\dots,N_m \leq M$, su suma es el menor submÃ³dulo que contiene
a todos ellos.

****** CaracterizaciÃ³n de la suma
Sea $M$ un $A\text{-mÃ³dulo}$,

 1. Dados submÃ³dulos $N_1,\dots,N_m$ de $M$, tenemos que

    \[
    N_1+\dots+N_m = \left\{ n_1+\dots+n_m \mid n_i \in N_i \right\}.
    \]

 2. Dado $X = \{m_1,\dots,m_n\} \subseteq M$, tenemos que $RX= Rm_1 + \dots + Rm_{n}$.

******* DemostraciÃ³n
Comprobamos que un mÃ³dulo que los contenga debe contener a todos
los elementos de esa forma, ademÃ¡s, forman un mÃ³dulo, asÃ­ que es
el menor.

***** Homomorfismo de mÃ³dulos
Un aplicaciÃ³n entre $A\text{-mÃ³dulos}$ $f\colon M \to N$ es *homomorfismo de mÃ³dulos*
si $f(am) = af(m)$ para $a \in A, m \in M$.

***** Cociente de mÃ³dulos
Sea $L < M$ un submÃ³dulo. El cociente $M/L$ tiene estructura de mÃ³dulo con

\[a(m+L) = am+L
\]

y la suma inducida en el cociente.

***** Primer teorema de isomorfÃ­a para mÃ³dulos
Para $f\colon M \to N$ homomorfismo de mÃ³dulos, $\mathrm{ker}(f)$ e $\mathrm{im}(f)$ son submÃ³dulos
y hay un isomorfismo $\widehat f\colon M/ \mathrm{ker}(f) \to \mathrm{im}(f)$ dado por

\[
\widehat f(m+ \mathrm{ker}(f)) = f(m)
\]

****** DemostraciÃ³n
Aplicamos primero el primer teorema de isomorfÃ­a en grupos. Y comprobamos
que ademÃ¡s $\widehat f$ es $A\text{-lineal}$ por ser $f$ isomorfismo de mÃ³dulos.

***** Bases y mÃ³dulos libres
Un conjunto de generadores de un $A\text{-mÃ³dulo}$ $M$ es *base* si cada $m \in M$
se escribe Ãºnicamente como

\[
m = \sum_{i=1}^n a_im_i
\]

****** MÃ³dulo libre
Un mÃ³dulo que admite una base se llama *mÃ³dulo libre*. Un ejemplo de
mÃ³dulo libre es $A^n$.

***** CaracterizaciÃ³n de bases
Un subconjunto $B \subseteq M$ no vacÃ­o finito es base si, y sÃ³lo si, para 
cualquier aplicaciÃ³n $f\colon B \to N$ existe un Ãºnico homomorfismo de
$R\text{-mÃ³dulos}$ $\overline{f} \colon M \to N$ con $\overline{f}_{|B} = f$.

\[\begin{tikzcd}
B \rar[hook]\drar[dashed, swap]{\exists! \overline{f}} & M \dar{f}\\
  & N
\end{tikzcd}\]

****** TODO DemostraciÃ³n

***** CaracterizaciÃ³n de finitamente generados
Para $M$ un $A\text{-mÃ³dulo}$,

 1. Si $M$ admite un conjunto de generadores $\left\{ m_1,\dots,m_n \right\}$, entonces
    $M \cong A^n/L$ para cierto submÃ³dulo $L$.
 2. Si $M$ es libre con base $\left\{ m_1,\dots,m_n \right\}$, entonces $M \cong A^n$.

****** TODO DemostraciÃ³n

***** Segundo teorema de isomorfÃ­a para mÃ³dulos
Sean $L,N \in {\cal L}(M)$ submÃ³dulos. Existe un isomorfismo

\[
\frac{L+N}{L} \cong \frac{N}{L \cap N}.
\]

****** DemostraciÃ³n
Aplicamos el primer teorema de isomorfÃ­a a la funciÃ³n 

\[
f\colon N \to \frac{L+N}{L}
\]

dada por $f(n) = n+L$. Es sobreyectiva y tiene como nÃºcleo a $L \cap N$.

***** Tercer teorema de isomorfÃ­a para mÃ³dulos
Sean $L \subseteq N \in {\cal L}(M)$. Existe un isomorfismo

\[
\frac{M/L}{N/L}\cong \frac{M}{N}
\]

AdemÃ¡s, hay una biyecciÃ³n creciente entre submÃ³dulos de $M$ conteniendo
a $L$ y ${\cal L}(M/L)$.

****** DemostraciÃ³n
Aplicamos priemr teorema de isomorfÃ­a a la aplicaciÃ³n

\[
f \colon M/L \to N/L
\]

dada por $f(m+L) = m+N$.

**** 1.4. MÃ³dulos simples. Teorema de Jordan-HÃ¶lder
***** MÃ³dulo simple
Un mÃ³dulo $M$ se dice simple si no tiene submÃ³dulos propios.

***** SubmÃ³dulo maximal
Un submÃ³dulo propio $N < M$ es *maximal* si lo es en ${\cal L}(M)$.

****** CaracterizaciÃ³n por simplicidad
Por tercer teorema de isomorfÃ­a, esto equivale a que $M/N$ es simple.

***** Serie de composiciÃ³n
Una cadena de submÃ³dulos $0 \subset M_1 \subset M_2 \subset \dots \subset M$ es una *serie de composiciÃ³n*
de $M$ si cada $M_{i-1}$ es maximal en $M_i$.

***** Teorema de Jordan-HÃ¶lder
Sea $M$ un $A$ mÃ³dulo de /dimensiÃ³n finita/ como $K$ espacio vectorial con
dos series de composiciÃ³n:

\[0 = M_0 \subset M_1 \subset\dots\subset M_n = M\]
\[0 = N_0 \subset N_1 \subset\dots\subset N_m = M\]

Entonces $n=m$ y existe una permutaciÃ³n con $M_i/M_{i-1} \cong N_{\sigma(i)}/N_{\sigma(i)-1}$.

****** Factores de composiciÃ³n
Los mÃ³dulos $M_i/M_{i-1}$ se llaman *factores de composiciÃ³n* de $M$ y estÃ¡n
Ãºnicamente determinados salvo isomorfismo y reordenaciÃ³n.

****** DemostraciÃ³n
Usaremos inducciÃ³n sobre $n$. En el caso $n=1$, $M$ es simple y no tiene
submÃ³dulos propios, luego todas sus series de composiciÃ³n son la misma.
En otro caso, no es simple y $n,m>1$.

******* Caso 1
Si $M_{n-1}=N_{n-1}$, aplicamos a ambos la hipÃ³tesis de inducciÃ³n y
ampliamos la permutaciÃ³n obtenida.

******* Caso 2
Si $M_{n-1} \neq N_{n-1}$, $M_{n-1}+N_{m-1} = M$ por maximalidad, y su intersecciÃ³n
tiene una serie de composiciÃ³n

\[
0 \subset L_1 \subset \dots \subset L_{k-1} \subset N_{m-1} \cap M_{n-1}
\]

que ademÃ¡s puede extenderse de dos formas a $M_{n-1}$ y $N_{m-1}$, sabiendo por 
segundo teorema de isomorfÃ­a que

\[
\frac{M_{n-1}}{N_{m-1}\cap M_{n-1}} \cong \frac{M}{N_{m-1}}
\quad\text{ y que }\quad
\frac{N_{n-1}}{N_{m-1}\cap M_{n-1}} \cong \frac{M}{M_{m-1}}
\]

son simples. Aplicando la hipÃ³tesis de inducciÃ³n dos veces, tenemos
dos permutaciones que nos dan

\[
L_i/L_{i-1} \cong M_{\tau i}/M_{\tau i - 1}
\quad\text{ y que }\quad
L_i/L_{i-1} \cong N_{\sigma i}/M_{\sigma i - 1}.
\]

CombinÃ¡ndolas tenemos lo pedido.

***** Longitud de un mÃ³dulo
El nÃºmero de factores de composiciÃ³n es la *longitud* del mÃ³dulo $\ell(M)$.

***** Longitud y cociente
Si $M$ es de dimensiÃ³n finita y $N \in {\cal L}(M)$. Entonces $\ell(M)=\ell(N)+\ell(M/N)$.

****** DemostraciÃ³n
Si tenemos series de composiciÃ³n

\[
0 \subset N_1 \subset N_2 \subset \dots \subset N
\qquad
\frac{N}{N} \subset \frac{M_1}{N} \subset \dots \subset \frac{M}{N}
\]

podemos aplicar el tercer teorema de isomorfÃ­a para tener
$M_j/M_{j-1} \cong \frac{M_j/N}{M_{j-1}/N}$ simple, y por tanto, una serie de composiciÃ³n

\[
0 \subset N_1 \subset \dots \subset N \subset M_1 \subset \dots \subset M.
\]

Como consecuencia, $\ell(M)=\ell(N)+\ell(M/N)$.

***** Longitud, suma e intersecciÃ³n
Sea $M$ un mÃ³dulo dimensiÃ³n finita con $N,L \in {\cal L}(M)$. Entonces:

\[\ell(N+L) + \ell(N\cap L) = \ell(N)+\ell(L)\]

****** DemostraciÃ³n
Aplicamos el [[*Segundo teorema de isomorfÃ­a para mÃ³dulos][segundo teorema de isomorfÃ­a]] y la [[*Longitud y cociente][longitud de un cociente]]
para tener

\[
\frac{L+N}{L} \cong \frac{N}{L \cap N},
\]

y por tanto $\ell(L+N) - \ell(L) = \ell(N) - \ell(L \cap N)$.

**** 1.5. Independencia lineal y sumas directas internas
***** Familia independiente
Una familia $\{N_i \mid i \in I\} \subset {\cal L}(M)$ es *independiente* si se verifica:

\[
N_j \cap \sum_{j\neq i} N_i = \{0\}
\]

***** Suma directa interna
Dada una familia independiente, $\sum_{i\in I} N_i \subset M$ se llama *suma directa interna*.

***** Suma directa externa e interna
Existe un Ãºnico homomorfismo de mÃ³dulos $\theta : \bigoplus_{i\in I} N_i \to \sum_{i\in I} N_i$, tal que
$\theta\iota_i(m) = m$ para cualquier $m \in N_i$.

****** DemostraciÃ³n
Extendiendo por linealidad la condiciÃ³n, el Ãºnico homomorfismo posible es:

\[
\theta((m_i)_{i \in I}) = \sum_{i \in I} m_i
\]

***** CaracterizaciÃ³n de familia independiente
Equivalen:

 1. La familia $\{N_i\mid i\in I\}$ es independiente.
 2. Toda subfamilia /finita/ $F \subset \{N_i\mid i\in I\}$ es independiente.
 3. La expresiÃ³n de cada $m = \sum_{i\in I} m_i$ con $m_i\in N_i$ es Ãºnica.
 4. Si $0 = \sum_{i\in I} m_i$, entonces $m_i = 0$.
 5. El homomorfismo canÃ³nico $\theta : \bigoplus_{i\in I} N_i \to \sum_{i\in I} N_i$ es inyectivo e isomorfismo.
 6. Para $J_1,J_2 \subset I$ con $J_1\cap J_2 = \varnothing$ se tiene $\sum_{i\in J_1} N_i \cap \sum_{i\in J_2} N_i = \{0\}$.

En este caso, notaremos la suma directa interna tambiÃ©n por $\bigoplus_{i \in I} N_i$.

****** DemostraciÃ³n
******* ImplicaciÃ³n 1 a 2
Trivial por definiciÃ³n de independencia.

******* ImplicaciÃ³n 2 a 3
Esto equivale a que cada $\sum_{i \in I} m_i = 0$ lleva a $m_i = 0$. Pero si hubiera
algÃºn $m_j$ no nulo, serÃ­a $m_j = - \sum_{i \in I, i\neq j} m_i$, contraviniendo independencia.

******* ImplicaciÃ³n 3 a 4
Trivial por la unicidad.

******* ImplicaciÃ³n 4 a 5
Desde lo anterior, se tiene que tiene nÃºcleo trivial y por tanto es
inyectivo. AdemÃ¡s, es sobreyectivo porque genera trivialmente todos los
elementos de $\sum N_i$. Es por tanto una biyecciÃ³n e isomorfismo.

******* ImplicaciÃ³n 5 a 6
Si no fuera asÃ­, existirÃ­an subconjuntos $J_1,J_2$ cumpliendo:

\[
\sum_{i \in J_1} m_i = \sum_{i \in J_2} n_i
\]

NÃ³tese que entonces la funciÃ³n $\theta$ darÃ­a la misma imagen para ambos
subconjuntos, contraviniendo inyectividad.

******* ImplicaciÃ³n 6 a 1
Trivial por el caso de $J_2$ con un elemento.

***** Ampliar familia independiente
Sea $\{N_i \mid i\in I\} \subset {\cal L}(M)$ es familia independiente y tenemos:

\[
N \cap \bigoplus_{i\in I} N_i = \{0\}
\]

Entonces, $\{N_i \mid i \in I\} \cup \{N\}$ es independiente.

****** DemostraciÃ³n
Si $n + \sum n_i = 0$, tenemos $n \in \bigoplus N_i \cap N$ y, por tanto, $n = 0$. Por 
independencia de la familia $\sum n_i = 0$, se llega a $n_i = 0$.

***** Suma directa en suma de simples
Sea $N \subset \sum_{i\in I} M_i$ suma de submÃ³dulos simples. Existe $\{M_i \mid i \in J \subseteq I\} \cup \{N\}$ 
independiente con:

\[
N \oplus \left( 
\bigoplus_{i \in J} M_i
\right) = \sum_{i \in I} M_i
\]

****** DemostraciÃ³n
Tomamos el conjunto $\Gamma$ de los subconjuntos $J \subseteq I$ tales que $\{M_i \mid i \in J\} \cup \{N\}$
es independiente. Veamos que es no vacÃ­o. Si para todo $N \cap M_i \neq 0$, 
debe tenerse por simplicidad $M_i \subset N$ y por tanto $\sum M_i \subset N$. AsÃ­, debe
existir algÃºn $M_i$ para el que $N \cap M_i = 0$.

Tomamos el maximal $J$. Para cualquier Ã­ndice $i \in I -J$, se tiene entonces
por maximalidad que $M_i \cap (N + \bigoplus_{j \in J} M_j) \neq 0$, pero eso implica por simplicidad
que $M_i \subseteq N + \bigoplus_{j\in J} M_j$.

***** Existencia de base para espacio vectorial finitamente generado
Sea $_DV$ espacio vectorial izquierdo sobre el anillo de divisiÃ³n $D$.
Para todo sistema de generadores no nulos $\{v_i\mid i\in I\}$ de $V$ existe un 
subconjunto tal que $V = \bigoplus_{j\in J} Dv_j$.

Todo espacio vectorial finitamente generado sobre $D$ tiene una base.

****** DemostraciÃ³n
Un anillo de divisiÃ³n es simple. Como $Dv_j \cong D$, tenemos que es un 
espacio suma de simples, luego [[*Suma directa en suma de simples][existe un conjunto]] de independientes que
genera el espacio.

**** 1.6. Independencia en familias infinitas
***** Suma directa externa infinita
Se define la *suma directa externa* $\bigoplus_{i\in I} N_i$ como el subconjunto del producto
cartesiano formado por las tuplas con un nÃºmero finito de valores no nulos.

**** 1.7. ClasificaciÃ³n de las Ã¡lgebras de divisiÃ³n reales de dimensiÃ³n finita
***** Determinante, traza, polinomio caracterÃ­stico y mÃ­nimo
Dada $a \in D$ en un Ã¡lgebra de divisiÃ³n sobre $k$, consideramos su *traza*,
su *determinante*, su *polinomio caracterÃ­stico* y su *polinomio mÃ­nimo*
como los del endomorfismo lineal multiplicaciÃ³n, $\lambda_a \colon D \to D$.

***** Lema de clasificaciÃ³n de Ã¡lgebras de divisiÃ³n
Sea $D$ Ã¡lgebra real de dimensiÃ³n finita mayor que $1$. Entonces:

\[
V = \{a \in D : a^2 \leq 0\} = \{a\in D\mid tr(a) = 0\}
\]

Luego es un subespacio vectorial con $D = \mathbb{R} \oplus V$. AdemÃ¡s, la dimensiÃ³n real
de $D$ es par.

****** DemostraciÃ³n
Llamamos $n = \mathrm{dim}_{\mathbb{R}}(D)$ y dado $a \in D$ consideramos su polinomio 
caracterÃ­stico

\[
p(X) = (X-r_1)\dots (X-r_k)q_1(X)\dots q_m(X)
\]

descompuesto en factores lineales y cuadrÃ¡ticos. Por Cayley-Hamilton,
tenemos que $p(a) = 0$, luego debe anularse algÃºn polinomio,

  * si $(a-r_i) = 0$ entonces $a \in \mathbb{R}$, y si $a^2 \leq 0$, nos da $a = 0$.
  * si $a \in D \setminus \mathbb{R}$, tendremos algÃºn $q_j(a) = 0$ como polinomio mÃ­nimo de $a$.

Por Ejercicio 18 tenemos $p = q^t$, con $2t=n$ en este caso. Por irreducibilidad
se tiene $q = (X-z)(X-\overline{z})$ para algÃºn complejo, asÃ­ que

\[\begin{aligned}
q(X) &= X^2 - 2 \mathrm{Re}(z) X + |z|^2 \\
p(X) &= X^{2t} - 2 \mathrm{Re}(z)t X^{2t-1} + \dots \\
p(X) &= X^{2t} - \mathrm{tr}(a) X^{2t-1} + \dots \\
\end{aligned}
\]

desde el desarrollo de $q$ y la definiciÃ³n de la traza como coeficiente del
polinomio caracterÃ­stico.

Sustituyendo en la primera ecuaciÃ³n desde la Ãºltima tenemos que

\[
a^2 - \frac{\mathrm{tr}(a)}{t}a + |z|^2 = 0,
\]

y que por tanto $a^2 \in \mathbb{R}^-$ si y sÃ³lo si $\mathrm{tr}(a) = 0$.

***** Teorema de Frobenius
Sea $D$ Ã¡lgebra de divisiÃ³n real de dimensiÃ³n finita. Entonces $D$ es isomorfa
a $\mathbb{R}$, $\mathbb{C}$, o $\mathbb{H}$.

****** DemostraciÃ³n
Si $D \not\cong \mathbb{R}$, aplicamos el [[*Lema de clasificaciÃ³n de Ã¡lgebras de divisiÃ³n][lema de clasificaciÃ³n]] para tener $D$ de dimensiÃ³n
par con $D = \mathbb{R}\oplus V$. Consideramos

\[
B(a,b) = \frac{ab+ba}{2}
= \frac{1}{2}\left( (a+b)^2-a^2-b^2 \right) \in \mathbb{R},
\]

una forma bilineal simÃ©trica definida negativa. Podemos diagonalizarla
para obtener una base donde $B(e_i,e_j) = 0$ si $i\neq j$ y $B(e_i,e_i) = -1$, es decir,
por definiciÃ³n de $B$,

\[
e_i^2 = -1
\quad\text{ y }\quad
e_ie_j = -e_je_i.
\]

En el caso $t=1$, tenemos $\mathbb{C}$. En el caso $t>1$, tenemos ademÃ¡s la restricciÃ³n
de que si tomamos $u = e_1e_2e_j$, se cumple

\[
u^2 = -e_1e_2e_1e_je_2e_j = 1,
\]

luego $0 = (u-1)(u+1)$ en un anillo de divisiÃ³n nos da $e_j = \pm e_1e_2$, asÃ­ que
debe tenerse $t=2$, con base $\left\{ 1,e_1,e_2,e_1e_2 \right\}$. En este caso, se comprueba que
hay un isomorfismo $\mathbb{H} \cong D$.

***** Corolario de Frobenius
La Ãºnica Ã¡lgebra de divisiÃ³n compleja de dimensiÃ³n finita es $\mathbb{C}$.

****** DemostraciÃ³n
NÃ³tese que en particular serÃ­a un Ã¡lgebra de divisiÃ³n real y no podrÃ­a
ser $\mathbb{H}$ porque no tiene a los complejos como centro.

**** 1.8. Idempotentes y anillos de matrices
***** Idempotente
Un elemento de un Ã¡lgebra $e \in R$ se llama *idempotente* si $e^2 = e$.
Son idempotentes triviales $0$ y $1$.

***** Conjunto completo de idempotentes ortogonales (CCIO)
Un conjunto de idempotentes no triviales $\{e_1,\dots,e_n\}$ es conjunto completo de
idempotentes ortogonales si:

\[
1 = e_1 + \dots + e_n
\]

Y ademÃ¡s, $e_ie_j = 0$ para $i \neq j$.

***** DescomposiciÃ³n de un CCIO
Sea $\{e_1,\dots,e_n\}$ un CCIO para $R$. Entonces $R = Re_1 \oplus \dots \oplus Re_n$.

****** DemostraciÃ³n
Cualquier elemento de $R$ se expresa como:

\[
r = r(e_1+e_2+\dots+e_n)
\]

Y la suma es directa porque si se tiene $x \in Re_j \cap \left(\sum_{i\neq j} Re_i \right)$, entonces:

\[
x = xe_j = \left(\sum_{i\neq j} xe_i\right)e_j = 0
\]

***** DescomposiciÃ³n en un CCIO
Sea $R = I_1 \oplus \dots \oplus I_n$ descomposiciÃ³n por ideales a izquierda no triviales.
Entonces, si $1 = e_1 + \dots + e_n$, para $e_i\in I_i$, $\{e_1,\dots,e_n\}$ forman un CCIO 
con $I_i = Re_i$.

****** DemostraciÃ³n
Si $x \in I_j$, $x = x\sum e_i$ y se tiene,

\[x - xe_j = 
\sum_{i\neq j} xe_i \in I_j \cap \left(\sum_{i\neq j} I_i\right) = 
\{0\}.\]

AsÃ­, hemos demostrado que

\[
I_j = \{ x \in R \mid xe_j = x\} = Re_i
\]

y que por tanto, $e_i^2 = e_i$. Por eso se tiene $\sum_{i\neq j} e_ie_j = 0$ y por independencia
lineal, se llega a $e_ie_j = 0$.

***** Matrices de descomposiciÃ³n
Llamamos al conjunto de matrices siguiente,

\[
Mat(e_iRe_j) = \left\{(r_{ij}) \mid r_{ij} \in e_iRe_j\right\}
\]

que es un subespacio vectorial multiplicativamente cerrado de $M_n(R)$. La
matriz diagonal

\[\begin{pmatrix}
e_1 & 0 & \dots & 0\\
0 & e_2 & \dots & 0 \\
\vdots & & & \vdots \\
0 & 0 & \dots & e_n
\end{pmatrix}
\]

es elemento neutro multiplicativo.

****** DemostraciÃ³n
Se comprueba trivialmente por tenerse:

\[
(e_ire_k)(e_kr'e_j) = e_i(re_kr')e_j \in e_iRe_j
\]

Multiplicando se comprueba ademÃ¡s que la diagonal es la unidad
multiplicativa.

***** DescomposiciÃ³n en matrices
La aplicaciÃ³n $\phi \colon R \to Mat(e_iRe_j)$ dada por $\phi(r) = (e_ire_j)_{ij}$ es un isomorfismo
de K-Ã¡lgebras.

****** DemostraciÃ³n
******* Es homomorfismo de Ã¡lgebras
Por definiciÃ³n es lineal. Si calculamos la componente $(i,j)$ de
$\phi(r)\phi(s)$, tenemos

\[
\sum_k e_ire_ke_kse_j =
\sum_k e_ire_kse_j =
e_ir \left(\sum_k e_k\right) se_j =
e_irse_j
\]

que es la componente $(i,j)$ de $\phi(rs)$. AdemÃ¡s, $\phi(1)$ es claramente la unidad.

******* Es isomorfismo
Supongamos que $\phi(r)=0$, entonces se tiene

\[
r = \left(\sum_i e_i\right)r \left( \sum_{j} e_{j} \right)
= \sum_{i,j} e_{i}re_{j} = 0
\]

y la funciÃ³n es inyectiva. Para comprobar que es sobreyectiva, simplemente
tomamos una matriz $(r_{ij})$ de la forma, y comprobamos que por ortogonalidad
e idempotencia se tiene

\[
\phi \left( \sum_{i,j} r_{ij} \right) = (r_{ij})
\]

***** DescomposiciÃ³n de endomorfismos
Sea $M = M_1 \oplus M_2 \oplus \dots \oplus M_n$ un A-mÃ³dulo con $M_i \cong N$. Se tiene

\[
\mathrm{End}(M) = M_n(\mathrm{End}(N)).
\]

****** TODO DemostraciÃ³n

***** DescomposiciÃ³n en ideales bilÃ¡teros
Sea $R = I_1\oplus I_2\oplus \dots \oplus I_n$ descompuesto en ideales bilÃ¡teros. Sea $\left\{ e_1,\dots,e_n \right\}$
su CCIO asociado. Entonces $e_i \in Z(R)$, idempotente central.

***** DescomposiciÃ³n en Ã¡lgebras
Sea $\{e_1,\dots,e_n\}$ un CCIO centrales de $R$. Entonces $Re_i$ es un Ã¡lgebra con unidad
y tenemos un isomorfismo de Ã¡lgebras $R \cong Re_1\times Re_2 \times \dots \times Re_n$ definido
por $r \mapsto (re_1,\dots,re_n)$.

***** Idempotente central primitivo
Un idempotente central es *primitivo* si $Re$ no es suma directa de dos ideales
propios de $R$.

***** DescomposiÃ³n en centrales primitivos
Si $R$ tiene un CCIO centrales primitivos, este conjunto es Ãºnico.

****** TODO DemostraciÃ³n

**** 1.9. El Ã¡lgebra de enfomorfismos de un mÃ³dulo semisimple
***** Complemento
Para $N \subseteq M$ submÃ³dulo, un *complemento* de $N$ es un $X$ tal que

\[
M = N \oplus X.
\]

En caso de que tenga complemento lo llamamos *sumando directo*.

***** MÃ³dulos semisimples
Un mÃ³dulo de dimensiÃ³n finita se dice *semisimple* si todo submÃ³dulo
es un sumando directo.

***** CaracterizaciÃ³n de semisimples
Sea $M$ mÃ³dulo con dimensiÃ³n finita como K-espacio vectorial. Equivalen:

  1) $M$ es semisimple.
  2) $M$ es suma directa finita de submÃ³dulos simples.
  3) $M$ es suma finita de submÃ³dulos simples.

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Tomamos una familia maximal de submÃ³dulos simples linealmente
independientes. Si el complemento de su suma no fuera nulo, entonces
contendrÃ­a algÃºn submÃ³dulo simple (por finitud) que serÃ­a linealmente
independiente, contraviniendo maximalidad.

******* Segunda implicaciÃ³n
Trivial.

******* Tercera implicaciÃ³n
Trivial porque podemos tomar [[*Suma directa en suma de simples][suma directa en suma de simples]] para
encontrar el complemento.

***** Lema de Schur
Sean $M,M'$ simples con $f\colon M \to M'$ homomorfismo de mÃ³dulos. Se tiene $f=0$
o $f$ isomorfismo.

****** Corolario: anillo de endomorfismos de un mÃ³dulo simple
El anillo de los endomorfismos de un mÃ³dulo simple es un anillo de
divisiÃ³n.

****** DemostraciÃ³n
Si no es nula, el nÃºcleo es un submÃ³dulo propio, luego debe ser inyectiva.
La imagen entonces serÃ¡ un submÃ³dulo propio no nulo y serÃ¡ sobreyectiva.

***** SubmÃ³dulos y cocientes de semisimples
Si $M$ es un semisimple de dimensiÃ³n finita, entonces todo submÃ³dulo de $M$
y todo cociente de $M$ es semisimple.

****** DemostraciÃ³n
Si existe un epimorfismo de mÃ³dulos $M \to N$, se tiene $N$ semisimple.
Cualquier cociente tendrÃ¡ la proyecciÃ³n como epimorfismo hacia Ã©l y
cualquier submÃ³dulo $N \subseteq M$ serÃ¡ cociente por su complemento como

\[
\frac{M}{X} =
\frac{N \oplus X}{X} \cong
\frac{N}{N \cap X} \cong N.
\]

******* El epimorfismo da la semisimplicidad
Por [[*Lema de Schur][Lema de Schur]], se tiene que la imagen de un simple serÃ¡ simple o
nula. AsÃ­, podemos escribir

\[
N = \sum_{i\in I} f(M_i)
\]

y tendremos que es suma de simples y por [[*CaracterizaciÃ³n de semisimples][caracterizaciÃ³n]], semisimple.

***** Unicidad de la descomposiciÃ³n en simples
Sea $M = M_1\oplus \dots \oplus M_n = N_1 \oplus \dots \oplus N_m$ semisimple descompuesto como suma
directa de simples. Entonces $n=m$ y se tiene $M_i \cong N_{\sigma i}$ para alguna 
permutaciÃ³n.

****** DemostraciÃ³n
Tenemos dos series de composiciÃ³n

\[\begin{aligned}
\left\{ 0 \right\} &= M_0 \subset
M_1 \subset 
M_1 \oplus M_2 \subset 
&\dots& \subset
M_1 \oplus \dots \oplus M_n &= M \\
\left\{ 0 \right\} &= N_0 \subset
N_1 \subset 
N_1 \oplus N_2 \subset 
&\dots& \subset
N_1 \oplus \dots \oplus N_n &= M \\
\end{aligned}\]

en las que los factores son simples, explÃ­citamente por segundo
teorema de isomorfÃ­a,

\[
\frac{M_j \oplus \dots \oplus M_0}{M_{j-1}\oplus \dots\oplus M_0} \cong
\frac{M_j}{(M_{j-1} \oplus \dots \oplus M_0) \cap M_j} \cong M_j.
\]

Pero aplicando [[*Teorema de Jordan-HÃ¶lder][Jordan-HÃ¶lder]], $M_j \cong N_{\sigma j}$.

***** Componentes isotÃ³picas de un mÃ³dulo
Sea $M = M_1\oplus \dots \oplus M_n$ descomposiciÃ³n finita en mÃ³dulos simples. Podemos
escoger mÃ³dulos simples $\Sigma_1,\dots,\Sigma_t$ y una particiÃ³n $\{1,\dots,n\} = \Lambda_1 \cup \dots \cup \Lambda_t$
tal que $M_i \cong \Sigma_j$ si y sÃ³lo si $i \in \Lambda_j$.

Podemos tomar $M_{\Lambda_j} = \bigoplus_{i\in \Lambda_j} M_i$ para descomponer en *componentes isotÃ³picas*

\[
M = M_{\Lambda_1}\oplus \dots \oplus M_{\Lambda_t}
\]

y llamar a $n_j$ la *multiplicidad* $\Sigma_j$ en $M$. Las componentes con su multiplicidad
son invarriantes llamados *estructura del mÃ³dulo*.

***** Estructura de los endomorfismos
Sea $M$ con estructura $(\Sigma_1,n_1),\dots,(\Sigma_t,n_t)$ entonces $\Delta_j= \mathrm{End}(\Sigma_j)$ es una
$K\text{-Ã¡lgebra}$ de dimensiÃ³n finita y hay un isomorfismo

\[ \mathrm{End}(M) \cong
\mathrm{M}_{n_1}(\Delta_1) \times \dots \times \mathrm{M}_{n_t}(\Delta_t)
\]

****** TODO DemostraciÃ³n
**** 1.10. Ãlgebras semisimples de dimensiÃ³n finita
***** Ãlgebras semisimples
Un Ã¡lgebra $A$ de dimensiÃ³n finita es *semisimple* si todo A-mÃ³dulo de
dimensiÃ³n finita es semisimple.

***** CaracterizaciÃ³n de Ã¡lgebras semisimples
Un Ã¡lgebra de dimensiÃ³n finita es semisimple si y sÃ³lo si es semisimple
como A-mÃ³dulo.

****** DemostraciÃ³n
Si $M$ es un mÃ³dulo finito-dimensional, es el cociente de un libre $A^n$.
Como $A^n$ es semisimple por serlo $A$, su [[*SubmÃ³dulos y cocientes de semisimples][cociente]] $M$ es semisimple.

***** Estructura de los mÃ³dulos de un Ã¡lgebra semisimple
Sea $A$ es un Ã¡lgebra semisimple con estructura $(n_1,\Sigma_1),\dots,(n_t,\Sigma_t)$ como
$A\text{-mÃ³dulo}$, entonces todo $A\text{-mÃ³dulo}$ finito tiene estructura $(m_1,\Sigma_1),\dots,(m_t,\Sigma_t)$
para algunos $m_1,\dots,m_t$. En particular, todo $A\text{-mÃ³dulo}$ simple es isomorfo a 
un $\Sigma_j$.

****** DemostraciÃ³n
Los mÃ³dulos de dimensiÃ³n finita son cocientes de $A^n$, y la estructura de
$A^n$ es simplemente $(nn_1,\Sigma_1),\dots,(nn_t,\Sigma_t)$, y sabemos que la estructura de los
submÃ³dulos y de los cocientes es la misma con coeficientes menores.

***** Ãlgebra de matrices sobre anillo de divisiÃ³n es semisimple
Dada $\Delta$ Ã¡lgebra de divisiÃ³n finito-dimensional, $M_n(\Delta)$ es un Ã¡lgebra 
semisimple con estructura $(n, \Sigma)$ para $\Delta \cong \mathrm{End}(\Sigma)^{op}$. AdemÃ¡s, $M_n(\Delta)^{op} \cong M_n(\Delta^{op})$.

****** DemostraciÃ³n
******* El Ã¡lgebra de matrices es semisimple
Tomamos $A_j$ el ideal izquierda de $M_n(\Delta)$ con base $\left\{ E_{1j},\dots,E_{nj} \right\}$.
Comprobamos que es simple, ya que cualquiera de sus elementos no
nulos genera $M_n(\Delta)E_{jj}$, por ser $\Delta$ anillo de divisiÃ³n.

AsÃ­, vemos que $M_n(\Delta)$ es semisimple por ser

\[ M_n(\Delta) = A_1 \oplus \dots \oplus A_n. \]

******* Estructura unimodular
Veamos que existe un isomorfismo de mÃ³dulos $A_1 \cong A_j$, explÃ­citamente

\[
f(a_1E_{11}+\dots +a_nE_{n1}) = a_1 E_{1j} + \dots + a_n E_{nj}
\]

es trivialmente isomorfismo de espacios vectoriales y se comprueba que
es homomorfismo de mÃ³dulos por tenerse

\[
f\left(\left( \sum_{}  \right)\right)
\]

***** Teorema de Wedderburn
Una $K\text{-Ã¡lgebra}$ de dimensiÃ³n finita es semisimple ssi es isomorfa a un
Ã¡lgebra de la forma

\[ \mathrm{M}_{n_1}(\Delta_1) \times \dots \times \mathrm{M}_{n_t}(\Delta_t).
\]

de forma Ãºnica para algunas $\Delta_1,\dots,\Delta_t$ Ã¡lgebras de divisiÃ³n de dimensiÃ³n
finita. AdemÃ¡s, esta factorizaciÃ³n es esencialmente Ãºnica.

****** TODO DemostraciÃ³n

***** Centro de Ã¡lgebra semisimple de dimensiÃ³n finita
Si $A$ es semisimple de dimensiÃ³n finita, $Z(A)$ es producto finito de cuerpos
extensiÃ³n finita de $k$. El nÃºmero de factores es el nÃºmero de $A\text{-mÃ³dulos}$ simples
no isomorfos en la estructura de $A$.

****** TODO DemostraciÃ³n
***** Semisimplicidad del Ã¡lgebra opuesta
Si $A$ es semisimple, entonces $A^{op}$ es semisimple.

****** TODO DemostraciÃ³n

***** Teorema de Molien
Un Ã¡lgebra compleja $A$ de dimensiÃ³n finita es semisimple ssi es
isomorfa a exactamente una de la forma

\[ \mathrm{M}_{n_1}(\mathbb{C}) \times \dots \mathrm{M}_{n_t}(\mathbb{C})
\]

cumpliendo $\mathrm{dim}_{\mathbb{C}}(A) = n_1^2+\dots+n_t^2$.

*** 2. Representaciones de grupos finitos
**** 2.1. Representaciones lineales de grupos finitos y mÃ³dulos
***** 2.1. RepresentaciÃ³n
Una *representaciÃ³n* $k\text{-lineal}$ de $G$ es un homomorfismo de grupos

\[\rho \colon G \to \mathrm{GL}(V).\]

****** Espacio de representaciÃ³n
Llamamos a $V$ /espacio de representaciÃ³n/ y a su dimensiÃ³n la
/dimensiÃ³n de la representaciÃ³n/. 

****** DimensiÃ³n finita
AquÃ­ consideraremos sÃ³lo representaciones de dimensiÃ³n finita.

***** Ãlgebra de grupo
Para $G$ grupo finito y $k$ cuerpo, el /Ã¡lgebra de grupo/ $kG$ se define
como el $k\text{-espacio}$ vectorial libre sobre $G$ con el producto dado por la
extensiÃ³n bilineal del producto sobre $G$.

***** RelaciÃ³n entre representaciÃ³n y mÃ³dulo del Ã¡lgebra de grupo
La aplicaciÃ³n que asigna cada homomorfismo de Ã¡lgebras $\Pi\colon kG \to \mathrm{End}_k(V)$
su restricciÃ³n $\rho \colon G \to \mathrm{GL}(V)$ es una biyecciÃ³n a las representaciones.

Las representaciones $k\text{-lineales}$ de $G$ son las estructuras de $kG\text{-mÃ³dulo}$
sobre $V$.

****** DemostraciÃ³n
Sabemos que cada homomorfismo $G \to \mathrm{GL}(V)$ extiende de manera Ãºnica
por ser una base de $kG$ y extiende al producto por estar definido
precisamente por extensiÃ³n. Cada aplicaciÃ³n restringe a $GL(V)$ por
ser los elementos de $G$ invertibles en $kG$ y es homomorfismo de
grupos.

***** Subespacio invariante
Un subespacio $W \leq V$ es $\rho\text{-invariante}$ si $\rho(g)(W) \leq W$ para cualquier $g \in G$.

/Equivalentemente, es un $kG\text{-mÃ³dulo}$/.

***** RepresentaciÃ³n irreducible
Una representaciÃ³n no nula es irreducible si no tiene espacios invariantes
propios.

/Equivalentemente, el $kG\text{-mÃ³dulo}$ es simple/.

**** 2.2. Representaciones completamente reducibles. Teorema de Maschke
***** 2.8. RepresentaciÃ³n completamente reducible
Una representaciÃ³n se llama *completamente reducible* si es nula
o el espacio de representaciÃ³n es suma directa de espacios irreducibles.

/Equivalentemente, el $kG\text{-mÃ³dulo}$ es semisimple/.

***** 2.9. Teorema de Maschke
Sea $G$ grupo finito y $k$ cuerpo con $\mathrm{char}(k) \nmid |G|$. Toda representaciÃ³n aquÃ­ es
completamente reducible.

/Equivalentemente, $kG$ es un Ã¡lgebra semisimple/.

****** DemostraciÃ³n
Dada $\varphi \colon V \to U$ $k\text{-lineal}$, definimos

\[
\widetilde \varphi(v) = \frac{1}{|G|}\sum_{g \in G}g \varphi(g^{-1}v),
\]

usando que $\mathrm{char}(k) \nmid |G|$, y es un homomorfismo de $kG\text{-mÃ³dulos}$,

\[\begin{aligned}
\widetilde\varphi(hv) = 
\frac{1}{|G|}\sum_{g \in G}hh^{-1}g\varphi(g^{-1}hv) =
h \frac{1}{|G|} \sum_{k \in G} k \varphi(k^{-1}v) = h \widetilde\varphi(v).
\end{aligned}\]

Sea ahora $V$ un $kG\text{-mÃ³dulo}$ con $W \leq V$. Consideramos $\pi\colon V \to V/W$, y
usando el complemento como espacio vectorial, creamos $\varphi\colon V/W \to V$
lineal con $\pi\circ\varphi = \mathrm{id}_{V/W}$ y tomamos la $\widetilde \varphi$. Tenemos entonces

\[
\pi(\widetilde\varphi(x)) =
\pi \left( \frac{1}{|G|}\sum_{g \in G}g\varphi(g^{-1}x) \right) =
\frac{1}{|G|} g\pi(\varphi(g^{-1}x)) =
\frac{1}{|G|} \sum_{g \in G} gg^{-1}x = x,
\]

y por tanto $\pi \circ \widetilde\varphi = \mathrm{id}_{V/W}$. Si tomamos $U = \operatorname{Im} \widetilde\varphi$, vemos que $V = W \oplus U$;
luego todo submÃ³dulo es un sumando directo.

***** 2.10.a. Representaciones equivalentes
Dos representaciones $k\text{-lineales}$ de $G$ se llaman *equivalentes* si los
$kG\text{-mÃ³dulos}$ son isomorfos.

***** 2.10.b. RepresentaciÃ³n regular
La representaciÃ³n regular $\rho_{reg}$ es la asociada al propio $kG$ como $k\text{-mÃ³dulo}$.
Cuando $\mathrm{char}(k) \nmid |G|$ es, por [[*Teorema de Maschke][Teorema de Maschke]], suma de irreducibles, que
notaremos como

\[\rho \sim
\rho_1^{n_1}\oplus \dots\oplus \rho_t^{n_t}
\]

para ciertas multiplicidades $n_i$.

***** 2.10.c. Constituyentes
Usando la estructura de las Ã¡lgebras semisimples, sabemos que cualquier
representaciÃ³n $k\text{-lineal}$ de $G$ serÃ¡ de la forma

\[\rho \sim
\rho_1^{m_1}\oplus \dots\oplus \rho_t^{m_t}
\]

para ciertas multiplicidades $m_i$. Llamamos a las $\rho_i$ con multiplicidad positiva
las *constituyentes* de $\rho$.

***** 2.11. Multiplicidades en la representaciÃ³n regular
Cuando $\mathrm{char}(k) \nmid |G|$, si las representaciones $k\text{-lineales}$ irreducibles de $G$
son $(V_1,\rho_1),\dots,(V_{t},\rho_t)$ y tenemos $n_i$ la multiplicidad de cada una de ellas
en la representaciÃ³n regular y $d_i = \mathrm{dim}_k(\Delta_i)$ la dimnesiÃ³n asociada al
Ã¡lgebra de divisiÃ³n que da el teorema de Wedderburn; tenemos que
$\mathrm{dim}_k(V_i) = d_in_i$ y $|G| = d_1n_1^2 +\dots + d_tn_t^2$.

****** TODO DemostraciÃ³n

***** 2.12. Multiplicidades en al representaciÃ³n regular compleja
Sean $(V_1,\rho_1),\dots,(V_t,\rho_t)$ las representaciones irreducibles complejas de $G$.
Si las multiplicidades en la representaciÃ³n regular son $(n_1,\dots,n_t)$,
entonces $\mathrm{dim}_{\mathbb{C}} V_i = n_i$ para $i = 1,\dots,t$ y $|G| = n_1^2 + \dots + n_t^2$.

****** TODO DemostraciÃ³n
***** 2.13. DimensiÃ³n del centro del Ã¡lgebra-grupo
Sean $C_1,\dots,C_r$ las clases de conjugaciÃ³n de $G$. Entonces $\mathrm{dim}_k Z(kG) = r$.

****** TODO DemostraciÃ³n

***** 2.14. NÃºmero de representaciones irreducibles complejas
El nÃºmero de clases de conjugaciÃ³n de $G$ coincide con el nÃºmero de
representaciones irreducibles complejas de $G$.

****** TODO DemostraciÃ³n
**** 2.3. Caracteres
***** 2.15. CarÃ¡cter complejo
Para $(V,\rho)$ representaciÃ³n compleja de $G$, la aplicaciÃ³n $\chi_{\rho}\colon G \to \mathbb{C}$ dada
por $\chi_{\rho}(g) = \mathrm{tr}(\rho(g))$, se llama *carÃ¡cter* complejo de $\rho$.

****** CarÃ¡cter irreducible
El que procede de una representaciÃ³n irreducible.

****** Grado de un carÃ¡cter
DimensiÃ³n de $V$ como espacio vectorial complejo.

***** 2.16. CarÃ¡cter invariante por equivalencia
Dos representaciones complejas equivalentes proporcionan el mismo
carÃ¡cter.

****** DemostraciÃ³n
Si $(V,\rho)$ y $(W,\pi)$ son equivalentes por $T \colon V \to W$, para $g \in G$ tenemos
que $T\rho(g) = \pi(g)T$, luego $\rho(g) = T^{-1}\pi(g)T$, y sabemos que la traza se
preserva por semejanza.

***** 2.17.a. Exponente de un grupo
El *exponente* de un grupo $G$ es el mÃ­nimo comÃºn mÃºltiplo de los Ã³rdenes
de sus elementos.

***** 2.17.b. DiagonalizaciÃ³n de elementos de la representaciÃ³n
Sea $G$ con exponente $m$ y $(V,\rho)$ representaciÃ³n de grado $n$. Existen raÃ­ces
$m\text{-Ã©simas}$ de la unidad $\omega_1,\dots,\omega_n$ en las que diagonaliza $\rho(g)$. Se tiene
entonces

\[\chi_{\rho}(g) = \omega_1+\dots + \omega_n \]

y

\[\chi_{\rho}(g^{-1}) = \overline{\chi_{\rho}(g)}.
\]

****** TODO DemostraciÃ³n

***** 2.18. Cota del carÃ¡cter
Para $G$ con exponente $m$ y $(V,\rho)$ representaciÃ³n,

\[|\chi_{\rho}(g)| \leq \operatorname{deg} \chi_{\rho}.
\]

El caso de igualdad se tiene si y sÃ³lo si $\rho(g) = \omega \mathrm{id}_V$ para alguna raÃ­z
$m\text{-Ã©sima}$ de la unidad.

****** Caso de la identidad
En particular, $\chi_{\rho}(g) = \operatorname{deg} \chi_{\rho}$ si y sÃ³lo si $\rho(g) = \mathrm{id}_V$.

***** 2.19. NÃºcleo de un carÃ¡cter
Dado $\chi$ carÃ¡cter complejo, su *nÃºcleo* se define como

\[\ker \chi = \left\{ g \in G \mid \chi(g) = \chi(1) \right\}.
\]

****** TODO El nÃºcleo es un subgrupo normal

***** ExtensiÃ³n de representaciones y caracteres
Dada una representaciÃ³n $\rho \colon G \to \mathrm{Aut}(V)$, notamos por $\tilde{\rho}(g) \colon \mathbb{C}G \to \mathrm{End}_{\mathbb{C}}(V)$ la
extensiÃ³n al Ã¡lgebra-grupo. Dado un carÃ¡cter $\chi_{\rho} \colon G \to \mathbb{C}$, notamos por 
$\widetilde{\chi_{\rho}} \colon \mathbb{C}G \to \mathbb{C}$ a la extensiÃ³n al Ã¡lgebra grupo.

***** Caracteres irreducibles complejos
Los *caracteres irreducibles* complejos de $G$ son los dados por sus
representaciones irreducibles.

****** DimensiÃ³n del espacio de representaciÃ³n
NÃ³tese que para cualquier carÃ¡cter irreducible se tiene
# Â¿Necesitamos que sea irreducible?

\[n_i = \operatorname{dim}_{\mathbb{C}} V_{i} = \chi_i(1).\]

***** CarÃ¡cter de la suma directa
Si $(W,\pi)$ es una representaciÃ³n con $W = W_{1} \oplus \dots \oplus W_m$, se tiene que

\[\chi_{\pi} = \chi_{\pi_1} + \dots + \chi_{\pi_m}.
\]

****** TODO DemostraciÃ³n

***** CarÃ¡cter regular
El *carÃ¡cter regular* es el carÃ¡cter de la representaciÃ³n regular.
Cumple que

 1) \[\chi_{reg}(g) = \left\{\begin{array}{ll} |G|,  & \mbox{si } g=1 \\0, & \mbox{si }  g \neq 1.
    \end{array} 
    \right.\]
 2) $\chi_{reg} = \chi_1(1)\chi_1 + \dots + \chi_t(1)\chi_t$.

****** TODO DemostraciÃ³n

**** 2.4. La tabla de caracteres
***** TODO Tabla de caracteres
***** TODO Teorema de Frobenius
**** 2.5. Funciones de clase. Reciprocidad
***** Producto interno
En el espacio vectorial $\mathbb{C}^G$, de dimensiÃ³n $|G|$, definimos el siguiente
*producto interno*

\[(\varphi,\psi) = \frac{1}{|G|} = \sum_{g \in G}\overline{\varphi(g)}\psi(g).
\]

****** Conjunto ortonormal
NÃ³tese que los $\left\{ \chi_1,\dots,\chi_t \right\}$ forman un /conjunto ortonormal/ de vectores
en $\mathbb{C}^G$.

****** Base ortonormal
Si $G$ es /abeliano/, tiene tantas clases de conjugaciÃ³n como elementos.
Tenemos $t = |G|$ y los caracteres irreducibles son una base ortonormal
de $\mathbb{C}^G$.

***** Funciones de clase
Una *funciÃ³n de clase* de $G$ es una aplicaciÃ³n $\varphi\colon G \to \mathbb{C}$ constante sobre
cada clase de conjugaciÃ³n de $G$. Es decir,

\[
\varphi(hgh^{-1}) = h\varphi(g) h^{-1}.
\]

Al subespacio complejo de funciones de clase lo llamamos ${\cal C}(G)$.

***** Base ortonormal de las funciones de clase
Los caracteres irreducibles $\mathrm{Irr}(G) = \left\{ \chi_1,\dots,\chi_t \right\}$ forman una base ortonormal
de ${\cal C}(G)$.

****** TODO DemostraciÃ³n

***** Equivalencia por caracteres
Dos representaciones complejas de $G$ son equivalentes si, y sÃ³lo si,
proporcionan el mismo carÃ¡cter.

****** TODO DemostraciÃ³n

***** RestricciÃ³n e inducciÃ³n
Fijados $H \leq G$, definimos

 * la *restricciÃ³n* $(-)_H\colon {\cal C}(G) \to {\cal C}(H)$ de funciones de clase.
 * la *inducciÃ³n* $(-)^G\colon {\cal C}(H) \to {\cal C}(G)$ de funciones de clase.

La /inducciÃ³n/ se define como

\[
\varphi^G(g) = \frac{1}{|H|}\sum_{x \in G}\varphi^{\bullet}(x^{-1}gx),
\]

donde $\varphi^{\bullet}(g) = \varphi(g)$ cuando $g \in H$ y $\varphi^{\bullet}(g) = 0$ cuando $g \notin H$.

***** Reciprocidad de Frobenius
Sea $H$ un subgrupo de $G$, $\varphi \in {\cal C}(H)$ y $\psi \in {\cal C}(G)$. Entonces

\[(\psi_H,\varphi) = (\psi,\varphi^G).
\]

****** TODO DemostraciÃ³n

***** La inducciÃ³n de un carÃ¡cter es carÃ¡cter
Si $\varphi$ es carÃ¡cter de $H$, entonces $\varphi^G$ es un carÃ¡cter de $G$.

****** TODO DemostraciÃ³n

***** TODO Constituyentes

*** Ejercicios de clase
**** Ejercicio 1
#+begin_statement
Comprobar que $K$ es una K-Ã¡lgebra.
#+end_statement

Tenemos que $K$ es un espacio vectorial sobre sÃ­ mismo y su propio
producto es una aplicaciÃ³n bilineal sobre $K$, ya que cumple:

  - $(a+b)c = ac+bc$, por axiomas de anillo.
  - $a(b+c) = ab+ac$, por axiomas de anillo.
  - $a(bc) = (ab)c = b(ac)$, el producto es conmutativo y asociativo.

AdemÃ¡s es un Ã¡lgebra asociativa y unital.

**** Ejercicio 2
#+begin_statement
Calcular el centro del Ã¡lgebra de matrices $M_n(K)$.
#+end_statement

Supongamos $A \in Z(M_n(K))$, si tomamos las matrices que sÃ³lo tienen una 
entrada unidad y el resto ceros $E_{ij} = (\delta_{ij})_{i,j}$. AsÃ­, tenemos:

\[
E_{ii}A = \begin{pmatrix}
0 & \dots & 0 \\
^{i)} a_{i1} & \dots & a_{in} \\
0 & \dots & 0 \\
\end{pmatrix}
\qquad
AE_{ii} = \begin{pmatrix}
0 & ^{i)}a_{1i} & 0 \\
\vdots & \vdots & \vdots \\
0 & a_{ni} & 0 \\
\end{pmatrix}
\]

Por lo tanto $a_{ij} = 0$ para $i \neq j$. AdemÃ¡s,

\[
E_{ij}A = \begin{pmatrix}
0 & ^{j)}\dots & 0 \\
^{i)} \dots & a_{ii} & \dots \\
0 & \dots & 0 \\
\end{pmatrix}
\qquad
AE_{ij} = \begin{pmatrix}
0 & ^{j)}\dots & 0 \\
^{i)}\dots & a_{jj} & \dots \\
0 & \dots & 0 \\
\end{pmatrix}
\]

Por tanto, $A = \lambda I$. Se cumple que $(\lambda I) B = \lambda (I B) = \lambda B = \lambda (B I) = B (\lambda I)$,
y el centro es de la forma

\[
\left\{
\lambda I \mid \lambda \in K
\right\}
\]

**** Ejercicio 3
#+begin_statement
Comprobar que $Im(u) \subseteq Z(A)$, siendo $u : K \longrightarrow A$, $u(\alpha) = \alpha 1_A$.
#+end_statement

Usando la bilinealidad:

\[
u(\alpha) a =
(\alpha 1) a =
\alpha (1a) =
1 (\alpha a) =
(\alpha a) 1 =
a (\alpha 1)
\]

**** Ejercicio 4
#+begin_statement
Supongamos $A$ anillo y $K$ cuerpo. Dado un homomorfismo de anillos $u : K \longrightarrow A$,
demostrar que $A$ es una K-Ã¡lgebra si defino su estructura de K-espacio 
vectorial como sigue:

\[
\forall\alpha \in K, a \in A:\quad \alpha a = u(\alpha) a
\]

Es decir, podemos definir alternativamente un Ã¡lgebra sobre $K$ como un
homomorfismo de anillos $u : K \longrightarrow Z(A)$, el *homomorfismo de estructura*.
#+end_statement

Debemos comprobar que la multiplicaciÃ³n del anillo es bilineal sobre la
estructura de espacio vectorial:

  - $(a+b)c = ac+bc$
  - $a(b+c) = ab+ac$
  - $(\alpha a)c = (u(\alpha) a) c = a u(\alpha) c$

Por lo que forma un K-Ã¡lgebra.

**** Ejercicio 5
#+begin_statement
Comprobar que $Z(\mathbb{H}) = \mathbb{R}$.
#+end_statement

Supongamos un elemento en el centro $z = a+bi+cj+dk$, deberÃ­a conmutar con
$i,j$, asÃ­ que:

\[
0 = zi-iz = (ai-b-ck+dj) - (ai-b+ck-dj) = 2(dj-ck)
\]
\[
0 = zj-jz = (aj+bk-c-di) - (aj-bk-c+di) = 2(bk-di)
\]

De donde tenemos $b=c=d=0$, y por tanto, el elemento debe estar en $\mathbb{R}$.

**** Ejercicio 6
#+begin_statement
Dados $q,p\in \mathbb{H}$, escritos como suma de vector y escalar, se tiene la fÃ³rmula:

\[
(a+v)(b+w) = ab + aw + bv - v\cdot w + v \wedge w
\]
#+end_statement

Los tres primeros tÃ©rminos se tienen porque el producto escalar coincide
con el producto de un real por un cuaterniÃ³n. Los dos Ãºltimos tÃ©rminos se
tienen como sigue. Si tomamos $v = xi+yj+zk$, $w = oi+pj+qk$; y los
interpretamos como vectores como $v = (x\ y\ z)$, $w = (o\ p\ q)$:

\[
vw = (-xo-yp-qz) + (pxk-qxj - oyk+qyi + ozj-pzi)
\]

Y comprobamos que:

\[(x\ y\ z)(o\ p\ q) = xo+yp+zq\]

\[\begin{vmatrix}
i&j&k \\
x&y&z \\
o&p&q \\
\end{vmatrix}
=
pxk-qxj - oyk+qyi + ozj-pzi
\]

**** TODO Ejercicio 7
#+begin_statement
Demostrar que un grupo abeliano $(V,+)$ junto a una acciÃ³n $A\times V \to V$ es
un mÃ³dulo ssi verifica las cuatro condiciones siguientes:

  1. $(a+a')v = av + a'v$
  2. $a(v+v') = av+av'$
  3. $a(a'v) = (aa')v$
  4. $1v = v$
#+end_statement
**** Ejercicio 8
#+begin_statement
Definir un submÃ³dulo.
#+end_statement

Un submÃ³dulo debe tener estructura de mÃ³dulo y una inclusiÃ³n al mÃ³dulo
del que es submÃ³dulo. Exigimos entonces, para que tenga estructura de mÃ³dulo,
que sea cerrado respecto a la suma y al producto por elementos del Ã¡lgebra.
NÃ³tese que dentro de los elementos del Ã¡lgebra estÃ¡n los elementos del 
cuerpo base del Ã¡lgebra.

**** Ejercicio 9
#+begin_statement
Sea $N_1,\dots,N_m \in {\cal L}(M)$. Demostrar que:

\[
N_1+\dots+N_m
=
\{m_1+\dots+m_n \mid m_i \in N_i \}
\]
#+end_statement

Primero notamos que es un mÃ³dulo, ya que:

 - $a(m_1+\dots+m_n) = am_1+\dots+am_n$
 - $(m_1+\dots+m_n)+(m'_1+\dots+m'_n) = (m_1+m'_1) + \dots + (m_n+m'_n)$

DespuÃ©s notamos que si un mÃ³dulo contiene a $N_1,\dots,N_m$ debe contener
todas las sumas de sus elementos por ser cerrado para la suma. AsÃ­,
este es el mÃ­nimo mÃ³dulo conteniendo a $N_i$.

**** Ejercicio 10
#+begin_statement
Â¿Para quÃ© valores del Ã¡ngulo el giro en el plano da sÃ³lo submÃ³dulos propios?
Es decir, Â¿cuÃ¡ndo es $\mathbb{R}^2$ simple como $\mathbb{R}[T]$ mÃ³dulo con $T$ giro?
#+end_statement

Sea $M$ un submÃ³dulo de $\mathbb{R}^2$ con $v \not\in M$. Si $T(v),v$ son linealmente 
independientes, el espacio $\langle Tv,v \rangle$ serÃ¡ $\mathbb{R}^2$ y no podrÃ¡ existir un mÃ³dulo
propio. En otro caso, $\langle v \rangle$ serÃ¡ un mÃ³dulo propio.

Para tener $Tv,v$ independientes, es necesario tener un giro mÃºltiplo de $\pi$.

**** TODO Ejercicio 11 (ââ)
#+begin_statement
Calcular todos los $\mathbb{R}[X]\text{-mÃ³dulos}$ de $\mathbb{P}_n$ para la acciÃ³n de derivaciÃ³n.
#+end_statement

**** Ejercicio 12?
#+begin_statement
Sean $M = S_1\oplus \dots \oplus S_t$, $N = T_1\oplus \dots \oplus T_n$ con $S_i,T_i$ simples y cumpliendo
$S_i \not\cong T_j$. Probar que todo homomorfismo de mÃ³dulos $f \colon M \to N$ es $0$.
#+end_statement

Similar al [[*Ejercicio 23 (â)][ejercicio 23]].

*** Ejercicios de los apuntes
**** Ejercicio 1 (*)
#+begin_statement
Calcular el centro del Ã¡lgebra de matrices $M_n(K)$.
#+end_statement

**** Ejercicio 2
#+begin_statement
Escribir la demostraciÃ³n de la ProposiciÃ³n 1.7.
#+end_statement

***** La imagen es subÃ¡lgebra
Trivialmente, la imagen es espacio vectorial y $f(a)f(b) = f(ab)$.

***** El nÃºcleo es un ideal
El nÃºcleo es subespacio vectorial y ademÃ¡s,

\[
f(ak) = f(a)f(k) = 0 = f(k)f(b) = f(kb)
\]

para cualquier $f(k) = 0$.

***** IsomorfÃ­a
Notamos primero que $\widehat f$ es el mismo que obtendrÃ­amos aplicando el
primer teorema de IsomorfÃ­a entre espacios vectoriales. AsÃ­, sabemos
que estÃ¡ bien definido y que es una funciÃ³n lineal biyectiva.

Comprobaremos simplemente que preserva el producto, probando asÃ­ que
es un isomorfismo de k-Ã¡lgebras; pero esto es trivial por la estructura
de Ã¡lgebra con la que hemos dotado al cociente:

\[
\widehat f((a+I)(b+I)) = 
\widehat f(ab+I) = f(ab) = f(a)f(b) 
= \widehat f(a+I) \widehat f(b+I).
\]

**** Ejercicio 3
#+begin_statement
Supongamos que $K$ es un cuerpo, y $A$ es un anillo (no necesariamente
conmutativo). Sea $u : K \to A$ un homomorfismo de anillos tal que
$Im(u) \subseteq Z(A)$, donde $Z(A)$ denota el centro de $A$, definido de manera
obvia. Comprobar que si definimos la acciÃ³n de $K$ sobre $A$ dada por
$\alpha a = u(\alpha) a$, para todo $\alpha \in K, a \in A$, entonces $A$ es una k-Ã¡lgebra.
#+end_statement

Comprobamos primero que $A$ es un k-espacio vectorial. Con su suma es
un grupo abeliano, y por ser el producto sobre ella distributivo y
el homomorfismo de anillos unital y asociativo:

 - $u(\alpha)(a+b) = u(\alpha)a + u(\alpha)b$
 - $u(1)a = 1a = a$
 - $u(\alpha)u(\beta)(a+b) = u(\alpha\beta)(a+b)$
 - $u(\alpha+\beta)a = u(\alpha)a + u(\beta)a$

Ahora comprobaremos simplemente que el producto del anillo es una
operaciÃ³n bilineal en este espacio vectorial.

  - $(a+b)c = ac+bc$
  - $a(b+c) = ab+ac$
  - $(\alpha a)c = (u(\alpha) a) c = a u(\alpha) c$

Donde hemos usado distributividad del producto y que $u(\alpha) \in Z(A)$.

**** Ejercicio 4
#+begin_statement
Demostrar que, realmente, $End_K(V)$, con las operaciones reciÃ©n descritas
(suma, producto escalar y composiciÃ³n), es una K-Ã¡lgebra.
#+end_statement

La suma se define por $(f+g)(v) = f(v)+g(v)$, lo que da un grupo abeliano
con el neutro $0(v) = 0$; ademÃ¡s, con $(\alpha f)(v) = \alpha f(v)$, nos da un espacio
vectorial.

Comprobamos ademÃ¡s que la composiciÃ³n es bilineal:

  1. $((f+g)\circ h)(v) = f(h(v)) + g(h(v)) = (f\circ h + g\circ h)(v)$
  2. $(f\circ (g+h))(v) = f(g(v)) + f(h(v)) = (f\circ g + f\circ h)(v)$
  3. $(\alpha f \circ g)(v) = \alpha (f\circ g)(v)) = f(\alpha g(v)) = (f \circ \alpha g)(v)$

Donde en el primer y segundo punto usamos la definiciÃ³n de suma; y
en el tercer punto usamos la linealidad de la funciÃ³n para conmutar
el elemento del cuerpo y la aplicaciÃ³n de la funciÃ³n.

**** Ejercicio 5 (*)
#+begin_statement
Comprobar todas las afirmaciones hechas en el Ejemplo 11.
#+end_statement

***** Estructura del espacio cociente
Sabemos que cada ideal no nulo lo genera un polinomio por ser un
Dominio de Ideales Principales, que ademÃ¡s podemos suponer mÃ³nico por ser $K$ 
un cuerpo. Como ademÃ¡s los polinomios forman un dominio euclÃ­deo con el
grado como funciÃ³n euclÃ­dea, podemos escribir cualquier polinomio $t$ como

\[
t(X) = r(X) + p(X)q(X), \text{ para } \mathrm{deg}(r) < n,
\]

y por tanto, ${\cal B} = \left\{ 1+I, x+I,\dots, x^{n-1}+I \right\}$ es un sistema generador. Sabemos
que es linealmente independiente porque si no lo fuese, tendrÃ­amos una
relaciÃ³n lineal que darÃ­a lugar a que un polinomio de grado menor que $n$
estuviera en el ideal. AsÃ­, ${\cal B}$ es base.

***** Matriz compaÃ±era
Podemos comprobar que la matriz compaÃ±era es la que representa a $\lambda_{x+I}$
por tenerse que $(x+I)(x^{i-1}+I) = (x^{i}+I)$ para $i < n$ y que

\[x^{n}+I = p_0-p_1x-\dots-p_{n-1}x^{n-1} + I\]

Sabemos ahora que $\lambda : A \to \mathrm{End}(A)$ es un homomorfismo inyectivo de Ã¡lgebras
que lleva $\lambda_{x+I} = \tilde N(p)$ y que por ser inyectivo preserva la independencia
lineal de la base. AsÃ­, la imagen de los elementos de la base es una base
de la imagen, y tenemos que el Ã¡lgebra $A$ es isomorfa a la subÃ¡lgebra de
$M_n(K)$

\[
\left\{ a_0I+a_1\tilde N(p) + \dots + a_{n-1}\tilde N(p)^{n-1} \mid
a_0,a_1,\dots,a_{n-1} \in K \right\} \subseteq M_n(K).
\]

**** Ejercicio 6
#+begin_statement
Expresar el cuerpo $\mathbb{Q}(\sqrt{2})$ como una $\mathbb{Q}$ subÃ¡lgebra de un Ã¡lgebra de matrices
sobre $\mathbb{Q}$.
#+end_statement

Empezamos notando que

\[
\mathbb{Q}\left(\sqrt{2}\right) = \frac{\mathbb{Q}}{(X^2-2)},
\]

y que podemos por tanto aplicar el razonamiento del ejemplo 11 para saber
que si la matriz compaÃ±era del polinomio $p(x) = x^2-2$ es

\[\tilde N(p) = \begin{pmatrix}
0 & 2 \\
1 & 1
\end{pmatrix},\]

el Ã¡lgebra serÃ¡ isomorfa a la subÃ¡lgebra de $M_2(\mathbb{Q})$ dada por

\[
\left\{ aI + b\tilde N(p) \mid a,b \in \mathbb{Q} \right\}.
\]

**** Ejercicio 7
#+begin_statement
Sea

\[\mathbb{H} = \left\{ \begin{pmatrix}
\alpha & -\overline{\beta} \\
\beta & \overline{\alpha}
\end{pmatrix} \mid \alpha,\beta \in \mathbb{C} \right\}\]

  1. Demostrar que $\mathbb{H}$ es una subÃ¡lgebra real de $M_2(\mathbb{C})$ y que $Z(\mathbb{H}) = \mathbb{R}$.
  2. Demostrar que todo elemento no nulo de $\mathbb{H}$ es una unidad.
  3. Demostrar que las matrices

     \[\mathbf{1} = \begin{pmatrix}1 & 0 \\0 & 1\end{pmatrix},\; 
     \mathbf{i} = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix},\;
     \mathbf{j} = \begin{pmatrix}i & 0 \\ 0 & -i\end{pmatrix},\;
     \mathbf{k} = \begin{pmatrix}0 & i \\ i & 0\end{pmatrix}
     \]

     forman una base de $\mathbb{H}$ como espacio vectorial real.
  4. Comprobar las identidades

     \[
     i^2=j^2=k^2=-1,\; ij=k,\; jk=i,\; ki=j
     \]

El Ã¡lgebra asÃ­ construida se llama Ã¡lgebra de los cuaterniones de Hamilton.
#+end_statement

***** Primer punto
Comprobamos que es cerrada bajo el producto por reales

\[\lambda\begin{pmatrix}
\alpha & -\overline{\beta} \\
\beta & \overline{\alpha}
\end{pmatrix} = \begin{pmatrix}
\lambda\alpha & -\lambda\overline{\beta} \\
\lambda\beta & \lambda\overline{\alpha}
\end{pmatrix} = \begin{pmatrix}
\lambda\alpha & -\overline{\lambda\beta} \\
\lambda\beta & \overline{\lambda\alpha}
\end{pmatrix}
\]

y que es cerrada bajo su producto

\[\begin{pmatrix}
\alpha & -\overline{\beta} \\
\beta & \overline{\alpha}
\end{pmatrix}\begin{pmatrix}
\gamma & -\overline{\delta} \\
\delta & \overline{\gamma}
\end{pmatrix} = \begin{pmatrix}
\alpha\gamma-\overline{\beta}\delta & -\overline{\delta}\alpha-\overline{\beta}\overline{\gamma} \\
\beta\gamma+\overline{\alpha}\delta & -\overline{\delta}\beta + \overline{\gamma}\overline{\alpha}
\end{pmatrix} = \begin{pmatrix}
\alpha\gamma-\overline{\beta}\delta & -\left(\overline{\beta\gamma+\overline{\alpha}\delta}\right)\\
\beta\gamma+\overline{\alpha}\delta & \overline{\alpha\gamma-\overline{\beta}\delta}
\end{pmatrix}.
\]

AdemÃ¡s, si tomamos una matriz en el centro, debe cumplir

\[\begin{aligned}\begin{pmatrix}
a & -\overline{b} \\
b & \overline{a}
\end{pmatrix}\begin{pmatrix}
c & -\overline{d} \\
d & \overline{c}
\end{pmatrix} = \begin{pmatrix}
ac-\overline{b}d & -a\overline{d}-\overline{bc} \\
bc+\overline{a}d & \overline{ac} - b\overline{d}
\end{pmatrix} &=\\ =\begin{pmatrix}
ac-b\overline{d} & -\overline{b}c-\overline{da} \\
ad+\overline{c}b & \overline{ac}-\overline{b}d
\end{pmatrix} &= \begin{pmatrix}
c & -\overline{d} \\
d & \overline{c}
\end{pmatrix}\begin{pmatrix}
a & -\overline{b} \\
b & \overline{a}
\end{pmatrix},\end{aligned}
\]

asÃ­ que $\overline{b}d = b\overline{d}$ y $\overline{a}d+bc = ad + \overline{c}d$. Tomando $c=0$ y $d=1$ llegamos a que
$a,b \in \mathbb{R}$; y tomando $d = i$, que $b=0$. AsÃ­, las Ãºnicas matrices en el centro
serÃ¡n las que representan a los reales, de la forma

\[\left\{\begin{pmatrix}\lambda & 0 \\ 0 & \lambda
\end{pmatrix}\mid \lambda \in \mathbb{R}\right\}\]

***** Segundo punto
Simplemente comprobar que cada elemento tiene una inversa a derecha

\[\begin{pmatrix}
a & -\overline{b} \\ b & \overline{a}
\end{pmatrix}\begin{pmatrix}
\frac{\overline{a}}{a\overline{a}+b\overline{b}} & 
\frac{\overline{b}}{a\overline{a}+b\overline{b}} \\ 
\frac{-b}{a\overline{a}+b\overline{b}} & 
\frac{a}{a\overline{a}+b\overline{b}}
\end{pmatrix} = \begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}\]

usando que si $a\overline{a}+b\overline{b} = 0$, es porque $a=b=0$ y el elemento es nulo.

***** Tercer punto
Sabiendo que los complejos son un espacio vectorial de dimensiÃ³n $2$
con base $\{1,i\}$, podemos escribir los elementos de $\mathbb{H}$ como

\[\begin{pmatrix}
a+bi & -c+di \\
c+di & a-bi
\end{pmatrix} = a\begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}+
b\begin{pmatrix}
i & 0 \\ 0 & -i
\end{pmatrix}+
c\begin{pmatrix}
0 & 1 \\ -1 & 0
\end{pmatrix}+
d\begin{pmatrix}
0 & i \\ i & 0
\end{pmatrix}\]

que trivialmente es una descomposiciÃ³n Ãºnica por independencia lineal.

***** Cuarto punto
Podemos comprobar trivialmente los cÃ¡lculos.

**** Ejercicio 8
#+begin_statement
Dado un A-mÃ³dulo $V$, demostrar que $\mathrm{Ann}_A(V) = \left\{ a \in A \mid av=0\; \forall v\in V \right\}$ es un
ideal de $A$. Dotar a $V$ de estructura de $A/\mathrm{Ann}_{A}(V)\text{-mÃ³dulo}$ fiel (es decir,
la representaciÃ³n correspondiente es fiel).
#+end_statement

Si $a \in \mathrm{Ann}_A(V)$ tenemos que para cualquier $r \in A$ y $v \in V$, $rav = r0 = 0$ y 
$(ar)v = a(rv) = 0$. Podemos dotar a $V$ de estructura de mÃ³dulo en el cociente
como

\[
\left( a+ \mathrm{Ann}(V) \right)v = av.
\]

Esta representaciÃ³n es fiel porque si tenemos $\forall v\in V\colon av = bv$, entonces
se tiene $a-b \in \mathrm{Ann}(V)$.

**** Ejercicio 9
#+begin_statement
Dar una demostraciÃ³n del Lema 1.25.
#+end_statement

***** Primer punto
Es claro que el menor submÃ³dulo que contenga a $N_1 \cup \dots \cup N_m$ debe contener
en particular a todas las sumas y por tanto

\[
\left\{ n_1+\dots+n_m \mid n_i \in N_i \right\} \subseteq N_1 + \dots + N_m.
\]

Si ademÃ¡s probamos que es un submÃ³dulo, tendremos que debe ser el menor
conteniendo a la uniÃ³n. Es cerrado para la suma por tenerse

\[
(n_1+\dots+n_m)+(n'_1+\dots+n'_m) =
(n_1+n'_1)+\dots+(n_m+n_m')
\]

y cerrado para el producto por elementos del anillo por tenerse

\[
a(n_1+\dots+n_m) = an_1+\dots+an_m.
\]

***** Segundo punto
De la misma forma, es claro que el menor submÃ³dulo conteniendo a $X$ debe
contener al menor mÃ³dulo conteniendo a cada uno de sus elementos, y por
tanto al menor submÃ³dulo conteniendo a todos esos submÃ³dulos. Sabemos
entonces que

\[
RX \supseteq Rm_1 + \dots + Rm_n.
\]

Pero ademÃ¡s, una suma de mÃ³dulos es un submÃ³dulo, asÃ­ que este es el menor
submÃ³dulo que contiene a $X$.

**** Ejercicio 10
#+begin_statement
Dados $A\text{-mÃ³dulos}$ por la izquierda $M,N$ y una aplicaciÃ³n $f\colon M \to N$,
demostrar que $f$ es homomorfismo de $A\text{-mÃ³dulos}$ si, y sÃ³lo si,
$f(am+a'm') = af(m) + a'f(m')$ para todo $a,a'\in A;\; m,m'\in M$.
#+end_statement

***** Si es homomorfismo de mÃ³dulos cumple la regla
Aplicando primero linealidad y luego dos veces la condiciÃ³n de homomorfismo
de mÃ³dulos tenemos

\[
f(am+a'm') = f(am)+f(a'm') = af(m) + a'f(m').
\]

***** Si cumple la regla, es homomorfismo de mÃ³dulos
La linealidad la comprobamos tomando $a=a'=1$, la unidad del Ã¡lgebra,

\[
f(m+m') = f(1m+1m') = 1f(m) + 1f(m') = f(m) + f(m').
\]

Y la condiciÃ³n de homomorfismo de mÃ³dulos se comprueba tomando $a' = m'=0$,

\[
f(am) = f(am+0) = af(m) + 0f(0) = af(m).
\]

**** Ejercicio 11
#+begin_statement
Demostrar que un conjunto de generadores $\{m_i \mid i \in I\}$ de un mÃ³dulo $_RM$ es una
base si, y sÃ³lo si, la igualdad $\sum_{i\in I}r_im_i = 0$ para $r_i \in R$ implica $r_i = 0$ para
todo $i \in I$.
#+end_statement

***** Si es una base, se tiene la condiciÃ³n
Si tenemos una base $\{m_i \mid i \in I\}$, en particular el $0$ se escribe de forma 
Ãºnica como $0 = \sum_{i\in I} 0m_i$. AsÃ­, cualquier otra forma de escribir $0 = \sum_{i\in I} r_i m_i$
nos da $r_i = 0$.

***** Si se tiene la condiciÃ³n, es una base
Si tenemos la condiciÃ³n y tenemos dos formas distintas de escribir un
elemento, tendrÃ­amos en particular

\[
\sum_{i\in I} r_im_i = m = \sum_{i\in I}s_im_i
\quad\text{ y }\quad
\sum_{i \in I} (r_i-s_i)m_i = 0.
\]

Lo que nos llevarÃ­a a $r_i = s_i$ para cumplir la condiciÃ³n.

**** Ejercicio 12
#+begin_statement
Sea $\theta \in \mathbb{R}$ y $T_\theta : \mathbb{R}^2\to\mathbb{R}^2$ el endomorfismo que gira los vectores un Ã¡ngulo $\theta$
en sentido contrario de las agujas del reloj. Consideremos la correspondiente
estructura de $\mathbb{R}[X]$ mÃ³dulo definida por $T_\theta$ sobre $\mathbb{R}^2$. Discutir para quÃ© valores
de $\theta$ es este mÃ³dulo simple.
#+end_statement

Supongamos un $v \in M$, subespacio vectorial. Como $Tv \in M$, tenemos dos casos,

  * si $Tv,v$ son linealmente dependientes, se tiene $Tv = \lambda v$ y por tanto debe
    tenerse $\theta = k\pi$ para algÃºn $k \in \mathbb{Z}$. El mÃ³dulo no serÃ­a simple, ya que se
    tendrÃ­a $T^2v = v$.
  * si no son linealmente dependientes, se tiene un espacio de dimensiÃ³n al
    menos $2$, que debe ser por tanto el total. El mÃ³dulo serÃ­a simple.

Es decir, salvo en el caso $\theta = k\pi$, el mÃ³dulo es simple.

**** Ejercicio 13
#+begin_statement
Sea $M$ un $A\text{-mÃ³dulo}$. Demostrar que $M$ es simple si, y sÃ³lo si, $M = Am$ para
todo $0 \neq m \in M$.
#+end_statement

***** Supongamos M simple
Entonces $Am$ es un submÃ³dulo no nulo, que debe ser por tanto $M$.

***** Supongamos la caracterizaciÃ³n
Sea un submÃ³dulo de $M$ que contiene a algÃºn elemento no nulo $m$. Por
las propiedades de submÃ³dulo, debe contener tambiÃ©n a todo $Am = M$.
AsÃ­, no existen submÃ³dulos propios.

**** Ejercicio 14 (*)
#+begin_statement
Consideramos $T\colon \mathbb{R}^3 \to \mathbb{R}^3$ una aplicaciÃ³n lineal, y la estructura de $\mathbb{R}[X]\text{-mÃ³dulo}$
correspondiente sobre $\mathbb{R}^3$. Discutir los posibles valores de la longitud de
$\mathbb{R}^3$ como $\mathbb{R}[X]\text{-mÃ³dulo}$. Poner un ejemplo de $T$ para el que se alcance cada longitud.
#+end_statement

La longitud debe ser como mÃ¡ximo $3$, su dimensiÃ³n como espacio vectorial
sobre $\mathbb{R}$. Estudiaremos los casos posibles.

*Longitud 1.*
Probaremos que no puede tenerse una cadena de longitud $1$; es decir, que
$\mathbb{R}^3$ sea simple. Toda aplicaciÃ³n lineal $T$ nos da una ecuaciÃ³n polinÃ³mica

\[ | T - \lambda I | = 0
\]

de grado $3$ con coeficientes reales, que debe tener al menos una soluciÃ³n
en los reales. Esto nos da un vector propio y por tanto un subespacio que
queda fijo por la acciÃ³n de $T$; es decir, un submÃ³dulo.

*Longitud 2.*
Tomamos $T$ la aplicaciÃ³n que gira un plano mientras deja fija la recta
ortogonal a Ã©l; especÃ­ficamente,

\[T = \begin{pmatrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1
\end{pmatrix}
\]

nos da el subespacio $\left\langle e_3 \right\rangle$ fijo bajo su acciÃ³n. Por otro lado, el
submÃ³dulo $\left\langle e_1,e_2 \right\rangle \cong \mathbb{R}^3/ \left\langle e_3 \right\rangle$ es simple; la rotaciÃ³n no dejarÃ¡ ninguna
recta fija, no hay vectores propios. AsÃ­, tenemos una serie de
composiciÃ³n

\[
0 \subset \left\langle e_1,e_2 \right\rangle \subset \mathbb{R}^3.
\]

*Longitud 3.*
Simplemente tomando la identidad y retirando a cada paso una dimensiÃ³n
del espacio vectorial

\[
0 \subset \left\langle e_1 \right\rangle
\subset \left\langle e_1,e_2 \right\rangle
\subset \left\langle e_1,e_2,e_3 \right\rangle.
\]

**** Ejercicio 15
#+begin_statement
Sea $\mathbb{P}_{n}$ el espacio vectorial real de las funciones polinÃ³micas en una variable
de grado menor o igual que $n$. Sea $T\colon \mathbb{P}_n \to \mathbb{P}_n$ la aplicaciÃ³n lineal que asigna
a cada polinomio su derivada. Calcular una serie de composiciÃ³n de $\mathbb{P}_n$ visto
como $\mathbb{R}[X]\text{-mÃ³dulo}$ via $T$.
#+end_statement

Tenemos una base del espacio vectorial dada por $\left\{ 1,x,\dots,x^n \right\}$, podemos generar
una serie de composiciÃ³n donde vemos que cada uno es un submÃ³dulo cerrado para
la derivaciÃ³n y que cada cociente es simple por ser de dimensiÃ³n $1$ en los reales
como

\[
0 \subset 
\left\langle 1 \right\rangle \subset
\left\langle 1,x \right\rangle \subset 
\dots \subset
\left\langle 1,x,\dots,x^n \right\rangle.
\]

**** Ejercicio 16 (**)
#+begin_statement
En las condiciones del Ejercicio 15, calcular todos los $\mathbb{R}[X]\text{-submÃ³dulos}$ de $\mathbb{P}_n$.
#+end_statement

Probaremos que los Ãºnicos submÃ³dulos de $\mathbb{P}_n$ son de la forma $\left\langle 1,x,x^2,\dots,x^k \right\rangle$.
Si tomamos un polinomio en un submÃ³dulo podemos suponerlo mÃ³nico por estar
en un cuerpo; y como ademÃ¡s es de caracterÃ­stica $0$, sus derivadas serÃ¡n cada
una de un grado menor. AsÃ­, dado $p = x^k+ \dots +a_1x + a_0 \in \mathbb{P}_n$ tendremos

\[\begin{aligned}
p =& x^k+& a_{k-1}x^{k-1} +& \dots &+& a_1x &+& a_0 \\
\partial p =&  &kx^{k-1}+& \dots &+& 2a_2x &+ &a_1 \\
\dots \\
\partial^n p =&  && && && k! \\
\end{aligned}\]

Lo que constituye una base del espacio de polinomios de dimensiÃ³n $k$ 
equivalente a $\left\langle 1,x,x^2,\dots,x^k \right\rangle$ gracias a que estamos en un cuerpo. AsÃ­,
cada submÃ³dulo serÃ¡ el submÃ³dulo de los polinomios de grado menor o igual
a $k$ para $k$ el grado de su polinomio de mayor grado.

**** Ejercicio 17 (**)
#+begin_statement
Supongamos $T \colon V \to V$ un endomorfismo $K\text{-lineal}$, donde $V$ es un espacio vectorial
de dimensiÃ³n finita que consideramos, como de costumbre, como $K[X]\text{-mÃ³dulo}$.
Supongamos que el polinomio mÃ­nimo $m(X)$ de $T$ es irreducible en $K[X]$ (ver
Ejemplo 14 para el concepto de polinomio mÃ­nimo). Demostrar que existen 
$K[X]\text{-submÃ³dulos}$ simples $V_1,\dots,V_t$ de $V$ tal que $V = V_1\oplus \dots \oplus V_{t}$ como
$K[X]\text{-mÃ³dulo}$.
#+end_statement

Como $m(X)$ es irreducible y estamos en un DIP el ideal que genera,
$(m(X))$, es maximal, y por tanto el cociente

\[
k \cong \frac{K[X]}{(m(X))}
\]

es un cuerpo. Y $V$ es un $k\text{-espacio vectorial}$ ya que por el primer teorema de 
isomorfÃ­a tenemos que $K[X] \to \mathrm{End}_K(V)$ descompone en una proyecciÃ³n y una
inyecciÃ³n

\[\begin{tikzcd}
K[X] \rar[two heads] & 
\displaystyle\frac{K[X]}{(m(X))} \rar[hook] &
\mathrm{End}_K(V).
\end{tikzcd}\]

Ahora, si $V$ tiene una base finita como $K\text{-espacio vectorial}$, sabiendo que $K \subseteq k$,
tenemos que $V$ tiene un sistema de generadores finito como $k\text{-espacio vectorial}$.
Por el Corolario 1.45 existe entonces un subconjunto de ese sistema de 
generadores tal que

\[
V = \bigoplus_{j \in J} kv_j = V_1 \oplus \dots \oplus V_t.
\]

NÃ³tese que cada uno de ellos es un submÃ³dulo simple por ser isomorfos a $k$.

**** Ejercicio 18 (**)
#+begin_statement
En las condiciones del Ejercicio 17, demostrar que el polinomio caracterÃ­stico
de $T$ es $m(X)^t$.
#+end_statement

Por Cayley-Hamilton, sabemos que $T$ cumple su ecuaciÃ³n caracterÃ­stica, y por
tanto, $m(X)$ divide a su polinomio caracterÃ­stico. 

Por otro lado, supongamos que tenemos un factor irreducible $p$ del
polinomio caracterÃ­stico; este tendrÃ¡ alguna raÃ­z $\lambda$ en la clausura
algebraica de $K$. Es decir, tendremos un vector propio con coeficientes
en $\overline{K}$ cumpliendo $Tv = \lambda v$.

Si aplicamos el polinomio mÃ­nimo evaluado en $T$ a ese vector tendremos

\[
0 = m(T)v = m(\lambda)v,
\]

asÃ­ que $\lambda$ es una raÃ­z de $m$ en la clausura algebraica. Ahora, como el
polinomio irreducible de $\lambda$ en $K$ sigue siendo $p$, concluimos que $p \mid m$.

En general, hemos demostrado que todo factor irreducible del polinomio
caracterÃ­stico divide al polinomio mÃ­nimo. Cuando ademÃ¡s el polinomio
mÃ­nimo es irreducible, se tiene que el polinomio caracterÃ­stico debe
ser de la forma $m(X)^s$.

Ahora comprobaremos que $s=t$. En efecto, tenemos que si $\mathrm{gr}(m) = n$,
entonces, por construcciÃ³n, $\mathrm{dim}_Kk=n$ y por ser $V_i \cong k$, tenemos
$\mathrm{dim}_{K}(V) = tn$; que debe ser el grado del polinomio caracterÃ­stico,
a la vez que debe ser $sn$.

**** Ejercicio 19
#+begin_statement
Demostrar que si $R$ es un dominio de integridad conmutativo, entonces $R$ no
tiene idempotentes no triviales.
#+end_statement

Si $e^2=e$, entonces $e(e-1) = 0$; lo que, en un dominio de integridad implica
que $e = 0$ Ã³ $e-1 = 0$.

NÃ³tese que no hemos usado la conmutatividad.

**** Ejercicio 20
#+begin_statement
Dar un CCIO para $R = M_n(k)$.
#+end_statement

Sean $E_{ii}$ las matrices nulas excepto por un $1$ en la entrada $i,i$. Se comprueba
trivialmente que $E_{ii}E_{jj} = 0$ para cualesquiera $i \neq j$, y que $E_{ii}^2 = 1$. Forman
ademÃ¡s un conjunto completo por tenerse:

\[
I = E_{11} + E_{22} + \dots + E_{nn}
\]
**** TODO Ejercicio 21
#+begin_statement
Comprobar las afirmaciones realizadas en el Ejemplo 17.
#+end_statement
**** Ejercicio 22
#+begin_statement
Sea $\left\{ e_1,\dots,e_n \right\}$ un ccio para $R$. Demostrar que los idempotentes $e_1,\dots,e_n$ son
centrales si, y sÃ³lo si, $e_iRe_j = 0$ para todo $i \neq j$.
#+end_statement

***** Si son centrales, cumplen la condiciÃ³n
Si son centrales, se tiene que, para cualquier $r \in R$,

\[
e_ire_j = re_ie_j = 0
\]

por ortogonalidad.

***** Si cumplen la condiciÃ³n, son centrales
Sabiendo que cumplen que $e_ire_j = 0$ para cualquier $r \in R$, tenemos

\[
e_ir = e_ir\left(\sum_j e_j\right) = \sum_j e_ire_j = e_ire_i = \sum_j e_jre_i = re_i
\]

por ser completos.

**** Ejercicio 23 (*)
#+begin_statement
Sean $M$ y $N$ mÃ³dulos semisimples con descomposiciones como sumas directas
de submÃ³dulos simples $M = S_1 \oplus \dots \oplus S_t$ y $N = T_1\oplus \dots \oplus T_s$. Supongamos que
$S_i$ no es isomorfo a $T_j$ para todo $i = 1,\dots,t$, $j = 1,\dots,s$. Demostrar que
todo homomorfismo de mÃ³dulos de $M$ a $N$ es cero.
#+end_statement

Desde el ejemplo 17, sabemos que, en los endomorfismos de un mÃ³dulo suma
directa, la composiciÃ³n de inclusiÃ³n y proyecciÃ³n en las distintas componentes
nos da un ccio. Vamos a llamar $q_i \colon M \to M$ al endomorfismo que proyecta e
incluye en la componente $i\text{-Ã©sima}$; y vamos a llamar $p_j \colon N\to N$ al que hace
lo mismo en la componente $j\text{-Ã©sima}$ de $N$. Sabemos que

\[
q_1 + \dots + q_t = \mathrm{id}
\quad\text{ y que }\quad
p_1 + \dots + p_s = \mathrm{id}.
\]

Ahora, calculamos que

\[\begin{aligned}
f &= (q_1+\dots+q_t) \circ f \circ (p_1+\dots+p_s) \\
  &= \sum_{i=1}^t\sum_{j=1}^s q_i\circ f\circ p_j = 0,
\end{aligned}\]

ya que $q_i\circ f\circ p_j\colon S_i\to T_i$ debe ser nulo o isomorfismo por el Lema de Schur
y hemos supuesto que no es isomorfismo.

# NÃ³tese que no es exactamente esto, sino que hay que partir q en sus
# componentes para igualar a cero.

**** TODO Ejercicio 24
#+begin_statement
Sea $M$ un mÃ³dulo semisimple de dimensiÃ³n finita con estructura
$\left( n_1,\Sigma_1 \right),\dots,(n_t,\Sigma_t)$. Si $N$ es un submÃ³dulo de $M$, demostrar que su estructura
es $\left( m_1,\Sigma_1 \right),\dots,(m_t,\Sigma_t)$ para ciertos $m_j \leq n_j$ (admitimos que $m_j=0$ significa
que $\Sigma_j$ no aparece en la estructura de $M$).
#+end_statement
**** TODO Ejercicio 25
#+begin_statement
Establecer un enunciado anÃ¡logo al del Ejercicio 24 para cada cociente de $M$.
#+end_statement
**** TODO Ejercicio 26
#+begin_statement
Dada una $K\text{-Ã¡lgebra}$ $A$, demostrar que la aplicaciÃ³n $\rho\colon A \to \mathrm{End}(A)^{op}$ definida
por $\rho(a)(a') = a'a$ es un isomorfismo de $K\text{-Ã¡lgebras}$.
#+end_statement

**** TODO Ejercicio 27 (*)
#+begin_statement
Sea $B$ un Ã¡lgebra. Demostrar que la aplicaciÃ³n que asigna a cada matriz
su traspuesta da un isomorfismo de Ã¡lgebras $M_n(B)^{op} \cong M_n(B^{op})$.
#+end_statement

**** TODO Ejercicio 28
#+begin_statement
Sea $\varphi\colon R \to S$ un isomorfismo de $K\text{-Ã¡lgebras}$, e $I,J$ ideales por la izquierda
de $R$. Demostrar que $\varphi(I),\varphi(J)$ son ideales por la izquierda de $S$ y que dado
cualquier homomorfismo de $R\text{-mÃ³dulos}$ $f \colon I \to J$, la aplicaciÃ³n $\widehat f\colon \varphi(I) \to \varphi(J)$
definida por $\widehat f(y) = \varphi f \varphi^{-1}(y)$ para $y \in \varphi(I)$ es un homomorfismo de $S\text{-mÃ³dulos}$.
#+end_statement

**** TODO Ejercicio 29
#+begin_statement
Sean $R_1,\dots,R_n$ $K\text{-Ã¡lgebras}$ y $R = R_1\times \dots \times R_n$. Los ideales por la izquierda
de $R$ son de la forma $I_1\times \dots\times I_n$, con $I_i$ ideal por la izquierda de $R_i$ para
$i = 1,\dots,n$. AnÃ¡loga descripciÃ³n tienen los ideales bilÃ¡teros de $R$.
#+end_statement

**** TODO Ejercicio 30 (*)
#+begin_statement
Sea $A$ un Ã¡lgebra simple finito-dimensional. Demostrar que $R = \mathrm{M}(A)$ es un
Ã¡lgebra simple de dimensiÃ³n finita. Demostrar que asimismo que si $\Sigma$ es un
$A\text{-mÃ³dulo}$ simple y $M$ es un $R\text{-mÃ³dulo}$ simple, entonces $\mathrm{End}(\Sigma)$ y $\mathrm{End}(M)$
son Ã¡lgebras isomorfas.
#+end_statement

**** Ejercicio 31 (*)                                                                                       :export:
#+begin_statement
Demostrar que $T_q \in SO(V)$ para todo cuaternio $q$ de norma $1$.
#+end_statement

Sabiendo que los cuaternios se expresan como $\mathbb{H} = \mathbb{R} \oplus V$ escribimos $q = a + bu$, 
donde $u \in V$ y $\|u\| = 1$, lo que nos da $u^2 = -u(-u) = -1$. Como $q$ es de norma $1$ 
debe cumplir

\[
1 = qq^{\ast} = a^2 - ub^2 = a^2 + b^2,
\]

luego puede expresarse como $q = \cos \theta + \sin\theta u$ para algÃºn $\theta \in \mathbb{R}$. Pero entonces
tenemos un cuaternio $w = \cos(\theta/2) + \sin(\theta/2) u$ que al elevarlo al cuadrado
nos da $q$, ya que

\[ w^2 =
\left( \cos \frac{\theta}{2} + \sin \frac{\theta}{2}u \right)^2 =
\cos^2 \frac{\theta}{2} - \sin^2 \frac{\theta}{2} + 
2 \sin \frac{\theta}{2}\cos \frac{\theta}{2}u =
\cos \theta + \sin \theta u = q.
\]

Finalmente, como $T_q = T_w \circ T_w$ siendo isometrÃ­as, tenemos que
debe cumplirse que $|T_q| = |T_w|^2 = 1$ y por tanto, $T_q \in SO(V)$.

**** Ejercicio 32 (*)                                                                                       :export:
#+begin_statement
Calcular explÃ­citamente una representaciÃ³n real no trivial de grado $2$ del
grupo de permutaciones $S_3$.
#+end_statement

Partimos de la idea de que $D_6 = S_{3}$, asÃ­ que cada elemento representarÃ¡
una simetrÃ­a del triÃ¡ngulo equilÃ¡tero con centro en el origen y un
vÃ©rtice en $(1\ 0)$.

En $\mathbb{R}^2$, llamamos $r,s,t$ a las rectas de Ã¡ngulos $0,2\pi/3,4\pi/3$. Consideraremos
las trasposiciones como simetrÃ­as respecto de estas rectas, y las permutaciones
de tres elementos serÃ¡n rotaciones compuestas de dos simetrÃ­as. ExplÃ­citamente,
las trasposiciones de dos elementos son

\[
(2\ 3) \mapsto \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix},
\quad
(1\ 2) \mapsto \begin{pmatrix} -1/2 & \sqrt{3}/2 \\ \sqrt{3}/2 & 1/2 \end{pmatrix},
\quad
(1\ 3) \mapsto \begin{pmatrix} -1/2 & -\sqrt{3}/2 \\ -\sqrt{3}/2 & 1/2 \end{pmatrix},
\]

y las rotaciones son

\[
(1\ 2\ 3) \mapsto 
\begin{pmatrix} -1/2 & -\sqrt{3}/2 \\ \sqrt{3}/2 & -1/2 \end{pmatrix},
\quad
(1\ 3\ 2) \mapsto
\begin{pmatrix} -1/2 & \sqrt{3}/2 \\ -\sqrt{3}/2 & -1/2 \end{pmatrix}.
\]

Y podemos comprobar sobre ellas que cumplen la tabla de multiplicaciÃ³n
del grupo.

**** Ejercicio 33
#+begin_statement
Comprobar que la multiplicaciÃ³n definida sobre $KG$ es asociativa. Su elemento
neutro es $1e$, donde $e$ es el elemento neutro de $G$.
#+end_statement

Tenemos por bilinealidad

\[
\left( \sum_{g \in G} \lambda_gg \right)
\left( \sum_{h \in G} \mu_hh \sum_{k \in G} \delta_kk \right) =
\sum_{g \in G} \lambda_gg \sum_{j,k \in G} \mu_h\delta_k hk =
\sum_{g,j,k \in G} \lambda_g\mu_h\delta_k g(hk),
\]

mientras que

\[
\left( \sum_{g \in G} \lambda_gg \sum_{h \in G} \mu_hh \right)
\left( \sum_{k \in G} \delta_kk \right) =
\sum_{g \in G} \lambda_g\mu_hgh \sum_{j,k \in G} \delta_kk =
\sum_{g,j,k \in G} \lambda_g\mu_h\delta_k (gh)k,
\]

que son iguales por asociatividad del producto de grupo. Hemos demostrado
en general que cualquier bilineal que extienda un operador binario sobre
los vectores de la base es asociativo.

**** TODO [#A] Ejercicio 34
#+begin_statement
Calcular todos los subespacios invariantes para la representaciÃ³n de $Q_8$ 
del Ejemplo 20.
#+end_statement

**** TODO [#A] Ejercicio 35
#+begin_statement
Calcular todos los subespacios invariantes para la representaciÃ³n de $S_3$
del Ejemplo 32.
#+end_statement

**** TODO Ejercicio 36
**** TODO Ejercicio 37
**** TODO Ejercicio 38
**** TODO Ejercicio 39
**** Ejercicio 40 (*)                                                                                       :export:
#+begin_statement
Calcular razonadamente la tabla de caracteres de $Q_8$.
#+end_statement

Puede comprobarse multiplicando que el grupo tiene cinco clases de conjugaciÃ³n,
con representantes dados por $1,-1,i,j,k$, por lo que tiene cinco representaciones
irreducibles complejas. Sabemos que los cuadrados de las dimensiones de esas 
representaciones deben sumar el orden del grupo, es decir 

\[ 8 = n_1^2 + n_2^2 + n_3^2 + n_4^2 + n_5^2,\]

y como no pueden tenerse dos $n_i > 1$, la Ãºnica soluciÃ³n posible es que tengan
dimensiones $1,1,1,1,2$, determinando el caracter de la identidad en cada una
de las representaciones.

 * El /primer carÃ¡cter/ serÃ¡ el dado por la representaciÃ³n irreducible trivial
   que envÃ­a cada elemento del grupo a $1 \in \mathbb{C}$.
 * El /Ãºltimo carÃ¡cter/, de dimensiÃ³n $2$ es el dado por la representaciÃ³n de los
   cuaternios como matrices complejas que conocemos del Ejercicio 7. Es ademÃ¡s
   irreducible por tenerse $(\chi_5,\chi_5) = (2^2+(-2)^2)/8 = 1$.
 * Para cualquiera de $i,j,k$, existe un subgrupo normal de $Q_8$ generado por
   el elemento con $4$ elementos, como por ejemplo $\left\{ 1,-1,i,-i \right\}$. Si llamamos a
   este grupo $A$, se tiene que $Q_8/A \cong \mathbb{Z}_2$, el Ãºnico grupo de cardinalidad $2$.
   Este grupo tiene una representaciÃ³n irreducible en $\mathbb{C}$ como $1,-1$, y por
   el Ejercicio 38, sabemos que la composiciÃ³n de representaciÃ³n irreducible
   con una proyecciÃ³n a un cociente por un subgrupo normal es una 
   representaciÃ³n irreducible, asÃ­ que lo es

   \[
   \rho_{A}\colon Q_8 \overset{\pi}\longrightarrow Q_8 / A \cong \mathbb{Z}_2 
   \longrightarrow \mathbb{C}.
   \]

   Esta representaciÃ³n la podemos repetir para los $A$ generados por $i,j,k$,
   dÃ¡ndonos las tres representaciones restantes con caracteres $\chi_2,\chi_3,\chi_4$ 
   respectivamente. NÃ³tese que cada una de ellas
   envÃ­a al elemento $1$ a los elementos del grupo y a $-1$ a los elementos fuera
   del grupo.

Tenemos finalmente la tabla de caracteres irreducibles

\[\begin{tabular}{c|ccccc}
    & 1 &  1 & 2 & 2 & 2 \\
Q_8 & 1 & -1 & i & j & k \\
\hline
\chi_1 & 1 &  1 &  1 &  1 &  1 \\
\chi_2 & 1 &  1 & 1  & -1  & -1   \\
\chi_3 & 1 &  1  & -1   & 1   &  -1  \\
\chi_4 & 1 &  1  &  -1  & -1   & 1   \\
\chi_5 & 2 & -2 &  0 &  0 &  0 \\
\end{tabular}\]

**** TODO Ejercicio 41 (*)
#+begin_statement
Calcular razonadamente la tabla de caracteres del grupo dihÃ©drico $D_4$.
#+end_statement

Tomamos la presentaciÃ³n del grupo dihÃ©drico

\[
\left\langle r,s \mid r^4=s^2=e, sr = r^{-1}s \right\rangle
\]

y comprobamos multiplicando que tiene $8$ elementos y $5$ clases de conjugaciÃ³n
con representantes $e,r,s,r^2,sr$, por lo que tiene cinco representaciones
irreducibles complejas. Sabemos que los cuadrados de las dimensiones de esas 
representaciones deben sumar el orden del grupo, es decir 

\[ 8 = n_1^2 + n_2^2 + n_3^2 + n_4^2 + n_5^2,\]

y como no pueden tenerse dos $n_i > 1$, la Ãºnica soluciÃ³n posible es que tengan
dimensiones $1,1,1,1,2$, determinando el caracter de la identidad en cada una
de las representaciones.

 * El /primer carÃ¡cter/ serÃ¡ el dado por la representaciÃ³n irreducible trivial
   que envÃ­a cada elemento del grupo a $1 \in \mathbb{C}$.

 * Tenemos tres subgrupos normales generados por $\left\langle e,r^2,s \right\rangle$, $\left\langle e,r^2,r \right\rangle$ y $\left\langle e,r^2,sr \right\rangle$,
   cada uno de ellos con cuatro elementos. Si llamamos a cualquiera de ellos
   $A$, tenemos la representaciÃ³n irreducible dada por la composiciÃ³n de la
   proyecciÃ³n al cociente con la representaciÃ³n irreducible de $\mathbb{Z}_2$ en los
   complejos

   \[
   \rho_{A}\colon D_4 \overset{\pi}\longrightarrow D_4 / A \cong \mathbb{Z}_2 
   \longrightarrow \mathbb{C}
   \]

   que es irreducible en virtud del Ejercicio 38. Esto nos da las
   representaciones con caracteres $\chi_2,\chi_3,\chi_4$ que envÃ­an cada elemento del
   grupo $A$ al $1$ y cada elemento fuera del grupo al $-1$.

 * El Ãºltimo carÃ¡cter proviene de la representaciÃ³n matricial

   \[
   r \mapsto \begin{pmatrix}
   0 & -1 \\ 1 & 0
   \end{pmatrix} 
   \qquad
   s \mapsto \begin{pmatrix}
   1 & 0 \\ 0 & -1
   \end{pmatrix},
   \]
   
   # la irreducibilidad se debe comprobar (Ï,Ï)=1
   que podemos comprobar que cumple las relaciones de la presentaciÃ³n.
   Por el caracter que define, que no es suma de otros dos caracteres 
   irreducibles, sabemos que es forzosamente la representaciÃ³n irreducible
   que nos falta.

Tenemos finalmente la tabla de caracteres irreducibles como

\[\begin{tabular}{c|ccccc}
    & 1 &  1 & 2 & 2 & 2 \\
D_4 & $e$ & $r^2$ & $s$ & $r$ & $sr$ \\
\hline
\chi_1 & 1 &  1 &  1 &  1 &  1 \\
\chi_2 & 1 &  1 & 1  & -1  & -1   \\
\chi_3 & 1 &  1  & -1   & 1   &  -1  \\
\chi_4 & 1 &  1  &  -1  & -1   & 1   \\
\chi_5 & 2 & -2 &  0 &  0 &  0 \\
\end{tabular}\]

NÃ³tese que es la misma tabla de caracteres que $Q_8$.

**** Ejercicio 42 (**)                                                                                      :export:
#+begin_statement
Calcular razonadamente la tabla de caracteres del grupo dihÃ©drico $D_n$, 
para $n \geq 2$.
#+end_statement

Tomamos la presentaciÃ³n del grupo dihÃ©drico

\[
\left\langle r,s \mid r^n=s^2=e, sr = r^{-1}s \right\rangle,
\]

y sabemos que es un grupo de $2n$ elementos. 

***** Clases de conjugaciÃ³n del caso impar
Cuando $n$ sea impar, sus clases de conjugaciÃ³n, que obtenemos
conjugando con los generadores, serÃ¡n las siguientes:

 * La clase trivial $\left\{ 1 \right\}$.
 * Las clases de la forma $\left\{ r^i,r^{-i} \right\}$, donde $0 < i < n$, que se puede comprobar
   que permanecen invariantes por conjugaciÃ³n de los generadores. Tenemos
   $(n-1)/2$ clases de este tipo, cada una con dos elementos ya que por
   ser $n$ impar, $r^i\neq r^{-i}$.
 * La clase $\left\{ sr^i \mid 0 \leq i < n \right\}$, que se genera desde $s$ usando que $r^isr^{-i} = sr^{-2i}$ 
   y que $n$ es impar. Es una clase de $n$ elementos.

Sumando las cardinalidades de todas ellas, observamos que no hay mÃ¡s, ya
que

\[
2n = 1 + 2 \frac{n-1}{2} + n.
\]

Tenemos $(n+3)/2$ clases de conjugaciÃ³n, luego tendremos $(n+3)/2$
representaciones irreducibles complejas.

***** Representaciones en el caso impar
Podemos considerar los caracteres y representaciones siguientes:

 * el caracter trivial dado por la *representaciÃ³n irreducible trivial*
   que envÃ­a cada elemento del grupo a $1 \in \mathbb{C}$.

 * el grupo generado por $\left\langle r \right\rangle$ es normal y tenemos $D_n/\left\langle r \right\rangle \cong \mathbb{Z}_2$, luego una
   representaciÃ³n dada por la proyecciÃ³n y la representaciÃ³n irreducible
   de $\mathbb{Z}_2$ en los complejos es irreducible. La llamamos $\chi_2$.

 * si interpretamos los grupos dihÃ©dricos como grupos de simetrÃ­as de
   los polÃ­gonos, podemos escribir representaciones bidimensionales
   que toman $r$ como cualquiera de las rotaciones de Ã¡ngulos $2\pi k/n$ en
   los complejos y $s$ como la simetrÃ­a, es decir,

   \[
   r \mapsto \begin{pmatrix}
   e^{2\pi i k/n} & 0 \\
   0 & e^{-2\pi i k/n} \\
   \end{pmatrix}, \quad
   s \mapsto \begin{pmatrix}
   0 & 1 \\
   1 & 0 \\
   \end{pmatrix}.\]
   
   Comprobaremos que para $k>0$ todas ellas son irreducibles. Los
   espacios que deja invariantes la primera son claramente $(1\ 0)$ y
   $(0\ 1)$ con dos valores propios $e^{2\pi i k/n} \neq e^{-2\pi i k/n}$
   (aquÃ­ usamos $n$ impar), pero no son invariantes bajo simetrÃ­as,
   por lo que no existen subespacios invariantes y la representaciÃ³n
   es irreducible. A sus caracteres los llamamos $\chi_{k+2}$.

Dentro de las Ãºltimas, existirÃ¡n algunas que serÃ¡n equivalentes. Notamos
que la caracterÃ­stica de $r$ bajo la representaciÃ³n dada por $k$ es $2\cos(2\pi k/n)$,
por lo que cada $k$ entre $1, \dots, (n-1)/2$ da una caracterÃ­stica distinta y por
tanto una representaciÃ³n no equivalente a las demÃ¡s. Con estas tenemos en
total $(n+3)/2$ representaciones irreducibles distintas, por lo que el resto
serÃ¡n equivalentes.

***** Tabla de caracteres del caso impar
Tenemos finalmente la tabla de caracteres irreducibles como

\[
\small
\begin{tabular}{c|cccccc}
    & 1 &  2 & 2 & \dots & 2 & $n$ \\
D_n & $e$ & $r$ & $r^2$ & $\dots$ & $r^{(n-1)/2} & $s$ \\
\hline
\chi_1 & 1 &  1 &  1 &  \dots &  1 & 1 \\
\chi_2 & 1 &  1 & 1  &  \dots  & 1 & -1  \\
\chi_{2+1} &  2 & 2\cos(2\pi 1/n) &  2\cos(2 \pi 2/n)  & \dots   & 2\cos(2 \pi (n-1)/2n)  & 0 \\
\chi_{2+2} &  2 & 2\cos(2\pi 2/n) &  2\cos(2 \pi 4/n)  & \dots   & 2\cos(2 \pi 2(n-1)/2n)  & 0 \\
\dots & \dots & \dots & \dots  & \dots  & \dots & \dots \\
\chi_{2+(n-1)/2} &  2 & 2\cos(2\pi (n-1)/2n) &  2\cos(2 \pi 2(n-1)/2n)  & \dots   & 2\cos(2 \pi (n-1)(n-1)/4n)  & 0 \\
\end{tabular}\]

***** Clases de conjugaciÃ³n del caso par
Cuando $n$ es par, sus clases de conjugaciÃ³n, que obtendremos conjugando
con los generadores, serÃ¡n las siguientes:

 * La clase trivial $\left\{ 1 \right\}$.
 * La clase que forma $\left\{ r^{n/2} \right\}$.
 * Las clases de la forma $\left\{ r^i,r^{-i} \right\}$, donde $0 < i < n$ y ademÃ¡s exigimos que
   $i = n/2$ para evitar el caso $r^i = r^{-i}$. Tenemos $(n-2)/2$ clases de este
   tipo, cada una con $2$ elementos;
 * La clase $\left\{ sr^{2a} \mid 0 \leq a < n/2 \right\}$, que es cerrada para conjugaciÃ³n
   gracias a la paridad de $n$. Es una clase con $n/2$ elementos.
 * La clase $\left\{ sr^{2a+1} \mid 0 \leq a < n/2 \right\}$, de nuevo con $n/2$ elementos.
 
Sumando las cardinalidades de todas ellas, observamos que no hay mÃ¡s, ya
que

\[
2n = 1 + 1 + 2\frac{n-2}{2} + \frac{n}{2} + \frac{n}{2}.
\]

Tenemos por tanto $n/2 + 3$ clases de conjugaciÃ³n y representaciones 
irreducibles complejas distintas.

***** Representaciones en el caso par
Podemos considerar los caracteres y representaciones siguientes

 * el caracter trivial dado por la *representaciÃ³n irreducible trivial*
   que envÃ­a cada elemento del grupo a $1 \in \mathbb{C}$.

 * el grupo generado por $\left\langle r \right\rangle$ es normal y tenemos $D_n/\left\langle r \right\rangle \cong \mathbb{Z}_2$, luego una
   representaciÃ³n dada por la proyecciÃ³n y la representaciÃ³n irreducible
   de $\mathbb{Z}_2$ en los complejos es irreducible. La llamamos $\chi_2$.

 * el grupo generado por $\left\langle r^2 \right\rangle$ es normal y tenemos $D_n/\left\langle r^2 \right\rangle \cong \mathbb{Z}_2 \times \mathbb{Z}_2$, luego
   podemos buscar las representaciones irreducibles del grupo de Klein. Las
   cuatro representaciones irreducibles unidimensionales de este grupo abeliano
   podemos obtenerlas con la trivial y enviando dos de sus elementos no nulos
   al $-1$. Esto nos da los caracteres $\chi_3,\chi_4$ nuevos ademÃ¡s de los dos anteriores.

 * si interpretamos los grupos dihÃ©dricos como grupos de simetrÃ­as de
   los polÃ­gonos, podemos escribir representaciones bidimensionales
   que toman $r$ como cualquiera de las rotaciones de Ã¡ngulos $2\pi k/n$ en
   los complejos y $s$ como la simetrÃ­a, es decir,

   \[
   r \mapsto \begin{pmatrix}
   e^{2\pi i k/n} & 0 \\
   0 & e^{-2\pi i k/n} \\
   \end{pmatrix}, \quad
   s \mapsto \begin{pmatrix}
   0 & 1 \\
   1 & 0 \\
   \end{pmatrix}.\]
   
   Comprobaremos que para $0<k<n/2$ todas ellas son irreducibles. Los
   espacios que deja invariantes la primera son claramente $(1\ 0)$ y
   $(0\ 1)$ con dos valores propios $e^{2\pi i k/n} \neq e^{-2\pi i k/n}$
   (aquÃ­ usamos $k < n/2$), pero no son invariantes bajo simetrÃ­as,
   por lo que no existen subespacios invariantes y la representaciÃ³n
   es irreducible. A sus caracteres los llamamos $\chi_{k+2}$.

Con esto tenemos los $n/2+3$ caracteres irreducibles.

***** Tabla de caracteres del caso par

\[
\small
\begin{tabular}{c|cccccccc}
    & 1  & 1 & 2 & \dots & 2 & $n/2$ & $n/2$ \\
D_n & $e$   & $r^{n/2}$ & $r$ & $\dots$ & $r^{n/2-1} & $s$ & $sr$ \\
\hline
\chi_1 & 1 &1 & 1 & \dots & 1 & 1 & 1 \\
\chi_2 & 1 &1& 1 & \dots & 1 & -1 & -1 \\
\chi_3 & 1 & (-1)^{n/2}& 1 &\dots & (-1)^{n/2-1} & 1 & -1 \\
\chi_4 & 1 & (-1)^{n/2} & 1 &\dots  & (-1)^{n/2-1} & -1 & 1 \\
\chi_{2+1} &  2 & 2\cos(2 \pi 1/2) & 2\cos(2\pi 1/n) & \dots & 2\cos(2 \pi (n/2-1)/n) & 0 & 0\\
\chi_{2+2} &  2 & 2\cos(2 \pi 2/2) & 2\cos(2\pi 2/n) & \dots  & 2\cos(2 \pi 2(n/2-1)/n) & 0 & 0 \\
\dots & \dots & \dots & \dots  & \dots  & \dots & \dots & \dots \\
\end{tabular}\]

**** DONE Ejercicio 43
#+begin_statement
Sea $G$ un grupo abeliano finito, y sea $\widehat G$ el conjunto de los caracteres 
complejos irreducibles de $G$. Demostrar que el producto inducido por el
de nÃºmeros complejos dota a $\widehat G$ de estructura de grupo.
#+end_statement

Todas las representaciones irreducibles de un grupo abeliano son de 
dimensiÃ³n $1$, luego $\chi_{\rho}(g) = \rho(g)$. La irreducibilidad se tiene por ser
todas de dimensiÃ³n 1.

**** TODO Ejercicio 44 (**)
#+begin_statement
Sea $G$ un grupo abeliano finito, y $\widehat G$ el grupo definido en el Ejercicio 43.
Demostrar que existe un isomorfismo de grupos $G \cong \widehat G$.
#+end_statement

**** TODO Ejercicio 45
#+begin_statement
Sea $G$ un grupo finito y $g \in G$. Demostrar que $g$ es conjugado con $g^{-1}$ si,
y sÃ³lo si, $\chi(g) \in \mathbb{R}$ para todo carÃ¡cter complejo irreducible $\chi$ de $G$.
#+end_statement
**** TODO Ejercicio 46
#+begin_statement
Demostrar que $\varphi^G \in {\cal C}(G)$ para cada $\varphi \in {\cal C}(H)$, y que $\varphi^G(1) = |G:H|\varphi(1)$.
#+end_statement
**** TODO Ejercicio 47 (*)
#+begin_statement
Sea $G$ un grupo abeliano finito, y $H$ un subgrupo de $G$. Demostrar que la
aplicaciÃ³n $(-)_H\colon {\cal C}(G) \to {\cal C}(H)$ es sobreyectiva. Identificar su nÃºcleo.
#+end_statement
**** Ejercicio 48 (**)                                                                                      :export:
#+begin_statement
Calcular la tabla de caracteres complejos de $A_5$.
#+end_statement

***** Clases de conjugaciÃ³n
Para determinar las clases de conjugaciÃ³n usaremos crucialmente
que

\[
\sigma (a\ b\ \dots) \sigma^{-1} = (\sigma(a)\ \sigma(b)\ \dots).
\]

Tenemos las siguientes clases:

 * la clase trivial con el Ãºnico elemento $()$.
 * la clase de ciclos de longitud $3$ con todos los elementos de la forma
   $(a\ b\ c)$, que pueden conseguirse desde cualquiera de ellos usando que
   $(a\ d\ e)(a\ b\ c)(d\ a\ e) = (d\ b\ c) = (c\ d\ b)$. Esta clase tiene cardinalidad
   $5\cdot 4\cdot 3/3 = 20$.
 * la clase de pares de trasposiciones disjuntas de la forma $(a\ b)(c\ d)$, 
   que pueden conseguirse desde cualquiera de ellos usando que
   $(e\ b\ a)(a\ b)(c\ d)(e\ a\ b) = (a\ e)(c\ d)$. Esta clase tiene cardinalidad
   $5\cdot 4\cdot 3\cdot 2/4\cdot 2 = 15$.
 * la clase de los ciclos de longitud 5 que se obtienen desde una
   permutaciÃ³n par desde $(1\ 2\ 3\ 4\ 5)$, que son todos aquellos que podemos
   obtener conjugando. Esta clase tiene cardinalidad $5!/5\cdot 2 = 12$.
 * la clase de los ciclos de longitud 5 que se obtienen desde una
   permutaciÃ³n impar desde $(2\ 1\ 3\ 4\ 5)$. NÃ³tese que todos los ciclos de
   longitud 5 deben ser como este o como los anteriores segÃºn cÃ³mo
   sea la permutaciÃ³n que los lleva a $(1\ 2\ 3\ 4\ 5)$ por conjugaciÃ³n.
   Esta clase tiene cardinalidad $5!/5\cdot 2 = 12$.

Comprobamos que la suma de la cardinalidad de las clases de conjugaciÃ³n
es la cardinalidad del grupo completo,

\[
60 = 1 + 20 + 15 + 12 + 12.
\]

Y como hay $5$ clases de conjugaciÃ³n, existirÃ¡n $5$ representaciones
irreducibles complejas, cuyas dimensiones ademÃ¡s deberÃ¡n sumar la
cardinalidad del grupo, es decir,

\[
60 = n_1^2+n_2^2+n_3^2+n_4^2+n_5^2.
\]

Sabiendo que la primera serÃ¡ la representaciÃ³n trivial, podemos
buscar exhaustivamente soluciones a $59 = n_2^2+n_3^2+n_4^2+n_5^2$ y
encontrar que la Ãºnica, salvo reordenaciÃ³n, es $3,3,4,5$. Esas
deben ser las dimensiones de nuestras representaciones.

***** Tabla de caracteres
Calculamos la tabla de caracteres usando que:

 - tenemos claramente la representaciÃ³n trivial $\chi_1$.
 - desde $S_{5}$ tenemos la representaciÃ³n $\psi$ dada por permutar
   los vectores de una base de dimensiÃ³n $5$. Si la restringimos
   tenemos $\psi_{A_5}$. Esta representaciÃ³n tiene un espacio
   invariante claro en $\left\langle (1,1,1,1,1) \right\rangle$ sobre el que actÃºa trivialmente. 
   Puede descomponerse entonces en la suma de dos representaciones,
   siendo una de ellas la trivial, y sabemos entonces que el caracter
   del otro sumando de la representaciÃ³n serÃ¡ $\psi_{A_5} - \chi_1$, por lo que
   nos queda una fila

   \[
   \small
   \begin{tabular}{c|cccccc}
   & 1    & 15             & 20          & 12                & 12                \\
   A_5        & $()$ & $(1\ 2)(3\ 4)$ & $(1\ 2\ 3)$ & $(1\ 2\ 3\ 4\ 5)$ & $(1\ 2\ 3\ 5\ 4)$ \\
   \hline
   \chi_4     & 5-1=4    & 1-1=0              & 2-1=1           & 0-1=-1                 & 0-1=-1                 \\ 
   \end{tabular}\]

   Comprobamos ademÃ¡s que el caracter $\chi_4$ asÃ­ obtenido es irreducible, 
   ya que cumple que $(\chi_4,\chi_4) = (4^2+20+12+12)/60 = 1$.

 - podemos obtener la segunda columna aplicando el teorema de ortogonalidad
   consigo misma para obtener $4 = 1^2 + x^2 + y^2 + z^2$ y con la primera
   columna para obtener $0 = 1 + 3x + 3y + 5z$. Si diagonalizamos la 
   representaciÃ³n de un elemento de esa clase de conjugaciÃ³n, como tiene
   orden $2$ la diagonal estarÃ­a formada por $\pm 1$; como ademÃ¡s las dimensiones
   son $5,3,3$, todos impares, nunca podrÃ­a ser $0$ su traza. AsÃ­, la Ãºnica
   soluciÃ³n a la primera ecuaciÃ³n es $x^2 = y^2 = z^2 = 1$ y la soluciÃ³n a la
   segunda ecuaciÃ³n es $x=-1, y=-1, z=1$.

 - la Ãºltima fila de la tabla de caracteres la podemos completar usando
   las relaciones de ortogonalidad. Sabemos que

   \[\begin{aligned}
   40 + 20b^2 + 12c^2 + 12d^2 &= 60 \\
   5 +15 + 20b   + 12c   + 12d   &= 0  \\
   20 + 20b   - 12c   - 12d   &= 0  \\
   \end{aligned}\]

   de donde deducimos primero que $c+d=0$, luego $b=-1$ y $c=d=0$.
   Hemos llegado a la conclusiÃ³n de que

   \[\small
   \begin{tabular}{c|cccccc}
   & 1    & 15             & 20          & 12                & 12                \\
   A_5        & $()$ & $(1\ 2)(3\ 4)$ & $(1\ 2\ 3)$ & $(1\ 2\ 3\ 4\ 5)$ & $(1\ 2\ 3\ 5\ 4)$ \\
   \hline
   \chi_5     & 5    & 1              & b=-1           & c=0                 & d=0                 \\ 
   \end{tabular}\]   

 - la tercera columna de la tabla podemos obtenerla de nuevo con relaciones
   de ortogonalidad, tenemos $3 = 1 + u^2 + v^2 + 1 + 1$ y $1 -u-v-1=0$, 
   luego $u = -v$ y $u=v=0$.

 - las dos Ãºltimas entradas estÃ¡n sujetas a las mismas condiciones, asÃ­
   que esperamos obtenerlas como soluciones distintas del mismo sistema
   de ecuaciones de ortogonalidad. Aplicando ortogonalidad tenemos
   $3^2 + 15 + 12p^{2}+ 12q^{2} = 60$ y $3 -15+12p+12q=0$, luego $p=1-q$,
   y como $p^2+(1-p)^2 = 3$, tenemos que las dos soluciones posibles
   serÃ¡n las de $p^2 - p - 1 = 0$, es decir $p=\pm(1+\sqrt{5})/2$.


AsÃ­, la tabla de caracteres acaba quedando como

\[\small
\begin{tabular}{c|cccccc}
           & 1    & 15             & 20          & 12                & 12                \\
A_5        & $()$ & $(1\ 2)(3\ 4)$ & $(1\ 2\ 3)$ & $(1\ 2\ 3\ 4\ 5)$ & $(1\ 2\ 3\ 5\ 4)$ \\
\hline
\chi_1     & 1    & 1              & 1           & 1                 & 1                 \\ 
\chi_2     & 3    & -1             & 0           & (1+\sqrt{5})/2    & (1-\sqrt{5})/2    \\ 
\chi_3     & 3    & -1             & 0           & (1-\sqrt{5})/2    & (1+\sqrt{5})/2    \\ 
\chi_4     & 4    & 0              & 1           & -1                & -1                \\ 
\chi_5     & 5    & 1              & -1          & 0                 & 0                 \\ 
\end{tabular}\]
** Inferencia estadÃ­stica
# ##
# Estos apuntes se han reescrito desde los apuntes de A. Hermoso Carazo y
# M.D. Ruiz Medina para la asignatura de Inferencia EstadÃ­stica del grado
# de matemÃ¡ticas de la Universidad de Granada.
# ##

*** Prerrequisitos
**** Distribuciones
***** FunciÃ³n generatriz de momentos
Se define para una variable aleatoria $X$ con funciÃ³n de distribuciÃ³n
$f$ como:

\[
M_X(t) = 
\mathbb{E}(e^{tX}) =
\int_\Omega e^{tx}f(x) \;dx
\]

****** CÃ¡lculo de momentos
Se cumple que:

\[
\mathbb{E}[X^n] = \frac{\partial^n}{\partial t^n} M_X(0)
\]

***** FunciÃ³n caracterÃ­stica
Se define para una variable aleatoria $X$ con funciÃ³n de distribuciÃ³n $f$:

\[
\varphi_X(t) = \mathbb{E}[e^{itX}] = \int_\Omega e^{itx}f(x)\;dx
\]

****** CÃ¡lculo de momentos
Se cumple que:

\[
\varphi_X^{(n)}(0) = i^n\mathbb{E}[X^n]
\]

**** Varianza
***** Varianza
La varianza se define equivalentemente como:

\[Var(X) = E\Big[(X-EX)^2\Big] = E[X^2] - E[X]^2\]

***** Covarianza
La covarianza se define equivalentemente como:

\[cov(X,Y) = E[(X-EX)(Y-EY)] = E[XY] - E[X]E[Y]\]

NÃ³tese que $cov(X,X) = Var(X)$. NÃ³tese ademÃ¡s se comporta como el 
[[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][producto interno]] de un espacio prehilbertiano.

***** Varianza de la suma
La varianza de una suma cumple:

\[
Var(X+Y) = Var(X) + Var(Y) + 2cov(X,Y)
\]

En el caso general:

\[Var\left(\sum X_i\right) = \sum_i\sum_j cov(X_i,X_j)\]

***** Cauchy-Schwarz para la covarianza
Se tiene la desigualdad:

\[cov(X,Y)^2 \leq Var(X)Var(Y)
\]

****** DemostraciÃ³n
Sabiendo que la varianza es siempre no negativa:

\[
0 \leq Var\left(X - \frac{cov(X,Y)}{Var(Y)} Y\right) =
Var(X) - \frac{\left(cov(X,Y)\right)^2}{Var(Y)}
\]

****** DemostraciÃ³n por Cauchy-Schwarz
Se comprueba que la covarianza da un producto escalar que genera
un [[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][espacio cociente]] prehilbertiano. Aplicamos Cauchy-Schwarz.

**** Esperanza condicional
***** Esperanza condicional en caso discreto
Definimos la esperanza condicional de dos variables discretas como:

\[\mathbb{E}[X|Y] = \sum_x xP(X=x\mid Y=y) = \sum_x x\frac{P(X=x, Y=y)}{P(Y=y)}\]

***** Esperanza condicional en el caso continuo
MÃ¡s generalmente se define para el caso continuo:

\[
\mathbb{E}[X|Y] = \int_X x f_{X|Y}(x|y) dx = \int_X x \frac{f_{X,Y}(x,y)}{f_Y(y)} dx
\]

***** Ley de esperanza total
La esperanza condicional cumple:

\[\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]\]

****** DemostraciÃ³n en el caso discreto
Se tiene:

\[\begin{aligned}
E[E[X|Y]] &= \int_Y f(y)  \left(\int_X x \frac{f(x,y)}{f(y)} dx \right) dy \\Ã±
&= \int_X x \int_Y f(x,y) dy dx \\&= \int_X x f(x) dx = E[X]
\end{aligned}\]

NÃ³tese que asumimos una conmutatividad de las integrales discretas.

****** DemostraciÃ³n en el caso continuo
Puede consultarse la [[https://en.wikipedia.org/wiki/Law_of_total_expectation#Proof_in_the_general_case][Ley de la esperanza total]].

**** Desigualdades
***** Desigualdad de Chebyshev
Para una variable aleatoria $X$ de segundo orden:

\[
P(|X-\mathbb{E}[X]| \geq a) \leq \frac{Var(X)}{a^2}
\]

**** Convergencia
***** Convergencia casi segura
Una sucesiÃ³n de variables aleatorias converge de forma casi segura a
otra $X_n \overset{c.s.}\longrightarrow X$ cuando el conjunto de sucesos que lo hacen tiene 
probabilidad 1.

\[
P\left(\lim_{n\to\infty} X_n = X\right) = 1
\]

***** Convergencia en probabilidad
Una sucesiÃ³n de variables aleatorias converge en probabilidad a
otra $X_n \overset{P}\longrightarrow X$ cuando:

\[
\lim_{n\to\infty} P\left(|X_n-X| \geq \varepsilon\right) = 0
\]

para cualquier $\varepsilon$.

****** Equivalentemente
Si consideramos su complemento:

\[
\lim_{n\to\infty} P(|X_n-X| < \varepsilon) = 1
\]

***** Convergencia en distribuciÃ³n
Una sucesiÃ³n de variables aleatorias converge en ley o en distribuciÃ³n 
a otra $X_n \overset{d}\longrightarrow X$, si se tiene que, dadas sus funciones de distribuciÃ³n,
convergen en los puntos en los que es continua:

\[
\forall x: F \mbox{ continua en } x:
\quad
\lim_{n\to\infty} F_n(x) = F(x)
\]

****** Equivalentemente
Se tiene $X_n\overset{d}\longrightarrow X$ si para cualquier $t$ real:

\[
\lim_{n\to\infty} E\left[ e^{tX_n} \right] = E\left[e^{tX}\right]
\]

***** Implicaciones
La convergencia casi segura implica la convergencia en probabilidad, que
implica a su vez la convergencia en distribuciÃ³n.

****** TODO DemostraciÃ³n

***** Ley dÃ©bil de los grandes nÃºmeros
Si $X_1,X_2,\dots$ es una sucesiÃ³n infinita de variables aleatorias 
independientes con la misma esperanza y varianza, entonces:

\[
\overline{X}_n = \frac{1}{n}(X_1+\dots+X_n)
\]

converge en probabilidad a $\mu$:

\[
\lim_{n\to\infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0
\]

***** Ley fuerte de los grandes nÃºmeros
Si $X_1,X_2,\dots$ es una sucesiÃ³n infinita de variables aleatorias 
independientes e idÃ©nticamente distribuidas con $E\left[|X_i|\right] < \infty$ y
valor esperado $\mu$, entonces:

\[
P\left(
\lim_{n\to\infty} \overline{X}_n = \mu
\right) = 1
\]

*** Distribuciones discretas
**** 1. DistribuciÃ³n uniforme
***** DefiniciÃ³n
Se define sobre un conjunto finito de valores $\{x_i\}$ con la misma 
probabilidad como:

\[f(x_i|n) = \frac{1}{n}\]
**** 2. DistribuciÃ³n binomial
***** DefiniciÃ³n
Determina la probabilidad de $x$ aciertos en $n$ experimentos de Bernoulli.
La funciÃ³n de distribuciÃ³n de $B(x|n,p)$ es:

\[
f(x|n,p) = {n \choose x}p^x (1-p)^{n-x}
\]

****** Esperanza

\[\mathbb{E}[X] = np\]

****** Varianza

\[Var[X] = np(1-p)\]

**** 3. DistribuciÃ³n multinomial
***** DefiniciÃ³n
Deriva de una binomial con $k$ salidas distintas de probabilidades $p_1,\dots,p_k$
como:

\[
f(X|n,p_1,\dots,p_k) = \frac{n!}{X_1!X_2!\dots X_n!}p^{X_1}p^{X_2}\dots p^{X_k}
\]

**** 4. DistribuciÃ³n de Poisson
***** DefiniciÃ³n
Definimos la distribuciÃ³n de Poisson $Poi(\lambda)$ como:

\[
f(n|\lambda) = \frac{e^{-\lambda}\lambda^n}{n!}
\]

****** Es una distribuciÃ³n
Comprobamos que suma la unidad:

\[
\sum_{n=1}^\infty f(n|\lambda) =
\sum_{n=1}^\infty e^{-\lambda}\frac{\lambda^n}{n!} = 1
\]

***** FunciÃ³n generatriz de momentos
La funciÃ³n generatriz se calcula como:

\[
M_X(t) = \sum_{n=1}^\infty e^{-\lambda}\frac{(e^t\lambda)^n}{n!}
= e^{\lambda(e^t-1)}
\]

****** Esperanza
Desde la funciÃ³n generadora:

\[
\mathbb{E}[X] = \frac{\partial M_X}{\partial t}(0) = \lambda
\]

****** Varianza
Desde la funciÃ³n generadora:

\[
Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \lambda
\]

***** Suma de Poisson
Para $X \leadsto Poi(\lambda_1)$, $Y \leadsto Poi(\lambda_2)$ independientes, su suma sigue la
distribuciÃ³n con el parÃ¡metro suma.

\[
X + Y \leadsto Poi(\lambda_1+\lambda_2)
\]

*** Distribuciones continuas
**** 1. DistribuciÃ³n normal
***** DefiniciÃ³n
Definimos la distribuciÃ³n normal ${\cal N}(\mu,\sigma^2)$ como aquella con funciÃ³n de
densidad:

\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

****** Imagen de la distribuciÃ³n
#+BEGIN_SRC R :file images/normal.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dnorm, seq(-3, 3, 0.1),
                          mean = 0, sd = 1+0.1*i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "DistribuciÃ³n normal, variando ÏÂ²."))
#+END_SRC

#+RESULTS:
[[file:images/normal.png]]

****** Es una distribuciÃ³n
Tenemos que comprobar que integra la unidad sobre los reales, y
de hecho, tomando cambio de variable $y = (x-\mu)/\sqrt{2\sigma^2}$ queda:

\[\begin{aligned}
\int^{+\infty}_{-\infty} 
\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2} =
\frac{1}{\sqrt{\pi}}\int^{+\infty}_{-\infty} 
e^{-y^2} = 1
\end{aligned}\]

Que es la [[https://en.wikipedia.org/wiki/Gaussian_integral][integral de Gauss]].

***** FunciÃ³n caracterÃ­stica
La funciÃ³n caracterÃ­stica de ${\cal N}(\mu,\sigma^2)$ es:

\[\varphi_X(t) = e^{it\mu - t^2\sigma^2/2}\]

****** TODO DemostraciÃ³n
Usamos la definiciÃ³n de funciÃ³n caracterÃ­stica y completamos
cuadrados para tener:

\[\begin{aligned}
\varphi_X(t) &= 
\mathbb{E}\left[e^{itX}\right] &= 
\int_{-\infty}^{+\infty}
e^{itx}\frac{1}{\sqrt{2\pi\sigma^2}} 
e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx \\&=
\frac{1}{\sqrt{2\pi\sigma^2}} 
\int_{-\infty}^{+\infty}
e^{it\mu}
\end{aligned}\]

***** Suma de normales
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ e $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. Entonces $X+Y\leadsto {\cal N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.

****** TODO DemostraciÃ³n

***** Producto por escalar
Si $X \leadsto {\cal N}(\mu,\sigma^2)$, entonces $kX \leadsto {\cal N}(k\mu,k\sigma^2)$.

****** TODO DemostraciÃ³n
***** Teorema de Cramer
Sean $X,Y$ independientes. Si $X+Y$ es normal, $X$ e $Y$ son normales.

****** TODO DemostraciÃ³n
**** 2. DistribuciÃ³n ÏÂ² de Pearson
***** DistribuciÃ³n chi cuadrado
Es un caso particular de la distribuciÃ³n gamma, $X \leadsto \chi^2(k) = \Gamma(k/2,1/2)$.
Al parÃ¡metro $k$ se le llama *nÃºmero de grados de libertad*.

****** GrÃ¡fica de la funciÃ³n de densidad
#+BEGIN_SRC R :file images/chi.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dchisq, seq(0, 6, 0.1),
                          df = i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "DistribuciÃ³n ÏÂ²(n), variando n."))
#+END_SRC

#+RESULTS:
[[file:images/chi.png]]

****** FunciÃ³n de densidad

 \[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]

******* TODO DemostraciÃ³n
***** FunciÃ³n generatriz de momentos

\[M_X(t) = \frac{1}{(1-2t)^{k/2}}\], para $t < 1/2$.

****** TODO DemostraciÃ³n
****** Esperanza y varianza

 - $E[X] = k$
 - $Var[X] = 2k$

******* DemostraciÃ³n
Se calculan desde la funciÃ³n generatriz.

***** Propiedad de reproductividad
Si tengo una serie de variables independientes distribuidas 
por $X_i \leadsto \chi^2(k_i)$, entonces:

\[\sum_{i=1}^n X_i \leadsto \chi^2 \left(\sum_{i=1}^n k_i \right)\]

***** RelaciÃ³n con la normal
Dadas variables independientes $X_i \leadsto {\cal N}(0,1)$,

 \[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]

****** TODO DemostraciÃ³n

***** Teorema central del lÃ­mite de LÃ¨vy
Para valores pequeÃ±os, pueden usarse tablas. Para valores grandes
de $n$, podemos aproximarla mediante el Teorema Central del LÃ­mite
como:

\[ \chi^2(n) \approx {\cal N}(n,2n)\]

****** TODO DemostraciÃ³n                                                                                   :extra:
**** 3. DistribuciÃ³n t de Student
***** DefiniciÃ³n 
Dadas dos variables independientes $X \leadsto {\cal N}(0,1)$ e $Y \leadsto \chi^2(n)$,
tenemos:

\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]

****** FunciÃ³n de densidad

\[ f(t) 
= \frac
{\Gamma\left(\frac{n+1}{2}\right)}
{\Gamma\left(\frac{n}{2}\right) \sqrt{n\pi}} 
\left(
1 + \frac{t^2}{n}
\right)^{-\frac{n+1}{2}}
\], $t \in \mathbb{R}$

******* TODO DemostraciÃ³n

****** GrÃ¡fica de la funciÃ³n de densidad
#+BEGIN_SRC R :file images/tstudent.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dt, seq(0, 3, 0.1),
                          df=i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "T de Student, variando n."))
#+END_SRC

#+RESULTS:
[[file:images/tstudent.png]]

***** Momentos
Tenemos que $\exists E[T^k] \iff k < n$. Cuando existen, se tiene

 - $E[T] = 0$
 - $Var[T] = \frac{n}{n-2}$

****** TODO DemostraciÃ³n

***** AproximaciÃ³n por la normal
Tabulada para $n$ pequeÃ±os y aproximada por ${\cal N}(0,1)$ para valores
grandes.
**** 4. DistribuciÃ³n F de Snedecor
***** DefiniciÃ³n
*F de Snedecor*. Dadas dos variables independientes $X \leadsto \chi^2(m)$ e
$Y \leadsto \chi^2(n)$, su cociente nos da:

\[F = \frac{X/m}{Y/n} \leadsto F(m,n)\]

****** GrÃ¡fica de la funciÃ³n de densidad
#+BEGIN_SRC R :file images/fsnedecor.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(df, seq(0, 2, 0.04),
                          df1=i, df2=i-1,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "F de Snedecor, variando n y m."))
#+END_SRC

#+RESULTS:
[[file:images/fsnedecor.png]]

****** FunciÃ³n de densidad

\[g(t)
= \frac
{\Gamma(\frac{m+n}{2})}
{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}
\left(\frac{m}{n}\right)^{\frac{m}{2}}
t^{m/2-1}
\left(1+\frac{m}{n}t\right)^{-\frac{m+n}{2}}\], para $t>0$.

******* TODO DemostraciÃ³n

***** Momentos
Tenemos que $\exists E[T^k] \iff k < n/2$.

 - $n > 2 \Rightarrow \exists E[F] = \frac{n}{n-2}$
 - $n > 4 \Rightarrow \exists Var[F] = \frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)}$

****** TODO DemostraciÃ³n

***** Propiedades
\[ F \leadsto F(m,n) \iff F^{-1} \leadsto F(n,m)\]
\[T \leadsto t(n) \iff T^2 \leadsto F(1,n)\]

***** AproximaciÃ³n
La distribuciÃ³n estÃ¡ tabulada y las tablas incluyen aproximaciones 
para valores grandes de $n$ y $m$.

**** 5. DistribuciÃ³n exponencial
***** DistribuciÃ³n exponencial
Dado un $\lambda>0$, definimos la distribuciÃ³n exponencial, $\operatorname{Exp}(\lambda)$, como aquella
con funciÃ³n de densidad:

\[f(x) = \lambda e^{-\lambda x}\qquad \forall x \in \mathbb{R}^+_0\]

****** Es una distribuciÃ³n
Trivialmente integrando:

\[
\int_0^\infty \lambda e^{-\lambda x} = 
-\left[ e^{-\lambda x} \right]^\infty_0 = 1
\]

***** Suma de exponenciales
La suma de variables exponenciales es una distribuciÃ³n Gamma:

***** Caso particular de la distribuciÃ³n Gamma
La exponencial es un caso particular de la distribuciÃ³n Gamma:

\[
Exp(\lambda) = \Gamma(1,\lambda)
\]
**** 6. DistribuciÃ³n de Dirichlet
***** DistribuciÃ³n de Dirichlet
Dado un vector de reales $\alpha_1,\alpha_2,\dots,\alpha_n$, definimos la distribuciÃ³n $Dir(\alpha)$ 
como la que tiene funciÃ³n de densidad:

\[
f(x) = \frac{1}{B(\alpha)} \prod_{i=1}^K x_i^{\alpha_i-1}
\]

donde,

\[
B(\alpha) =
\frac
{\prod_{i=1}^K \Gamma(\alpha_i)}
{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}
\]

***** Momentos
****** Esperanza

\[
E[X_i] = \frac{\alpha_i}{\sum_k \alpha_k}
\]

****** Varianza

\[
Var[X_i] = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}
\]
**** 7. DistribuciÃ³n Gamma
***** FunciÃ³n Gamma
Se define la funciÃ³n gamma $\Gamma : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\Gamma(\alpha) = \int^\infty_0 t^{\alpha-1}e^{-t} dt\]

****** La integral estÃ¡ definida
Por un lado, $t^{a-1}e^{-t} < t^{a-1}$, integrable en $[0,b]$. Por otro lado,

\[\lim_{t \to \infty} \frac{t^{\alpha-1}e^{-t}}{e^{-t/2}} = 0\]

Por lo que $t^{\alpha-1}e^{-t} < e^{-t/2}$ integrable, a partir de algÃºn punto.
Partimos la integral como:

\[\int_0^b t^{\alpha-1}e^{-t}dt + \int^{\infty}_b t^{\alpha-1}e^{-t}dt
< \infty\]

***** Propiedades de la funciÃ³n Gamma
Sea $\alpha > 0$, se verifica:

  1. $\Gamma(1) = 1$.
  2. $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$.
  3. $\Gamma(n+1) = n!$ para $n \in \mathbb{N}$.
  4. $\Gamma(\alpha)\Gamma(1-\alpha) = \frac{\pi}{\sin(\alpha\pi)}$ para $0<\alpha<1$.
  5. $\Gamma(1/2) = \sqrt{\pi}$.
  6. $\Gamma(\alpha) = \beta^\alpha \int^\infty_0 t^{\alpha-1}e^{-\beta t} dt$ para $\beta > 0$.

****** DemostraciÃ³n
******* Punto 1
Trivial.
******* Punto 2
Integral por partes.
******* Punto 3
InducciÃ³n sobre los dos primeros apartados.
******* TODO Punto 4
******* Punto 5
Trivial desde el punto anterior.
******* Punto 6
Cambio de variable $\varphi(t) = \beta t$.

***** DistribuciÃ³n Gamma
Dados $\alpha,\beta > 0$, definimos la distribuciÃ³n Gamma $\Gamma(\alpha,\beta)$ como aquella con
funciÃ³n de densidad:

\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\]

para $x>0$. 

****** Imagen de la distribuciÃ³n
#+BEGIN_SRC R :results graphics :file images/gamma.png
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dgamma, seq(0, 4, 0.05),
                          shape = i, rate = 1,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "DistribuciÃ³n gamma, variando Î±."))
#+END_SRC

#+RESULTS:
[[file:images/gamma.png]]
***** Propiedades de la distribuciÃ³n Gamma
La funciÃ³n de densidad de una distribuciÃ³n $\Gamma(\alpha,\beta)$ verifica:

  1. Cuando $0<\alpha<1$, $f$ es decreciente y $\lim_{x\to 0} f(x) = \infty$.
  2. Cuando $\alpha = 1$, $f$ es decreciente y $f(0)=1$.
  3. Cuando $\alpha>1$, $f$ es creciente en $[0,(\alpha-1)/\beta]$ y decreciente 
     en $[(\alpha-1)/\beta,\infty]$.

Y sobre convexidad y concavidad se tiene:

  1. Si $0<\alpha\leq 1$, es convexa.
  2. Si $1 < \alpha \leq 2$, es cÃ³ncava en $[0,(\alpha-1+\sqrt{\alpha+1})/\beta]$ y convexa
     en $[(\alpha-1+\sqrt{\alpha+1})/\beta,\infty]$.
  3. Si $2 < \alpha$, es cÃ³ncava en $[(\alpha-1-\sqrt{\alpha+1})/\beta,(\alpha-1+\sqrt{\alpha+1}/\beta)]$ y
     convexa en todo el resto del dominio.

****** TODO DemostraciÃ³n
***** Suma de Gammas
Para $X \leadsto \Gamma(\alpha_1,\beta)$, $Y \leadsto \Gamma(\alpha_2,\beta)$, independientes:

\[
X+Y \leadsto \Gamma(\alpha_1+\alpha_2,\beta)
\]
**** 8. DistribuciÃ³n Beta
***** FunciÃ³n Beta
Se define la funciÃ³n beta $\beta : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\beta(x,y) = \int^1_0 t^{x-1}(1-t)^{y-1}dt\]

****** EstÃ¡ bien definida
Por la [[*RelaciÃ³n con la funciÃ³n Gamma][relaciÃ³n con la funciÃ³n Gamma]] sabemos que debe estar
bien definida.

***** RelaciÃ³n con la funciÃ³n Gamma
Para cada $x,y$ se tiene:

\[\frac{\Gamma(x)\Gamma(y)}{\Gamma(xy)} = \beta(x,y)\]

****** TODO DemostraciÃ³n

***** DistribuciÃ³n Beta
Dados $p,q>0$, definimos la distribuciÃ³n Beta $\beta(p,q)$ como aquella con 
funciÃ³n de densidad:

\[f(x) = \frac{1}{\beta(p,q)} x^{p-1}(1-x)^{q-1}\]
*** 1. IntroducciÃ³n a la inferencia estadÃ­stica. EstadÃ­sticos muestrales
**** Planteamiento de un problema de inferencia
***** Modelo estadÃ­stico
Un modelo estadÃ­stico $(X,{\cal P})$ consta de:

  - $X : (\Omega, {\cal A},{\cal P}) \longrightarrow (\mathbb{R},{\cal B},P_X)$ variable aleatoria que describe el 
    objeto de estudio.
  - ${\cal P}$ familia de distribuciones que pueden ser la de $X$.

***** Modelo estadÃ­stico paramÃ©trico
Cuando se conoce la forma funcional de $P_X$ y sÃ³lo desconocemos un 
parÃ¡metro tenemos una familia paramÃ©trica de distribuciones $F(x,\theta)$ 
para $\theta$.

***** Modelo estadÃ­stico no paramÃ©trico
Cuando la forma funcional de $P_X$ es desconocida.

***** Muestra aleatoria simple
Una muestra aleatoria simple es un vector $(X_1,\dots,X_n)$
de variables independientes idÃ©nticamente distribuidas. 

****** RealizaciÃ³n muestral 
Una realizaciÃ³n muestral a un valor concreto obtenido al
observar la muestra.

****** Espacio muestral
Conjunto de todas las posibles realizaciones.

**** FunciÃ³n de distribuciÃ³n empÃ­rica
***** FunciÃ³n de distribuciÃ³n muestral
La *funciÃ³n de distribuciÃ³n empÃ­rica* es una funciÃ³n de 
distribuciÃ³n razonable que podemos obtener desde una 
realizaciÃ³n muestral.

 \[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(X_i < x)} \]

***** Propiedades de la funciÃ³n de distribuciÃ³n empÃ­rica
Fijado un $x \in \mathbb{R}$, $F^\ast(x)$ es una variable aleatoria siguiendo por 
definiciÃ³n una binomial:

\[ nF^\ast(x) \leadsto {\cal B}(n, F(x))\]

Calculamos su *esperanza* y *varianza* desde Bernoulli como:

 - Esperanza: $E[F^\ast(x)] = F(x)$
 - Varianza: $Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}$

Aplicando entonces el Teorema Central del LÃ­mite:

\[ \frac{F^\ast(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \leadsto {\cal N}(0,1) \]

***** Teorema de Glivenko-Cantelli
Las funciones de distribuciÃ³n muestrales convergen 
casi seguramente uniformemente a la teÃ³rica.

\[ P\left\{ \lim_{n \rightarrow \infty} 
\sup_{x \in \mathbb{R}} |F^\ast_n(x) - F(x)| = 0\right\} = 1\]

****** Equivalentemente
Con probabilidad 1 se tiene que, al tomar sucesivas observaciones 
independientes y considerar las correspondientes funciones de 
distribuciÃ³n muestrales:

\[\forall x \in \mathbb{R}: \forall \epsilon>0: \exists n_\epsilon : \forall n \geq n_\epsilon:
\quad F^\ast_n(x) - \epsilon < F_X(x) < F^\ast_n(x) + \epsilon\]

****** DemostraciÃ³n
[[http://matematicas.unex.es/~nogales/estadisticamatematica/TGC.pdf][Teorema de Glivenko-Cantelli]].

**** EstadÃ­sticos muestrales
***** EstadÃ­stico muestral
Dada una muestra aleatoria simple, un *estadÃ­stico muestral* es una 
funciÃ³n sobre ella $T : (\mathbb{R}^n,{\cal B}^n)\longrightarrow (\mathbb{R}^k,{\cal B}^k)$ medible e independiente 
de cualquier parÃ¡metro desconocido.

***** Momentos muestrales no centrados
Para cada $k \in \mathbb{N}$:

\[A_k = \frac{1}{n}\sum_{i=1}^n X_i^k\]

***** Momentos muestrales centrados
Para cada $k \in \mathbb{N}$:

\[B_k = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^k\]

***** Media muestral
Caso particular,

\[A_1 = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X}\]

***** Varianza muestral
Caso particular,

\[B_2 = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2\]

***** Cuasivarianza muestral

\[S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2\]

*** 2. Muestreo de poblaciones normales
**** Muestreo de la normal
***** Lema de Fisher
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple con $X \leadsto {\cal N}(\mu,\sigma^2)$.
Los estadÃ­sticos $\overline{X}$ y $S^2$ son independientes.

****** TODO DemostraciÃ³n

***** A1. Inferencia de la media con varianza conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$, y $\overline{X}$ su media muestral:

\[
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)
\]

****** DemostraciÃ³n
Usando las propiedades de la suma de normales y la linealidad de la
esperanza y cuadracidad de la varianza tenemos:

\[
\overline{X} \leadsto {\cal N}(\mu,\sigma^2/n)
\]

Desde donde simplemente normalizamos.

***** A2. Inferencia de la media con varianza desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$ con $\overline{X}$ su media muestral y $S^2$ su cuasivarianza muestral,
entonces:

\[
\frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]

****** DemostraciÃ³n
Por la definiciÃ³n de t de Student, sabiendo:

\[
\frac{\overline{X} - \mu}{S/\sqrt{n}}
=
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)S^2}{\sigma^2}/n-1}}
\leadsto
t(n-1)
\]

***** B1. Inferencia de la varianza con media conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$, entonces:

\[
\sum_{i=1}^n \left(\frac{X_i-\mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]

****** DemostraciÃ³n
Usando que la suma de cuadrados de normales estÃ¡ndar es una
distribuciÃ³n chi cuadrado.

***** B2. Inferencia de la varianza con media desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$ con $S^2$ su cuasivarianza muestral, entonces:

\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]

****** DemostraciÃ³n
Usamos la independencia entre $X_i-\overline{X}$ y $\overline{X}-\mu$ para escribir:

\[
\sum_{i=1}^n (X_i - \mu)^2
=
\sum_{i=1}^n (X_i - \overline{X})^2 +
\sum_{i=1}^n (\overline{X} - \mu)^2
\]

Ahora bien, sabemos que:

\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]

\[
n\left(\frac{\overline{X} - \mu}{\sigma}\right)^2}
\leadsto
\chi^2(1)
\]

Y desde aquÃ­, por unicidad de las funciones generadoras de momentos
se tiene:

\[
\sum_{i=1}^n \frac{(X_i-\overline{X})^2}{\sigma^2}
\leadsto
\chi^2(n-1)
\]

**** Muestreo de dos normales
***** ExtensiÃ³n del lema de Fisher
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes.
Los vectores $(\overline{X},\overline{Y})$ y $(S^2_1,S^2_2)$ son independientes.

****** TODO DemostraciÃ³n
***** Inferencia sobre diferencia de medias con varianzas conocidas
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes:

\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1}{\sigma_1^2} + \frac{(n_2-1)S^2_2}{\sigma_2^2}}
\sqrt{\frac{\sigma_1^2/n_1 + \sigma_2^2/n_2}{n_1+n_2-2}}
}
\leadsto
t(n_1+n_2-2)
\]

****** DemostraciÃ³n
El numerador sigue una distribuciÃ³n ${\cal N}(0,\sigma^2_1/n_1+\sigma^2_2/n_2)$, asÃ­ que lo
dividimos para una normal estÃ¡ndar. Cada uno de los sumandos de
la otra raÃ­z forma una chi cuadrada, que al sumarse da $\chi(n_1+n_2-2)$.

Usamos entonces la definiciÃ³n de t de Student.
***** Inferencia sobre diferencia de medias con varianzas iguales
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma^2)$ independientes:

\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1+ (n_2-1)S^2_2}{n_1+n_2-2}}
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]

****** DemostraciÃ³n
Desde el caso anterior, tomando las varianzas iguales.

****** DemostraciÃ³n alternativa
El numerador sigue una ${\cal N}(0,\sigma^2(1/n_1+1/n_2))$. Podemos dividirlo por
la raÃ­z de la varianza e incluir otra varianza en la raÃ­z de las
cuasivarianzas muestrales para tener una $\chi^2(n_1+n_2-2)$.

Aplicamos definiciÃ³n de t de Student.
***** Inferencia sobre cociente de varianzas con media conocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes con muestras de
tamaÃ±os $n_1$ y $n_2$:

\[
\frac
{\sum_{i=1}^{n_1}(X_i-\mu_1)^2 / n_1\sigma_1^2}
{\sum_{i=1}^{n_2}(X_i-\mu_2)^2 / n_2\sigma_2^2}
\leadsto
F(n_1,n_2)
\]

****** DemostraciÃ³n
Desde la definiciÃ³n de la F de Snedecor, sabiendo que cada factor
es una chi cuadrada.

***** Inferencia sobre cociente de varianzas con media desconocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes:

\[
\frac
{S_1^2/\sigma_1^2}
{S_2^2/\sigma_2^2}
\leadsto
F(n-1,m-1)
\]

****** DemostraciÃ³n
Aplicando la definiciÃ³n de F de Snedecor y sabiendo que son dos
distribuciones chi cuadrado.

*** 3. Suficiencia y completitud
**** EstadÃ­sticos suficientes
***** EstadÃ­stico suficiente
Un estadÃ­stico $t$ es *suficiente* para un parÃ¡metro $\theta$ cuando una vez 
conocido no puede obtenerse mÃ¡s informaciÃ³n de sobre $\theta$ de los datos;
esto es:

\[\Pr(\theta| t,x) = \Pr(\theta|t)\]

****** DefiniciÃ³n equivalente
De forma equivalente, es *suficiente* si la distribuciÃ³n condicionada 
al estadÃ­stico es independiente del parÃ¡metro $\theta$:

 \[\Pr(x|t,\theta) = \Pr(x|t)\]

***** Teorema de factorizaciÃ³n de Fisher-Neyman
$T$ es suficiente para una familia $\theta \in \Theta$ ssi existen funciones no negativas
$g$,$h$ tales que la distribuciÃ³n $f_\theta$ es:

\[f_\theta(x) = h(x)g_\theta(T(x))\]

Donde $g_\theta$ sÃ³lo depende de $x$ a travÃ©s de $T$ y $h$ no depende de $\theta$.

****** TODO DemostraciÃ³n
***** Propiedades de los estadÃ­sticos suficientes
Los estadÃ­sticos suficientes cumplen:

  1. Si $T$ es suficiente para $\{P_\theta \mid \theta \in \Theta\}$, lo es para $\{P_\theta \mid \theta \in \Theta' \subset \Theta\}$.
  2. Si $T$ es suficiente y $T = h'(U)$, $U$ es suficiente.
  3. Toda transformaciÃ³n biunÃ­voca de suficiente es suficiente.

****** DemostraciÃ³n
******* Punto 1
Si cumple la factorizaciÃ³n para un conjunto, lo cumple para tambiÃ©n
un subconjunto.

******* Punto 2
Por el teorema de factorizaciÃ³n:

\[f_\theta(x) = h(x)g_\theta(h'(U(x)))\]

******* Punto 3
Trivial desde lo anterior usando la inversa.

**** EstadÃ­sticos completos
***** Familia de distribuciones completa
Una familia $\{P_\theta \mid \theta \in \Theta\}$ es completa si dada $X \leadsto P_\theta$ se tiene que
para cada $g$ medible:

\[
E_\theta[g(X)] = 0,\;\forall\theta\in\Theta
\implies
P_\theta(g(X) = 0) = 1,\;\forall\theta\in\Theta
\]

***** EstadÃ­stico completo
Un estadÃ­stico $T$ es *completo* cuando para cualquier funciÃ³n medible $g$,
se tiene:

\[ E_\theta [g(T)] = 0, \; \forall\theta\in\Theta
\implies
P_\theta(g(T) = 0) = 1,\; \forall\theta\in\Theta\]

**** Suficiencia y completitud en familias exponenciales
***** Familia exponencial k-paramÃ©trica
Una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramÃ©trica si:

 1. $\Theta$ es intervalo de $\mathbb{R}^k$.
 2. Los valores de la variable no dependen de $\theta$, esto es:
    $\{{ x \mid f_{\theta}(x) > 0 \} = \{{ x \mid f_{\theta'}(x) > 0 \}$ para cualesquiera $\theta,\theta' \in \Theta$.
 3. La familia es de la forma:

    \[f_\theta(x) = exp\left\{\sum_{h=1}^k {Q_h(\theta) T_h(x) + S(x) + D(\theta)}\right\}\]

***** Teorema de suficiencia y complitud
Si una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramÃ©trica, cualquier muestra
aleatoria simple tambiÃ©n lo es:

\[
f^n_\theta(x_1,\dots,x_n) = 
exp\left\{
\sum^k_{h=1} Q_h(\theta) \left(
\sum^n_{i=1} T_h(x_i)
\right) +
\sum^n_{i=1} S(x_i) + nD(\theta)
\right\}
\]

TeniÃ©ndose ademÃ¡s:

 1. $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ estadÃ­stico *suficiente* para $\theta$.
 2. Si $k \leq n$, y $(Q_1(\Theta), \dots Q_k(\Theta))$ contiene un abierto;
    $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ es *completo*.

****** TODO DemostraciÃ³n

***** Ejemplo: la normal para la media
La familia $\{{\cal N}(\mu,\sigma^2) \mid \mu \in \mathbb{R}\}$ es uniparamÃ©trica escribiendo la funciÃ³n
de distribuciÃ³n como:

\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\left( \frac{x^2}{2\sigma^2} - 2\frac{x\mu}{2\sigma^2} + \frac{\mu^2}{2\sigma^2} \right)
\right\}
\]

De aquÃ­ tenemos el $T(x) = x$ suficiente para $\mu$. Y con para una muestra 
tenemos $T(x_1,\dots,x_n) = x_1 + \dots + x_n$.

***** Ejemplo: la normal para la varianza
La familia $\{{\cal N}(\mu,\sigma^2) \mid \sigma\in\mathbb{R}\}$ es uniparamÃ©trica escribiendo la funciÃ³n
de distribuciÃ³n como:

\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\frac{1}{2\sigma^2}\left(x^2 - 2x\mu + \mu^2 \right)
\right\}
\]

AsÃ­ $T(x) = \sum_{i=1}^n (x_i - \mu)^2$ es suficiente. AdemÃ¡s es completo porque
tenemos que $Q(\sigma) = -\frac{1}{2\sigma^2}$ tiene en la imagen un intervalo abierto.

***** Ejemplo: distribuciÃ³n de Poisson
La familia de Poisson $\{Poi(\lambda) \mid \lambda \in \mathbb{R}\}$ tiene como estimador suficiente 
del parÃ¡metro a la suma de las muestras. Tenemos:

\[
f_\lambda(x) = \frac{1}{\prod x_i!} e^{-n\lambda} \lambda^{\sum x_i}
\]

Luego por Fisher-Neyman, sabemos que $\sum x_i$ es suficiente.

*** 4. EstimaciÃ³n puntual
**** Planteamiento del problema de estimaciÃ³n
***** Estimador puntual
Un estimador puntual de $\theta$ es un estadÃ­stico $T$ tomando valores en el 
dominio del parÃ¡metro, $\Theta$.

***** FunciÃ³n de pÃ©rdida y de riesgo
La *funciÃ³n de pÃ©rdida*, $L(\theta,t)$, nos dice la pÃ©rdida asociada a estimar 
un parÃ¡metro si su verdadero valor es otro.

\[
L : \Theta \times \Theta \longrightarrow \Theta
\]

***** FunciÃ³n de riesgo
La *funciÃ³n de riesgo* es la que asocia a cada valor del parÃ¡metro la 
pÃ©rdida media asociada al estimador.

\[ R^L_T(\theta) = E_\theta [L(\theta,T)] \]

***** Estimador Ã³ptimo
El *estimador Ã³ptimo*, $T$, dada una funciÃ³n de pÃ©rdida, es el que minimiza 
uniformemente la funciÃ³n de riesgo:

\[ R^L_T(\theta) \leq R^L_{T''}(\theta),\quad \forall \theta \in \Theta,\; \forall T''\]

***** TODO Ejemplo de estimador Ã³ptimo
**** EstimaciÃ³n de menor error cuadrÃ¡tico
***** FunciÃ³n de pÃ©rdida cuadrÃ¡tica
La funciÃ³n de pÃ©rdida cuadrÃ¡tica, ${\cal L}(\theta, t) = (t - \theta)^2$, hace a la funciÃ³n de 
riesgo de un estimador su error cuadrÃ¡tico medio:

\[R^L_T(\theta) = E_\theta[(T - \theta)^2]\]

NÃ³tese que en el caso de $E[T] = \theta$, se tiene $R^L_T(\theta) = Var_\theta[T]$.

**** EstimaciÃ³n insesgada de mÃ­nima varianza
***** Estimador insesgado
Un estimador $T$ de $g(\theta)$, es *insesgado* o *centrado* si:

 $E_\theta[T] = g(\theta)$

***** UMVUE: Estimador insesgado uniformemente de mÃ­nima varianza
Un estimador $T$ insesgado y de segundo orden es *UMVUE* para $g(\theta)$ si para 
cualquier otro estimador insesgado $T'$ se tiene que:

\[ Var_\theta[T] \leq Var_\theta[T']\]

****** De segundo orden
Lo llamamos de segundo orden cuando existe el momento de segundo orden:

\[
\exists \mathbb{E}_\theta[T^2(X_1,\dots,X_n)]
\quad
\forall \theta \in \Theta
\]

***** Propiedades del UMVUE
El estimador UMVUE cumple:

 - Unicidad: El UMVUE de cualquier funciÃ³n paramÃ©trica, si existe, es Ãºnico.
 - Linealidad: Si $T,Q$ son UMVUE para $g,h$; $aT+bQ$ es UMVUE para $ag+bh$.

****** Unicidad
Si existieran dos UMVUE con $Var(T) = Var(T')$, tendrÃ­amos:

\[\begin{aligned}
Var\left(\frac{1}{2}(T+T')\right) &= 
\frac{1}{4}
\left(
Var(T) + Var(T') + 2cov(T,T')
\right) \\& \leq
\frac{1}{4}
\left(
Var(T) + Var(T') + 2\sqrt{Var(T)Var(T')}
\right) \\& = Var(T)
\end{aligned}\]

La igualdad se da por ser UMVUE, y entonces, $cov(T,T') = Var(T)$.
De aquÃ­ $cov(T-T',T-T') = 0$, haciendo constante la diferencia entre los
dos. La diferencia entre ellos debe ser constantemente $0$ por ser ambos 
insesgados.

****** TODO Linealidad
***** Teorema de RaÃ³-Blackwell
Si $T$ es suficiente para $\theta$ y $S$ es un estimador insesgado de $g(\theta)$ de 
segundo orden:

  - $E[S \mid T]$ es estimador insesgado de $g(\theta)$ de segundo orden.
  - $Var_\theta[E[S \mid T]] \leq Var_\theta[S]$

Es decir, $E[S \mid T]$ serÃ¡ normalmente mejor estimador y nunca peor que $S$.

****** DemostraciÃ³n
Sabemos $E[S|T] = E[S] = \theta$ por la [[*Ley de esperanza total][ley de esperanza total]]. La desigualdad
entre varianzas la vemos como:

\[\begin{aligned}
E\Big[(E[S|T] - \theta)^2 \Big] &= 
E\Big[E[S-\theta | T]^2 \Big] \leq
E\Big[E[(S-\theta)^2|T] \Big] = E\Big[(S-\theta)^2\Big]
\end{aligned}\]

Donde volvemos a usar la ley de esperanza total. La desigualdad viene
de que la varianza es positiva, o de la desigualdad de Jensen para el
cuadrado.

***** Teorema de Lehmann-ScheffÃ©
Para $T$ suficiente y completo para $\theta$; si $g(\theta)$ admite un estimador insesgado 
de segundo orden $S$, entonces existe el UMVUE de $g(\theta)$ y estÃ¡ dado por:

\[ \mathbb{E} [S \mid T]\]

De otra forma, un estimador insesgado que es funciÃ³n de estimador completo
y suficiente es el UMVUE.

****** DemostraciÃ³n
Por [[*Teorema de RaÃ³-Blackwell][RaÃ³-Blackwell]], sabemos que es un estimador insesgado; y que, dado
cualquier otro estimador insesgado $Q$, tenemos que:

\[ Var[E[Q|T]] \leq Var[Q]\]

Ahora bien, dado otro, tendrÃ­amos:

\[
E\Big[ E[S|T] - E[Q|T] \Big] = 0
\]

Y como $T$ es completo y ambos son dependientes de $T$, eso implica que:

\[P\Big(
E[S|T] - E[Q|T] = 0
\Big) = 1\]

Por lo tanto, ambos son el UMVUE.

***** CÃ¡lculo del UMVUE
Dado $T$ suficiente y completo. Para calcular el UMVUE de $g(\theta)$ podemos:

  1. Buscar un estimador insesgado y de segundo orden cualquiera de $g(\theta)$.
     Entonces $\mathbb{E}[S|T]$ serÃ¡ el UMVUE.
  2. Buscar $h(T)$ tal que $\mathbb{E}_\theta[h(T)] = g(\theta)$, un estimador insesgado que es
     sÃ³lo funciÃ³n de $T$. Se cumplirÃ¡ $\mathbb{E}[h(T)|T] = h(T)$.

**** EstimaciÃ³n eficiente
***** Condiciones de regularidad de FrÃ©chet-Cramer-Rao
Una familia $\{P_\theta \mid \theta\in\Theta\}$ es *regular* en el sentido de FrÃ©chet-Cramer-Rao
si cumple que:

  1. $\Theta$ es intervalo abierto de $\mathbb{R}$.
  2. $\forall \theta,\theta'\in\Theta : \{x \mid f_\theta(x) > 0\} = \{x \mid f_{\theta'}(x) > 0\} = \chi$
  3. Tenemos $f_\theta(x)$ derivable respecto a $\theta$ para todo $x \in \chi$ con:

     \[ \int_\chi \frac{d f_\theta(x)}{d\theta} dx = 
     \frac{d}{d\theta} \int_\chi f_\theta(x) dx = 
     0, \quad \forall \theta\in\Theta\]
   
     O, cuando la distribuciÃ³n es discreta:
   
     \[\sum_\chi \frac{d f_\theta(x)}{d\theta}
     = 
     \frac{d}{d\theta}\sum_\chi f_\theta(x)
     = 
     0\]

***** FunciÃ³n de informaciÃ³n de Fisher
Si $\{P_\theta : \theta \in \Theta\}$ es regular, definimos la *funciÃ³n de informaciÃ³n* asociada
a $X$ como:

\[I_X(\theta) = E_\theta\left[\left( \frac{d}{d\theta} \ln(f_\theta(X))
\right)^2\right]\]

Y la funciÃ³n de informaciÃ³n asociada a una muestra como:

\[
\[I_{X_1,\dots,X_n}(\theta) = 
E_\theta\left[\left( \frac{d}{d\theta}\ln(f_\theta(X_1,\dots,X_n))
\right)^2\right]\]

***** Propiedades de la funciÃ³n de informaciÃ³n
La funciÃ³n de informaciÃ³n tiene como propiedades:

  1. $I_X(\theta) \geq 0$.

  2. En el caso $I_X(\theta) = 0$, $f_\theta(X)$ no depende de $\theta$.

  3. \[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = 0\].

  4. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = I_X(\theta) \].

  5. \[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = 0\].

  6. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = I_{X_1,\dots,X_n}(\theta) \].

  7. Aditividad, $I_{X_1,\dots,X_n}(\theta) = nI_X(\theta)$.
 
****** DemostraciÃ³n
******* Punto 3
Derivando y asumiendo las condiciones de regularidad:

\[
\mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ln f_\theta(x)\right]
=
\int_\chi \left(\frac{\partial}{\partial\theta} \ln f_\theta(x)\right) f_\theta(x)\; dx
=
\int_\chi \frac{\partial}{\partial\theta} f_\theta(x)\;dx
= 0
\]

***** FunciÃ³n de informaciÃ³n bajo Cramer-RaÃ³
Bajo las hipÃ³tesis de regularidad fuertes de FrÃ©chet-Cramer-RaÃ³:

\[
I(\theta) = E_{\theta}\left[
-\frac{\partial^2}{\partial\theta^2} \log f_\theta(X)
\right]
\]

Es decir, debemos exigir que:

\[
\int_X \frac{\partial^2}{\partial\theta^2} f_\theta(x) dx =
\frac{\partial^2}{\partial\theta^2} \int_X  f_\theta(x) dx
\]

****** DemostraciÃ³n
Notando primero la siguiente igualdad:

\[
\frac{\partial^2}{\partial\theta^2}\log f_\theta(x) =
\frac{1}{f_\theta(x)}\frac{\partial^2}{\partial\theta^2}f_\theta(x) -
\left(\frac{\partial}{\partial\theta}\log f_\theta(x)\right)^2
\]

Y simplemente tomamos esperanzas, sabiendo que por las condiciones de
regularidad:

\[
\mathbb{E}\left[
\frac{1}{f_\theta(x)}
\frac{\partial^2}{\partial\theta^2} f_\theta(x)
\right]
=
\frac{\partial^2}{\partial\theta^2}
\int_X f_\theta(x)\;dx 
= 
\frac{\partial^2}{\partial\theta^2}
1
=
0
\]

***** EstadÃ­stico regular
Un estadÃ­stico $T$ es regular en el sentido de FrÃ©chet-Cramer-RaÃ³, si
siendo una distribuciÃ³n discreta:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\sum_{x \in \chi^n} T(x)f_\theta(x) \\&= 
\sum_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x)
\end{aligned}\]

O, siendo una distribuciÃ³n continua:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\int_{x \in \chi^n} T(x)f_\theta(x) \;dx \\&= 
\int_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x) \;dx
\end{aligned}\]

***** Cota de FrÃ©chet-Cramer-RaÃ³
Si $\{P_\theta \mid \theta \in \Theta\}$ es regular, la funciÃ³n de informaciÃ³n se acota
$0 < I_X(\theta) < \infty$, y $T$ es un estadÃ­stico regular, de segundo orden e
insesgado en una funciÃ³n derivable $g(\theta)$, se tiene:

  1. \[Var_\theta[T] \geq \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]

  2. Para todo $\theta \in \Theta$ tal que $g'(\theta) \neq 0$:

     \[Var_\theta[T] = \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]
     
     ssi existe $a(\theta) \neq 0$ tal que:

     \[P_\theta\left(
     \frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = 
     a(\theta)[T(X_1,\dots,X_n) - g(\theta)]
     \right) = 1\]

****** DemostraciÃ³n
******* Primer punto
Llamamos primero:

\[V_\theta = \frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\]

Tenemos que:

  - $Var_\theta(V_\theta) = I_{X_1,\dots,X_n}(\theta)$
  - $\mathbb{E}(V_\theta) = 0$
  - $Cov(T,V_\theta) = \mathbb{E}[TV_\theta] - \mathbb{E}[T]\mathbb{E}[V]$

Calculando:

\[\begin{aligned}
Cov(T,V_\theta) 
&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\right)
f_\theta(x_1,\dots,x_n)\;dx 
\\&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} \int_{\chi^n} 
T(x_1,\dots,x_n)
\left(f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} g(\theta)
\end{aligned}\]

Y finalmente aplicamos la desigualdad de Cauchy-Schwartz a la 
covarianza entre $T,V_\theta$ para tener:

\[
Cov(T,V_\theta) \leq Var[T]Var[V_\theta]
\]
    
***** Estimador eficiente
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con la funciÃ³n de informaciÃ³n acotada 
$0 < I_X(\theta) < \infty$ y $g(\theta)$ funciÃ³n paramÃ©trica derivable. Un estimador $T$ de $g(\theta)$ 
es *eficiente* si es insesgado, regular, y su varianza alcanza la
cota de FrÃ©chet-Cramer-RaÃ³ en todo punto:

\[Var_\theta[T] = \frac{(g'(\theta))^2}{I_{X_1,\dots,X_n}(\theta)},
\qquad \forall\theta \in \Theta\]

***** CaracterizaciÃ³n de estimadores eficientes
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con $0 < I_X(\theta) < \infty$ y $g(\theta)$ funciÃ³n paramÃ©trica
derivable y *no constante*. Un estimador $T$ es eficiente ssi existe un $a(\theta)$
cumpliendo:

  1. \[P_\theta\left(\frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = a(\theta)[T(X_1,\dots,X_n) - g(\theta)]\right) = 1\]

  2. \[I_{X_1,\dots,X_n}(\theta) = a(\theta)g'(\theta)\]

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Si tenemos un $T$ eficiente, tomamos el $T$ de la cota y $g'(\theta)\neq 0$,
de ahÃ­ tenemos la primera igualdad.

***** Ejemplo: distribuciÃ³n binomial
Dada la familia de distribuciones $\{B(k_0,p) \mid p \in (0,1)\}$, veremos que es
regular.

****** Es regular
El intervalo $p \in (0,1)$ es abierto y $\chi = (0,\dots,k_0)$.

Si la expresamos exponencialmente es mÃ¡s fÃ¡cil calcular su derivada
y relacionarla con una esperanza:

\[
f_p(x)
= 
\exp\left\{\log{k_0\choose x} + x \log p + (k_0-x)\log (1-p)\right\}
\]

Tenemos:

\[
\sum_\chi \frac{\partial f_p(x)}{\partial p}
=
\sum_\chi \frac{x-pk_0}{p(1-p)} f_p(x)
=
\mathbb{E}\left[\frac{X-pk_0}{p(1-p)} \right] = 0
\]

****** FunciÃ³n de informaciÃ³n
Desde la derivada podemos calcular la funciÃ³n de informaciÃ³n:

\[
I_{X_1,\dots,X_n}(p) = \frac{nk_0}{p(1-p)}
\]

****** CaracterizaciÃ³n del estimador eficiente
Calcularemos para usar la caracterizaciÃ³n:

\[
\frac{\partial}{\partial p} \ln f_p(x_1,\dots,x_n)
=
\sum \frac{\partial}{\partial p} f_p(x_i)
=
n\frac{\overline{x}-pk_0}{p(1-p)}
\]

AsÃ­, para cumplir la caracterizaciÃ³n, tiene sentido tomar:

  - $g(p) = pk_0$
  - $T(X_1,\dots,X_n) = \overline{X}$
  - $a(p) = \frac{n}{p(1-p)}$

**** EstimaciÃ³n de mÃ¡xima verosimilitud
***** FunciÃ³n de verosimilitud
Para cada realizaciÃ³n muestral se define la funciÃ³n de verosimilitud
de la realizaciÃ³n, $L_{x_1,\dots,x_n} : \Theta \longrightarrow \mathbb{R}^+_0$, como:

\[L(\theta) = f_\theta(x_1,\dots,x_n)\]

***** Estimador de mÃ¡xima verosimilitud
Tenemos $\hat\theta$ estimador de mÃ¡xima verosimilitud de $\theta$ cuando la estimaciÃ³n
asociada a cada realizaciÃ³n muestral maximiza la verosimilitud:

\[L_{x_1,\dots,x_n}\left(\hat\theta(x_1,\dots,x_n)\right)
= \max_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)\]

***** EcuaciÃ³n de mÃ¡xima verosimilitud
El procedimiento habitual para hallar el estimador de mÃ¡xima 
verosimilitud es derivar e igualar:

\[
\frac{\partial}{\partial\theta_j} \ln L_{X_1,\dots,X_n}(\theta_1,\dots,\theta_k)
= 0
\]

NÃ³tese que esto sÃ³lo nos da un punto crÃ­tico.

****** DemostraciÃ³n
Usando simplemente que el logaritmo es creciente y la caracterizaciÃ³n
de los mÃ¡ximos.

***** Propiedades del estimador de mÃ¡xima verosimilitud
Un estimador de mÃ¡xima verosimilitud $\hat\theta$ de $\theta$ cumple:

  1. Consistencia:

     \[\lim_{n \to \infty}\hat\theta(x_1,\dots,x_n) = \theta\]

  2. Normalidad asintÃ³tica. Para $n$ suficientemente grande, sus errores
     pueden aproximarse por una normal:

     \[\sqrt{n}(\hat\theta(x_1,\dots,x_n) - \theta) \leadsto {\cal N}(0,1/I_X(\theta))\]

****** TODO DemostraciÃ³n

***** RelaciÃ³n con estadÃ­sticos suficientes
Si $\{P_\theta \mid \theta \in \Theta\}$ admite estadÃ­stico suficiente $T$, entonces $\hat\theta$ es funciÃ³n
de $T$.

****** DemostraciÃ³n
Por Teorema de factorizaciÃ³n de Fisher-Neyman, tenemos que la
funciÃ³n de distribuciÃ³n se escribirÃ¡ como:

\[
f_\theta(x) = h(x)g_\theta(T)
\]

Entonces para maximizarla habrÃ¡ que maximizar $g_\theta(T)$.

***** RelaciÃ³n con estimadores eficientes
Si $T$ es estimador eficiente de $\theta$, entonces $T$ es el Ãºnico estimador 
mÃ¡ximo verosÃ­mil de $\theta$.

****** TODO DemostraciÃ³n

***** FunciÃ³n de verosimilitud de una funciÃ³n paramÃ©trica
Se define la funciÃ³n de verosimilitud de $g : \Theta \longrightarrow \Lambda$ asociada a
una realizaciÃ³n, $M_{x_1,\dots,x_n} : \Lambda \longrightarrow \mathbb{R}^+_0$, como:

\[M_{x_1,\dots,x_n}(\lambda)
= \sup_{\theta \in g^{-1}(\lambda)} L_{x_1,\dots,x_n}(\theta) \]

***** Estimador de mÃ¡xima verosimilitud de una funciÃ³n paramÃ©trica
SerÃ¡ $\hat\lambda$ estimador mÃ¡ximo verosÃ­mil de $\lambda$ cuando:

\[M_{x_1,\dots,x_n}(\hat\lambda(x_1,\dots,x_n)) 
= \max_{\lambda \in \Lambda} M_{x_1,\dots,x_n}(\lambda) \]

***** Teorema de invarianza de Zenha
Si $\hat\theta$ es estimador mÃ¡ximo verosÃ­mil de $\theta$, entonces $g(\hat\theta)$ es estimador
mÃ¡ximo verosÃ­mil de $g(\theta)$.

****** TODO DemostraciÃ³n
Se cumple:

\[
M(\lambda') = sup_{\theta \in g^{-1}(\lambda')} L(\theta)
\leq
sup_{\theta \in \Theta} L(\theta)
=
L(\hat\theta)
=
M(g(\hat\theta))
\]

**** MÃ©todo de los momentos
***** DescripciÃ³n
El estimador de una funciÃ³n dependiente en los momentos
poblacionales es el mismo dependiendo en los momentos muestrales.

\[g(\theta) = h(m_{\theta,1},\dots,m_{\theta,k}) 
\quad\Rightarrow\quad
\widehat{g(\theta)}(X_1,\dots,X_n) = h(A_1,\dots,A_k)\]

****** Momentos poblacionales
Definimos los momentos poblacionales como:

\[m_{\theta,j} = E_\theta[X^j]\]

****** Momentos muestrales
Definimos los momentos muestrales como:

\[A_j = \frac{1}{n}\sum_{i=1}^n X^j_i\]

**** MÃ©todo de mÃ­nimos cuadrados
***** DescripciÃ³n
Si $X_i$ son las observaciones aleatorias de una magnitud $\varphi(t,\theta)$ con
errores $\varepsilon_i$; es decir:

\[X_i = \varphi(t_i,\theta) + \varepsilon_i\]

Entonces el estimador de mÃ­nimos cuadrados de $\theta$ es el que minimice
la suma de cuadrados de los errores:

\[\sum^n_{i=1}(X_i - \varphi(t_i,\theta))^2\]

*** 5. EstimaciÃ³n por intervalos de confianza
**** Definiciones y mÃ©todos de construcciÃ³n
***** Intervalo de confianza
Para $X \leadsto P_\theta$, un intervalo de confianza $\alpha$ para $\theta$ es un intervalo 
aleatorio $(I_1,I_2)$ tal que para cualquier $\theta \in \Theta$:

\[P_\theta\left(
I_1(X_1,\dots,X_n) \leq \theta \leq I_2(X_1,\dots,X_n)
\right)
\geq 1 - \alpha\]

***** Intervalo de confianza de menor longitud esperada uniformemente
Un interavlo $(I_1,I_2)$ es el de menor longitud esperada uniformemente 
si para cualquier otro $(I_1',I_2')$ al mismo nivel, se tiene:

\[
E_\theta[I_2(X_1,\dots,X_n) - I_1(X_1,\dots,X_n)]
\leq
E_\theta[I_2'(X_1,\dots,X_n) - I_1'(X_1,\dots,X_n)]
\]

***** Intervalos mediante desigualdad de Chevychev
Si $T$ es estimador insesgado de $\theta$ con varianza uniformemente acotada:

  - $E_\theta[T(X_1,\dots,X_n)] = \theta$
  - $Var_\theta[T(X_1,\dots,X_n)] \leq c$

Por lo que por Chevychev tenemos, dado $k>0$, un intervalo de confianza
para $\theta$ al nivel de confianza $1 - c/k^2$:

\[P\left(T - k \leq \theta \leq  T + k \right) \geq 1 - c/k^2\]

****** TODO DemostraciÃ³n

***** Pivote para un parÃ¡metro
Un pivote es una funciÃ³n $T(X_1,\dots,X_n,\theta)$ tal que fijado cualquier $\theta$,
$T(X_1,\dots,X_n,\theta)$ es una variable con distribuciÃ³n independiente de $\theta$.

***** Intervalos obtenidos mediante el mÃ©todo pivotal
Dado un pivote $T$ estrictamente monÃ³tono respecto a $\theta$, y dos valores
$\lambda_1,\lambda_2$, tales que:

\[P_\theta(\lambda_1 < T < \lambda_2) \geq 1 - \alpha\]

Tomamos las soluciones $\hat\theta_1, \hat\theta_2$, cumpliendo $T(X_1,\dots,\hat\theta_1) = \lambda_1$ y
$T(X_1,\dots,\hat\theta_2) = \lambda_2$; y ellas forman un intervalo de confianza:

  - $P_\theta(\hat\theta_1 < \theta < \hat\theta_2) \geq 1 - \alpha$, para $T$ creciente.
  - $P_\theta(\hat\theta_2 < \theta < \hat\theta_1) \geq 1 - \alpha$, para $T$ decreciente.

****** TODO DemostraciÃ³n

***** Un pivote en distribuciones continuas
Si $X$ es continua con $F_\theta$ funciÃ³n de distribuciÃ³n, un pivote es:

\[ T(X_1,\dots,X_n,\theta) = -2 \sum_{i=1}^n \ln F_\theta(X_i) \leadsto \chi^2(2n)\]

****** TODO DemostraciÃ³n
***** Un pivote dado un estadÃ­stico
Sea $S$ un estadÃ­stico de distribuciÃ³n continua con $F^S_\theta$ funciÃ³n de 
distribuciÃ³n. Un pivote es:

\[T(X_1,\dots,X_n,\theta) = F^S_\theta(S(X_1,\dots,X_n)) \leadsto U(0,1)\]

****** TODO DemostraciÃ³n
**** Ejemplos de intervalos de confianza
***** A1. Intervalo para la media de una normal con varianza conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2_0)$. El intervalo para $\mu$ de menor longitud
media uniforme a nivel de confianza $1-\alpha$ serÃ¡:

\[\left(
\overline{X}-z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}},
\overline{X}+z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}}
\right)\]

donde $z_{\alpha/2}$ cumple que \[P\left(Z > z_{\alpha/2}\right) = \alpha/2\] con $Z \leadsto {\cal N}(0,1)$.

****** Pivote
Usamos como pivote a la normalizada:

\[T(X_1,\dots,X_n,\mu) = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \leadsto {\cal N}(0,1)\]

****** Intervalos candidatos
Usando el pivote, tenemos el siguiente candidato a intervalo de
confianza:

\[
1 - \alpha > 
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{\sigma_0 / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}}
\right)
\]

Debiendo tenerse que, si $\Phi$ es la funciÃ³n de distribuciÃ³n de la
normal $\Phi(\lambda_2)-\Phi(\lambda_1) = 1 - \alpha$.

****** Longitud media
Buscamos el que minimice la longitud media:

\[E_\mu \left[
\left( \overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}} \right) -
\left( \overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} \right)
\right] 
= (\lambda_2-\lambda_1)\frac{\sigma_0}{\sqrt{n}}\]

Por lo que tratamos de minimizar $(\lambda_2-\lambda_1)$.

****** MinimizaciÃ³n
Usamos multiplicadores de Lagrange para definir:

\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]

Calculando las derivadas parciales tenemos:

\[\begin{aligned}
-1-\lambda\Phi'(\lambda_1) &= 0\\
1 + \lambda\Phi'(\lambda_2) &= 0
\end{aligned}
\]

Luego debe tenerse $\Phi'(\lambda_1) = \Phi'(\lambda_2)$. Sabiendo que $\Phi'$ es la funciÃ³n
de distribuciÃ³n de la normal, tenemos $\lambda_1 = \pm \lambda_2$. Como deben ser
distintos para cumplir la restricciÃ³n, tenemos $\lambda_1 = -\lambda_2$.

****** ConclusiÃ³n
La restricciÃ³n nos fuerza a $\Phi(\lambda_2) - \Phi(-\lambda_2) = 1 - \alpha$, luego estamos
buscando el $z_{\alpha/2}$ que cumple, para una normalizada $Z$, $P(Z > z_{\alpha/2}) = \alpha/2$.

***** A2. Intervalo para la media de una normal con varianza desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\mu$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ serÃ¡:

\[
\left(
\overline{X} - t_{n-1;\alpha/2}\frac{S}{\sqrt{n}},\,
\overline{X} + t_{n-1;\alpha/2}\frac{S}{\sqrt{n}}
\right)
\]

donde $t_{n-1;\alpha/2}$ cumple que $P(Z \leq t_{n-1;\alpha/2}) = \alpha/2$ con $Z \leadsto T(n-1)$.

****** Pivote
Usaremos como pivote la t de Student:

\[
T = \frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]

****** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1 - \alpha =
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{S / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{S}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{S}{\sqrt{n}}
\right)
\]

Donde, si $\Phi$ es la funciÃ³n de distribuciÃ³n de $t(n-1)$, tenemos
que $1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$.

****** Longitud media
Queremos minimizar la longitud esperada del intervalo:

\[
\mathbb{E}\left[
\left(\overline{X}-\lambda_1\frac{S}{\sqrt{n}}\right) - 
\left(\overline{X}-\lambda_2\frac{S}{\sqrt{n}}\right)
\right]
=
(\lambda_2-\lambda_1)\mathbb{E}\left[
\frac{S}{\sqrt{n}}
\right]
\]

Buscamos por tanto minimizar $\lambda_2-\lambda_1$.

****** MinimizaciÃ³n
Usamos multiplicadores de Lagrange para definir:

\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]

De donde deducimos $\Phi'(\lambda_1) = \Phi'(\lambda_2)$. Como la t de Student es simÃ©trica
y monÃ³tona en cada mitad, tenemos $\lambda_1 = -\lambda_2$.

****** ConclusiÃ³n
Buscamos entonces el $t_{\alpha/2}$ que cumple $P(Z > t_{\alpha/2}) = \alpha/2$.

***** B1. Intervalo para la varianza de una normal con media conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\sigma^2$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ es:

\[
\left(
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;\alpha/2}}
,\quad
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;1-\alpha/2}}
\right)
\]

Donde $\chi^2_{n;\alpha/2}$ cumple que $P(Z > \chi^2_{n;\alpha/2}) = \alpha/2$ para $Z \leadsto \chi^2(n)$.

****** Pivote
Tomamos como pivote a la funciÃ³n:

\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi(n)
\]

****** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1-\alpha = P\left(
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_2}
\leq
\sigma^2
\leq
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_1}
\right)
\]

Donde, si $\Phi$ es la funciÃ³n de distribuciÃ³n de $\chi(n)$, tenemos que
$1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$. Buscamos minimizar $1/\lambda_1-1/\lambda_2$.

****** MinimizaciÃ³n
Usamos minimizadores de Lagrange. Definimos:

\[
F(\lambda_1,\lambda_2) = 
\frac{1}{\lambda_2}-\frac{1}{\lambda_1} + 
\lambda(\Phi(\lambda_2)-\Phi(\lambda_1) - (1-\alpha))
\]

Calculando las derivadas parciales tenemos:

\[
\lambda\Phi'(\lambda_2) - \frac{1}{\lambda_2^2} = 0
\]
\[
\lambda\Phi'(\lambda_1) - \frac{1}{\lambda_1^2} = 0
\]

Y por tanto se minimiza cuando $\Phi(\lambda_1)/\Phi(\lambda_2) = \lambda_2^2/\lambda_1^2$. En la prÃ¡ctica
se usa el intervalo de colas iguales.

****** ConclusiÃ³n
El intervalo de colas iguales nos da $\lambda_1 = \chi^2_{n;1-\alpha/2}$ y $\lambda_2 = \chi^2_{n;\alpha/2}$.

***** B2. Intervalo para la varianza de una normal con media desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\sigma^2$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ es:

\[
\left(
\frac{(n-1)S^2}{\chi^2_{n-1;\alpha/2}}
,\quad
\frac{(n-1)S^2}{\chi^2_{n-1;1-\alpha/2}}
\right)
\]

Donde $\chi^2_{n-1;\alpha/2}$ cumple que $P(Z > \chi^2_{n-1;\alpha/2}) = \alpha/2$ para $Z \leadsto \chi^2(n-1)$.

****** Pivote
Tomamos como pivote:

\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]

***** C1. Intervalo para la diferencia de medias de normales de varianza dada
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\mu_1-\mu_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - 
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
,\quad
\overline{X}-\overline{Y} +
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
\right)
\]

Donde $z_{\alpha/2}$ cumple que $P(Z>z_{\alpha/2}) = \alpha/2$ para $Z\leadsto {\cal N}(0,1)$.

****** Pivote
Usamos como pivote:

\[
\frac
{\overline{X}-\overline{Y}-(\mu_1-\mu_2)}
{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}
\leadsto
{\cal N}(0,1Ã±)
\]

***** C2. Intervalo para la diferencia de medias de normales de varianza igual
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2)$, $Y \leadsto {\cal N}(\mu_2, \sigma^2)$. El intervalo para $\mu_1-\mu_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

donde,

\[
S_p = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S^2_2}{n_1+n_2-2}}
\]

y donde $t_{n_1+n_2-2;\alpha/2}$ cumple que $P(Z > t_{\alpha/2}) = \alpha/2$ con $Z \leadsto t(n_1+n_2-2)$.

****** Pivote
Tomamos como pivote a la funciÃ³n:


\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
S_p
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]

****** Intervalos candidatos
Usando el pivote tenemos el siguiente invervalo de confianza:

\[
1-\alpha = P\left(
\overline{X}-\overline{Y} - \lambda_1S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\leq
\mu_1-\mu_2
\leq
\overline{X}-\overline{Y} + \lambda_2S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

Donde, si $\Phi$ es la funciÃ³n de distribuciÃ³n de $t(n_1+n_2-2)$, tenemos
que $1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

****** MinimizaciÃ³n
Usaremos minimizadores de Lagrange para deducir de nuevo que
$\Phi'(\lambda_1) = \Phi'(\lambda_2)$, por monotonÃ­a y simetricidad, $\lambda_1=\lambda_2$.

****** ConclusiÃ³n
Buscamos entonces el $t_{\alpha/2}$ que cumple $P(Z > t_{\alpha/2}) = \alpha/2$.

***** D1. Intervalo para el cociente de varianzas de normales de media dada
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\sigma_1^2/\sigma_2^2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
F_{n_2,n_1;1-\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
,
F_{n_2,n_1;\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
\right)
\]

donde, $F_{n_2,n_1;\alpha/2}$ cumple que $P(Z > F_{\alpha/2}) = \alpha/2$ con $Z \leadsto F(n_2,n_1)$.

****** Pivote
Tomamos como pivote a la funciÃ³n:

\[
\frac
{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2 / n_2\sigma_2^2}
{\sum_{i=1}^{n_2} (X_i-\mu_1)^2 / n_1\sigma_1^2}
\leadsto
F(n_2,n_1)
\]

****** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1 - \alpha = P
\left(
\lambda_1\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\leq
\frac{\sigma^2_1}{\sigma^2_2}
\leq
\lambda_2\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\right)
\]

Donde, si $\Phi$ es la funciÃ³n de distribuciÃ³n de $F(n_1,n_2)$, tenemos que
$1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

****** MinimizaciÃ³n
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a $\Phi'(\lambda_2) = \Phi'(\lambda_1)$. NÃ³tese que en este caso la distribuciÃ³n no es
simÃ©trica.

****** ConclusiÃ³n
Buscamos entonces:

  - el $F_{n_2,n_1;1-\alpha/2}$ que cumple $P(Z > F) = 1-\alpha/2$.
  - el $F_{n_2,n_1;\alpha/2}$ que cumple $P(Z>F) = \alpha/2$.

***** D2. Intervalo para el cociente de varianzas de normales de media desconocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2_1)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\sigma^2_1/\sigma^2_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
F_{n_2-1,n_1-1; 1-\alpha/2}\frac{S_1^2}{S_2^2}
,
F_{n_2-1,n_1-1; \alpha/2}\frac{S_1^2}{S_2^2}
\right)
\]

donde, $F_{n_2-1,n_1-1; 1-\alpha/2}$ cumple que $P(Z > F_{\alpha/2}) = \alpha/2$ con $Z \leadsto F(n_2-1,n_1-1)$.

****** Pivote
Tomamos como pivote a la funciÃ³n:

\[
\frac{S_2^2/\sigma_2^2}{S_1^2/\sigma_1^2}
\leadsto
F(n_2-1,n_1-1)
\]

****** Intervalos candidatos
Usando el pivote llegamos al siguiente intervalo de confianza:

\[
1-\alpha = P
\left(
\frac{\sigma_1^2}{\sigma_2^2}\lambda_1
\leq
\frac{S_2^2}{S_1^2}
\leq
\frac{\sigma_1^2}{\sigma_2^2}\lambda_2
\right)
\]

Donde, si $\Phi$ es la funciÃ³n de distribuciÃ³n de $F(n_2-1,n_1-1)$, tenemos
que $1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

****** MinimizaciÃ³n
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a $\Phi'(\lambda_2) = \Phi'(\lambda_1)$. NÃ³tese que en este caso la distribuciÃ³n no es
simÃ©trica.

****** ConclusiÃ³n
Buscamos entonces:

  - el $F_{n_2-1,n_1-1;1-\alpha/2}$ que cumple $P(Z > F) = 1-\alpha/2$.
  - el $F_{n_2-1,n_1-1;\alpha/2}$ que cumple $P(Z>F) = \alpha/2$.

**** TODO Intervalos unilaterales
*** 6. Contraste de hipÃ³tesis
**** Planteamiento del problema
***** Problema de contraste de hipÃ³tesis
Dada $(X_1,\dots,X_n)$ una muestra aleatoria simple de $X \leadsto P_\theta$, para
$\theta \in \Theta_0 \cup \Theta_1$, llamamos:

 - *HipÃ³tesis nula*: $H_0 : \theta \in \Theta_0$
 - *HipÃ³tesis alternativa*: $H_1 : \theta \in \Theta_1$

a dos hipÃ³tesis posibles.

***** Test de hipÃ³tesis
El *test de hipÃ³tesis* es un estadÃ­stico $\varphi$ tomando valores en $[0,1]$, que 
da la posibilidad de rechazar $H_0$ dada una realizaciÃ³n muestral. Se 
llama:

  - *Test no aleatorizado*, si toma valores $0,1$.
  - *Test aleatorizado*, si toma valor distinto de $0,1$.

***** Tipos de errores de un test de hipÃ³tesis
Hay dos tipos de erorres:

  - *Error de tipo 1*: Rechazar $H_0$ siendo cierta. Falso negativo.
  - *Error de tipo 2*: Aceptar $H_0$ siendo falsa. Falso positivo.

***** FunciÃ³n de potencia de un test
Dado un test $\varphi$, su funciÃ³n de potencia $\beta_\varphi : \Theta \longrightarrow [0,1]$ se define:

\[\beta_\varphi(\theta) = E_\theta[\varphi(X_1,\dots,X_n)]\]

Que es la probabilidad media de rechazar $H_0$ bajo $P_\theta$.

***** TamaÃ±o del test
El tamaÃ±o del test es $\sup_{\theta \in \Theta_0} \beta_\varphi(\theta)$, la mÃ¡xima probabilidad media de
cometer un error de tipo 1.

***** Nivel de significaciÃ³n de un test
Un test $\varphi$ tiene nivel de significaciÃ³n $\alpha$ si su tamaÃ±o es menor o igual
que $\alpha$. Es decir,

\[
\forall \theta \in \Theta_0, \quad
\beta_\varphi(\theta) =
E_\theta[\varphi(X_1,\dots,X_n)] \leq
\alpha
\]

***** Test uniformemente mÃ¡s potente
Un test con nivel de significaciÃ³n $\alpha$ es uniformemente mÃ¡s potente a 
dicho nivel si para cualquier otro test $\varphi'$ con nivel de significaciÃ³n
$\alpha$, se tiene:

\[\beta_{\varphi'}(\theta) \leq \beta_\varphi(\theta)
\quad \forall \theta \in \Theta_1\]

**** Lema de Neyman-Pearson
***** El problema de contraste
Fijado un nivel de significaciÃ³n, encontrar el test uniformemente mÃ¡s
potente a dicho nivel.

***** Lema de Neyman-Pearson
Sea $X \longrightarrow \{P_{\theta_0}, P_{\theta_1}\}$ y $(X_1,\dots,X_n)$ una muestra aleatoria simple
con funciones de densidad $f_0,f_1$. Consideramos el problema de contraste 
con $H_0 : \theta = \theta_0$ y $H_1 : \theta = \theta_1$.

  1. Cualquier test de la forma:

     \[
     \varphi(\tilde X) = 
     \threepartdef
     {1}{f_1(\tilde X) > kf_0(\tilde X)}
     {\gamma(\tilde X)}{f_1(\tilde X) = kf_0(\tilde X)}
     {0}{f_1(\tilde X) < kf_0(\tilde X)}
     \]
     
     con $k \in \mathbb{R}^+_0$ y $\gamma(X_1,\dots,X_n) \in [0,1]$, es de mÃ¡xima potencia entre todos
     los de nivel de significaciÃ³n $\alpha = E_{\theta_0}[\varphi]$, su tamaÃ±o.
     
  2. Para todo $\alpha \in (0,1]$ existe un test de la forma anterior con
     $\gamma(X_1,\dots,X_n) = \gamma$ constante y tamaÃ±o $\alpha$.

  3. Si $\varphi'$ es de mÃ¡xima potencia al nivel de significaciÃ³n $\alpha = E_{\theta_0}[\varphi']$, 
     entonces $\varphi'$ es de la forma anterior con probabilidad 1 bajo $P_{\theta_0}$ y $P_{\theta_1}$.

  4. El test de mÃ¡xima potencia entre todos los de nivel de significaciÃ³n
     0 es:
     
     \[
     \varphi_0(\tilde X) = \twopartdef
     {1}{f_0(\tilde X) = 0}
     {0}{f_0(\tilde X) > 0}
     \]

****** DemostraciÃ³n
******* Punto 1
Dado otro test $\varphi'$, como toma valores en $[0,1]$ tenemos que:

\[
(\varphi-\varphi')(f_1-kf_0) \geq 0
\]

Podemos entonces integrar para tener:

\[
\int (\varphi(x)-\varphi'(x))(f_1(x)-kf_0(x)) \;dx \geq 0
\]

Conociendo las funciones de potencia $\int \varphi f_i = \beta_\varphi(\theta_i)$ y $\int \varphi' f_i = \beta_{\varphi'}(\theta_i)$ , 
tenemos:

\[
\beta_{\varphi}(\theta_1) - \beta_{\varphi'}(\theta_1) +
k(\beta_{\varphi'}(\theta_0) - \beta_{\varphi}(\theta_0))
\geq 0\]

Usando ahora que $\beta_{\varphi'}(\theta_0) \leq \beta_\varphi(\theta_0) = \alpha$, tenemos que $\beta_\varphi(\theta_1) \geq \beta_{\varphi'}(\theta_1)$.
AsÃ­, nuestro test es el mÃ¡s potente uniformemente.

******* Punto 3
Cuando es de mÃ¡xima potencia, se da el caso de igualdad en la Ãºltima
ecuaciÃ³n, que lleva el caso de igualdad a la integral. Como es una
integral de tÃ©rminos positivos, debe ser distinta de cero sÃ³lo en
un conjunto de medida nula.

**** TODO DescripciÃ³n mediante p-valores
**** Test de la razÃ³n de verosimilitudes
***** Test de la razÃ³n de verosimilitudes
Sea $(X_1,\dots,X_n) \in \chi^n$ una muestra aleatoria simple de $X \leadsto \{P_\theta \mid \theta \in \Theta_0 \cup \Theta_1\}$.
El test de razÃ³n de verosimilitudes para el problema de contraste con
$H_0 : \theta \in \Theta_0$ y $H_1 : \theta \in \Theta_1$; se define como:

\[
\varphi(X_1,\dots,X_n) = \twopartdef
{1}{\lambda(X_1,\dots,X_n) < c}
{0}{\lambda(X_1,\dots,X_n) \geq c}
\]

donde se define:

\[\lambda(x_1,\dots,x_n) = \frac
{\sup_{\theta \in \Theta_0} L_{x_1,\dots,x_n}(\theta)}
{\sup_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)}
\]

siendo $L$ la funciÃ³n de verosimilitud y $c \in (0,1]$ una constante que se 
determina imponiendo el tamaÃ±o o nivel de significaciÃ³n requerido.

**** Dualidad entre tests de hipÃ³tesis y regiones de confianza
***** Dualidad
Sea $X \leadsto \{P_\theta \mid \theta \in \Theta\}$. Para cada $\theta_0 \in \Theta$ consideramos un conjunto $A(\theta_0) \subseteq \chi^n$
y para cada relizaciÃ³n se define:

\[
\varphi_{\theta_0}(x_1,\dots,x_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } (x_1,\dots,x_n) \notin A(\theta_0) \\
0 & \mbox{if } (x_1,\dots,x_n) \in A(\theta_0) \\
\end{array} 
\right.
\]

Y con esto se define:

\[
S(x_1,\dots,x_n) = \{\theta\in \Theta \mid (x_1,\dots,x_n) \in A(\theta)\}
\]

Cada uno de los tests $\varphi_{\theta_0}$ tiene nivel de significaciÃ³n $\alpha$ ssi $S$ es una
regiÃ³n de confianza para $\theta$ a nivel de confianza $1-\alpha$.

**** Ejemplos
***** Contrastes sobre la media de una normal con varianza conocida
****** HipÃ³tesis: valor de la media
Si tomamos $H_0:\mu=\mu_0$ y $H_1:\mu\neq\mu_0$, podemos crear un [[*Test de la razÃ³n de verosimilitudes][TRV]] como:

\[
\varphi(X_1,\dots,X_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| > z_{\alpha/2} \\
0 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| \leq z_{\alpha/2} \\
\end{array} 
\right.
\]

******* CÃ¡lculo
Sabiendo que la funciÃ³n de verosimilitud es:

\[
L_{x_1,\dots,x_n}(\mu) = 
\frac{1}{(\sigma_0^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma_0^2}
\]

Calculamos el test:

\[
\lambda = 
\frac{\sup_{\mu = \mu_0} L(\mu)}{\sup_{\mu\in\mathbb{R}} L(\mu)} = 
\frac{L(\mu_0)}{L(\overline{x})} =
\exp\left\{\frac{-n(\overline{x}-\mu_0)^2}{2\sigma^2_0}\right\}
\]

Y podemos tomar raÃ­ces para tener otro test equivalente que, al
seguir una distribuciÃ³n normal, podemos ajustar para tener el
parÃ¡metro $\alpha$ pedido.

****** HipÃ³tesis: media menor que un valor
Si tomamos $H_0 : \mu \leq \mu_0$ y $H_1 : \mu > \mu_0$, podemos crear un TRV como:

\[
\varphi(X_1,\dots,X_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} > z_\alpha \\
0 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \leq z_\alpha \\
\end{array} 
\right.
\]

******* CÃ¡lculo
Si en este caso calculamos la $\lambda$ tenemos que:

\[
\lambda(x_1,\dots,x_n) = 
\frac{\sup_{\mu\leq\mu_0} L(\mu)}{L(\overline{x})} =
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \leq 0 \\
\frac{L(\mu_0)}{L(\overline{x})} 
& \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq 0 \\
\end{array} 
\right.
\]

****** HipÃ³tesis: media mayor que un valor
Si tomamos $H_0 : \mu \geq \mu_0$ y $H_1 : \mu < \mu_0$, podemos crear un TRV como:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < z_{1-\alpha} \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq z_{1-\alpha}
\end{array} 
\right.
\]

******* CÃ¡lculo
Si en este caso calculamos la $\lambda$ tenemos que:

\[
\lambda(x_1,\dots,x_n) 
=
\frac{\sup_{\mu\geq\mu_0} L(\mu)}{L(\overline{x})}
=
\left\{\begin{array}{ll} 
1& \mbox{if } \overline{x} \geq \mu_0 \\
\frac{L(\mu_0)}{L(\overline{x})}& \mbox{if } \overline{x} \leq \mu_0
\end{array} 
\right.
\]

DÃ¡ndonos un test:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \lambda(x_1,\dots,x_n) < c \\
0 & \mbox{if } \lambda(x_1,\dots,x_n) \geq c
\end{array} 
\right.
=
\left\{\begin{array}{ll} 
0 & \mbox{if } \overline{x} \geq \mu_0 \\
1 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} < c \\
0 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} \geq c
\end{array} 
\right.
\]

Las dos primeras condiciones colapsan cuando tomamos la raÃ­z y
comparamos con ella:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < c \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq \min(0,c)
\end{array} 
\right.
\]

Ahora ajustamos la $c$ para que nos dÃ© la significancia $\alpha$:

\[
\alpha = 
\sup_{\mu \geq \mu_0} 
P\left( \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} < c' \right)
=
\sup_{\mu \geq \mu_0}
P\left( \frac{\overline{X}-\mu}{\sigma_0/\sqrt{n}} < \frac{\mu_0-\mu}{\sigma_0/\sqrt{n}} + c' \right)
\]

Tomamos entonces como $c' = z_{1 - \alpha}$. NÃ³tese que es negativo.

***** Contrastes sobre la varianza de una normal con media conocida
****** HipÃ³tesis: valor de la varianza
Si tomamos $H_0 : \sigma^2 = \sigma_0^2$ y $H_1 : \sigma^2 \neq \sigma_0^2$, creamos un TRV como:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < \chi^2_{n;1-\alpha/2} 
   \mbox{ Ã³ } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > \chi^2_{n;\alpha/2} \\
0& \mbox{if } \chi^2_{n;1-\alpha/2} \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq \chi^2_{n;\alpha/2}
\end{array} 
\right.
\]

******* CÃ¡lculo
Sabiendo que la funciÃ³n de verosimilitud es:

\[
L_{x_1,\dots,x_n}(\sigma) = 
\frac{1}{(\sigma^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma^2}
\]

Usamos que el estimador mÃ¡ximo verosÃ­mil de $\sigma^2$ es:

\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\mu_0)^2}{n}
\]

Calculamos el test:

\[
\lambda = 
\frac{\sup_{\sigma = \sigma_0} L(\sigma)}{\sup_{\sigma\in\mathbb{R}} L(\sigma)} = 
\frac{L(\sigma_0)}{L(\widehat\sigma)} =
\left(\frac{\widehat\sigma^2}{\sigma_0^2}\right)^{n/2}
\exp\left\{
\frac{-n\widehat\sigma_0^2}{2\sigma_0^2}+\frac{n}{2}
\right\}
\]

La funciÃ³n $xe^{-x}$ tiene un mÃ¡ximo antes de ir hacia $-\infty$ por ambos
lados. AsÃ­, podemos escribir:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < c_1 
   \mbox{ Ã³ } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > c_2 \\
0& \mbox{if } c_1 \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq c_2
\end{array} 
\right.
\]

NÃ³tese que sigue una distribuciÃ³n $\chi^2(n)$ como suma de $n$ normales.

***** Contrastes sobre la varianza de una normal con media desconocida
Se tendrÃ¡ como estimador mÃ¡ximo verosÃ­mil a:

\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n}
\]

*** 7. TeorÃ­a general de modelos lineales
**** Modelo lineal general y modelo de Gauss Markov
***** Modelo lineal general
El modelo general lineal queda descrito por:

\[\mathbf{Y = X\beta + \varepsilon}\]

****** Vector observable
$Y = (Y_1,\dots,Y_n)$ es un vector aleatorio observable.

****** Matriz de diseÃ±o
Una matriz conocida $X$ de dimensiÃ³n $n \times k$, cuyo rango determina el 
rango del modelo.

****** Vector de efectos
Un vector desconocido $\beta = (\beta_1,\dots,\beta_k)$.

****** Vector de errores
Un vector aleatorio no observable $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)$ representando el 
error entre $Y$ y $X\beta$.

***** Modelo de Gauss-Markov
Modelo lineal donde las componentes del vector de errores son variables
aleatorias de segundo orden, centradas, homocedÃ¡sticas (igual varianza)
e incorreladas:

  - $E[\varepsilon_i] = 0$
  - $E[\varepsilon_i^2] = \sigma^2$
  - $E[\varepsilon_i\varepsilon_j] = 0$

****** Enunciado vectorial
Las condiciones sobre el vector de errores equivalen a exigir:

  - $E[\varepsilon] = 0$
  - $E[\varepsilon\varepsilon^T] = \sigma^2 I_{n \times n}$

****** Objetivo del modelo
Inferir $\beta$ y $\sigma^2$ a partir de observaciones del vector $Y$.

**** EstimaciÃ³n de mÃ­nimos cuadrados del vector de efectos
***** Modelo
En el modelo de Gauss-Markov queremos minimizar la suma de
cuadrados de los errores:

\[S^2(\beta) = 
\sum^n_{i=1} \varepsilon^2_i =
\|Y - X\beta\|^2\]

****** MinimizaciÃ³n
Para minimizarlo, calculamos la derivada:

\[\frac{\partial}{\partial \beta_h} S^2(\beta) = 
-2 x_{ih} \sum^n_{i=1} \left(
Y_i - \sum^k_{j=1} x_{ij}\beta_j
\right) = 0
\]

Y obtenemos las ecuaciones normales siguientes:

\[
\sum^n_{i=1} Y_i x_{ih} = \sum^n_{i=1}\sum^k_{j=1} x_{ij}x_{ih}\beta_j
\]

Que pueden expresarse matricialmente como:

\[X^TY = (X^TX)\beta\]

***** Estimador de mÃ­nimos cuadrados de beta
Llamamos $\widehat\beta$ al estimador de mÃ­nimos cuadrados de $\beta$.

  - Existencia: existe al menos un estimador de mÃ­nimos cuadrados de $\beta$.
  - Unicidad: garantizada cuando el modelo es de rango mÃ¡ximo por tenerse
    la soluciÃ³n \[\widehat\beta(Y) = (X^TX)^{-1}X^TY\].

**** Funciones estimables
***** FunciÃ³n lineal estimable
Una $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable si admite un estimador insesgado,
lineal en las componentes de $Y$. Es decir:

\[\exists \widehat\psi(Y) = c_1Y_1 + \dots + c_nY_n\]

tal que $E[\widehat\psi(Y)] = \psi(\beta)$.

***** Teorema de Gauss-Markov
Si $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable, admite un Ãºnico UMVUE. 
Dicho estimador es:

\[\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y) \]

Donde $\widehat{\beta}(Y)$ es un estimador de mÃ­nimos cuadrados de $\beta$.

****** TODO DemostraciÃ³n

***** Propiedades del estimador de mÃ­nimos cuadrados en modelos de rango mÃ¡ximo
Sea $\widehat\beta(Y) = (X^TX)^{-1}X^TY$ el estimador de mÃ­nimos cuadrados en un modelo 
de rango mÃ¡ximo.

  1. $\widehat\beta_j(Y)$ es el estimador lineal insesgado de mÃ­nima varianza de $\beta_j$.
  2. Las varianzas y covarianzas vienen dadas: $Cov(\widehat\beta(Y)) = \sigma^2(X^TX)^{-1}$.
  3. Toda funciÃ³n lineal de las componentes de $\beta$ es estimable con 
     estimador lineal insesgado de mÃ­nima varianza
     $\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y)$.

****** TODO DemostraciÃ³n

**** Modelo estimado
***** Modelo estimado
Siendo $\widehat\beta$ el estimador de mÃ­nimos cuadrados, llamamos:

  - Modelo estimado: $\widehat Y = X\widehat\beta$
  - Residuos mÃ­nimo-cuadrÃ¡ticos: $R = Y - X\widehat\beta$

***** Propiedades del modelo estimado
El modelo estimado cumple:

  1. $\widehat Y_i$ es el estimador lineal insesgado de mÃ­nima varianza de $E[Y_i]$.
  2. Los residuos son centrados $E[R_i] = 0$.
  3. El vector de residuos es ortogonal al vector estimado:

     \[X^TR = 0,\quad \widehat{Y}^TR = 0\]

***** Varianza residual
Siendo $r$ el rango de $X$, la *varianza residual* es un estimador
insesgado de $\sigma^2$:

\[
S^2_R = \frac{1}{n-r}\sum_{i=1}^n R_i^2 = \frac{1}{n-r}\|Y-X\widehat\beta\|^2
\]

**** Inferencia bajo hipÃ³tesis de normalidad
***** HipÃ³tesis de normalidad
La hipÃ³tesis de normalidad asume que los errores se distribuyen
bajo una distribuciÃ³n normal:

\[
Y_i = \sum_{j=1}^k x_{ij}\beta_j + \varepsilon_i 
\leadsto 
{\cal N}\left(\sum_{j=1}^k x_{ij}\beta_j, \sigma^2 \right)
\]

Para $Y_1,\dots,Y_n$ independientes.

****** Equivalentemente
Podemos expresar los errores como:

\[\varepsilon \leadsto {\cal N}(0,\sigma^2)\]

***** FunciÃ³n de mÃ¡xima verosimilitud
La funciÃ³n de mÃ¡xima verosimilitud bajo la hipÃ³tesis de normalidad queda
como:

\[
L_y(\beta,\sigma^2) = 
\frac{1}{(2\pi)^{n/2}\sigma^n}
\exp\left\{
-\frac{\sum_{i=1}^n(y_i - \sum_{j=1}^k x_{ij}\beta_j)^2}{2\sigma^2}
\right\}
\]

***** Estimadores mÃ¡ximo verosÃ­miles de efectos
Los estimadores mÃ¡ximo verosÃ­miles de $\beta$ son $\widehat\beta$, estimadores de mÃ­nimos
cuadrados.

****** TODO DemostraciÃ³n

***** Estimador mÃ¡ximo verosÃ­mil de la varianza
El estimador mÃ¡ximo verosÃ­mil de la varianza es:

\[
\widehat\sigma^2 = \frac{1}{n}\sum_{i=1}^n R_i^2 = \frac{n-r}{n}S^2_R
\]

*** 8. Inferencia Bayesiana
# ##
# Este capÃ­tulo se ha escrito siguiendo los apuntes de estadÃ­stica
# de AndrÃ©s Herrera, Nuria RodrÃ­guez, Javier Poyatos, MarÃ­a del Mar Ruiz y 
# Juan Luis SuÃ¡rez.
#
# Pueden consultarse los apuntes originales en:
#   https://github.com/andreshp/math-notes/tree/master/StatisticalInference
# ##

**** 8.1. IntroducciÃ³n
***** Ley de la probabilidad total
La ley de la probabilidad total establece, para $A_i$ una particiÃ³n del
espacio de sucesos:

\[
P(B) = \sum_{i=1}^n P(B|A_i)P(A_i)
\]

***** Teorema de Bayes
Para un espacio de probabilidad $(\Omega,{\cal A},P)$, si tenemos una particiÃ³n dada
por $A_1,\dots,A_n$ con probabilidad no nula:

\[P(A_i|B) 
=
\frac{P(A_i \cap B)}{P(B)} 
= 
\frac{P(B|A_i)P(A_i)}{P(B)}
\]

***** DistribuciÃ³n a priori
Sea $X$ variable aleatoria con distribuciÃ³n $f(x|\theta)$, con $\theta \in \Theta$. A una 
distribuciÃ³n $\pi(\theta)$ establecida con informaciÃ³n previa sobre $\theta$ se le
llama *distribuciÃ³n a priori*.

***** DistribuciÃ³n condicionada
Dada una muestra $\tilde{X} = (X_1,\dots,X_n)$ y una distribuciÃ³n a priori $\pi(\theta)$,
tenemos una distribuciÃ³n conjunta con funciÃ³n de densidad:

\[
f(\tilde x,\theta) = f(\tilde x|\theta)\pi(\theta)
\]

***** DistribuciÃ³n marginal
La distribuciÃ³n marginal de $\tilde X$ la definimos como:

\[
m(\tilde x) 
= 
\int_{\Omega} f(\tilde x,\theta) d\theta
=
\int_{\Omega} f(\tilde x|\theta) \pi(\theta) d\theta
\]

***** DistribuciÃ³n a posteriori
Dada una realizaciÃ³n de la muestra y una distribuciÃ³n a priori, definimos
una distribuciÃ³n a posteriori como:

\[
\pi(\theta|\tilde x) = 
\frac{f(\tilde x|\theta)\pi(\theta)}{m(\tilde x)} =
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
\]

**** 8.2. EstadÃ­stica clÃ¡sica
Se destacan las siguientes diferencias con la estadÃ­stica clÃ¡sica.
En la estadÃ­stica clÃ¡sica:

  1. La probabilidad se limita a sucesos con frecuencias relativas.
  2. El parÃ¡metro $\theta$ es fijo, completamente desconocido.
  3. Se usan estimadores de mÃ¡xima verosimilitud o insesgados.
  4. Los tests de hipÃ³tesis se construyen fijando un tamaÃ±o $\alpha$.

Mientras que en la estadÃ­stica bayesiana:
  
  1. La probabilidad se puede establecer previa a cualquier suceso.
  2. El parÃ¡metro $\theta$ es una variable aleatoria.
  3. El mÃ©todo de muestreo es irrelevante.
  4. Podemos calcular la probabilidad de que una hipÃ³tesis sea cierta.

**** 8.3. Familias conjugadas
***** Familia conjugada
Sea ${\cal F} = \{\pi_i(\theta) \mid i \in I\}$ una familia de distribuciones a priori. Se llama
*conjugada* respecto a una familia de densidades $P = \{f(x|\theta) \mid \theta \in \Theta\}$ si
para cada $\pi(\theta) \in {\cal F}$ y $f(x\mid \theta) \in P$, se tiene que $\pi(\theta\mid \tilde x) \in {\cal F}$.

***** Lema de caracterizaciÃ³n de familias conjugadas
Para $\pi(\theta),\Pi(\theta) \in {\cal F}$, equivalen:

  1. $f(\tilde x|\theta)\pi(\theta) \propto \Pi(\theta)$
  2. $\pi(\theta|\tilde x) = \Pi(\theta)$

****** DemostraciÃ³n
******* Primera implicaciÃ³n
Por definiciÃ³n:

\[
\pi(\theta|\tilde x) 
=
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
=
\frac{M\Pi(\theta)}{M \int_\Omega \Pi(\theta) d\theta}
=
\Pi(\theta)
\]

Usando que $\int_\Omega \Pi = 1$ por ser distribuciÃ³n.

******* Segunda implicaciÃ³n
Por definiciÃ³n:

\[
f(\tilde x|\theta)\pi(\theta) = \Pi(\theta)\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta
\]

***** CaracterizaciÃ³n de familias conjugadas
Una familia de distribuciones a priori ${\cal F}$ es conjugada respecto a ${\cal P}$ ssi
el producto de cualesquiera dos distribuciones de ambas familias vuelve
a ser una distribuciÃ³n de la familia de distribuciones a priori, salvo
alguna constante.

\[\forall f \in {\cal P}, \pi \in{\cal F}: \exists k:\quad 
kf(x|\theta)\pi(\theta) \in {\cal F}\]

***** Ejemplos de familias conjugadas
****** Beta para Bernoulli
La familia de distribuciones Beta es una familia conjugada para:

  - distribuciones de Bernoulli.
  - distribuciones binomiales.
  - distribuciones binomiales negativas.

Se tiene:

  - $X \leadsto B(n,\theta)$
  - $\theta \leadsto \beta(p,q)$
  - $\theta|x \leadsto \beta(x+p,n-x+q)$

****** Gamma para Poisson
La familia de distribuciones Gamma es una familia conjugada para
distribuciones de Poisson.

Se tiene:

  - $X\leadsto Poi(\theta)$
  - $\theta \leadsto \Gamma(\alpha,\beta)$
  - $\theta|\tilde x \leadsto \Gamma(\sum x_i + \alpha, n + \beta)$

****** Normales para normales con varianza conocida
La familia de distribuciones normales es conjugada para las 
distribuciones normales de varianza conocida.

Se tiene:

  - $X \leadsto {\cal N}(\mu,\sigma^2)$
  - $\mu \leadsto {\cal N}(\eta,\tau)$
  - $\mu|\tilde x \leadsto {\cal N}\left(\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}, \frac{\sigma^2\tau^2}{n\tau^2+\sigma^2}\right)$

****** Dirichlet para multinomiales
La familia de distribuciones de Dirichlet es conjugada para la
familia de distribuciones multinomiales.

Se tiene:

  - $X_1,\dots,X_n \leadsto Multi(\theta_1,\dots,\theta_k)$
  - $\theta_1,\dots,\theta_k \leadsto Dir(\alpha_1,\dots,\alpha_k)$
  - $\theta_1,\dots,\theta_n|x_1,\dots,x_n \leadsto Dir(x_1+\alpha_1,\dots,x_k+\alpha_k)$

**** 8.4. Distribuciones objetivas
***** DistribuciÃ³n de Jeffreys
Para una familia $\{f(x|\theta) \mid \theta\in\Theta\}$, la distribuciÃ³n de Jeffreys se define 
como:

\[\pi^J(\theta) \propto \sqrt{{\cal I}_X(\theta)}\]

para la informaciÃ³n de Fisher.

**** 8.5. Convergencia de distribuciones a posteriori
***** Convergencia en un espacio paramÃ©trico discreto
Sea $\Theta = \{\theta_1,\dots,\theta_k\}$, cuando el tamaÃ±o de la muestra diverge, la distribuciÃ³n
a posteriori degenera en $\theta_0$, el valor verdadero del parÃ¡metro.

\[
\pi(\theta\mid X_1,\dots,X_n) 
\overset{P_{\theta_0}}{\underset{n \to \infty}\longrightarrow} \theta_0
\]

NÃ³tese que los $\theta_i$ deben generar distribuciones distintas para aplicar
este resultado.

****** DemostraciÃ³n
Dada una distribuciÃ³n a priori $\pi$, llamamos $\pi(\theta_j) = p_j \in [0,1]$. Llamamos
$\theta_t$ al verdadero parÃ¡metro, y tomamos $X_1,\dots,X_n$ con la distribuciÃ³n
dada por $f(x|\theta_t)$.

\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n f(X_j|\theta_i)}
{\displaystyle \sum_{r=1}^k\left(\prod_{j=1}^n f(X_j|\theta_r) \right) p_r}
\]

Si multiplicamos por $\prod_{j=1}^n f(X_j|\theta_t)$ tenemos:

\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}}
{\displaystyle \sum_{r=1}^k\left(
\prod_{j=1}^n \frac{f(X_j|\theta_r)}{f(X_j|\theta_t)}
\right) p_r}
\]

Tomando logaritmos estudiamos las variables aleatorias $Z_j = \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}$,
que son i.i.d. y por la Ley fuerte de los grandes nÃºmeros, tenemos que
converge casi seguramente respecto a la probabilidad que define $\theta_t$:

\[
\frac{1}{n}\sum_{j=1}^n \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
\mathbb{E}\left[\log \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)} \right]
\]

Que ademÃ¡s sabemos (no trivialmente) que es negativo. Tenemos entonces
por continuidad del logaritmo que:

\[
\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
0
\]

Aplicando esto a la probabilidad a posteriori tenemos que sÃ³lo converge
a uno en el valor $\theta_t$ y converge a cero en todos los demÃ¡s.

***** Nota: Influencia de la distribuciÃ³n a priori
NÃ³tese que la distribuciÃ³n a priori ha sido independiente de la 
convergencia a la distribuciÃ³n a posteriori degenerada.

***** Nota: Estimadores bayesianos
Los modelos bayesianos asignan probabilidad 1 a la hipÃ³tesis correcta
cuando el tamaÃ±o de la muestra diverge.

**** 8.6. Test de hipÃ³esis bayesianos
***** Probabilidad a posteriori de un modelo
Dados dos modelos $M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}$ y $M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}$,
la probabilidad de que se cumpla el primero condicionada a una muestra es:

\[
\pi(M_1|x) = \frac{\pi(M_1)m(x|M_1)}{\pi(M_1)m(x|M_1) + \pi(M_2)m(x|M_2)}
\]

****** Modelos
Cada modelo $M : \{f(x|\theta,M), \pi(\theta|M), \pi(M)\}$ viene dado por:

  1. Una funciÃ³n de distribuciÃ³n condicionada a cada parÃ¡metro $\theta$.
  2. Una probabilidad para cada parÃ¡metro, condicionada al modelo.
  3. Probabilidad de que se cumpla el modelo.

Necesitamos $\pi(M_1)+\pi(M_2) = 1$ en el caso de comparar esos dos modelos.

***** Factor de Bayes
Dados dos modelos $M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}$ y $M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}$,
Definimos el factor de Bayes como:

\[
B_{21}(x) = \frac{m(x|M_2)}{m(x|M_1)}
\]

cociente entre distribuciones marginales.

Cuanto mÃ¡s alto es, mÃ¡s baja es la probabilidad a posteriori del modelo $M_1$.

***** MÃ©todo de Leamer: motivaciÃ³n
Si usamos distribuciones impropias para realizar tests de hipÃ³tesis y
las multiplicamos por coeficientes para que sean integrables, el factor
de Bayes se verÃ­a afectado arbitrariamente por estos coeficientes.

***** MÃ©todo de Leamer: muestras de entrenamiento
Una *muestra de entrenamiento* $\tilde x_1 \subset \tilde x$ es una sublista de la muestra original.
Se llama *propia* si $0 < m(\tilde x_1 | M) < \infty$. Se llama *minimal* si es propia y
ninguna sublista suya lo es.

***** MÃ©todo de Leamer
Dada una muestra de entrenamiento, sabemos que $\pi(\theta|M)f(\tilde x_1|\theta,M)$
integrarÃ¡ y podremos usarlo como distribuciÃ³n a priori.

Interesa utilizar una muestra minimal para que se pierdan el menor
nÃºmero de elementos en la muestra para el test de hipÃ³tesis.

**** TODO 8.7. Probabilidades subjetivas
*** Ejercicios
**** Tema 1. IntroducciÃ³n a la inferencia estadÃ­stica
***** Ejercicio 1
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X$. 
Dar el espacio muestral y calcular la funciÃ³n masa de probabilidad 
de $(X_1,\dots,X_n)$ en cada uno de los siguientes casos:

  1. $X \longrightarrow \{{\cal B}(k_0,p); p \in (0,1)\}$ binomial
  2. $X \longrightarrow \{{\cal P}(\lambda); \lambda\in\mathbb{R}^+\}$ Poisson
#+end_statement

****** Punto 1
El espacio muestral es $\{0,1,\dots,k_0\}^n$, una palabra $k_0\text{-aria}$ de $n$ letras. 
Usando independencia:

\[P(x_1,\dots,x_n) = \prod P(x_i) 
= \prod_{i=1}^n \left({k_0 \choose x_i} p^{x_i}(1-p)^{k_0-x_i} \right)\]
****** Punto 2
El espacio muestral es $\mathbb{N}^n$, palabras en los naturales.
Usando independencia:

\[P(x_1,\dots,x_n) = \prod P(x_i) = \prod_{i=0}^n e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\]

***** Ejercicio 2
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X$. Dar el espacio
muestral y calcular la funciÃ³n masa de probabilidad de $(X_1,\dots,X_n)$ en cada uno de
los siguientes casos:

  1. $X \longrightarrow \{U(a,b); a,b\in\mathbb{R}; a < b\}$ uniforme
  2. $X\longrightarrow \{{\cal N}(\mu,\sigma^2)\}$ normal
#+end_statement
****** Punto 1
El espacio muestral aquÃ­ es $[a,b]^n$, donde por independencia tengo como funciÃ³n
de densidad:

\[f(x_1,\dots,x_n) = \prod f(x_i) = \left(\frac{1}{b-a}\right)^n\]

****** Punto 2
El espacio muestral es $\mathbb{R}^n$, siendo la funciÃ³n de densidad:

\[f(x_1,\dots,x_n) = 
\prod_{i=0}^n \frac{1}{\sigma\sqrt{2\pi}} 
e^{-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2}\]

***** Ejercicio 4
#+begin_statement
Se dispone de una muestra aleatoria simple de tamaÃ±o 40 de una
distribuciÃ³n exponencial de media 3, Â¿cuÃ¡l es la probabilidad de que
los valores de la funciÃ³n de distribuciÃ³n muestral y la teÃ³rica, en
$x=1$, difieran menos de 0.01?  Aproximadamente, Â¿cuÃ¡l debe ser el
tamaÃ±o muestral para que dicha probabilidad sea como mÃ­nimo 0.98?
#+end_statement

****** Probabilidad de que difieran
Tenemos que $nF^\ast_X(1) \leadsto {\cal B}(n,F(1))$, luego podemos calcular la probabilidad
como:

\[\begin{aligned}
P\Big( F(1) - 0.01 < F^\ast(1) < F(1) + 0.01 \Big) = \\
P\Big( 10.93 < 40F^\ast(1) < 11.73 \Big) = \\
P\Big(10 < 40F^\ast(1) < 12) =\\
P\Big(11 = 40F^\ast(1)) =\\
{40 \choose 11} F(1)^{11}(1-F(1))^{40-11} \approx\\
0.1318
\end{aligned}\]

Sabiendo que $F(1) = 1 - e^{-1/3} \approx 0.283$ y que $F^\ast$ es variable discreta.

******* CÃ¡lculos
#+BEGIN_SRC R :results output
f1 = 1-exp(-1/3)
f1
n = 40
n*(f1 + 0.01)
n*(f1 - 0.01)
dbinom(11,n,0.3)
#+END_SRC

#+RESULTS:
: [1] 0.2834687
: [1] 11.73875
: [1] 10.93875
: [1] 0.1318644

****** TODO TamaÃ±o muestral
Llamamos $\sqrt{\frac{1}{n}F(1)(1-F(1))} = \sigma_n$, y esta vez aplicamos el Teorema 
Central del LÃ­mite para tener que:

\[\frac{F^\ast(1) - F(1)}{\sigma_n} \leadsto {\cal N}(0,1)\]

Lo que buscamos es que:

\[\begin{aligned}
P\Big( -0.01 < F^\ast(1)-F(1) < 0.01 \Big) > 0.98 \\
P\Big( \frac{-0.01}{\sigma_n} < \frac{F^\ast(1)-F(1)}{\sigma_n} < \frac{0.01}{\sigma_n} \Big) > 0.98 \\
\end{aligned}\]

Dada $\Phi$ funciÃ³n de distribuciÃ³n de la normal tipificada, 
tenemos que:

\[\begin{aligned}
\Phi\left(\frac{0.01}{\sigma_n}\right) -
\Phi\left(\frac{-0.01}{\sigma_n}\right) > 0.98 \\
2\Phi\left(\frac{0.01}{\sigma_n}\right) > 0.98 +1 \\
1 -\Phi\left(\frac{0.01}{\sigma_n}\right) < 0.01
\end{aligned}\]

Usando la tabla de la normal, tenemos:

\[\frac{0.01}{\sigma_n} \geq 2.33\]

Desde donde calculamos:

\[n = 10978\]

# Esto debe estar mal

******* CÃ¡lculos
#+BEGIN_SRC R :results output
# Lookup on the normal distribution table
qnorm(1-0.01)
#+END_SRC

#+RESULTS:
: [1] 2.326348

***** Ejercicio 7
#+begin_statement
Dada una muestra aleatoria simple $(X_1,\dots,X_n)$ de una variable $X$, obtener
la distribuciÃ³n en el muestreo de $\overline{X}$ en los casos:

  1. $X \leadsto B(1,p)$
  2. $X\leadsto P(\lambda)$
  3. $X \leadsto exp(\lambda)$
#+end_statement

****** Punto 1
Por independencia y suma de binomiales:

\[
\overline{X} \leadsto \frac{1}{n}B(n,p)
\]

NÃ³tese que deja de ser una binomial.

****** Punto 2
Por independencia y suma de Poisson:

\[
\overline{X} \leadsto \frac{1}{n}Poi(n\lambda)
\]

****** Punto 3
Por independencia y suma de Gammas:

\[
\overline{X} = \frac{1}{n}\Gamma(n,\lambda)
\]

***** Ejercicio 10
Desde la distribuciÃ³n de la estimaciÃ³n de la normal.

**** Tema 2. Distribuciones en el muestreo de poblaciones normales
***** Ejercicio 1
#+begin_statement
Se toma una muestra aleatoria simple de tamaÃ±o $5$ de una variable aleatoria
con distribuciÃ³n ${\cal N}(2.5, 36)$. Calcular:

  1. Probabilidad de que la cuasivarianza muestral estÃ© comprendida entre
     $1.863$ y $2.674$.
  2. Probabilidad de que la media muestral estÃ© comprendida entre $1.3$ y $3.5$,
     supuesto que la cuasivarianza muestral estÃ¡ entre $30$ y $40$.
#+end_statement

****** Probabilidad de la Cuasivarianza
Buscamos:

\[\begin{aligned}
P\Big(1.863 \leq S^2 \leq 2.674 \Big) &\leq 
P\Big(\frac{n-1}{\sigma^2}1.863 \leq \frac{n-1}{\sigma^2}S^2 \leq \frac{n-1}{\sigma^2}2.674 \Big)
\end{aligned}\]

Y sabiendo que $\frac{n-1}{\sigma^2}S^2 \leadsto \chi^2(n-1)$, sea $\phi$ la funciÃ³n de distribuciÃ³n para
tener que la probabilidad serÃ¡:

\[\phi\left(\frac{n-1}{\sigma^2}2.674\right) - 
\phi\left(\frac{n-1}{\sigma^2}1.863\right) =
0.010 - 0.005 = 0.005
\]

Consultando la tabla de Poisson.

******* CÃ¡lculos
#+BEGIN_SRC R
n = 5
s2 = 36
pchisq((n-1)/s2 * 2.674, df=n-1) - pchisq((n-1)/s2 * 1.863, df=n-1)
#+END_SRC

#+RESULTS:
: 0.00499959549303851

****** Probabilidad de la Media Muestral
La suposiciÃ³n de que la cuasivarianza muestral estÃ¡ entre 30 y 40 no
aporta nada porque la media y ella son estadÃ­sticos independientes por
el Lema de Fisher.

Buscamos:

\[P\Big(  
1.3 \leq \overline{X} \leq 1.5
\Big) = 
P\Big(  
\frac{1.3 - \mu}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq 
\frac{1.5 - \mu}{\sigma/\sqrt{n}}
\Big)
\]

Y como $\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)$, siendo $\phi$ la distribuciÃ³n de la normal, calculamos
la probabilidad usando las tablas de la normal.

\[
\phi(-0.3726) - \phi(-0.4472) = 0.3557 - 0.3300 = 0.0257
\]

******* CÃ¡lculos
#+BEGIN_SRC R
n = 5
m = 2.5
s2 = 36
s = sqrt(36)
pnorm((1.5-m)/(s/sqrt(n))) - pnorm((1.3-m)/(s/sqrt(n)))
#+END_SRC

#+RESULTS:
: 0.0273336344978247

***** Ejercicio 3
#+begin_statement 
Â¿De quÃ© tamaÃ±o mÃ­nimo habrÃ­a que seleccionar una muestra de una variable
con distribuciÃ³n normal ${\cal N}(\mu,4)$ para poder afirmar, con probabilidad mayor
que $0.9$, que la media muestral diferirÃ¡ de la poblacional menos de $0.1$?
#+end_statement

Buscamos:

\[P\Big(
\frac{-0.1}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq
\frac{0.1}{\sigma/\sqrt{n}}
\Big)\]

Sabiendo que $\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)$, calculamos:

\[\begin{aligned}
1 - 2\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\geq 0.9 \\
\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\leq 0.05 \\
\end{aligned}\]

Usando la tabla de la distribuciÃ³n, tenemos que:

\[\frac{0.1}{2/\sqrt{n}} = 1.65\]

Desde donde: $n = 1089$.

****** CÃ¡lculos
#+BEGIN_SRC R
s2 = 4
s = sqrt(s2)
q = qnorm(1-0.05)
((s*q)/0.1)^2
#+END_SRC

#+RESULTS:
: 1082.21738163816

***** Ejercicio 7
Comprobando sumas se llega a que siguen una Poisson.

**** Tema 3. Suficiencia y complitud
***** Ejercicio 1
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X \leadsto B(k,p)$
y sea $T(X_1,\dots,X_n) = \sum^n_{i=1} X_i$. Probar, usando la definiciÃ³n y aplicando el
teorema de factorizaciÃ³n, que $T$ es suficiente para $p$.
#+end_statement

****** Usando la definiciÃ³n
Llamamos $S = \sum^n_{i=1} X_i$. Veremos que $P(x_1,\dots,x_n\mid S)$ no depende de $p$.
En el caso $x_1+\dots+x_n \neq S$, la probabilidad es $0$ y claramente 
independiente de $p$. En el otro caso, tenemos:

\[\begin{aligned}
P(x_1,\dots,x_n\mid S) = 
\frac{P(x_1,\dots,x_n)}{P(S)} =
\frac
{\prod {k \choose x_i} p^{x_i}(1-p)^{k-x_i}}
{{nk \choose S} p^{S}(1-p)^{nk-S}} =
\frac
{\prod {k \choose x_i}}
{{nk \choose S}}
\end{aligned}\]

Que no depende de $p$. Hemos usado que $S = \sum_{i=1}^n X_i \leadsto {\cal B}(nk,p)$ para 
calcular la probabilidad de que valga un valor concreto.

****** Usando el teorema de factorizaciÃ³n
Factorizamos la funciÃ³n de densidad como:

\[
f(x_1,\dots,x_n) =
\prod_{i=1}^n {k \choose x_i} p^{x_i}(1-p)^{k-x_i} =
\left(p^{\sum x_i}(1-p)^{nk - \sum x_i}\right)
\left(\prod^{k}_{i=1} {k\choose x_i}\right)
\]

El primer factor sÃ³lo depende de los datos a travÃ©s de la suma y el
segundo factor no depende de la probabilidad.

***** Ejercicio 3
#+begin_statement
Sea $(X_1,X_2,X_3)$ una muestra aleatoria simple de una variable $X \leadsto B(1,p)$.
Probar que el estadÃ­stico $X_1+2X_2+3X_3$ no es suficiente.
#+end_statement

Llamamos $S=X_1+2X_2+3X_3$. Vamos a calcular $P(1,1,0\mid S=3)$ y 
comprobaremos que depende de $p$. NÃ³tese que puede llegarse a $S=3$ de dos
formas, como $1+2+0$ y como $0+0+3$.

\[
P(1,1,0\mid S=3) =
\frac{P(1,1,0)}{P(S=3)} =
\frac{p^2(1-p)}{p^2(1-p)+p(1-p)^2}=
p
\]

**** Tema 4. EstimaciÃ³n puntual. MÃ©todos de estimaciÃ³n
***** Ejercicio 2
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de $X \leadsto B(1,p)$ y sea 
$T = \sum_{i=1}^n X_i$.

  1. Probar que si $k \in \mathbb{N}$ y $k\leq n$ el estadÃ­stico

     \[\frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]

     es un estimador insesgado de $p^k$. Â¿Es este estimador el UMVUE?
  2. Probar que si $k>n$, no existe ningÃºn estimador insesgado para $p^k$.
  3. Â¿Puede afirmarse que $\frac{T}{n}\left(1-\frac{T}{n}\right)^2$ es insesgado para $p(1-p)^2$?
#+end_statement

****** Punto 1. Es insesgado.
Llamamos $M = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}$. Comprobamos que es insesgado calculando 
la esperanza. Dividimos entre los casos $i\leq k$ que son nulos y los demÃ¡s.
Usamos $P(T = i) = {n \choose i}p^i(1-p)^{n-i}$.

\[\begin{aligned}
\mathbb{E}[M] &=
\sum_{i=1}^n {n \choose i} p^i (1-p)^{n-i}
\frac{i(i-1)\dots(i-k+1)}{n(n-1)\dots(n-k+1)} \\&=
\sum_{i=1}^n p^i (1-p)^{n-i}
\frac{(n-k)!}{(i-k)!((n-k)-(i-k))!} \\&=
p^k \sum_{i=1}^n p^{i-k} (1-p)^{(n-k)-(i-k)}
{n-k \choose i-k} \\&= p^k
\end{aligned}\]

Usando binomio de Newton en el Ãºltimo paso.

****** Punto 1. Es el UMVUE.
Usaremos el teorema de Lehmann-ScheffÃ©, sabiendo que $M$ es un estimador
insesgado de $p^k$; y que $T$ es suficiente y completo para $p$.

Para ver que $T$ es completo, calculamos:

\[\begin{aligned}
\mathbb{E}[g(T)] 
&= \sum_{t=0}^n g(t) {n \choose t} p^t(1-p)^{n-t} \\
&= (1-p)^n \sum_{t=0}^n g(t) {n \choose t} \left(\frac{p}{1-p}\right)^t \\
\end{aligned}\]

Para que se anule siempre, debe anularse el polinomio
$\sum g(t) {n \choose t}r^t$ para $r \in \mathbb{R}^+$, lo que implica $g(t) = 0$.

Pero ahora, por Lehmann-ScheffÃ©, tenemos que el UMVUE serÃ¡:

\[\mathbb{E}[M\mid T] = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]

Que es un UMVUE.

****** Punto 2.
Supongamos un estimador $Q$ que fuera insesgado para $p^q$. TendrÃ­amos:

\[\mathbb{E}[Q(X)] = p^q\]

Es decir, llamando $R(T) = \sum_{\sum x_i = T} Q(X)$,

\[
\sum^n {n \choose k} R(t) p^t(1-p)^{n-t} = p^q
\]

Pero esto nos darÃ­a un polinomio de grado $q$ sobre $p$, que no puede ser 
nulo.

****** TODO Punto 3.
No. Si calculamos la esperanza usando linealidad obtenemos algo distinto.

#+BEGIN_SRC sage
n,p = var('n p')
m1 = n*p
m2 = m1*(1-p+m1)
m3 = m1*(1-3*p+3*m1+2*p^2 - 3*n*p^2 + n^2*p^2)
(p - 2*m2/n^2 + m3/n^3).normalize()
#+END_SRC

#+RESULTS:
: (n^2*p^3 - 2*n^2*p^2 - 3*n*p^3 + n^2*p + 5*n*p^2 + 2*p^3 - 2*n*p - 3*p^2 + p)/n^2

***** Ejercicio 3
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable
$X \leadsto \{{\cal P}(\lambda)\mid \lambda > 0\}$. Encontrar, si existe, el UMVUE para $\lambda^s$, siendo
$s \in \mathbb{N}$ arbitrario.
#+end_statement

Por familia uniparamÃ©trica demostramos $T = \sum X_i$ suficiente y completo.
Probando llegamos a que $\frac{1}{n^s}T(T-1)\dots(T-s)$ es insesgado.

***** Ejercicio 7
****** Punto 1
Se comprueba que es familia exponencial uniparamÃ©trica. Con
$T(X) = X$ y por tanto con $\sum_i T(X_i)$ como estadÃ­stico suficiente.
AdemÃ¡s, es completo porque $Q(\Theta)$ tiene un abierto en su imagen.

****** DONE Punto 2

***** Ejercicio 8
****** DONE Punto 1

**** Tema 5. Intervalos de confianza
***** Ejercicio 1
El mÃ­nimo es $n=44$.
***** Ejercicio 2
****** Primer punto
Intervalo de $(170.75, 179.75)$.

****** Segundo punto
Da un $n \geq 865$.
***** Ejercicio 5
Se llega a $t = 2.1788$.

***** Ejercicio 7
****** Primer punto
Calculamos:

  - $\overline{X} = 37.2$
  - $\overline{Y} = 16.88$
  - $S_X^2 = 482.137$
  - $S_Y^2 = 208.517$
  - $n_1 = 6$
  - $n_2 = 5$

Y tenemos un intervalo de confianza para el cociente de varianzas.
Calculando primero desde las tablas (?), con $\alpha = 0.95$:

  - $F_{1-\alpha/2} =$
  - $F_{\alpha/2} =$
***** Ejercicio 9
Tomamos el estimador:

\[
T = \frac{1}{n}\sum_{i=1}^n X_i
\]

Si partimos de la desigualdad de Chebyshev con la cota $c = 1/n$ a la
varianza, llegamos a $k = 1/\sqrt{n\alpha}$, que nos da el intervalo:

\[
\left(
\frac{1}{n} \sum X_i + \frac{1}{\sqrt{n\alpha}}
,\quad
\frac{1}{n} \sum X_i - \frac{1}{\sqrt{n\alpha}}
\right)
\]

Con la confianza $\alpha$.
**** Tema 6. Contraste de hipotÃ©sis
**** Ejercicio 1
#+begin_statement
Se toma una observaciÃ³n de una variable con distribuciÃ³n de Poisson para
contrastar que la media vale 1 frente a que vale 2.

  1. Construir un test no aleatorizado con nivel de significaciÃ³n 0.05 
     para el contraste planteado. Calcular las probabilidades de cometer
     error de tipo 1 y de tipo 2, el tamaÃ±o y la potencia del test frente
     a la hipÃ³tesis alternativa.
  2. Â¿CÃ³mo debe aleatorizarse el test para alcanzar el tamaÃ±o 0.05?Â¿CuÃ¡l
     es la potencia de este test?
#+end_statement

Usaremos el lema de Neyman-Pearson para crear un test donde el problema
de contraste es: $H_0 : \lambda = 2$, $H_1 : \lambda = 1$, para una distribuciÃ³n $Poi(\lambda)$.

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) \geq kf_0(X) \\
0 & \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]

Sabemos que este test tendrÃ¡ tamaÃ±o $\mathbb{E}_{\lambda=2}[\varphi]$, calculamos:

\[f_1(x) = \frac{1}{ex!}\]

\[
f_0(x) = \frac{2^x}{e^2x!}
\]

La condiciÃ³n $f_1(X) \geq kf_0(X)$ equivale a:

\[
x \leq \log_2 \frac{e}{k}
\]

Siendo $\Phi$ la funciÃ³n de distribuciÃ³n de una Poisson $Poi(2)$, que es la 
distribuciÃ³n que sigue aquÃ­ $x$, tenemos, consultando las tablas:

\[
\mathbb{E}_{\lambda=2}[\varphi] = 
\Phi\left(\log_2\frac{e}{k}\right) = 
0.05
\]

# Tenemos que resolver esto y no parece que en las tablas se dÃ© nada
# sensato. Repasar el ejercicio.
**** Ejercicio 2
#+begin_statement
Una urna contiene 10 bolas, blancas y negras. Para contrastar que el 
nÃºmero de bolas blancas es 5 frente a que dicho nÃºmero es 6 Ã³ 7, se
extraen tres bolas con reemplazamiento y se rechaza $H_0$ sÃ³lo si se 
obtienen 2 Ã³ 3 bolas blancas. Calcular el tamaÃ±o de este test y la
potencia frente a alternativas.
#+end_statement

***** TamaÃ±o del test
Llamamos $X$ a la variable dada por cada extracciÃ³n, siendo 1 si es blanca
y 0 si es negra. Vemos que $X \leadsto B(1,\theta/10)$. Tenemos como hipÃ³tesis la
hipÃ³tesis nula $H_0 : \theta = 5$ y la alternativa $H_1 : \theta \in \{6,7\}$. Nuestro test estÃ¡
definido por:

\[
\varphi(X_1,X_2,X_3) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum X_i = 2,3 \\
0 & \mbox{if } \sum X_i = 0,1
\end{array} 
\right.
\]

Calculamos el tamaÃ±o sabiendo que $Z = \sum X_i \leadsto B(3,\theta/10)$:

\[
\sup_{\theta = 5} \beta_\varphi(\theta) = E_{\theta=5}[\varphi]
=
P(Z = 2) + P(Z=3) = \frac{1}{2}
\]

***** Potencia frente alternativas
Calculamos:

\[\beta_\varphi(6)\]
\[\beta_\varphi(7)\]

**** Ejercicio 3
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria
con distribuciÃ³n de Poisson de parÃ¡metro $\lambda$. Encontrar el test mÃ¡s potente
de tamaÃ±o $\alpha$ para resolver el problema de contraste:

\[
H_0 : \lambda = \lambda_0
\]
\[
H_1 : \lambda = \lambda_1
\]

AplicaciÃ³n: En una centralita telegÃ³nica el nÃºmero de llamadas por minuto
sigue una distribuciÃ³n de Poisson. Si en cinco minutos se han recibido 12
llamadas, Â¿puede aceptarse que el nÃºmero medio de llamadas por minuto es
1.5, frente a que dicho nÃºmero es 2, al nivel de significaciÃ³n 0.05?
Calcular la potencia del test obtenido.
#+end_statement

***** Desarrollo teÃ³rico
Por Neyman-Pearson, el test mÃ¡s potente de tamaÃ±o $\alpha$ serÃ¡ de la forma:

\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } f_1(x_1,\dots,x_n) > kf_0(x_1,\dots,x_n) \\
\gamma & \mbox{if } f_1(x_1,\dots,x_n) = kf_0(x_1,\dots,x_n) \\
0 & \mbox{if } f_1(x_1,\dots,x_n) < kf_0(x_1,\dots,x_n)
\end{array} 
\right.
\]

La desigualdad es equivalente a:

\[
\frac{e^{-n\lambda_1}\lambda_1^{\sum x_i}}{\prod x_i!} 
> 
k \frac{e^{-n\lambda_0}\lambda_0^{\sum x_i}}{\prod x_i!} 
\]

\[
k = \frac
{n(\lambda_0-\lambda_1)-c}
{\log\left(\lambda_0/\lambda_1\right)}
> \sum x_i
\leadsto
Poi(n\lambda_0)
\]

Por tanto podemos tomar una distribuciÃ³n de Poisson siendo $\rho_\alpha$ el valor
que da $\alpha$ en la funciÃ³n de distribuciÃ³n y tener:

\[
c = n(\lambda_0-\lambda_1) + \rho_\alpha\log(\lambda_0/\lambda_1)
\]

Tenemos por tanto un test de la forma:

\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i < \rho_\alpha \\
0 & \mbox{if } \sum x_i \geq \rho_\alpha
\end{array} 
\right.
\]

El tamaÃ±o del test entonces serÃ¡ $\alpha$.

***** AplicaciÃ³n
En este caso tenemos $\sum x_i = 12$, para $n=5$. Para nivel de significaciÃ³n
$\alpha = 0.05$, tenemos $P(Poi(10) > \rho_{0.05}) = 0.05$.

Tenemos:

\[
P(Poi(10) > 14) = 0.05
\]

Y como $12 < 14$, se el test da la hipÃ³tesis alternativa.
# Esto hay que comprobarlo, que lo he hecho muy rÃ¡pido y creo que le
# dado la vuelta.

**** Ejercicio 4
#+begin_statement
Sea $(X_1,\dots,X_n)$ muestra aleatoria simple de una variable con distribuciÃ³n
${\cal N}(\mu,\sigma^2_0)$. Deducir el test mÃ¡s potente de tamaÃ±o arbitrario para contrastar
hipÃ³tesis simples sobre $\mu$.
#+end_statement

Aplicaremos Neyman-Pearson para crear un test con dos hipÃ³tesis,
$H_0 : \mu =\mu_0$, $H_1:\mu =\mu_1$, de la forma:

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) > kf_0(X) \\
\gamma(X) & \mbox{if } f_1(X) = kf_0(X)\\
0& \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]

Que serÃ¡ el de mayor potencia con nivel de significaciÃ³n $E_{\mu_0}[\varphi]$.
Calculamos la significaciÃ³n sabiendo que la condiciÃ³n $f_1 > kf_0$ nos
da:

\[
f_1(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_1)^2}{2\sigma^2}}
\]

\[
f_0(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_0)^2}{2\sigma^2}}
\]

Simplificando la condiciÃ³n:

\[
\sum x_i > \frac{\log k - (\mu_0^2-\mu_1^2)}{2(\mu_1-\mu_0)} = k'
\]

Y usamos la normal con $P(Z \geq z_\alpha) = \alpha$ para obtener el valor necesario para
$k'$ si queremos un test de tamaÃ±o $\alpha$:

\[
k' = z_\alpha\sigma/\sqtr{n} + \mu_0
\]

El test mÃ¡s potente de tamaÃ±o $\alpha$ es entonces:

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i > k' \\
0& \mbox{if } \sum x_i \leq k'
\end{array} 
\right.
\]

**** DONE Ejercicio 5
#+begin_statement
Deducir el test mÃ¡s potente de tamaÃ±o $\alpha$ para contrastar $H_0:\theta=\theta_0$
frente a $H_1:\theta=\theta_1$ y calcular su potencia. Â¿CuÃ¡l es el test Ã³ptimo fijado
un nivel de significaciÃ³n arbitrario?
#+end_statement

**** Ejercicio 6
#+begin_statement
Deducir el test mÃ¡s potente de tamaÃ±o arbitrario para contrastar $H_0 : \theta = \theta_0$
frente a $H_1 : \theta = \theta_1$, basÃ¡ndose en una muestra de tamaÃ±o $n$ de una variable
aleatoria con funciÃ³n de densidad


\[
f_\theta(x) = \frac{\theta}{x^2}, \quad x > \theta
\]

Deducir el test Ã³ptimo para un nivel de significaciÃ³n arbitrario.
#+end_statement


**** Ejercicio 8
Minimizando y calculando se llega a:

\[
\lambda(X) =
\frac{1}{\theta_0^n} e^{(1-1/\theta_0)\sum x_i}
\]

**** Ejercicio 9
#+begin_statement
En base a una observaciÃ³n $X \leadsto B(n,p)$, deducir el test de razÃ³n de 
verosimilitudes para contrastar la hipÃ³tesis de que el parÃ¡metro $p$ no
supera un determinado valor, $p_0$.
#+end_statement

**** Ejercicio 10
#+begin_statement
Sea $X$ variable con funciÃ³n de densidad

\[
f_\theta(x) = \theta x^{\theta-1},\quad 0<x<1
\]

BasÃ¡ndose en una observaciÃ³n de $X$, deducir el test de razÃ³n de 
verosimilitudes de tamaÃ±o arbitrario para contrastar:

\[
H_0 : \theta \leq \theta_0
\]
\[
H_1 : \theta > \theta_0
\]
#+end_statement

**** Ejercicio 13
#+begin_statement
Un profesor asegura que tiene un nuevo mÃ©todo de enseÃ±anza mejor que el
usado tradicionalmente. Para comprobar si tiene razÃ³n se selecciona de
forma aleatoria e independiente dos grupos de alumnos, A y B, utilizÃ¡ndose
el nuevo mÃ©todo en el grupo A y el tradicional con el B. A final de curso
se hace un examen a los alumnos, obteniÃ©ndose las siguientes puntuaciones.

  - Grupo A: 6, 5, 4, 7, 3, 5.5, 6, 7, 6
  - Grupo B: 5, 4, 5, 6, 4, 6, 5, 3, 7

Supuesto que las puntuaciones de cada grupo siguen una distribuciÃ³n normal,
Â¿proporcionan estos datos evidencia para rechazar el nuevo mÃ©todo, con
un nivel de significaciÃ³n 0.05?
#+end_statement

Usaremos la dualidad entre intervalos de confianza y tests de hipÃ³tesis
para buscar si el 0 estÃ¡ en un intervalo de confianza 0.95 de la diferencia
de ambas medias. Las varianzas son desconocidas pero las suponemos iguales.

El intervalo para $\mu_1-\mu_2$ de menor longitud media uniforme a nivel de
confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

Y simplemente calculamos.
** InformÃ¡tica grÃ¡fica

 - curena@ugr.es
 - [[http://lsi.ugr.es/curena]]
 - [[http://lsi.ugr.es/doce/ig/17-18]]

1 punto por trabajo aparte.
Defensa de prÃ¡cticas con 1 semana antelaciÃ³n.

*** Tema 1. IntroducciÃ³n
**** 1. IntroducciÃ³n
**** 2. Proceso de visualizaciÃ³n
**** 3. LibrerÃ­a OpenGL
**** 4. ProgramaciÃ³n del cauce grÃ¡fico
***** 4.1. Cauce programable
En la GPU se ejecutan dos etapas y entre ambas la rasterizaciÃ³n y
recortado de polÃ­gonos.

 * *TransformaciÃ³n* de coordenadas de vÃ©rtice a ventana; realizado por
   el /vertex shader/ que se ejecuta al llamar a =glVertex=.
 * *Sombreado*, cÃ¡lculo del color de pixel; realizado por el
   /fragment shader/.

Hay dos opciones para seleccionar shaders

 * *cauce de funcionalidad fija*, predefinidos hasta OpenGL 3.0;
 * *cauce programable*, escrito en GLSL, mÃ¡s flexible y eficiente,
   compilado en tiempo de ejecuciÃ³n.

El cauce grÃ¡fico fluye entonces como:

 1) CPU, aplicaciÃ³n.
 2) ImplementaciÃ³n de OpenGL.
 3) Vertex shader para cada vÃ©rtice.
 4) RasterizaciÃ³n.
 5) Fragment shader para cada pixel.
 6) Framebuffer.

***** 4.2. Shaders bÃ¡sicos
Un *program* es un /vertex shader/ con un /fragment shader/. Se
almacenan en =char*=, se compilan con OpenGL y se enlazan.

****** TODO Programar un vertex shader
****** TODO Programar un fragment shader
***** 4.3. CreaciÃ³n y ejecuciÃ³n de programas
Un *program* tiene un =GLuint= identificador.

 * =glCreateShader=
 * =glShaderSource=
 * =glCompileShader=
 * =glCreateProgram=
 * =glAttachShader=
 * =glLinkProgram=
 * =glUseProgram=

***** 4.4. Funciones auxiliares
**** 5. ApÃ©ndice: puntos, vectores y marcos
*** Tema 2. Modelado de objetos
**** 2.1. Modelos geomÃ©tricos
***** 2.1.1. IntroducciÃ³n
El modelo geomÃ©trico mÃ¡s general son los conjuntos pero no permiten
una representaciÃ³n computacional clara. Se usan

 * *voxels*, cuadrÃ­culas de volÃºmenes,
 * *fronteras*, polÃ­gonos planos.

**** 2.2. Modelos de fronteras
***** 2.2.1. Elementos y adyacencia
Los modelos de fronteras usan mallas de polÃ­gonos, normalmente
mallas de triÃ¡ngulos. Se consideran adyacencias entre vÃ©rtices,
aristas y caras bajo un *marco de referencia local de la malla*.

La malla tiene como atributos

 * normales de las caras,
 * normales de los vÃ©rtices,
 * colores de caras, 
 * colores de vÃ©rtices, que luego interpolarÃ¡n las caras;
 * coordenadas de textura,
 * vectores bitangentes.

***** 2.2.2. Lista de triÃ¡ngulos
La estructura mÃ¡s simple es la *lista de triÃ¡ngulos aislados*, una
entrada para cada tres vÃ©rtices, $9n$ floats; consume mucha memoria
innecesaria. =Objeto3D=, =MallaTA=

****** VisualizaciÃ³n
Puede hacerse

 * con =glBegin/glEnd= llamando a =glVertex3fv=,
 * con =glDrawArrays=, usando $3n$ tuplas de coordenadas.

***** 2.2.3. Mallas como tiras de triÃ¡ngulos
Cada triÃ¡ngulo es adyacente al anterior y tenemos $3(n+2)$ floats.
En algunos casos hay que usar varias o repetir vÃ©rtices. La complejidad
de representarlas luego es mayor. =MallaTT=

***** 2.2.4. Mallas indexadas
Usar dos tablas

 * *tabla vÃ©rtices*, con entrada por vÃ©rtice,
 * *tabla triÃ¡ngulos*, llamando a tabla vÃ©rtices.

Mucho mÃ¡s efiicente =MallaInd=, $3n$.

***** 2.2.5. RepresentaciÃ³n con aristas aladas
Las aristas tienen dos caras adyacentes, hay una tabla de vÃ©rtices y
otrade aristas, donde la segunda guarda

 * vÃ©rtice inicial,
 * vÃ©rtice final,
 * triÃ¡ngulo a la izquierda,
 * triÃ¡ngulo a la derecha,
 * arista anterior en el triÃ¡ngulo izquierda,
 * arista siguiente en el triÃ¡ngulo izquierda,
 * arista anterior en el triÃ¡ngulo derecha,
 * arista siguiente en el triÃ¡ngulo derecha.

Podemos asÃ­ resolver adyacencias, pueden usarse tambiÃ©n

 * tabla de aristas de vÃ©rtice,
 * tabla de aristas de triÃ¡ngulo.

***** 2.2.6. RepresentaciÃ³n con atributos
A los vÃ©rtices se les puede asignar

 * colores,
 * normales,
 * coordenadas de textura,
 * otros atributos.

Pueden asociarse con =glVertex= o usando =glDrawArrays= con
=glDrawElements=. A las caras no se pueden asignar atributos
directamente, pero se pueden cambiar al enviar la cara.

***** 2.2.7. VisualizaciÃ³n de mallas en modo diferido
Se envÃ­a todo una sÃ³la vez a la GPU. Puede hacerse con

 * display lists (obsoletas),
 * vertex buffer objects (VBO).

El VBO se crea tomando un identificador, generando luego el VBO,
asignÃ¡ndolo al dentificador y haciendo la transferencia a GPU,
puede desactivarse luego.

#+BEGIN_SRC c++
GLuint id_vbo;
glGenBuffers(1, &id_vbo);
glBindBuffer(tipo, id_vbo);
glBufferData(tipo, tamanio, puntero, GL_STATIC_DRAW);
glBindBuffer(tipo, 0);
#+END_SRC

La malla se puede visualizar muchas veces entonces sin enviar datos a
GPU usando =glBindBuffer= y =glDrawElements=. Colores y normales se
pueden almacenar en los mismos VBOs.

**** 2.3. Transformaciones geomÃ©tricas
***** 2.3.1. TransformaciÃ³n geomÃ©trica
Todas las mallas deben acabar apareciendo en *coordenadas del mundo*.
Se usan transformaciones geomÃ©tricas matriciales para mostrar los
objetos.

Se consideran matrices 4x4 donde el Ãºltimo vector indica si es un punto
y las coordenadas de ese punto. Se transforma sobre un marco de coordenadas
$R$ desde $p = R(x,y,z,w)^t$ a $p' = R(x',y',z',w')^t$; viene asÃ­ determinada
por tres funciones lineales

\[\begin{aligned}
x' &= f_x(x,y,z,w) \\
y' &= f_y(x,y,z,w) \\
z' &= f_z(x,y,z,w) \\
w' &= w
\end{aligned}\]

que dependen del marco de referencia.

***** 2.3.2. Transformaciones usuales en IG
Todas ellas son afines y coherentes, $T(p-q) = Tp - Tq$.

****** TraslaciÃ³n
Para puntos $\mathrm{Tra}[d](p) = p + d$ y para vectores $\mathrm{Tra}[d](v) = v$.
Queda como

\[\begin{aligned}
x' &= f_x(x,y,z,w) &= x + d_xw \\
y' &= f_y(x,y,z,w) &= y + d_yw \\
z' &= f_z(x,y,z,w) &= z + d_zw \\
w' &= w
\end{aligned}\]

y puede escribirse como =MAT_Traslacion(dx,dy,dz)=.

****** Escalado

****** Cizalla

****** RotaciÃ³n

****** ComposiciÃ³n

****** RepresentaciÃ³n matricial
***** 2.3.3. Matrices y marcos de coordenadas
Si las coordenadas del marco $B$ en $A$ vienen dadas por $a,b,c,d$,
la matriz de cambio de $B$ a $A$ viene dada por

\[M_{A,B} = \begin{pmatrix}
a_x & b_x & c_x & d_x \\
a_y & b_y & c_y & d_y \\
a_z & b_z & c_z & d_z \\
0 & 0 & 0 & 1 \\
\end{pmatrix}\]

y se calculan las coordenadas como $Mc_{A} = c_B$.

***** 2.3.4. RepresentaciÃ³n de matrices en memoria
Se usa el tipo =Matriz4f=.

***** 2.3.5. Transformaciones en OpenGL
OpenGL almacena

 * *matriz de modelado* (N), pasa de coordenadas de objeto a coordenadas
   del mundo; posiciona un objeto en la escena;

 * *matriz de vista* (V), pasa de coordenadas del mundo a coordenadas de
   ojo, relativas a la cÃ¡mara;

 * *modelview* (M), compone modelado y vista $M = VN$.

La modelview puede especificarse por composiciÃ³n

#+BEGIN_SRC c++
glMatrixMode(GL_MODELVIEW);
glLoadIdentity();
gluLookAt(..);     // Vista
glMultMatrix(..);  // Modelado
#+END_SRC

La gestiÃ³n directa de matrices es obsoleta a partir de OpenGL3.1.

***** 2.3.6. GestiÃ³n de matriz de modelado en GLSL
**** 2.4. Modelos jerÃ¡rquicos, representaciÃ³n y visualizaciÃ³n
*** Tema 3. VisualizaciÃ³n
**** 3.1. Cauce grÃ¡fico y definiciÃ³n de la cÃ¡mara
***** 3.1.1. El cauce grÃ¡fico del algoritmo Z-buffer
El algoritmo Z-buffer elimina partes ocultas (EPO) en 3D y se
implementa en hardware. Tiene 4 pasos.

 * TransformaciÃ³n de coordenadas de vÃ©rtices, proyecciÃ³n a la
   pantalla.
 * Recortado de polÃ­gonos fuera de zona visible.
 * RasterizaciÃ³n y EPO, cÃ¡lculo de pÃ­xeles donde proyectar.
 * IluminaciÃ³n y texturaciÃ³n.

****** Sistemas de coordenadas

 * (OC) Coordenadas de *objeto*, propias de cada objeto fuera de escena.
 * (WC) Coordenadas de *mundo*, colocando los objetos en la escena.
 * (EC) Coordenadas de *cÃ¡mara* u *ojo*, relativas a la cÃ¡mara virtual.
 * (CC) Coordenadas de *recortado*, distancias normalizadas relativas al
   rectÃ¡ngulo de la pantalla.
 * (NDC) Coordenadas *normalizadas de dispositivo*, de recortado dentro de
   la zona visible.
 * (DC) Coordenadas de *dispositivo*, en pixels.

****** Cambios de coordenadas

 * (N) La matriz de *modelado* pasa objeto a mundo.
 * (V) La matriz de *vista* pasa mundo a cÃ¡mara.
 * (P) La matriz de *proyecciÃ³n* pasa de cÃ¡mara a recortado.
 * (D) La matriz de *viewport* pasa normalizadas (NDC) a dispositivo (DC).

***** 3.1.2. TransformaciÃ³n de vista
La matriz de vista se define con

 * $o_c$, posiciÃ³n de observador (PRP),
 * $n$, normal al plano de proyecciÃ³n (VPN),
 * $a$, punto de atenciÃ³n (VRP), alternativa a especificar $n$,
 * $u$, direcciÃ³n que seÃ±ala el "arriba" de la imagen (VUP).

****** Construir del marco de referencia
A partir de los parÃ¡metros se pueden construir tres vectores
perpendiculares formando el *marco del observador*,

\[\begin{aligned}
n &= o - a \\
z_c &= \frac{n}{\|n\|} \\
x_c &= \frac{n \times u}{\|n \times u\|} \\
y_c &= z_c \times u_c
\end{aligned}\]

y este marco se representa en coordenadas de mundo $W$. =gluLookAt=
toma $o,a,u$ como parÃ¡metros.

****** CÃ¡lculo de matriz de vista dado un marco
Dado $p$ en coordenadas del mundo podemos tomar los productos escalares
de $p-o_c$ con los ejes $x_c,y_c,z_c$. La matriz de vista serÃ¡ entonces

\[V = \begin{pmatrix}
a_x & a_y & a_z & 0 \\
b_x & b_y & b_z & 0 \\
c_x & c_y & c_z & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 & -o_{x} \\
0 & 1 & 0 & -o_{y} \\
0 & 0 & 1 & -o_{z} \\
0 & 0 & 0 & 1 \\
\end{pmatrix}\]

donde $a = x_c, b = y_c, c = z_c$ son los tres ejes.

****** CÃ¡lculo de matriz de vista con Ã¡ngulos de Euler
Los Ã¡ngulos de Euler pueden construirse a partir de las coordenadas
del marco

\[
V = \mathrm{Rot}[\gamma,z] \cdot \mathrm{Rot}[\beta,y] \cdot \mathrm{Rot}[\alpha,x] \cdot \mathrm{Tra}[-o_c]
\]

***** 3.1.3. TransformaciÃ³n de proyecciÃ³n
Se proyecta sobre un *viewplane* de dos formas

 * *perspectiva*, con lÃ­neas proyectoras hacia un foco; hay un factor de
   escala que decrece afÃ­nmente con la distancia $s = 1/(ad_z + b)$;
 * *ortogrÃ¡fica*, con lÃ­neas proyectoras paralelos, es una proyecciÃ³
   afÃ­n simple.

****** El view-frustum
RegiÃ³n de la escena visible en el viewport. La transformaciÃ³n de
proyecciÃ³n debe transformarlo en un cubo de lado 2 centrado en el
origen, esta no es lineal pero puede serlo en cuatro dimensiones.

 * Es un ortoedro en proyecciÃ³n ortogrÃ¡fica.
 * Es una pirÃ¡mide truncada en proyecciÃ³n perspectiva.

****** ParÃ¡metros del view-frustum
Se interpretan en coordenadas de vista, y se usan para transformar
de vista a recortado (matriz P)

 * $n,f$, near y far, son los lÃ­mites en Z del view-frustum, se exigen
   positivos, determinan planos de recorte trasero y delantero;

 * $l,r,b,t$, bottom y top, lÃ­mites en X e Y, que se transformarÃ¡n en
   [-1,1];

 * $(r-l)/(t-b)$ debe ser la relaciÃ³n de aspecto del viewport.

****** TODO Matriz de proyecciÃ³n perspectiva
****** TODO Matriz de proyecciÃ³n ortogrÃ¡fica
****** Matrices en OpenGL
#+BEGIN_SRC c++
glFrustum(l,r,b,t,n,f); // perspectiva
glOrtho(l,r,b,t,n,f);   // ortogrÃ¡fica

gluPerspective(fovy,a,n,f) // perspectiva (alternativa)
#+END_SRC

donde para =gluPerspective= se asume $r = -l$ y $t = -b$ y se tiene

 * =fovy= es la apertura del campo de visiÃ³n, grados de 0 a 180;
 * =a= es la relaciÃ³n de aspecto $r/b$;
 * =n,f= son near y far.

**** 3.2. Modelos de iluminaciÃ³n
***** 3.2.1. RadiaciÃ³n visible
La *radiancia* $L(\lambda,p,v)$ determina el tono y brillo de un punto. 
Los colores pueden medirse en RGB usando mezcla aditiva, la
traducciÃ³n dependerÃ¡ del dispositivo.

***** 3.2.2. EmisiÃ³n y reflexiÃ³n de la radiaciÃ³n
La radiancia es suma de emitida y reflejada, para cada radiancia
incidente desde cada punto, se refleja una fracciÃ³n en cada direcciÃ³n
$v$,

\[
L(\lambda, p,v) = L_{em}(\lambda, p,v) + \sum_i L_{in}(\lambda,p,u_i)f_r(\lambda,p,v,u_i)
\]

***** 3.2.3. Simplificaciones en modelos computacionales
Se asume para calcular

 * fuentes puntuales no extensas,
 * la Ãºnica iluminaciÃ³n indirecta es constante,
 * objetos opacos,
 * no hay sombras arrojadas,
 * no hay dispersiÃ³n,
 * sÃ³lo funciona en RGB.

**** 3.3. Modelo de iluminaciÃ³n local (MIL) bÃ¡sico
***** 3.3.1. Elementos del modelo: normales y colores
La iluminaciÃ³n depende de la orientaciÃ³n caracterizada por el *vector
normal*.

***** 3.3.2. Fuentes de luz, materiales y reflexiÃ³n
 * Posicionales, con vector unitario

   \[
   l_i = \frac{q_i-p}{\|q_i-p\|}
   \]

 * Direccionales, a distancia infinita, direcciÃ³n constante.

***** 3.3.3. Componentes del modelo
 * Radiancia emitida $L_{em}(p)$, emisividad del material.

 * Reflectividad difusa $f_{ra}(p,v,l_i) = M_A(p)$.

 * Componente difusa dependiendo de la posiciÃ³n, independiente de la
   direcciÃ³n; $f_{rd}(p,v,l_i) = M_D(p) \max(0,n_p \cdot l_i)$.

 * Material difuso $M_A(p)$.

 * Componente pseudo-especular, el reflejo de la luz en objetos
   brillantes. *Modelo de Phong*,

   \[
   f_{rs}(p,v,l_i) = M_S(p) d_i [\max(0,r_i \cdot v)]^{e}
   \]

   con $r_i$ reflejado, $e$ exponente de brillo, $d_i$ midiendo si
   estÃ¡ de cara a la superficie.

***** 3.3.4. Modelo completo
\[
L(p,v) = M_E(p) + A_G(p) + \sum_{i=0}^{n-1}S_iC_i
\]

donde

\[
C_i = M_A(p) + M_D(p) \max(0,n\cdot l_i) + M_S(p) d_i (\max(0,r_i \cdot v))^e
\]

**** 3.4. IluminaciÃ³n en OpenGL
***** 3.4.1. IluminaciÃ³n vs asignaciÃ³n de colores
Obsoleta y eliminada a partir de OpenGL3.1. Cuando estÃ¡ activada se
usa el MIL para calcular el color.

#+BEGIN_SRC c++
glEnable(GL_LIGHTING);
glDisable(GL_LIGHTING);
#+END_SRC

Los parÃ¡metros del MIL son

 * $M_E$, emisividad,
 * $M_A,M_{D},M_S$, reflexividad difusa, ambiente y pseudoespecular,
 * $A_G$, luz ambiente,
 * $e$, exponente,
 * $S_{iA},S_{iD},S_{iS}$, luminosidad de cada fuente de luz,
 * $q_i,l_i$, posiciÃ³n y direcciÃ³n de cada fuente de luz.

El modelo de funcionalidad fija es parecido al MIL visto.

***** 3.4.2. DefiniciÃ³n de fuentes de luz
Hay luces =GL_LIGHTi= para =i = 0..8=. Pueden especificarse en varios
marcos de coordenadas con transformaciones.

#+BEGIN_SRC c++
// ActivaciÃ³n
glEnable(GL_LIGHTi);
glDisable(GL_LIGHTi);

// ConfiguraciÃ³n de colores
glLightfv(GL_LIGHTi, GL_AMBIENT, caf);
glLightfv(GL_LIGHTi, GL_DIFFUSE, caf);
glLightfv(GL_LIGHTi, GL_SPECULAR, caf);

// PosiciÃ³n/direcciÃ³n
glLightfv(GL_LIGHTi, GL_POSITION, tupla);
glLightfv(GL_LIGHTi, GL_DIRECTION, dirf);
#+END_SRC

***** 3.4.3. RepresentaciÃ³n de fuentes de luz
Clase =FuenteLuz=, se guardan en =ColeccionFL=.

***** 3.4.4. Vector hacia observador
El observador puede ser local o en el infinito.

#+BEGIN_SRC c++
glLightModeli(GL_LIGHT_MODEL_LOCAL_VIEWER, GL_FALSE); // Ortogonal
glLightModeli(GL_LIGHT_MODEL_LOCAL_VIEWER, GL_TRUE); // Perspectiva
#+END_SRC

***** 3.4.5. Normales de vÃ©rtices
Se pueden especificar con =glNormal= y normalizarse con
=glEnable(GL_NORMALIZE)=.

***** 3.4.6. Atributos materiales
El tÃ©rmino ambiente, emisividad y colores se controlan.

#+BEGIN_SRC c++
glLightModelf(GL_LIGHT_MODEL_AMBIENT, color);
glMaterialf(GL_FRONT_AND_BACK, GL_EMISSION, color);
glMaterialfv(GL_FRONT_AND_BACK, GL_AMBIENT, color);
glMaterialfv(GL_FRONT_AND_BACK, GL_DIFFUSE, color);
glMaterialfv(GL_FRONT_AND_BACK, GL_SPECULAR, color);
glMaterialfv(GL_FRONT_AND_BACK, GL_SHININESS, color);
#+END_SRC

***** 3.4.7. RepresentaciÃ³n de materiales
Clase =Material=.

**** 3.5. MÃ©todos de sombreado para Z-Buffer
***** 3.5.1. EvaluaciÃ³n del MIL con Z-Buffer
El MIL puede evaluarse

 * *sombreado plano* (flat shading): una vez por polÃ­gono.
 * *sombreado de vÃ©rtices* (smooth shading, Gouround): una vez por vÃ©rtice.
 * *sombreado de pixel* (pixel shading): una vez por pixel.

***** 3.5.2. Sombreado plano
Eficiente, discontinuidades y poco realista. Crea bandas Mach

***** 3.5.3. Sombreado en vÃ©rtices
Eficiente pero mÃ¡s realista, puede tener bandas Mach. Puede perder
zonas brillantes.

***** 3.5.4. Sombreado en pixeles
Costoso, mÃ¡s calidad y realismo.

***** 3.5.5. OpenGL: mÃ©todo de sombreado
SÃ³lo plano y vÃ©rtices

#+BEGIN_SRC c++
glShadeModel(GL_FLAT);
glShadeModel(GL_SMOOTH);
#+END_SRC

**** 3.6. VisualizaciÃ³n de texturas
***** 3.6.1. Detalles a pequeÃ±a escala
Rugosidades variando la normal y la reflectividad. Pueden usarse
polÃ­gonos de detalle, pero las *texturas* son mÃ¡s eficientes, llevan
texels a un cuadrado del espacio. Pueden ser procedurales.

***** 3.6.2. Coordenadas de textura
Aplican la textura al objeto.

***** 3.6.3. AsignaciÃ³n explÃ­cita de coordenadas de textura
Se asignan al modelar el objeto en escena.

***** 3.6.4. AsignaciÃ³n procedural de coordenadas de textura
Un subprograma =CoordText(p)= que las calcula en cada punto.

 * AsignaciÃ³n a vÃ©rtices, interpolando luego.
 * AsignaciÃ³n a puntos.

Se suelen usar:

 * funciones lineales,
 * coordenadas paramÃ©tricas,
 * coordenadas polares,
 * coordenadas cilÃ­ndricas.

En los casos de una superficie paramÃ©trica, podemos usar las coordenadas
como coordenadas de textura. Las esfÃ©ricas y cilÃ­ndricas pueden proporcionar
mejores resultados en algunos casos.

***** 3.6.5. Consulta de texels
El texel (i,j) tiene centro en un punto $(c_i,d_j)$ y puede
consultarse

 * el texel mÃ¡s cercano a ese punto,
 * una interpolaciÃ³n bilineal entre los cuatro texels.

La interpolaciÃ³n es mÃ¡s suave.

**** 3.7. Texturas en OpenGL
***** 3.7.1. ActivaciÃ³n y desactivaciÃ³n
#+BEGIN_SRC c++
glEnable(GL_TEXTURE_2D);
glDisable(GL_TEXTURE_2D);
#+END_SRC

Cuando se activan, el color de textura sustituye a reflexividades del
material y al color.

***** 3.7.2. Carga de texturas
OpenGL gestiona varias texturas por identificadores, en cada momento
habrÃ¡ una sola activa. Las texturas se guardan en RAM.

#+BEGIN_SRC c++
// Genera
GLuint idTex;
glGenTextures(1, &idTex);

// Asocia, con potencias o mipmaps
glTexImage2D(GL_TEXTURE_2D, 0,GL_RGB,ancho,alto,borde = 0, GL_RGB,GL_UNSIGNED_BYTE, texels);
gluBuild2DMipmaps(GL_TEXTURE_2D, GL_RGB, ancho,alto, GL_RGB, GL_UNSIGNED_BYTE, texels);

// Activa
glBindTexture(GL_TEXTURE_2D, idTex);
#+END_SRC

***** 3.7.3. ConfiguraciÃ³n de texturas
Determinan la apariencia de textura

 * color de texels,
 * selecciÃ³n de texels (cercano o interpolaciÃ³n),
 * selecciÃ³n fuera de rango (replicado o truncamiento),
 * coordenadas explÃ­citas o procedurales.

****** Texturas, reflectividades e iluminaciÃ³n
#+BEGIN_SRC c++
glLightModeli(GL_LIGHT_MODEL_COLOR_CONTROL, GL_SINGLE_COLOR); // Color en lugar de reflectividades
glLightModeli(GL_LIGHT_MODEL_COLOR_CONTROL, GL_SEPARATE_SPECULAR_COLOR); // Especular aparte
#+END_SRC

****** SelecciÃ³n de texels
#+BEGIN_SRC c++
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); // MÃ¡s cercano
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);  // InterpolaciÃ³n
#+END_SRC

****** Tipo de generaciÃ³n procedural
Dos tipos de generaciÃ³n,

#+BEGIN_SRC c++
glTexGeni(GL_S, GL_TEXTURE_GEN_MODE, GL_OBJECT_LINEAR); // Coordenadas de objeto
glTexGeni(GL_T, GL_TEXTURE_GEN_MODE, GL_OBJECT_LINEAR); // Coordenadas de ojo
#+END_SRC

****** EspecificaciÃ³n de coeficientes de generaciÃ³n procedural
Los coeficientes de las funciones lineales de generaciÃ³n

#+BEGIN_SRC c++
glTexGenfv(GL_S, GL_OBJECT_PLANE, coefsS);
glTexGenfv(GL_T, GL_OBJECT_PLANE, coefsT);
#+END_SRC

***** 3.7.4. AsignaciÃ³n explÃ­cita de texturas y VBOs
Puede hacerse con =glBegin/glEnd= usando =glTexCoord2f=. Pueden
crearse VBOs con coordenadas de textura.

***** 3.7.5. RepresentaciÃ³n de texturas
Clase =Textura=.

**** 3.8. Materiales en grafo de escena
***** 3.8.1. Modelo de aspecto
Los materiales dan un modelo de aspecto que puede insertarse en el
grafo de escena afectando a todas las entradas por debajo. Para esto
es cÃ³modo tener una =PilaMateriales=.

***** 3.8.2. ImplementaciÃ³n de materiales en el grafo
AÃ±adimos materiales a =EntradaNGE= y visualizamos con =visualizarGL=.

**** 3.9. VisualizaciÃ³n con cauce grÃ¡fico programable
***** 3.9.1. IntroducciÃ³n
La Ãºnica forma de evaluar el MIL es usar vertex+fragment shader en el
cauce grÃ¡fico con GLSL.

****** ParÃ¡metros del vertex shader
Hay parÃ¡metros de entrada al vertex shader, que se enviarÃ¡n con
=glVertexAttrib= y =glVertexAttribPointer=

 * *uniform*, mismo valor para todos los vÃ©rtices,
 * *vÃ©rtice*, potencialmente distintos para cada vÃ©rtice.

Los parÃ¡metros de salida se entregan interpolados al fragment shader.
Pueden declararse explÃ­citamente y estÃ¡n predefinidos algunos.

***** 3.9.2. Sombreado de pixeles (fragment shader)
El shader tiene

 * parÃ¡metros *uniform*, iguales en todos los pixeles =glUniform=,
 * parÃ¡metros *in*, interpolados a partir de los *out* del vertexShader.

El factor geomÃ©trico de la pseudo-especular puede calcularse usando
Blinn-Phong y puede escribirse una completa evaluaciÃ³n del MIL.

***** 3.9.3. Atributos vÃ©rtice genÃ©ricos
Es necesario usar atributos de vÃ©rtice para los parÃ¡metros de entrada.
Tienen,

 * localizaciÃ³n, identificador en la aplicaciÃ³n,
 * nombre, identificador en el fuente del vertexshader.

Se puede asociar localizaciÃ³n a los nombres con =glBindAttribLocation=
y se pueden enviar tablas de atributos genÃ©ricos.

*** Tema 4. InteracciÃ³n y animaciÃ³n
**** 4.1. IntroducciÃ³n
Buscamos un sistema grÃ¡fico interactivo que responda al usuario
interactivamente. HabrÃ¡ retroalimentaciÃ³n, tÃ©cnicas y funciones de
entrada que lean de dispositivos lÃ³gicos, cambiando su estado y
generando eventos.

***** Leer de dispositivos
Existen tres modos

 - modo de muestreo :: variables con el estado actual, la CPU debe
      muestrear a frecuencia suficiente.
 - modo de peticiÃ³n :: se hace una peticiÃ³n y se espera a que ocurra
      el evento determinado, pueden perderse eventos y tiempo
      esperando.
 - modo cola de eventos :: se aÃ±ade a una cola FIFO cada evento y se
      va procesando luego.

**** 4.2. Eventos en GLUT
GLUT gestiona los eventos con cola de eventos; cada evento va asociado
a un *callback*, una funciÃ³n que lo trata y toma parÃ¡metros de Ã©l.

***** Funciones de registro de callback
#+BEGIN_SRC c++
glutDisplayFunc(); // es necesario redibujar la imagen.
glutMouseFunc(); // pulsar/levantar de botones del ratÃ³n.
glutMotionFunc(); // movimiento del ratÃ³n con un botÃ³n pulsado.
glutPassiveMotionFunc(); // movimiento del ratÃ³n sin botÃ³n pulsado.
glutReshapeFunc(); // cambio de tamaÃ±o de la ventana.
glutKeyFunc(); // pulsar o levantar de tecla.
glutIdleFunc(); // ausencia de eventos externos.
glutTimerFunc(); // ha transcurrido un intervalo de tiempo.
#+END_SRC

***** Eventos de botones de ratÃ³n
Declaramos un callback 

#+BEGIN_SRC c++
void FGE_BotonRaton(GLint boton, GLint estado, GLint x, GLint y);
#+END_SRC

donde =boton= toma tres constantes (=GLUT_LEFT_BUTTON=,
=GLUT_RIGHT_BUTTON=, =GLUT_MIDDLE_BUTTON=) segÃºn el botÃ³n pulsado y
dos estados (=GLUT_UP=, =GLUT_DOWN=) segÃºn se haya pulsado o levantado
la =x,y= indica la posiciÃ³n del ratÃ³n en cada momento.

****** Ejemplo de callback de botones de ratÃ³n
#+BEGIN_SRC c++
int xClickIzq, yClickIzq; // posiciÃ³n del Ãºltimo click del botÃ³n izquierdo
[...]

void FGE_BotonRaton(int boton,int estado,int x,int y) {
  if (boton == GLUT_LEFT_BUTTON && estado == GLUT_DOWN) { 
    xClickIzq = x; 
    yClickIzq = y; 
  }
  else if
  [...]
}
#+END_SRC
***** Eventos de movimiento de ratÃ³n
****** Ejemplo de callback de movimiento de ratÃ³n
**** 4.3. Posicionamiento
La posiciÃ³n que introduce un usuario estÃ¡ en /coordenadas de dispositivo/
y es necesario pasarla a coordenadas de mundo.

***** Posicionamiento 2D
Las coordenadas de dispositivo $x,y$ se convierten a mundo $x',y'$ con una
transformaciÃ³n inversa

\[\begin{aligned}
x' &= X_{min} + x (X_{max} + X_{min}) / \mathrm{ancho}; \\
y' &= Y_{max} - y (Y_{max} + Y_{min}) / \mathrm{alto}; \\
\end{aligned}\]

donde los parÃ¡metros son los que determinan el ancho y alto del
dispositivo y mÃ­nimos y mÃ¡ximos del mundo.

#+BEGIN_SRC c++
glOrtho(Xmin,Xmax, Ymin,Ymax, Zmin,Zmax);
glViewport(x0, y0, ancho, alto);
#+END_SRC

***** Posicionamiento 3D
Se restringe a un plano no perpendicular al de proyecciÃ³n y se traza
una recta desde el centro de proyecciÃ³n por el punto introducido, que
cortarÃ¡ al plano dado.

**** 4.4. Control de cÃ¡maras
Dos usos de la cÃ¡mara

 - visualizaciÃ³n de objetos en modo *orbital* centrando el objeto;
 - exploraciÃ³n de escenario en *primera persona*, desplazando VRP y
   rotando VPN y VUP en torno al marco de coordenadas de la cÃ¡mara.

El *marco de coordenadas de vista* estÃ¡ determinado por tres versores
ortonormales $x_c, y_c, z_c$ y un origen $o_c$. La *matriz de vista* se obtiene
directamente desde ellas

\[V = \begin{pmatrix}
x_c(0) & y_c(0) & z_c(0) & -o_c \cdot x_c \\
x_c(1) & y_c(1) & z_c(1) & -o_c \cdot y_c \\
x_c(2) & y_c(2) & z_c(2) & -o_c \cdot z_c \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\]

y transforma coordenadas de mundo en coordenadas de cÃ¡mara.

***** 4.4.1. CÃ¡maras en modo primera persona

 * *Rotaciones*: se rotan VPN y VUP en torno a PRP (origen). VPN rota
   en horizontal o vertical y VUP permite rotar en torno a un centro.
   La rotaciÃ³n se hace con los ejes del marco.

 * *Traslaciones*: se desplaza PRP en la direcciÃ³n de traslaciÃ³n.
   La traslaciÃ³n se hace con los ejes del marco.

***** 4.4.2. CÃ¡maras orbitales
Las coordenadas esfÃ©ricas, el punto de atenciÃ³n y la distancia a Ã©l
fijan la cÃ¡mara. El marco de vista puede obtenerse desde estos
parÃ¡metros.

**** 4.5. SelecciÃ³n
Se pueden dar identificadores de selecciÃ³n a

 * triÃ¡ngulos,
 * mallas,
 * grupos de objetos,
 * nodos del grafo de escena.

Puede seleccionarse pixel en pantalla y buscar identificadores
proyectados en ese pixel. La bÃºsqueda puede hacerse por

 * *ray-casting*, por intersecciÃ³n de rectas,
 * *clipping*, recortando dentro de un view-frustum pequeÃ±o,
 * *rasterizaciÃ³n*, visualiza con identificadores.

En OpenGL hay un modo selecciÃ³n y puede usarse un framebuffer distinto
para rasterizar con identificadores.

***** 4.5.1. SelecciÃ³n de OpenGL [Obsoleto]
#+BEGIN_SRC c++
glRenderMode(GL_SELECT);
glRenderMode(GL_RENDER);
#+END_SRC

Hay una pila de nombres que se almacenan en un buffer de
selecciÃ³n. Durante la rasterizaciÃ³n, OpenGL registra nombres.

***** 4.5.2. SelecciÃ³n con frame-buffer invisible
Se puede usar de dos formas, la segunda mÃ¡s simple

 * crear un *frame-buffer object* (FBO);
 * *doble buffer* un back buffer y un front buffer.

****** VisualizaciÃ³n con identificadores
Codificamos los identificadores como colores en lugar de usar los
colores de los objetos. Cambiamos el color actual de OpenGL y desactivamos
la iluminaciÃ³n, las texturas, usar sombreado plano y triÃ¡ngulos planos.

Este es el *modo identificadores* de visualizaciÃ³n.

****** TransformaciÃ³n de identificadores a colores
Los identificadores deben ser =unsigned char= con una variante de
=glColor= que los acepta en lugar de valores flotantes. Puede
reconstruirse el unsigned de nuevo desde los tres colores.

**** 4.6. AnimaciÃ³n
#+BEGIN_SRC c++
glutPostRedisplay(); // Regenera la imagen
glutSwapBuffers();   // Intercambia buffer dibujado
glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH); // ActivaciÃ³n de buffer
#+END_SRC

La modificaciÃ³n en escena se puede hacer con

 * *keyframes*, configuraciones sobre las que se interpola,
 * *simulaciÃ³n fÃ­sica*, usando mecÃ¡nica clÃ¡sica,
 * *esqueletos*, que simplifican la animaciÃ³n,
 * *animaciÃ³n procedural*, objetos descritos por procedimientos.

*** Tema 5. Realismo en rasterizaciÃ³n, ray-tracing
**** 5.1. TÃ©cnicas realistas de rasterizaciÃ³n
***** 5.1.1. Mipmaps
La resoluciÃ³n de las texturas debe adaptarse a la imagen. Puede
solucionarse con *antialiasing* o con *mipmaps*, una serie de texturas
$\left\{ M_i \right\}$ donde cada una se obtiene promediando grupos de
$4$ pixels de la anterior, con resoluciones $2^{n-i} \times 2^{n-i}$.

El $i$ de textura que se usa crece con el logaritmo de la distancia.

***** 5.1.2. PerturbaciÃ³n de la normal
Las rugosidades a pequeÃ±a escala causarÃ­an un rendering muy lento; se
usa una textura que modifica la normal a pequeÃ±a escala (*bump-maps*).

 * La textura la da un campo de alturas, que puede ser generado
   procedural o dado como tonos de gris.

 * Sobre ese campo de alturas se calculan derivadas parciales o
   diferencias finitas $d_u,d_{v}$.

 * A las derivadas en un punto se les llama tangente y bitangente y
   definen un plano tangente perpendicular a la normal. Pueden
   obtenerse interpolando en algunos casos.

***** 5.1.3. Sombras arrojadas
Las sombras arrojadas plantean un problema similar a visibilidad. 

 * Lo soluciona el Algoritmo de Weiler-Atherton-Greenberg, para
   eliminaciÃ³n de partes ocultas.

 * MÃ¡s eficiente es usar Z-buffer.

***** 5.1.4. Superficies transparentes
La refracciÃ³n la calcula la ley de Snell, $n_i\sin(\theta_i) = n_j\sin(\theta_j)$.

 * El Z-buffer sÃ³lo puede tener en cuenta los rayos que van hacia el
   observador, pero puede adaptarse a superficies transparentes cuando
   no hay refracciÃ³n. Los colores en superficies transparentes dependen
   del orden de los polÃ­gonos.

 * La reflexiÃ³n especular no puede reproducirse con los mÃ©todos
   vistos.  Pueden usarse mapas de entorno tipo caja; y en espejos
   planos puede sintetizarse directamente una cÃ¡mara simÃ©trica.

**** 5.2. Ray-tracing
***** 5.2.1. El algoritmo de Ray-tracing
Ray-tracing reÃºne todos los efectos anteriores y es mÃ¡s sencillo y realista
que el Z-buffer. Cada pixel crea un rayo primario.

***** 5.2.2. EvaluaciÃ³n del MIL

 * Las sombras se calculan siguiendo el rayo hasta la fuente.
 * Las superficies especulares o con refracciÃ³n crean rayos
   secundarios.

***** 5.2.3. Esquema del algoritmo
La funciÃ³n es recursiva, devuelve un color y tendrÃ¡ algÃºn parÃ¡metro
para evitar la recursividad infinita.

*** Ejercicios
**** Ejercicio 12
***** apartado a
4by

vÃ©rtices = (n+1)*(m+1) = nm + n + m + 1
caras = 2nm

floats = 3*vertices = 3nm + 3n + 3m + 3
ints = 3*caras = 6nm

tamaÃ±o = 4*ints + 4*floats = 24nm + 12nm + 12n + 12m + 12 = 36nm + 12n + 12m + 12

***** apartado b
592908

***** apartado c
1/2

**** Ejercicio 13
verticestira = 2n+2
totalvert = verticestira * m
tamaÃ±o = 4*totalvert = 3*4m(2n+2) = 24nm + 24m
**** Ejercicio 39
La matriz de vista simplemente centrarÃ­a en la cÃ¡mara

\[
\mathrm{gluLookAt}((c_x,c_y,c_z), (c_x,c_y,c_z-1), (0,1,0))
\]

mientras que la de proyecciÃ³n describe el cuadro ortogonal

\[
\mathrm{glOrtho}\left(-\frac{s}{2},\frac{s}{2},-\frac{s}{2},\frac{s}{2},-\frac{s}{2},\frac{s}{2}\right).
\]
**** TODO Ejercicio 40
**** TODO Ejercicio 41
**** TODO Ejercicio 42
**** TODO Ejercicio 43
**** Ejercicio 46
# Tabla de coordenadas de textura
** IngenierÃ­a, empresa y sociedad
#
# Estos apuntes de empresa han sido escritos por Mario RomÃ¡n, tomando
# como base la docencia de Matilde Ruiz Arroyo para la asignatura de
# IngenierÃ­a, Empresa y Sociedad.
#
# Pueden copiarse y distribuirse bajo la siguiente licencia respetando
# la nota de autorÃ­a original:
#
#  Creative Commons Attribution-NonCommercial-ShareAlike 4.0 
#  International Public License
# 

*** Detalles de la asignatura
Profesora Matilde Ruiz, departamento de OrganizaciÃ³n de Empresas. 
(matilderuiz@ugr.es)

*EvaluaciÃ³n:*

- 70% Pruebas objetivas: la prueba final de junio y los exÃ¡menes parciales.
- 30% Actividades prÃ¡cticas: participaciÃ³n, ejercicios y asistencia.

*BibliografÃ­a:*

 - Fuentes Fuentes M. et al. /"Fundamentos de direcciÃ³n y administraciÃ³n de empresas"/

*** 1. La empresa y direcciÃ³n de empresas
**** 1.0. IntroducciÃ³n
***** EconomÃ­a
La *economÃ­a* se define como la ciencia que estudia la administraciÃ³n
eficiente de la escasez de recursos. TambiÃ©n como ciencia que
estudia la elecciÃ³n.

Este concepto proviene de las ideas de *Malthus*. La escasez justifica
la necesidad de reparto y la pobreza.

***** AdministraciÃ³n de empresas
La *administraciÃ³n de empresas* estudia la economÃ­a en el contexto de
una empresa, asÃ­ como la gestiÃ³n de proyectos, la planificaciÃ³n y el
liderazgo. Es una disciplina cientÃ­fica multidisciplinar y contempla
aspectos psicosociales.

**** 1.1. Concepto de empresa y de organizaciÃ³n
***** OrganizaciÃ³n
Una *organizaciÃ³n* es una unidad social deliberadamente destinada a un
objetivo especÃ­fico. Requiere constar de una estructura interna
deliberada y fijar el objetivo comÃºn.

****** DefiniciÃ³n de Gibson
La *organizaciÃ³n* es una unidad coordinada /deliberada/ formada por
mÃ¡s de una persona que trabaja para alcanzar un /objetivo comÃºn/.

****** DefiniciÃ³n de Etzioni
Unidades sociales deliberadamente constituidas para promover objetivos
especÃ­ficos.

****** ClasificaciÃ³n de organizaciones
Una clasificaciÃ³n genÃ©rica sencilla las separa segÃºn tengan o no
*Ã¡nimo de lucro*.

******* Ejemplo: Inditex
Industria de diseÃ±o textil INDITEX S.A. es una empresa multinacional
que tiene como objetivo maximizar el beneficio. SegÃºn la empresa,
su objetivo es "/escuchar a los clientes para ofrecerles las propuestas/
/de moda que desean/".

******* Ejemplo: Amigos del museo del Prado
En sus estatutos aclara que su fin primario es cultural. Puede
obtener beneficios econÃ³micos, pero serÃ¡n un fin secundario, sÃ³lo
un medio para su fin primario cultural:

"/tiene por fin particular todo lo relacionado con la promociÃ³n,/
/estÃ­mulo, apoyo y desarrollo de cuantas acciones culturales,/
/educativas y de otra Ã­ndole tengan relaciÃ³n con el Museo/"

***** DefiniciÃ³n de empresa
La *empresa* es una organizaciÃ³n que /transforma/ un conjunto de recursos
fÃ­sicos, monetarios y cognitivos en bienes y/o servicios, con el objetivo
principal de /obtener beneficios/.

****** Autores
Otras definiciones de empresa.

 - Unidad tÃ©cnico-econÃ³mica que combina elementos humanos y financieros.
 - Una organizaciÃ³n con Ã¡nimo de lucro.
 - Unidad de decisiÃ³n.
 - Sistema en el que se coordinan factores de producciÃ³n.
 - Conjunto de factores de producciÃ³n.

***** Â¿Es el beneficio el Ãºnico objetivo?
Hay posiciones distintas.

****** Friedman
SÃ­, hay que maximizarlo teniendo como Ãºnicas restricciones

 * las leyes,
 * las normas de la economÃ­a capitalista, y
 * evitar engaÃ±os y fraudes.

****** Freeman
No, no sÃ³lo hay que maximizar el beneficio, tambiÃ©n hay que satisfacer
a los *stakeholders*, que son todas las personas que tienen intereses en
la compaÃ±Ã­a.

***** Stakeholder
Los *stakeholders* son todas las partes interesadas en una empresa. Pueden
ser

 * *internas*, como los empleados, gerentes y propietarios.
 * *externas*, como los proveedores, clientes y la sociedad.

Originalmente, los stakeholders se definieron como los miembros de los grupos
sin cuyo apoyo la empresa dejarÃ­a de existir.

***** La empresa social
Las *empresas sociales* nacen con el objetivo de resolver problemas
sociales. No tienen Ã¡nimo de lucro, sino que buscan un objetivo
social.

Organizaciones con Ã¡nimo de lucro y sin Ã¡nimo de lucro pueden tener un
fin fundamental con beneficio social, medioambiental o polÃ­tico. Pero
a las empresas sociales se les exige que sean rentables por sÃ­ mismas.

En su filosofÃ­a se incluye que no reemplaza a la empresa tradicional,
sino que que quiere coexistir con ella. Como ejemplos:

 - plataformas sociales, como /Ashoka/ y /SocialEmprende/.
 - /DBS/.

***** Tipos de responsabilidad jurÃ­dica
Existen varias formas de responsabilidad, que indican cÃ³mo responderÃ¡ la
empresa ante las posibles pÃ©rdidas. La responsabilidad puede ser limitada
o ilimitada y solidaria o mancomunada.

****** Responsabilidad ilimitada
Se llama *responsabilidad ilimitada* cuando el propietario responde
con todo el patrimonio ante las posibles pÃ©rdidas de la
empresa. Existe cuando Ã©ste realiza la actividad con su propia persona
como personalidad jurÃ­dica.

****** Responsabilidad limitada
Se llama *responsabilidad limitada* a aquella que no obliga a responder
con el patrimonio a los socios ante posibles pÃ©rdidas. La responsabilidad
de cada socio estÃ¡ limitada por sus aportaciones.

****** Responsabilidad solidaria
La *responsabilidad solidaria* se reparte entre los socios equitativamente.

****** Responsabilidad mancomunada
La *responsabilidad mancomunada* hace que cada socio responda por su parte.

***** ClasificaciÃ³n de empresas segÃºn forma jurÃ­dica (individuales)
Constituidas por una persona fÃ­sica, que sigue usando su NIF y tributando
mediante el IRPF.

****** Empresario individual
Cuando el *empresario individual* responde por la empresa de manera
ilimitada. Se constituye como una mera persona fÃ­sica, que responde
con su patrimonio, utiliza su NIF como persona jurÃ­dica y paga el IRPF.

****** Emprendedor de responsabilidad limitada
El *emprendedor de responsabilidad limitada* es una forma jurÃ­dica
permitida segÃºn ciertos criterios. La Ãºnica diferencia es que la
responsabilidad del empresario individual pasa a limitarse. 

Aun asÃ­, sigue siendo un empresario individual. No tiene CIF y paga el
IRPF.

***** ClasificaciÃ³n de empresas segÃºn forma jurÃ­dica (societarias)
En una *forma societaria*, varias personas aportan capital o trabajo
bajo un contrato de asociaciÃ³n que da lugar a una personalidad jurÃ­dica
nueva.

****** Sociedad colectiva
En una *sociedad colectiva* la responsabilidad es /ilimitada y
mancomunada/ entre los socios, pero sÃ³lo participan del beneficio por
la parte que han aportado de capital y trabajo.

****** Sociedad comanditaria
En una *socidedad comanditaria* existen dos tipos de socios

 - *socios colectivos*, similares a los de las sociedades colectivas,
   aportando trabajo y capital y con responsabilidad ilimitada.
 - *socios comanditarios*, sÃ³lo aportan capital y no trabajan en la
   empresa. Su responsabilidad se limita a su capital y sus beneficios
   son proporcionales a su aportaciÃ³n.

****** Sociedad limitada (SL)
En una *sociedad limitada* el capital se divide en *participaciones 
sociales*. Estas se diferencian de las /acciones/ en que existen obstÃ¡culos
legales a su transmisiÃ³n directa; sÃ³lo pueden ser transmitidas si da el
visto bueno una /junta de socios/ o la transmisiÃ³n se hace a familiares o a
otros socios.

Tiene trÃ¡mites mÃ¡s costosos y lentos que otras formas societarias de
responsabilidad ilimitada. El capital social en ellas no puede ser inferior
a los 3000â¬ mÃ­nimos iniciales.

****** Sociedad de responsabilidad limitada unitaria (SLU)
Hay una *sociedad de responsabilidad limitada unitaria* cuando existe un
socio Ãºnico, que debe cumplir con una declaraciÃ³n de unipersonalidad y que
puede tomar decisiones unilateralmente.

****** Sociedad de responsabilidad limitada nueva empresa (SLNE)
La *sociedad limitada nueva empresa* es una especializaciÃ³n de la
sociedad limitada que facilita su creciÃ³n. Se da de alta en un
procedimiento telemÃ¡tico de 48 horas y se utilizan modelos genÃ©ricos
para constituirla.

****** Sociedad anÃ³nima (SA)
En una *sociedad anÃ³nima*, el capital se divide en partes iguales
(alÃ­cuotas) llamadas *acciones*. Las acciones se transmiten libremente
en el mercado financiero a terceros, suelen representar un voto y la
responsabilidad de los socios o /accionistas/ se limita a su
aportaciÃ³n. Es el Ãºnico tipo de sociedad que puede cotizar en Bolsa.

Necesita 60000â¬ como capital mÃ­nimo inicial.

****** Empresas de economÃ­a social
Las *empresas de economÃ­a social* suelen constituirse para solventar
crisis de las empresas. Suelen ser democrÃ¡ticas.

******* Sociedades cooperativas (SC)
Las *sociedades cooperativas* no tienen Ã¡nimo de lucro. Cada socio
aporta capital y trabajo; responde sÃ³lo con el capital aportado y
tiene un voto independientemente del capital. Es /limitada/ y 
/solidaria/.

Estas uniones de trabajo asociado suelen darse cuando los trabajadores
tienen un objetivo comÃºn fuerte. Tienen regulaciÃ³n estatal y
autonÃ³mica y en ocasiones exenciones fiscales (las andaluzas se llaman
SCA).

El beneficio se llama /retorno/. Las reservas (la parte del beneficio
que no se distribuye entre los socios), suelen ser mÃ¡s grandes. Suelen
tener una reserva obligatoria para invertir en la formaciÃ³n de los
socios.

/Ejemplo: cooperativa MondragÃ³n/

******* Sociedades laborales
Las *sociedades laborales* son sociedades anÃ³nimas o de responsabilidad
limitada donde los trabajadores poseen al menos el 51% del capital.

Suelen ser intentos de los trabajadores de evitar la quiebra de la
empresa.

***** ClasificaciÃ³n de empresas segÃºn tamaÃ±o
La clasificaciÃ³n segÃºn tamaÃ±o se realiza en base a varios criterios, tales
como

 * *nÃºmero de trabajadores* contratados en la empresa.

 * *volumen de negocio anual*, el total de ingresos por ventas
   contabilizadas. Equivale al valor total de los bienes y servicios
   vendidos en un aÃ±o.

 * *balance general anual (activo)*, bienes y derechos de la empresa, sin
   contar las obligaciones de pago, que pertenecerÃ­an al /pasivo/.

Por ellos se dividen en: grandes, medianas, pequeÃ±as y
microempresas. El 99.9% de empresas en EspaÃ±a son PYMEs (pequeÃ±as y
medianas empresas).

****** Empresas autÃ³nomas o asociadas
En el caso de que la empresa tenga relaciÃ³n con otras, sus datos
deberÃ¡n incluirse al calcular su tamaÃ±o. Una empresa se clasifica
segÃºn su relaciÃ³n con otras en

  * *autÃ³noma* si es totalmente independiente y no tiene participaciÃ³n en
    otras empresas, ni otras empresas en ella.
  * *asociada* si hay terceros con mÃ¡s del 25% de la empresa o tiene
    participaciÃ³n de mÃ¡s del 25% en otra empresa.

Aun asÃ­ existen empresas que se consideran autÃ³nomas a pesar de esto.
AdemÃ¡s, dos empresas estÃ¡n *vinculadas* cuando una puede ejercer
influencia dominante sobre la otra, ya sea nombrando miembros del
consejo de administraciÃ³n, mediante clÃ¡usulas estatutarias o contratos.

**** 1.2. Enfoque sistÃ©mico de la empresa
***** DefiniciÃ³n de sistema
Un *sistema* es un conjunto de elementos relacionados dinÃ¡micamente que
realizan una actividad para alcanzar un objetivo; operando con entradas
y proveyendo salidas.

***** Condiciones para la existencia de un sistema
Para considerar algo un sistema se le exigen

 * un *conjunto de elementos*, los factores productivos.
 * una *estructura de sistema*, jerarquÃ­a de la empresa.
 * un *plan comÃºn*, la misiÃ³n de la empresa.
 * unas *funciones caracterÃ­sticas*, las funciones tÃ©cnicas y
   administrativas que desarrollan la actividad.
 * un *conjunto de estados*, el balance de la empresa.

La empresa transforma materias primas (proceso tÃ©cnico), transforma
ahorro en capital (proceso financiero) y procesa informaciÃ³n (proceso
mental).

***** ClasificaciÃ³n de sistemas
Se clasifican en

 * *abiertos* si se relacionan con el entorno.
 * *cerrados* si no interaccionan con el entorno.
 * *naturales* si no influye el ser humano en su creaciÃ³n.
 * *artificiales* si se crean por voluntad humana.

La empresa se considera un sistema abierto artificial.

***** RetroalimentaciÃ³n o feedback
La empresa se considera *autorregulada* porque cuando se desvÃ­a de los
objetivos, el proceso de *retroalimentaciÃ³n o feedback* permite
conocer a la empresa que se han producido estas desviaciones y
corregirlas. Puede asÃ­ adaptarse al entorno.

La *homeostasis* es la capacidad de la empresa de mantener esa
estabilidad.

**** 1.3. Subsistemas funcionales de la empresa
***** Principio de jerarquÃ­a
El *principio de jerarquÃ­a* permite descomponer un sistema en
subsistemas y estudiarlos concretamente.

***** Subsistemas segÃºn criterio funcional
El *criterio funcional* de jerarquizaciÃ³n divide a la empresa en tantos
subsistemas como actividades desarrolle. Estos son

 * *subsistema de aprovisionamiento*, que adquiere los insumos.
 * *subsistema de producciÃ³n*, transforma insumos en productos.
 * *subsistema de comercializaciÃ³n*, decide precio, promociÃ³n y
   distribuciÃ³n.
 * *subsistema de recursos humanos*, selecciona y orienta
   trabajadores.
 * *subsistema financiero*, decide los fondos y aplica inversiones.
 * *subsistema de direcciÃ³n*, estrategia y gestiÃ³n de la empresa.

***** Sinergia
La *sinergia* es el aumento de productividad de varios sistemas cuando
interactÃºan entre ellos frente a cuando trabajan de manera aislada.

**** 1.4. La direcciÃ³n de empresas
***** Eficiencia y eficacia
La *eficacia* mide el nivel de cumplimiento de los objetivos. La *eficiencia*
mide el uso de la cantidad adecuada de recursos para lograr sus objetivos.

Una empresa bien dirigida debe ser eficaz y eficiente.

***** Funciones de los administradores
La gestiÃ³n empresarial define las siguientes *funciones del directivo*, que
son propias del subsistema de management o direcciÃ³n.

 * La *planificaciÃ³n* define la estrategia de la empresa y los planes para
   conseguir los objetivos.
 * La *organizaciÃ³n* diseÃ±a la estructura para realizar las tareas,
   asignando personal y decidiendo cÃ³mo se tomarÃ¡n decisiones.
 * La *direcciÃ³n* coordina a la organizaciÃ³n, motivando, comunicando y
   resolviendo potenciales conflictos.
 * El *control* vigila el desempeÃ±o de la organizaciÃ³n, lo compara con
   los objetivos y lo corrige en caso de que sea necesario.

*** 2. TeorÃ­as de la empresa y del empresario
**** 2.1. TeorÃ­as de la empresa
# Nada

**** 2.2. TeorÃ­as del empresario
***** EvoluciÃ³n histÃ³rica del empresario
Se consideran figuras distintas segÃºn la Ã©poca, en

 * *capitalismo mercantilista* (S. XVI-XVIII), existe un estado que es el
   agente econÃ³mico predominante y mercaderes, que simplemente comercian.
 * *revoluciÃ³n industrial* (S. XVIII-XIX), se desarrolla el pensamiento
   clÃ¡sico capitalista del empresario *Adam Smith* habla de la regulaciÃ³n
   del mercado por la mano invisible. En el siglo XIX, *Karl Marx* explica
   el beneficio como la extracciÃ³n de la plusvalÃ­a de los trabajadores
   mediante la propiedad privada de los medios de producciÃ³n.
 * *aportaciones posteriores*, Cantillon define en el S. XVIII el
   /entrepreneur/ y habla del talento del empresario. Say aporta la
   funciÃ³n directiva en el S. XIX.

***** TeorÃ­a del empresario riesgo. Knight (1921)
*Knight* desarrolla la figura del empresario como persona que asegura las
rentas de los factores productivos, adelantando el pago. El riesgo que
asume al aportar el dinero es lo que justifica el beneficio empresarial.

Los riesgos serÃ¡n

  - *tÃ©cnicos*, como cumplir con la producciÃ³n esperada.
  - *financieros*, al aportar el capital inicial.

***** TeorÃ­a del innovador de Schumpeter (1912)
*Schumpeter* diferencia en /TeorÃ­a del desenvolvimiento econÃ³mico/
entre capitalista y empresario. El empresario serÃ¡ el que aplica una
tecnologÃ­a existente a un problema real, el capitalista pone el dinero.

Se justifica el beneficio porque se dice que esa innovaciÃ³n es la que
desencadena el desarrollo econÃ³mico y social.  AsÃ­, el empresario que
innova es el motor del progreso econÃ³mico y social.

El cambio tecnolÃ³gico se desarrolla en un ciclo entre un *monopolio*
*temporal* del empresario sobre la innovaciÃ³n y lo pierde luego a una
*situaciÃ³n de equilibrio*.

# **** AutÃ³nomos
# Nos dicen que hay menos autÃ³nomos en EspaÃ±a que en Europa por
# culpa de las cuotas de autÃ³nomo. AquÃ­ artÃ­culos en contra:
#
# - [[http://www.ticbeat.com/empresa-b2b/desmontando-el-mito-de-la-cuota-de-autonomos-en-espana-y-europa/][Desmontando el mito de la cuota de autÃ³nomos en EspaÃ±a]]
# - [[http://www.elderecho.com/actualidad/Espana-quinto-Europa-autonomos-ATA_0_457500143.html][EspaÃ±a, quinto paÃ­s de Europa que mÃ¡s autÃ³nomos crea con 46.000 nuevas altas]]
#
# Pero hay miles a favor tambiÃ©n. OjalÃ¡ manejÃ¡ramos datos.

****** Proceso de cambio tecnolÃ³gico
Se definen tres fases

 - *invenciÃ³n*, generaciÃ³n de nuevas ideas, ajena a la actividad empresarial.
 - *innovaciÃ³n*, aplicaciÃ³n de la invenciÃ³n a un producto.
 - *imitaciÃ³n*, difusiÃ³n de la innovaciÃ³n.

***** Tecnoestructura de Galbraith (1950s)
*Galbraith* supera la concepciÃ³n de empresario como persona y deja que
delegue en la /tecnoestructura/, un grupo de personas, un Ã³rgano
colegiado, que dirige la empresa.

Separa propietario (capital de la empresa) y gestor (administraciÃ³n).

**** 2.3. Propiedad, direcciÃ³n y gobierno de la empresa
***** DefiniciÃ³n de directivo
El *directivo* supervisa la combinaciÃ³n de los recursos productivos. Sus
funciones son

 * fijar objetivos y toma decisiones.
 * coordinar la empresa.
 * coordinar relaciÃ³n de la empresa con el entorno.

Los directivos suelen ser los mismos propietarios, pero pueden ser gestores
contratados u otras personas al nombre del propietario.

***** DefiniciÃ³n de capitalista
El *capitalista* es el propietario del capital de la empresa.

***** Empresario
El *empresario* es un directivo capitalista. 

/Ejemplos: Amancio, Florentino./

****** Emprendedor
Llamamos *emprendedor* al empresario que es a su vez el creador de la
idea del negocio o del cambio y se implica a nivel gestor y
capitalista.

****** Empresario individual propietario
SegÃºn Cuervo (1997), es el empresario clÃ¡sico en el que convergen 
capitalista y directivo; sigue las nociones de [[*TeorÃ­a del empresario riesgo. Knight (1921)][empresario riesgo]] y
[[*TeorÃ­a del innovador de Schumpeter (1912)][empresario innovador]].

****** Empresario corporativo
Controla la empresa sin participar significativamente en el capital.
Es parte sÃ³lo de la tecnoestructura.

***** Estructura de la propiedad de la empresa
La *estructura de la propiedad* de una empresa es el modo en el que se
distribuye la propiedad del capital de la empresa entre sus
propietarios legales. Todo partÃ­cipe en el capital de la empresa tiene
la consideraciÃ³n legal de *propietario*; es decir, los propietarios
son las personas (fÃ­sicas y jurÃ­dicas) que aportan el dinero y los
bienes necesarios para la actividad productiva.

Los propietarios pueden haber accedido a la titularidad

  * creÃ¡ndola.
  * heredÃ¡ndola.
  * comprÃ¡ndola.

****** Grupos de propiedad
Se consideran los siguientes grupos de propiedad en la empresa

 * sector pÃºblico.
 * particulares y familias.
 * empresas industriales y servicios (capital empresarial).
 * entidades financieras (capital bancario).

La estructura de la propiedad refleja la importancia relativa de los
grupos. VarÃ­a segÃºn sectores y paÃ­ses y condiciona los objetivos que
persigue la empresa.

***** Estructura accionarial
La /estructura de la propiedad/ en sociedades anÃ³nimas se distribuye
entre los *accionistas*. Los hay de dos tipos

 * *accionistas de control*, si son activos en las decisiones de la
   empresa.
 * *accionistas pasivos*, si son simples inversores financieros.

La estructura accionarial completa se divide en

 * *Autocartera*, las acciones propias que la sociedad mantiene entre
   sus activos.
 * *Accionistas mayoritarios y de control*, que tienen control de la
   empresa.
 * *PequeÃ±os accionistas*, capital flotante en compraventa libre en el
   mercado financiero. Son propietarios sin poder en la empresa, para
   controlarla necesitan asociarse.
 * *Inversores institucionales*, sociedades de inversiÃ³n, fondos de
   pensiones o compaÃ±Ã­as de seguros que buscan la rentabilidad.

***** Gobierno corporativo
El *gobierno corporativo* es la estructura que concreta las relaciones
entre todos los /stakeholders/ para establecer los objetivos de la empresa
y los medios para controlarla.

****** MotivaciÃ³n
Cuando las empresas son pequeÃ±as, suele coincidir la propiedad y la
gestiÃ³n. Conforme crecen, deben dotarse de estructuras que limiten
conflictos y aseguren que los directores no toman decisiones
contrarias a los propietarios.

Se intenta paliar un problema que surge en un contexto de informaciÃ³n
asimÃ©trica; donde las partes interesadas pueden tener intereses
contrarios a la empresa.

****** Responsabilidad social corporativa
Los criterios de *responsabilidad social corporativa* (RSC) son cÃ³digos
de buen gobierno que buscan asegurar
 
 * la confianza y la transparencia.
 * el adecuado funcionamiento de los Ã³rganos y la separaciÃ³n entre ellos.
 * el control interno y la responsabilidad.

En particular, las empresas que cotizan en la comisiÃ³n del mercado de
valores deben seguir un cÃ³digo de gobierno especÃ­fico.

***** Mecanismos de control
Los *mecanismos de control* delimitan el modelo de gobierno corporativo que
sigue la empresa.

***** Mecanismos internos de control
Los *mecanismos internos* son los diseÃ±ados por la propia organizaciÃ³n.

****** Junta general de socios accionistas
En una /sociedad anÃ³nima/, la *Junta General de Socios* es una reuniÃ³n
de accionistas que toma acuerdos por mayorÃ­a. 

Sus competencias son:

 * nombrar al consejo de administraciÃ³n.
 * nombrar y destituir administradores.
 * disolver la sociedad.
 * elegir consejero ejecutivo.
 * adquirir de determinados bienes y tomar otras decisiones.

Es obligatoria y debe reunirse anualmente. NÃ³tese que no puede
administrar, tomar acuerdos contrarios a los estatutos o representar a
la sociedad; sÃ³lo puede nombrar a los /administradores/.

****** Consejo de administraciÃ³n
En /sociedades de capital/ (limitadas, anÃ³nimas y comandatarias), el
*consejo de administraciÃ³n* estÃ¡ formado por personas elegidas por los
propietarios que administran y representan a la empresa.

EstÃ¡ compuesto por:

 * *Consejeros internos o ejecutivos.* Delegados de la
   empresa. /Ejemplo: Jobs, Zuckerberg./
 * *Consejeros externos dominicales.* Significativos. Representan
   empresas con capital en la sociedad.
 * *Consejeros externos independientes.* Expertos en gestiÃ³n que son
   independientes. /Ejemplo: Felipe GonzÃ¡lez, JosÃ© MarÃ­a Aznar./

****** Caso de las empresas pequeÃ±as
Normalmente coinciden. SÃ³lo a partir de cierto tamaÃ±o tienen que
nombrar administradores.

***** Mecanismos externos de control
Los *mecanismos externos* son los que se derivan de la estructura de la
economÃ­a de mercado.

****** Oferta PÃºblica de AdquisiciÃ³n de Valores (OPA)
Intenta comprar parte de una empresa para controlar sus cambios de
direcciÃ³n. Puede intentarse por alterar su estructura financiera o
para construir imperios empresariales.

****** Mercados financieros
Con los fondos se determinarÃ¡ el valor de la empresa y la posibilidad
de ser controlada desde el exterior.

****** Mercado laboral de consejeros y directivos
La alta competencia del mercado laboral de consejeros y directivos hace
que funcionen correctamente. Se compra su reputaciÃ³n positiva, que deben
haber ganado en otras empresas previamente.

**** 2.4. La direcciÃ³n: funciones y niveles
La *direcciÃ³n* consiste en la integraciÃ³n de las distintas partes de la
empresa entre sÃ­. Se divide en varios niveles de jerarquizaciÃ³n de las 
decisiones.

***** Alta direcciÃ³n
La *alta direcciÃ³n* estÃ¡ formada por personas con responsabilidad
sobre toda la empresa que fijan los grandes objetivos.  Lo forma el
/comitÃ© ejecutivo/ de la empresa con el /director ejecutivo/ (tambiÃ©n
consejero delegado o CEO) y las personas que considere para participar
de los anÃ¡lisis gestores de la empresa. 

/CEOs, ComitÃ© Ejecutivo, Presidente Ejecutivo, Consejero Delegado./

***** DirecciÃ³n media
La *direcciÃ³n media* la forman los directivos que actÃºan como enlace
jerÃ¡rquico entre la alta direcciÃ³n y la direcciÃ³n de primera lÃ­nea.
Marcan objetivos a medio y corto plazo, que deben estar alineados con
los grandes objetivos.

/Directores departamentales, director financiero, director comercial./

***** DirecciÃ³n de primera lÃ­nea
La *direcciÃ³n de primera lÃ­nea* la constituyen supervisores y directivos
que toman decisiones en problemas diarios y rutinarios para la empresa.

/Jefes de equipo, capataces, jefe de planta./

*** 3. Entorno de la empresa
**** Objetivos
Conocer:

 - AnÃ¡lisis de entorno. PEST.
 - Comprender su utilidad en la direcciÃ³n estratÃ©gica.

En conjunto forma el anÃ¡lisis DAFO.

***** AnÃ¡lisis externo: amenazas y oportunidades
Provenientes del entorno.

 * *Amenaza*, aspecto negativo del entorno.
 * *Oportunidad*, aspecto positivo del entorno.

***** AnÃ¡lisis interno: debilidades y fortalezas
Provenientes de la propia empresa.

 * *Debilidad*, aspecto negativo interno.
 * *Fortaleza*, aspecto positivo interno.

**** 3.1. DefiniciÃ³n de entorno
Entendiendo la empresa como un sistema abierto que se relaciona con el
exterior, tiene sentido estudiar el entorno como el conjunto de
fuerzas externas con las que se relaciona. 

***** DefiniciÃ³n de entorno
El *entorno* es el conjunto de factores que, siendo /externos/ a la
empresa, tienen o pueden tener incidencia en sus actuaciones y
resultados. La viabilidad depende de la empresa depende de ellos.

*(!) Nada que sea parte de la empresa es entorno de la empresa.*

****** DefiniciÃ³n de Downey y Slocum
El *entorno* es un conjunto de personas y grupos con las que la
organizaciÃ³n tiene relaciones de intercambio y de las que depende su
viabilidad.

****** DefiniciÃ³n de Mintzberg
El *entorno* es todo aquello ajeno a la empresa como organizaciÃ³n.

***** RelaciÃ³n con el entorno
SegÃºn sus decisiones cambia su relaciÃ³n con el entorno. La empresa
puede influir, pero no controlar el entorno.

****** Factores estratÃ©gicos
El entorno estÃ¡ formado por los *factores estratÃ©gicos*, que son todos
los factores que pueden incidir en las actuaciones de la empresa son
los factores estratÃ©gicos, tanto en el presente como en el futuro. Son
*amenazas* y *oportunidades*.

****** Entorno relevante
La influencia del entorno puede ser diferente segÃºn tiempo y
organizaciÃ³n. Los factores estratÃ©gicos son los mÃ¡s influyentes y
sobre los que centra la atenciÃ³n el anÃ¡lisis de entorno; aquellos
que afectan a la empresa son parte de su entornro relevante.

****** AnÃ¡lisis del entorno
Debe sistematizarse el anÃ¡lisis de los factores estratÃ©gicos para 
tratar de estar preparado y anticiparse a los cambios.

***** LÃ­mites del entorno
Los lÃ­mites del entorno varÃ­an segÃºn la relaciÃ³n con el exterior.
Los proveedores pueden considerarse internos a la empresa. Son los
llamados *lÃ­mites difusos*.

***** Niveles del entorno
SegÃºn /Bueno (2002)/, se establecen varios niveles del entorno segÃºn
el Ã¡mbito geogrÃ¡fico.

 * Entorno *global*, a nivel mundial.
 * Entorno *internacional*, a nivel de una regiÃ³n internacional.
 * Entorno *nacional o domÃ©stico*, a nivel del paÃ­s.
 * Entorno *regional*, dentro de un paÃ­s o varios paÃ­ses.
 * Entorno *local*, a nivel de un nÃºcleo urbano.
 
La mayorÃ­a de las empresas en la actualidad funcionan a niveles globales,
debido a la /globalizaciÃ³n/ de la actividad econÃ³mica.

**** 3.2. CaracterÃ­sticas del entorno
Las *caracterÃ­sticas del entorno* son los atributos a los que se enfrenta
una organizaciÃ³n. 

Identifica /caracterÃ­sticas o atributos/ que definen unos entornos
frente a otros.

***** Entorno como fuente de recursos
La idea del entorno como *fuente de recursos* aparece en las teorÃ­as de
Aldrich, Pfeffer y Salancik. Estas fuentes de recursos se caracterizan por

 * si los recursos son *abundantes*.
 * si los recursos son *variados*.
 * si estÃ¡n *concentrados*.
 * si otras empresas *compiten* por ellos.

En base a estas caracterÃ­sticas se consideran distintos tipos de entorno.

****** Estable-Aleatorio
En un entorno *estable-aleatorio*, no hay competencia por los recursos,
estÃ¡n distribuidos y son abundantes.

/Ultramarinos de un pueblo. No hay competencia pero hay demanda estable./

****** PlÃ¡cido-Integrado
En un entorno *plÃ¡cido-integrado*, los recursos son estables pero estÃ¡n
concentrados; haciendo algunas posiciones en el entorno mÃ¡s ventajosas
que otras.

/Silicon Valley aprovecha recursos tecnolÃ³gicos de la zona./

****** Inestable-Reactivo
En un entorno *inestable-reactivo* los cambios son pocos pero
existen varias empresas con las mismas necesidades de recursos. Todos
los movimientos por los recursos serÃ¡n respondidos por los
competidores.

/Empresas de software intentando captar empleados./

****** Turbulento
En un entorno *turbulento* existe competencia entre las empresas y
ademÃ¡s las condiciones y los recursos estÃ¡n en continuo cambio.

/Empresas en zonas de inestabilidad polÃ­tica./

***** Entorno como fuente de incertidumbre
La idea del entorno como *fuente de incertidumbre* aparece en las
teorÃ­as de Duncan, Lawrence y Lorsch. Se considera el entorno como una
fuente de informaciÃ³n sobre la que la empresa toma decisiones. Estas
fuentes de informaciÃ³n se caracterizan por

 * la *predecibilidad e incertidumbre*, que viene dada por la cantidad
   de informaciÃ³n a la que se puede acceder en el entorno.

Se establecen cuatro dimensiones para medirlo.

****** DinÃ¡mico/Estable
Un entorno es *dinÃ¡mico* si hay variaciones numerosas e impredecibles;
y es *estable* si no hay cambios o son muy predecibles.

****** Complejo/Sencillo
Un entorno es *complejo* cuando se necesitan muchos conocimientos
especÃ­ficos para entenderlo; y *sencillo* cuando el conocimento es
comprensible.

****** Diverso/Integrado
Un entorno es *diverso* cuando hay un gran nÃºmero de clientes
distintos a los que abastecer; e *integrado* cuando son todos
similares y se concentran en Ã¡reas prÃ³ximas.

****** Hostil/Munificiente
Un entorno es *hostil* cuando existe mucha competencia por los
recursos; y *munificiente* cuando la competencia es pequeÃ±a.

***** Entorno general
El *entorno general* estÃ¡ integrado por un conjunto de factores que
ejercen influencia sobre todas las empresas dentro de un sistema
socioeconÃ³mico.

****** DefiniciÃ³n de Cuervo (2001)
El *entorno genÃ©rico* agrupo los elementos que afectan de manera
similar a todas las empresas en un espacio dado independientemente
de su sector.

***** Entorno general: AnÃ¡lisis PESTEL
El *anÃ¡lisis PESTEL* tiene como objetivo diagnosticar el entorno
general valorando el impacto que varias variables tienen en la
actuaciÃ³n de la empresa. Identifica las principales oportunidades y
amenazas.

****** Variables
El anÃ¡lisis *PESTEL* analiza las dimensiones

 * *polÃ­tica*, dada por la estabilidad, proteccionismo e
   intervencionismo del estado. Nivel de corrupciÃ³n de un paÃ­s.

 * *econÃ³mica*, dada por factores econÃ³micos como

   - la /inflaciÃ³n/, maracada por el IPC y los precios al
     consumo. Hay inflaciÃ³n y deflaciÃ³n segÃºn suban o bajen.
   - la /tasa de desempleo/.
   - el /tipo de cambio/ frente al dÃ³lar como base.
   - la /renta disponible/ de las familias, corregida por inflaciÃ³n.
   - los /tipos de interÃ©s/.

 * *social*, dada por preferencias como valores, creencias,
   desigualdad, religiÃ³n, feminismo, tradiciÃ³n, cultura.

 * *tecnolÃ³gica*, dada por el grado de infraestructuras del
   paÃ­s. Gasto en I+D. InversiÃ³n pÃºblica del gobierno y
   patentes. Leyes de proyecciÃ³n del conocimiento.

 * *ecolÃ³gica*, dada por la polÃ­tica medioambiental y el consumo de
   energÃ­a.

 * *legal*, dada por la legislaciÃ³n laboral, la normativa de fusiones
   y adquisiciones y las restricciones a la libre competencia.

****** Etapas del anÃ¡lisis
El anÃ¡lisis PEST se divide en varias etapas

 * en el *Paso 1*, se delimitan los factores estratÃ©gicos y se estudia
   quÃ© variables pueden tener mÃ¡s incidencia sobre la empresa. Se
   estudian /variables y factores clave/.

 * en el *Paso 2*, se describe la evoluciÃ³n esperada de los factores
   estratÃ©gicos del entorno.

 * en el *Paso 3*, se valoran y jerarquizan las oportunidades y
   amenazas.

****** RealizaciÃ³n de la tabla y perfil estratÃ©gico del entorno
Todo el anÃ¡lisis puede representarse en una tabla plantilla que
considera los factores principales del entorno. Pueden colocarse
tambiÃ©n en un perfil estratÃ©gico del entorno.

****** Consideraciones sobre el anÃ¡lisis PEST
El anÃ¡lisis PEST proporciona una herramienta fÃ¡cil de usar e
interpretar, pero tiene el problema de que

 * es un anÃ¡lisis muy subjetivo y cualitativo. 
 * este anÃ¡lisis debe entrar en detalles y por tanto debe tratar los
   factores mÃ¡s relevantes.
 * no todas las variables estarÃ¡n en un anÃ¡lisis PEST. Hay que ser
   selectivos y sÃ³lo considerar los factores estratÃ©gicos.
 * el impacto de un mismo entorno puede variar entre distintas
   industrias y entre empresas de una misma industria.

En ocasiones se utiliza el anÃ¡lisis PESTEL como una ampliaciÃ³n del
anÃ¡lisis PEST.

***** Entorno general: Diamante de Porter
***** Entorno especÃ­fico
El *entorno especÃ­fico o competitivo* lo forma el conjunto de empresas
que se dedican a la misma actividad econÃ³mica que la empresa que se
estÃ¡ analizando y que conforman su /sector/. El entorno especÃ­fico
marca las reglas de competencia.

****** Componentes de Hall (1996)
SegÃºn Hall, el entorno especÃ­fico estÃ¡ formado por

 * *proovedores* de los recursos necesarios.
 * *clientes*, que consumen los productos.
 * *competidores*, que compiten por los recursos y clientes.
 * *reguladores* que controlan y fiscalizan a las organizaciones.

***** Entorno especÃ­fico: Modelo de Porter (2009)
El *modelo de las cinco fuerzas competitivas de Porter*, desarrollado
en el 2009 mide el /grado de atractivo/ de una industria por cinco
fuerzas competitivas bÃ¡sicas, que definen la posibilidad de obtener
rentas superiores en la industria. A mÃ¡s rivalidad, menos posibilidad
de obtener rentas superiores.

TambiÃ©n se llama *modelo de rivalidad ampliada*.

****** 1. Competidores actuales
Grado de rivalidad con los competidores del sector. La amenaza que
representan depende de varios factores.

******* NÃºmero y equilibrio entre competidores
En las *industrias concentradas* hay pocos competidores y estÃ¡n muy
desequilibrados formando oligopolios; no hay competencia. En las
*industrias fragmentadas* hay muchos competidores y estÃ¡n equilibrados,
por lo que la competencia es mayor.

/Un ejemplo de industria concentrada es la farmacÃ©utica./
/Un ejemplo de industria fragmentada son los bares./

******* Ritmo de crecimiento del sector
El ritmo de crecimiento determina la competencia. Cuanto mÃ¡s reciente
es el sector, menos competencia hay. SegÃºn el ritmo de crecimiento una
industria se consideran las fases de

 * *introducciÃ³n*, donde la industria se estÃ¡ creando. Un ejemplo
   son los coches autÃ³nomos.

 * *crecimiento*, donde hay oportunidades y /menos competencia/. Un 
   ejemplo son los drones.

 * *madurez*, donde la industria se estabiliza. Un ejemplo son los
   electrodomÃ©sticos.
   
 * *declive*, donde las ventas se reparten y hay /mÃ¡s competencia/.
   Un ejemplo son los SMS.

Y la posibilidad de incrementar las ventas es mÃ¡s difÃ­cil
despuÃ©s. La intensidad de la competencia es mayor conforme
avanza.

******* Barreras de movilidad
Las *barreras de movilidad* son las que se encuentran las empresas
para ampliar su lÃ­nea de productos a otros segmentos. Si hay muchas
barreras, las empresas se quedarÃ¡n donde estÃ¡n y la competencia del
sector serÃ¡ menor.

******** Ejemplo: banca privada
Es muy difÃ­cil iniciarse en la banca privada. Hay unas barreras muy
grandes entre servicios usuales y los servicios privados. Los
contratos suelen estar blindados y los canales de distribuciÃ³n suelen
trabajar con las marcas que ya conocen.

******** Ejemplo: automociÃ³n
SÃ³lo unas pocas fabrican coches y camiones. Es difÃ­cil saltar del
diseÃ±o de unos al diseÃ±o de otros; se necesita tecnologÃ­a, cadenas
y materiales diferentes.

******* Barreras de salida
Las *barreras de salida* son el equivalente a la movilidad a nivel
del sector. Cuanto mÃ¡s difÃ­cil sea salir del sector, mÃ¡s se intentarÃ¡
sobrevivir y mayor serÃ¡ la competencia.

******** Ejemplo: activos especializados
Si hay activos especializados que no se amortizan, y que por ser tan
especÃ­ficos, es muy difÃ­cil vender.

******** Ejemplo: costes fijos de salida
Para cerrar, hay que hacer frente a costes como los costes de
despidos. Las empresas que no quieren pagarlos, se mantienen en el
sector.

******** Ejemplo: imagen de marca
En ocasiones se mantiene un sector sÃ³lo para mantener el prestigio de
la imagen de marca. El daÃ±o a la imagen es un coste de salida.

******** Ejemplo: empresas familiares
La barrera emocional de seguir negocio familiar tradicional constituye
una barrera de salida. EstÃ¡n dispuestas a continuar a pesar de las
pÃ©rdidas.

El ejemplo usual es la /almazara/.

******** Ejemplo: presiones externas
FÃ¡bricas que emplean a mucha gente en una regiÃ³n tienen la presiÃ³n
externa para seguir empleando a la gente.

******* Estructura de costes de las empresas
Un mayor peso de los costes fijos en la *estructura de costes* lleva
a operar a plena capacidad y por tanto a incrementar la intensidad
competitiva.

******** Estructura de costes
La *estructura de costes* es la proporciÃ³n entre costes fijos y
costes variables que cumplen $CT = CF + CV$ siendo
 
 * CF: los *costes fijos*. Hay que pagarlos siempre.
 * CV: los *costes variables*. Se pagan segÃºn cuÃ¡nto produzcas.
 * CT: los *costes totales*.

******** Margen de beneficio
El margen de beneficio es la diferencia del precio y el coste de
producciÃ³n unitario $\mathrm{margen_{unitario}} = p - c_v$. El margen bruto es el
margen unitario por nÃºmero de unidades

\[\mathrm{margen} = u(p - c_v).
\]

NÃ³tese que es distinto del beneficio (!), que tiene en cuenta tambiÃ©n
los costes fijos. El beneficio es $B = \mathrm{Ingresos} - CT$.

AsÃ­, cuando los costes fijos son muy grandes, es muy difÃ­cil entrar
en la competencia, asÃ­ que la competencia serÃ¡ menor. El *punto muerto*
es el punto a partir del cual se van a obtener beneficios.

******* Grado de diferenciaciÃ³n de los productos/servicios
La *diferenciaciÃ³n* de productos disminuye la intensidad de la
competencia. Cada uno vende cosas distintas y tiene una cartera de
clientes fieles que es difÃ­cil que cambien de proveedor.

/Ejemplo: compaÃ±Ã­as telefÃ³nicas tienen poca diferenciaciÃ³n./
/Ejemplo: las bebidas son diferentes./

******* Costes de cambio
Los *costes de cambio* son los que debe asumir un cliente para cambiar
de proveedor. Cuantos mÃ¡s altos, mÃ¡s difÃ­cil es que cambie un cliente
y menor es la competencia. Pueden ser tÃ©cnicos, de formaciÃ³n o
financieros.

/Ejemplo: es difÃ­cil el cambio de compaÃ±Ã­a telefÃ³nica por permanencias./

******* Capacidad productiva instalada
Cuando mÃ¡s *capacidad productiva* instalada, hay mÃ¡s necesidad de
vender los productos, y mÃ¡s alta es la competencia.

/Un planta produciendo 1000 unidades/dÃ­a debe producir a ese volumen./ 

******* Diversidad de competidores
Si los competidores difieren en estrategias, puede aumentar la
competitividad.

******* Intereses estratÃ©gicos
Cuando todas las empresas estÃ¡n interesadas en el sector, mayor serÃ¡
la competencia.

****** 2. Competidores potenciales
Los *competidores potenciales* son las empresas potencialmente
interesadas en entrar en el sector. La amenaza depende de dos
factores.

******* Barreras de entrada
Las *barreras de entrada* son los obstÃ¡culos que enfrentan las
empresas para entrar al sector. Se resumen en

 1. la *economÃ­a de escala*. Las empresas que entran al sector deben
    hacerlo a gran escala para beneficiarse de las reducciones de
    costes que da el incremento de volumen productivo; esto supone
    una barrera para los nuevos. Al caso contrario se le llama una
    /deseconomÃ­a de escala/. La sinergia es una causa de la economÃ­a
    de escala.

 2. la *diferenciaciÃ³n de productos*. Cuando todos tienen
    diferenciados sus productos, para entrar al sector habrÃ¡ que
    ofrecer algo diferente para poder competir al mismo nivel.

 3. las *necesidades de capital*. Un sector que necesita una inversiÃ³n
    muy grande tiene una barrera de entrada muy grande.

 4. los *costes de cambio*. Si los clientes estÃ¡n fidelizados, el que
    entre nuevo en el sector va a tener que saltar la barrera de los
    costes de cambio.

 5. el *acceso a canales de distribuciÃ³n*, que normalmente estarÃ¡n en
    manos de las empresas del sector.

 6. las desventajas en *costes de desarrollo*. Hay que pagar patentes
    o desarrollar tecnologÃ­as alternativas. Hay que encontrar buenas
    localizaciones, que las tienen las empresas del sector. La
    experiencia del sector la tienen las empresas.

 7. la *polÃ­tica gubernamental*. Pueden existir restricciones que
    limitan la entrada al sector. Normas reguladoras de la competencia
    que impiden la libre competencia en el mercado.

******* Represalias esperadas
Represalias de los competidores que estÃ¡n ya en el sector contra
los nuevos competidores. Las represalias son mayores cuanto mÃ¡s
recursos de defensa pueden invertir en la represalia.

/Empresas como las teleoperadoras pueden ejercerlos contra nuevas
empresas./

****** 3. Productos sustitutivos
Los *productos sustitutivos* son aquellos que cubren las mismas
funciones desde el punto de vista de los clientes usando otra tecnologÃ­a
u otras materias primas.

Si hay muchos productos sustitutivos, serÃ¡ menos atractivo el sector;
dependerÃ¡ ademÃ¡s de

  * el grado de *sustituciÃ³n*.
  * los *precios relativos* al sustitutivo.
  * *obsolescencia* que causan los sustitutivos.
  * *costes de cambio* al sustitutivo.

Pueden no tener siquiera el mismo CNAE, y no los consideramos dentro
del mismo sector, pero hay competencia entre ellos.

/Ejemplo: azÃºcar y sacarina./

/Ejemplo: aceite oliva y el girasol./

****** 4. Proveedores
A mayor poder de *negociaciÃ³n de los proveedores*, mÃ¡s podrÃ¡n influir en
el coste o los plazos de entrega de las empresas del sector. Cuanto
mÃ¡s concentrados estÃ©n y menos sustitutos haya para los proveedores,
mÃ¡s poder de negociaciÃ³n tendrÃ¡n.

******* IntegraciÃ³n vertical
La *integraciÃ³n vertical* es el proceso por el cual un proveedor o
un cliente pasa a integrar otra parte de la cadena de producciÃ³n.

El poder de negociaciÃ³n del proveedor aumenta cuanto mÃ¡s informado
estÃ© el proveedor y mÃ¡s fÃ¡cil sea que entre en el sector por
integraciÃ³n vertical.

/Ejemplo: Inditex realiza integraciÃ³n hacia atrÃ¡s al producir tela./

/Ejemplo: Apple realiza integraciÃ³n hacia alante al crear tiendas./
******* NÃºmero de proveedores y grado de concentraciÃ³n
******* Grado de diferenciaciÃ³n de los proveedores
******* Existencia de productos sustitutos al del proveedor
******* Importancia de nuestra empresa para el proveedor
******* Nivel y calidad de la informaciÃ³n
****** 5. Clientes
A mayor poder de *negociaciÃ³n de los clientes*, mÃ¡s podrÃ¡n influir en
lo que debe producir la empresa.

******* NÃºmero de clientes y grado de concentraciÃ³n
******* Grado de diferenciaciÃ³n de los productos
******* Existencia de sustitutos al producto
******* Amenaza de integraciÃ³n vertical
******* InformaciÃ³n de la que dispone el cliente
***** Entorno especÃ­fico: limitaciones y extensiones del modelo
Al modelo de las 5 fuerzas de Porter se le plantean las siguientes
crÃ­ticas y limitaciones

 * tiene un *carÃ¡cter estÃ¡tico*, requiere de una actualizaciÃ³n del
   anÃ¡lisis siempre que se pueda.
 * no tiene en cuenta *industrias auxiliares* y complementarias; no
   considera relaciones de colaboraciÃ³n.
 * la realidad es *heterogÃ©nea*, no todas las fuerzas ni todos los
   factores tienen la misma influencia.
 * no todos los *competidores* se encuentran afectados de la misma
   manera por las fuerzas del sistema. (Se intenta salvar esto
   mediante el anÃ¡lisis de los sectores estratÃ©gicos, que aplica un
   anÃ¡lisis particular en los distintos grupos estratÃ©gicos; es
   distinta la competencia de Inditex y la de una pequeÃ±a textil)

*** 4. La direcciÃ³n estratÃ©gica
**** 4.1. Concepto de direcciÃ³n estratÃ©gica
La *direcciÃ³n de empresa* es la gestiÃ³n diaria de la misma para lograr
eficacia y eficiencia. Este concepto contrastarÃ¡ con la idea de
direcciÃ³n estratÃ©gica.

***** Concepto de estrategia
La *direcciÃ³n estratÃ©gica* es el proceso de diseÃ±o e implantaciÃ³n de
una estrategia con la que responder al entorno. 

La idea de la direcciÃ³n estratÃ©gica surge porque la empresa debe
responder a un entorno que es turbulento y que presenta debilidades y
amenazas.

****** Estrategia
Del griego /stratos/ (ejÃ©rcito), /ego/ (lÃ­der, guÃ­a); se entiende
como la guÃ­a que marca el camino de la empresa hacia unos objetivos.

****** DefiniciÃ³n de estrategia (Johnson)
La *estrategia* es la direcciÃ³n en el sentido de orientaciÃ³n a largo
plazo para lograr ventajas en un entorno cambiante para conseguir
los beneficios de los stakeholders.

****** DefiniciÃ³n de estrategia (Porter)
La *estrategia* es una acciÃ³n ofensiva o defensiva para obtener una
posiciÃ³n competitiva en un sector; afrontar las cinco fuerzas
competitivas y conseguir un mayor rendimiento sobre la inversiÃ³n de la
empresa.

****** DefiniciÃ³n de estrategia (Grant)
La *estrategia* define cÃ³mo desplegarÃ¡ la empresa sus recursos en el
entorno para satisfacer los objetivos a largo plazo. La estrategia
aporta coherencia y cohesiÃ³n, dando sentido a toda la organizaciÃ³n.

****** DefiniciÃ³n de estrategia (Guerras y Navas)
La *estrategia* es la respuesta que se diseÃ±a para sobrevivir o para
responder al entorno.

***** Ideas bÃ¡sicas del concepto
El concepto de estrategia se basa
 
 * en la *respuesta* de la empresa su entorno. El entorno
   es cambiante y por tanto la estrategia tambiÃ©n lo serÃ¡.

 * Tiene el objetivo Ãºltimo de posicionar y mejorar la empresa frente
   a competidores. Busca la *ventaja competitiva*.

 * en conseguir los objetivos a largo plazo de los *stakeholders*.
   Refleja dÃ³nde quiere llegar la empresa.

 * en la *toma de decisiones* como proceso que la genera. Se evalÃºan
   distintas opciones segÃºn su ajuste a la empresa y al entorno.

 * en definir los *cursos de acciÃ³n*. En la estrategia se *definen las
   decisiones* que se toman en toda la organizaciÃ³n. El resto de
   decisiones se supeditan a ella.

***** Componentes de la estrategia: campo de actividad
El *campo de actividad* es el conjunto de productos y mercados que
componen la actividad econÃ³mica de la empresa. La estrategia define
/el campo de actividad/.

Puede estar integrado por distintas *Unidades EstratÃ©gicas de Negocio*
*(UEN)*, conjuntos homogÃ©neos para los que se puede definir una
estrategia especÃ­fica para ellos y diferente a las demÃ¡s.

****** Empresa diversificada
Las *empresas diversificadas* son aquellas que tienen varias Unidades
EstratÃ©gicas de Negocio, actuando en varios sectores distintos. Las
estrategias en cada uno de ellos serÃ¡n distintos.

Existen dos tipos de diversificaciÃ³n

 * se considera *diversificaciÃ³n relacionada* si se pueden compartir
   proveedores, canales de distribuciÃ³n o partes del proceso de
   producciÃ³n entre varias UEN.

 * se considera *diversificaciÃ³n no relacionada* si no se comparten
   partes del proceso de producciÃ³n entre UEN.

/Ejemplo: Danone hace diversificaciÃ³n relacionada/

/Ejemplo: Virgin hace diversificaciÃ³n no relacionada./

****** Estrategias distintas
Cada UEN tiene una estrategia competitiva distinta porque

 * se desarrolla en un *entorno* distinto.
 * tiene *competidores* distintos.
 * tiene *oportunidades* distintas.
 * las *competencias* y capacidades requeridas son diferentes.

******** Ejemplo: grupo PRISA
Tiene varias unidades distintas de educaciÃ³n, noticias, audiovisual...

******** Ejemplo: Danone
Productos lÃ¡cteos, agua, nutriciÃ³n mÃ©dica y nutriciÃ³n infantil. Aunque
sean grupos similares, las estrategias serÃ¡n distintas en cada uno de
ellos.

***** Componentes de la estrategia: capacidades distintivas
Las *capacidades distintivas* son el conjunto de recursos y capacidades
que permiten a la empresa realizar determinados procesos mejor que sus
competidores. Constituyen la base de la /ventaja competitiva/.

/Ejemplo: la capacidad de innovaciÃ³n./

***** Componentes de la estrategia: ventaja competitiva
La *ventaja competitiva* es el conjunto de caracterÃ­sticas por las que
se tiene una mejor posiciÃ³n competitiva respecto a los competidores.

No debe confundirse con las /bases de la ventaja competitiva/, que son
todos aquellos factores que provocan la ventaja competitiva. La ventaja
competitiva es el resultado final de la formulaciÃ³n e implantaciÃ³n de
la estrategia de la empresa.

***** Componentes de la estrategia: sinergia
La *sinergia* es el efecto que hace que los recursos y capacidades
integrados ofrezcan un rendimiento mayor al que ofrecerÃ­an de forma
aislada. La integraciÃ³n y la conexiÃ³n entre campos de actividad debe
formar parte de la estrategia.

En las empresas diversificadas las sinergias pueden conseguirse fÃ¡cilmente
si la diversificaciÃ³n es relacionada.

***** Niveles de la estrategia
La estrategia se aborda a distintos *niveles* relacionados entre sÃ­.
Las estrategias de cada nivel deben estar integradas y ser coherentes
entre sÃ­. Derivan de una estrategia corporativa global.

****** Nivel de estrategia corporativa
La *estrategia corporativa* responde a la pregunta de /Â¿dÃ³nde competir?/.  
Es la estrategia global de la empresa, la que define el campo de actividad.

****** Nivel de estrategia competitiva o negocio
La *estrategia competitiva* responde a la pregunta de /Â¿cÃ³mo competir?/. 
Persigue alcanzar una /ventaja competitiva/ en costes o en diferenciaciÃ³n.

****** Nivel funcional
La *estrategia funcional* determina las decisiones a tomar en cada
Ã¡rea funcional de la empresa teniendo en cuenta los recursos y
capacidades distintivas de la misma.

Implica tomar decisiones sobre

 * direcciÃ³n comercial o de mÃ¡rketing.
 * direcciÃ³n de la producciÃ³n.
 * direcciÃ³n financiera.
 * direcciÃ³n de recursos humanos.

****** Resumen
La estrategia corporativa decide los campos de actividad, dÃ³nde va a
diversificar la empresa. La estrategia de negocio se formula para cada
UEN distinto; y por Ãºltimo, la estrategia funcional decide cÃ³mo vamos
a aplicar los recursos dentro de cada Ã¡rea funcional.

|-------------+----------------------------------+------------------------|
| Corporativa | Campo de actividad de la empresa | DirecciÃ³n general      |
| De negocio  | UENs                             | Jefes de divisiÃ³n      |
| Funcionales | Ã¡reas funcionales                | Directores sectoriales |
|-------------+----------------------------------+------------------------|

**** 4.2. Proceso de direcciÃ³n estratÃ©gica
AnÃ¡lisis del entorno e interno. En base a Ã©l formulamos las direcciones
para los objetivos, decidiremos cuÃ¡l es la mÃ¡s adecuada y lo implementaremos.

Se divide en varias etapas.

***** Etapa de anÃ¡lisis estratÃ©gico
En la fase de *anÃ¡lisis estratÃ©gico* debe realizarse un diagnÃ³stico de
la situaciÃ³n de la empresa, partiendo del objetivo a largo plazo, la
misiÃ³n, valores y objetivos.

Partiendo del objetivo a largo plazo, se hace un anÃ¡lisis DAFO, con un
anÃ¡lisis tanto externo como interno.

****** AnÃ¡lisis DAFO
El anÃ¡lisis DAFO puede usarse en esta fase para analizar la situaciÃ³n
externa e interna de la empresa. Recoge debilidades y fortalezas,
amenazas y oportunidades.

***** Etapa de formulaciÃ³n de estrategias
En la fase de *formulaciÃ³n de estrategias* se diseÃ±an, evalÃºan y
seleccionan las estrategias que vamos a implementar, a niveles
coorporativo y competitivo.

***** Etapa de implantaciÃ³n
En la fase de *implantaciÃ³n de la estrategia* se pone en prÃ¡ctica la
estrategia seleccionada. Suele elaborarse un /plan estratÃ©gico/ que
recoja las decisiones tomadas. Puede suponer cambios de direcciÃ³n, de
equipamiento tÃ©cnico, de compromiso, de recursos y de control; y debe
adecuarse a la cultura organizativa de la empresa.

**** 4.3. Opciones estratÃ©gicas bÃ¡sicas
***** Opciones a nivel de estrategia corporativa
Las opciones a nivel de *estrategia corporativa* decidirÃ¡n la estrategia
global de la empresa; /dÃ³nde competirÃ¡/, quÃ© actividades vamos a
desarrollar y en quÃ© mercados vamos a tener presencia.

****** Estrategias de crecimiento
Las *estrategias de crecimiento* deben responder hacia dÃ³nde se quiere
crecer y cÃ³mo se quiere crecer. Para ello, nos valemos de la matriz de
Ansoff, que clasifica las estrategias de crecimiento.

|---------------------+------------------------+----------------------|
|                     | Productos Existentes   | Productos nuevos     |
|---------------------+------------------------+----------------------|
| Mercados Existentes | PenetraciÃ³n de mercado | Desarrollo productos |
| Mercados Nuevos     | Desarrollo de mercados | *DiversificaciÃ³n*    |
|---------------------+------------------------+----------------------|

Dentro de estas estrategias se consideran tres *estrategias de expansiÃ³n*, 
que implican crecimiento sin variar el campo de actividad 
considerablemente. Son

 * *estrategia de penetraciÃ³n en el mercado*, cuando se siguen explotando
   los mismos productos en los mismos mercados. Se busca incrementar
   la cuota de mercado. Se siguen estrategias de mÃ¡rketing.

 * *estrategia de desarrollo de productos*, cuando se desarrollan nuevas
   variedades de productos para los mercados habituales; o se migra
   hacia modelos superiores.

 * *estrategia de desarrollo de mercados*, cuando se introducen los
   mismos productos en nuevos mercados. Incluye la
   /internacionalizaciÃ³n/.

AdemÃ¡s, se considera una estrategia que sÃ­ supone un cambio sustancial,
la

 * *estrategia de diversificaciÃ³n*, busca explotar productos nuevos
   en mercados desconocidos. Incluye la /integraciÃ³n vertical/.

****** Formas de crecimiento
La empresa crece con distintos mÃ©todos

 * *crecimiento interno u orgÃ¡nico* cuando invierte en sÃ­ misma para
   incrementar su valor econÃ³mico y su capacidad productiva.

 * *crecimiento externo* cuando lleva a cabo adquisiciones o fusiones
   con otras empresas; o toma control sobre las decisiones de otra
   empresa. No implica crecimiento real de la economÃ­a.

 * *crecimiento hÃ­brido* cuando existen acuerdos temporales entre
   empresas que no varÃ­an su identidad jurÃ­dica. Ejemplos son 
   la /franquicia/, las /subcontratas/ o los /UTE/.

****** Estrategia de reestructuraciÃ³n
La *estrategia de reestructuraciÃ³n* es aquella que emprende cambios en
el *campo de actividad*, reduciendo la importancia de algunos negocios
y abandonando la explotaciÃ³n de unidades estratÃ©gicas. Al abandonar
negocios estamos cambiando nuestro campo de actividad.

***** Opciones a nivel de estrategia competitiva
Las opciones a nivel de *estrategia competitiva* decidirÃ¡n la estrategia
particular de cada UEN; /cÃ³mo competirÃ¡/, y quÃ© guÃ­as deberÃ¡ seguir la
empresa para obtener y mantener la ventaja competitiva.

|-----------+---------------------+----------------------------|
|           | Costes              | DiferenciaciÃ³n             |
|-----------+---------------------+----------------------------|
| Industria | Liderazgo en costes | DiferenciaciÃ³n de producto |
| Segmento  | SegmentaciÃ³n        | SegmentaciÃ³n               |
|-----------+---------------------+----------------------------|

****** Liderazgo en costes
La estrategia de *liderazgo en costes* persigue producir con costes
inferiores a los de los competidores. Sabiendo que los costes son

\[m = p - c\]

donde

 * $m$ es el margen de beneficio,
 * $p$ el precio unitario del producto, y
 * $c$ el coste unitario.

Puede explotarse la bajada del coste unitario manteniendo el
precio e incrementando su rentabilidad, o disminuyendo el precio
y esperando un aumento de ventas frente a los competidores.

****** DiferenciaciÃ³n
La estrategia de *diferenciaciÃ³n* busca que el producto sea percibido
como diferente o superior por los clientes. Por calidad, atenciÃ³n o
por ser diferente.

Puede explotarse la diferenciaciÃ³n aumentando el precio unitario y 
manteniendo los clientes gracias a esta diferenciaciÃ³n.

****** SegmentaciÃ³n
La estrategia de *segmentaciÃ³n* se centra en un segmento de los
clientes o se enfoca en sÃ³lo una parte de la poblaciÃ³n. Normalmente el
enfoque va acompaÃ±ado de una estrategia de costes o diferenciaciÃ³n.

****** Problema: stuck in the middle
Cuando se persiguen los objetivos de coste y diferenciaciÃ³n a la vez,
se corre el riesgo de no conseguir ninguno. A esto se le llama
quedarse /atrapado en el medio/.

***** Opciones a nivel funcional
Las *opciones estratÃ©gicas* a nivel funcional son especÃ­ficas y
variadas para cada Ã¡rea funcional. Se consideran compuestas de todas las
decisiones estratÃ©gicas derivadas de las decisiones de los niveles
superiores y se clasifican por Ã¡reas.

****** ProducciÃ³n
Se toman decisiones como

 * nivel de integraciÃ³n vertical.
 * nivel de capacidad mÃ¡xima por unidad productiva.
 * gestiÃ³n de la producciÃ³n.
 * localizaciÃ³n de la producciÃ³n.

****** MÃ¡rketing
Se toman decisiones como

 * elecciÃ³n del producto.
 * polÃ­tica de precios.
 * canales de distribuciÃ³n.

****** Recursos humanos
Se toman decisiones como

 * incentivos y promociÃ³n interna.
 * criterios de selecciÃ³n de personal.
 * remuneraciÃ³n.

****** Financiero
Se toman decisiones como

 * polÃ­tica de dividendos.
 * selecciÃ³n de inversores.
 * gestiÃ³n del riesgo.

**** Sobre misiÃ³n y visiÃ³n
En el proceso de direcciÃ³n estratÃ©gica, hay una parte de anÃ¡lisis
externo, anÃ¡lisis DAFO y anÃ¡lisis interno. En la formulaciÃ³n de
estrategias. Pero todo ello depende en primera instancia de la
visiÃ³n y misiÃ³n de las empresas.

/NÃ³tese que algunas empresas mezclan misiÃ³n y visiÃ³n./

***** La misiÃ³n
La *misiÃ³n* es el motivo de la empresa. Responde a la pregunta
de /quÃ©/ hace la empresa. Es la expresiÃ³n de la identidad de la
empresa y declaraciÃ³n de propÃ³sito general.

****** DiferenciaciÃ³n
Dos empresas competidoras en el mismo sector tendrÃ¡n normalmente
misiones distintas. HarÃ¡n Ã©nfasis en cosas distintas.

****** Empresas diversificadas
En empresas diversificadas, la misiÃ³n es el hilo conductor; pero
pueden enunciarse misiones distintas para las distintas unidades
en casos de diversificaciÃ³n extrema no relacionada.

***** La visiÃ³n
Enuncia la *visiÃ³n* ideal futura de la empresa. Se expresa en tÃ©rminos
de ideales, procurando que no sÃ³lo haga referencia a los stakeholders.

***** El eslogan
NÃ³tese que no se relacionan con el eslogan, que es sÃ³lo un
reconocimiento a nivel publicitario y comercial.

**** Modelos de negocio
La lÃ³gica de la estrategia competitiva es crear valor. Los modelos de
negocio estudian cÃ³mo construir la ventaja competitiva. El modelo de
negocio se suele plasmar en un lienzo de 9 bloques creado por
Osterwalder (el /Business Model Canvas/).

Dos empresas que satisfacen la misma necesidad a los clientes pueden
tener dos modelos de negocio diferentes.

***** Business model canvas
Debemos entender el problema del cliente y ofrecerle soluciones.
Osterwalder considera que el verdadero producto que ofrece una empresa
es su modelo de negocio.

***** Elementos del modelo de negocio
El orden y tamaÃ±o de los bloques en el canvas es semÃ¡ntico. La parte
izquierda es el backend mientras que la derecha es el frontend. Abajo
queda la parte financiera.

 * Propuesta de valor.
 * Segmentos de clientes.
 * Canales de comunicaciÃ³n con el cliente.
 * RelaciÃ³n con el cliente.
 * Fuentes de ingresos.
 * Asociaciones clave: competidores, proovedores.
 * Canales.
 * Recursos clave.
 * Actividades clave: verbos, acciones esenciales para construir y
   crear la propuesta de valor.
 * Fuentes de ingresos y de gastos.

*** Ejercicios
**** Ejercicio 1
#+begin_statement
En el modelo de las 5 fuerzas competitivas de Porter, Â¿quÃ©
representarÃ­a el nuevo servicio de llamadas de Whatsapp para las
empresas de telecomunicaciÃ³n en 2015?Â¿Crees que suponÃ­a en aquel
entonces una amenaza relevante?
#+end_statement

El servicio de llamadas de Whatsapp cubre la misma necesidad que
cubrÃ­an las llamadas tradicionales usando una tecnologÃ­a distinta, la
VoIP. Es por tanto un *producto sustitutivo* de las llamadas
tradicionales segÃºn el modelo de Porter.

En el 2015 parecÃ­a suponer una amenaza relevante porque era un
sustitutivo accesible a la mayorÃ­a de consumidores (700 millones de
personas usaban Whatsapp) y existÃ­an previsiones de que superarÃ­a a
las llamadas en 2018. AdemÃ¡s, los SMS eran un precedente de cÃ³mo un
proceso similar habÃ­a sido una amenaza para las telefÃ³nicas.

**** Ejercicio 2
#+begin_statement
Ante el lanzamiento en 2015 del nuevo servicio de llamadas de Whatsapp,
Â¿quÃ© reacciones se esperaban por parte de las compaÃ±Ã­as de telefonÃ­a mÃ³vil?
#+end_statement

SegÃºn CÃ©sar CÃ³rcoles y otros profesores de la UOC, las estrategias
podrÃ­an ser

 * *reducir el costo* de las llamadas, ofreciendo mÃ¡s minutos de voz en
   las tarifas. EstÃ¡n bajando los precios relativos al sustitutivo.
 * *incluir la VoIP* en sus servicios y promocionarla para vender mÃ¡s en
   las tarifas de internet.
 * *rebajar la calidad del servicio de VoIP* en detrimento del servicio
   de Whatsapp y otras aplicaciones similares. IrÃ­a contra sus propios
   usuarios y el principio de neutralidad de la red. SerÃ­a una represalia
   contra un potencial competidor.

**** Ejercicio 3
#+begin_statement
SegÃºn el informe de Cisco Systems citado en el artÃ­culo, "se prevÃ© que
en 2018 los minutos de llamadas por Wifi superen los minutos de
llamads por la red telefÃ³nica convencional". Ante la cercanÃ­a del
momento pronosticado, Â¿crees que se han cumplido las previsiones?Â¿CÃ³mo
calificarÃ­as, segÃºn los parÃ¡metros estudiados en clase, la amenaza de
este servicio para las operadoras de telefonÃ­a tradicionales?
#+end_statement

Las previsiones parecen no haberse cumplido y parece que todavÃ­a las
llamadas sobre WiFi no superaran a las llamadas tradicionales, aunque
seguramente la existencia de servicios de mensajerÃ­a y mensajes de voz
haya hecho caer ambas.

Como amenaza, debemos tener en cuenta parÃ¡metros como

 * el *coste de cambio* al VoIP es muy bajo. La mayorÃ­a de usuarios ya 
   lo tienen instalado.
 * el *acceso a canales de distribuciÃ³n* lo tienen ya las empresas de
   VoIP, que tienen clientes obtenidos gracias a ofrecer servicios de
   mensajerÃ­a.
 * el *grado de sustituciÃ³n*. Aunque la VoIP sustituye las llamadas en
   la mayorÃ­a de los casos, si se quieren llamadas de alta calidad y sin
   cortes, podrÃ­a necesitarse volver a las llamadas tradicionales.
 * los *precios relativos al sustitutivo* son mucho mÃ¡s bajos para las
   llamadas por VoIP en general debido a que aprovechan la red Wifi que
   se estÃ© usando.
 * la *obsolescencia* no es en este caso un problema para las llamadas
   tradicionales, que siguen teniendo normalmente mayor calidad que las
   llamadas VoIP.

**** Ejercicio 4
#+begin_statement
Â¿CuÃ¡l era la estrategia inicial pensada por Zuckerberg para poder
ofrecer servicios "efÃ­meros" de realidad aumentada e intercambio de
archivos para usuarios de smartphones?
#+end_statement

La estrategia pretendÃ­a eliminar o integrar en sÃ­ toda la competencia.
Estrategia de adquisiciÃ³n por vÃ­a de crecimiento externo.

EspecÃ­ficamente, comprar la principal empresa de la competencia, como
intentÃ³ en 2013, y crear una plataforma de realidad aumentada, con la
que el resto de potenciales competidores se verÃ­an obligados a
integrarse.

**** Ejercicio 5
#+begin_statement
Tras la marcha de los acontecimientos, Â¿cÃ³mo podemos clasificar a
Facebook Stories en el marco del modelo de rivalidad ampliada de
Porter, respecto de Snapchat (suponiendo que la industria de
referencia es la que ofrece el tipo de servicios descritos en el punto
1)?
#+end_statement

ActÃºa como un competidor actual. EspecÃ­ficamente sabemos que no es un
producto sustitutivo porque estÃ¡ usando exactamente las mismas
tecnologÃ­as, que, de hecho, ha imitado directamente en lugar de
innovar.

AdemÃ¡s, quiere realizar integraciÃ³n vertical para crear una plataforma
de realidad aumentada y convertirse en proveedor del resto de plataformas
que pudieran existir en el sector.

** TeorÃ­a de NÃºmeros y CriptografÃ­a
*** FactorizaciÃ³n
**** MÃ©todos bÃ¡sicos de factorizaciÃ³n
***** Fuerza bruta
Probando a dividir cada nÃºmero hasta $\sqrt{n}$.

***** MÃ©todo de Fermat
Escribimos un nÃºmero como diferencia de cuadrados para factorizarlo como
$n = t^2 - s^2 = (t+s)(t-s)$.

****** Soluciones en congruencias no triviales
Si $(t,s)$ es una soluciÃ³n no trivial de $x^2 \equiv y^2$, entonces $gcd(n,t+s)\neq 1$
y $gcd(n,t-s) \neq 1$.

**** MÃ©todo de factor base
Una *base* es un conjunto $B = \{p_0 = -1,p_1,\dots,p_h\}$ donde $p_0,p_1,\dots,p_n$
son enteros primos.

***** Conjunto de candidatos B-nÃºmeros
Un conjunto de candidatos B-nÃºmeros es $\mathbb{Z}_n$ escribiendo la
mitad de nÃºmeros mÃ¡s grandes dentro de la base, como:

\[
\mathbb{Z}_n =
\{0,1\dots,-2,-1\}
\]

Llamamos $\operatorname{abmod}$ a la clase de equivalencia del nÃºmero en el conjunto 
de candidatos.

***** B-nÃºmero
Un nÃºmero $b$ es B-nÃºmero respecto de $n$ si $\operatorname{abmod}(b^2,n)$ factoriza por los 
elementos de la base $B$.

***** Alfa-vectores
Dado $b$ un B-nÃºmero, con:

\[
\operatorname{abmod}(b^2,n) = p_0^{e_0}\dots p_h^{e_h}
\]

definimos el *Î±-vector* como $\alpha vect(b) = (e_0,e_1,\dots,e_h)$.

***** TODO Idea del algoritmo
Sea $S_0 = \{\alpha_0,\dots,\alpha_r\}$ un Î±-vector del conjunto de B-nÃºmeros.

**** MÃ©todos de elecciÃ³n de la base B y los B-nÃºmeros
***** Algoritmo: voy a tener suerte
Se eligen los $h \in \mathbb{N}$ primeros nÃºmeros primos. Escogemos dos Ã­ndices
$k_{max} \leq i_{max}$ y calculamos:

  - $\lfloor \sqrt{n}\rfloor, \lfloor \sqrt{2n}\rfloor, \dots, \lfloor \sqrt{k_{max}n}\rfloor$
  - $\lfloor \sqrt{n}\rfloor+1, \lfloor \sqrt{2n}\rfloor+1, \dots, \lfloor \sqrt{k_{max}n}\rfloor+1$
  - $\dots$
  - $\lfloor \sqrt{n}\rfloor + i_{max}, \lfloor \sqrt{2n}\rfloor+i_{max}, \dots, \lfloor \sqrt{k_{max}n}\rfloor+i_{max}$

Entre estos, buscamos B-nÃºmeros y calculamos los Î±-vectores 
correspondientes.

***** Algoritmo: voy a forzar la suerte
**** Fracciones continuas
***** FracciÃ³n continua
Notamos una fracciÃ³n continua como:

\[
[a_0,a_1,a_2,\dots] =
a_0 + \frac{1}{a_1+\frac{1}{a_2+\dots}}
\]

***** Propiedades de las fracciones continuas
Las fracciones continuas cumplen:

  1. Todo racional se expresa como fracciÃ³n continua finita.
  2. Todo real se expresa como fracciÃ³n continua.
  3. Se cumple la fÃ³rmula:

     \[\frac{a+\sqrt{d}}{b} = [a_0,[a_1,\dots,a_r]]\]

     Si notamos $[a_0,[a_1,\dots,a_r]] = [a_0,a_1,\dots,a_r,a_1,\dots,a_r,\dots]$.

***** CÃ¡lculo de la fracciÃ³n continua de un real
Sea $x \in \mathbb{R}$, tomamos $a_0 = \lfloor x \rfloor$ y podemos escribir recursivamente la 
fracciÃ³n continua como:

\[
x = a_0 + \frac{1}{x_1^{-1}}
\]
** Taller de GeometrÃ­a y TopologÃ­a
*** 1. Construcciones con regla y compÃ¡s
**** 1.1. Construcciones posibles
***** 1.1.0. Construcciones elementales
AxiomÃ¡ticamente consideramos realizables las siguientes construcciones
elementales:

  1. Dados dos puntos, puede construirse un *segmento* entre ellos.
  2. Todo segmento puede extenderse.
  3. Dados dos puntos, puede construirse un *cÃ­rculo* con centro y radio.
  4. Dadas dos rectas secantes, puede construirse su *punto* de corte.
  5. Dados cÃ­rculo y rectas, puede construirse su *punto* de corte.
  6. Dados cÃ­rculos tangentes, pueden construirse *puntos* de corte.

****** Equivalencia de compases                                                                            :extra:
Asumimos un compÃ¡s colapsable, pero podrÃ­amos usar uno no colapsable
con el mismo efecto, por la [[https://en.wikipedia.org/wiki/Compass_equivalence_theorem][equivalencia de compases]].

***** 1.1.1. Construcciones bÃ¡sicas
****** 1. TriÃ¡ngulo equilÃ¡tero sobre un segmento
****** 2. Copiar un segmento
****** 3. Cortar segmento de otro dado
****** 4. Bisecar Ã¡ngulo
****** 5. Mediatriz de un segmento
****** 6. Perpendicular a travÃ©s de un punto en una recta
****** 7. Perpendicular a travÃ©s de un punto fuera de una recta
****** 8. TriÃ¡ngulo con longitudes de lados dada
****** 9. Copiar un segmento a un segmento dado
****** 10. Copiar un Ã¡ngulo a un rayo
****** 14. Paralela a una recta a travÃ©s de un punto exterior
***** 1.1.2. Construcciones involucrando razones geomÃ©tricas
****** 15. Cortar un segmento en n partes iguales
****** 16. Cortar un segmento en un racional
****** 17. Media geomÃ©trica de dos segmentos
***** 1.1.3. Construcciones involucrando Ã¡reas
****** 19. Paralelogramo con igual Ã¡rea que un triÃ¡ngulo dado
***** 1.1.4. Circunferencias destacadas
****** 24. Centro de una circunferencia
****** 25. Circunferencia inscrita a un triÃ¡ngulo
Usando bisectrices.
****** 26. Circunferencia circunscrita a un triÃ¡ngulo
**** 1.2. Construcciones imposibles
***** 1.2.1. Elementos constructibles o realizables
Un real $x$ es constructible cuando podemos construir puntos $C,D$ tales que
$\overline{CD} = x$.

****** Subcuerpo de nÃºmeros constructibles
Los nÃºmeros constructibles forman un subcuerpo de $\mathbb{R}$. Llamamos $\mathfrak{C}$ al
cuerpo de los constructibles.

******* TODO DemostraciÃ³n
****** Los racionales son constructibles
Todo subcuerpo de $\mathbb{R}$ debe contener a los racionales.

****** Las raÃ­ces de constructible son constructibles
Si $x \in\mathfrak{C}$, entonces $\sqrt{|x|} \in \mathfrak{C}$.

***** 1.2.1. ExtensiÃ³n cuadrÃ¡tica
Dado $\mathbb{K}$ subcuerpo de $\mathbb{R}$ llamamos a:

\[
\mathbb{K}(\sqrt{e}) = \{ a+ b\sqrt{e} \mid a,b\in\mathbb{K}\}
\]

una extensiÃ³n cuadrÃ¡tica de $\mathbb{K}$.

***** 1.2.1. Teorema de Descartes
Un nÃºmero real es constructible ssi estÃ¡ en alguna extensiÃ³n cuadrÃ¡tica
iterada de $\mathbb{Q}$.

****** TODO DemostraciÃ³n

*** 2. GeometrÃ­a no euclÃ­dea
**** El Quinto Postulado de Euclides
***** Quinto postulado de Euclides
El Quinto Postulado de Euclides afirma que, dadas $r,s,t$ rectas
cortando $r$ a $t,s$ con Ã¡ngulos $\alpha,\beta$; si $\alpha+\beta < \pi$, entonces $s \cap t \neq \varnothing$.

***** Teorema de Legendre
El Quinto Postulado equivale a que la suma de los Ã¡ngulos de un
triÃ¡ngulo es exactamente $\pi$. Si la suma de los Ã¡ngulos de un triÃ¡ngulo
es $\pi$, se cumple el Quinto Postulado.

***** CuadrilÃ¡teros de Saccheri
Un cuadrilÃ¡tero $ABCD$ es *de Saccheri* cuando $\widehat{A},\widehat{B}$ son rectos y 
ademÃ¡s $\overline{AD}=\overline{BC}$.

***** CuadrilÃ¡tero de Lambert
Un cuadrilÃ¡tero es *de Lambert* si tiene tres Ã¡ngulos rectos.

***** Independencia del Quinto Postulado
Los siguientes resultados son independientes del Quinto Postulado

 * la suma de los Ã¡ngulos de un triÃ¡ngulo es menor o igual que $\pi$.
 * los dos Ã¡ngulos no rectos de un cuadrilÃ¡tero de Saccheri son
   iguales y menores o iguales que $\pi/2$.
 * el lado superior de un cuadrilÃ¡tero de Saccheri es menor que 
   el lado inferior.

***** ImplicaciÃ³n al Quinto Postulado
Los siguientes resultados equivalen al Quinto Postulado

 * los Ã¡ngulos de un triÃ¡ngulo suman $\pi$.
 * los Ã¡ngulos de todos los triÃ¡ngulos suman $\pi$.
 * un cuadrilÃ¡tero de Saccheri tiene todos los Ã¡ngulos rectos.

**** GeometrÃ­a hiperbÃ³lica
***** Axioma de Lobachevsky
Dada una recta $r$ y un punto $a \notin r$, existen al menos dos rectas $s_1,s_2$
distintas con $a \in s_1\cap s_2$, pero $s_1\cap r = s_2\cap r = \varnothing$.

***** Infinitas rectas
Dada una recta $r$ y un punto $a \notin r$, existen infinitas rectas que pasan
por $a$ y no contienen a $r$.

**** Paralelas en geometrÃ­a hiperbÃ³lica
***** Existencia de rectas dado un Ã¡ngulo
Dada una recta $r$ y un punto $a \notin r$, dado cualquier Ã¡ngulo $\alpha \in (0,\pi)$ y
siendo $a \in t_a \perp r$, existen dos rectas $l_1,l_2$ formando un Ã¡ngulo $\alpha$ con
$t_a$.

***** Existencia de paralelas
Dada una recta $r$ y un punto $a \notin r$, tomamos $t_{a}$ la perpendicular por $a$
y $l_{\alpha}$ la dada con Ã¡ngulo $\alpha$ por el teorema anterior. Existe un Ãºnico
$\beta \in (0,\pi/2)$ tal que

  * $l_{\beta}\cap r = \varnothing$,
  * $\forall a \in (0,\beta): l_{\alpha}\cap r \neq \varnothing$.
 
En este caso, la recta $l_{\beta}$ es *paralela*.

***** El paralelismo es independiente del punto elegido
Si una recta es paralela a otra por un punto, lo es por todos sus
puntos.

***** El Ã¡ngulo de paralelismo sÃ³lo depende de la distancia
El Ã¡ngulo de paralelismo de una recta por un punto sÃ³lo depende de
la distancia de la recta al punto.

***** Existen exactamente dos paralelas
Dada una recta y un punto exterior, existen exactamente dos paralelas
distintas que pasan por el punto.

***** SimetrÃ­a del paralelismo
Si $r$ es paralela a $s$, $s$ es paralela a $r$.

***** Ultraparalelas
Dos rectas se dicen ultraparalelas si no son secantes ni paralelas.

***** CaracterizaciÃ³n de ultraparalelas
Dos rectas son ultraparalelas si y sÃ³lo si admiten una perpendicular
comÃºn.

**** Defecto de triÃ¡ngulos
***** Defecto de un triÃ¡ngulo
El *defecto* de un triÃ¡ngulo $ABC$ es

\[ \mathrm{def}(ABC) =
\pi - {\widehat A} - \widehat B - \widehat C > 0.
\]

***** Los defectos de triÃ¡ngulos son aditivos
Dado un $D \in AB$ y $ABC$ un triÃ¡ngulo, se tiene

\[ \mathrm{def}(ABC) = \mathrm{def}(ACD) + \mathrm{def}(BCD).
\]

***** Dos triÃ¡ngulos son congruentes si tienen los mismos Ã¡ngulos
Dos triÃ¡ngulos con los mismos Ã¡ngulos en geometrÃ­a hiperbÃ³lica
deben ser congruentes.

***** Paralela a una y perpendicular a otra
Dadas dos semirectas formando un Ã¡ngulo agudo, existe una Ãºnica
perpendicular a una y paralela a la otra.

***** El defecto es el Ã¡rea
El defecto es igual al Ã¡rea del triÃ¡ngulo.

**** Semiplano de PoincarÃ©
***** DefiniciÃ³n
Llamamos $\mathbb{H}^2 = \left\{ (x,y) \in \mathbb{R}^2\mid y>0
\right\}$ y $\forall p = (x,y) \in \mathbb{H}^2$ y sobre Ã©l tomamos 
la mÃ©trica $g_p = \frac{1}{y^{2}}\left\langle \cdot,\cdot \right\rangle$.

*** Ejercicios
**** Formas del universo
***** Ejercicio 1
#+begin_statement
Como seres de dimensiÃ³n 3 en un mundo de tres dimensiones es fÃ¡cil
dibujar y entender posibles formas de Planilandia pero no de nuestro
propio Universo. Imaginar la forma de nuestro Universo en alguna de
las siguientes situaciones:

 * Hacemos una expediciÃ³n a una galaxia remota. Al llegar a ella
   descubrimos estar de vuelta en la tierra.

 * Un astrÃ³nomo acaba de descubrir que los mismos objetos se
   encuentran en posiciones distintas del Universo.

 * Buscando ondas de radio que detecten seÃ±ales extraterrestres,
   detectamos una seÃ±xal que procede de una galaxia lejana.
   Investigamos y vemos que se trata de la seÃ±al de un programa de TV
   emitido hace 50 aÃ±os.

A las posibles formas de nuestro Universo les llamaremos
variedades de dimensiÃ³n 3 y su estudio constituye la TopologÃ­a
3-dimensional.
#+end_statement

En cualquiera de esos casos podrÃ­amos estar ante un toro tridimensional.
Los objetos se repetirÃ­an en el espacio una y otra vez, y todo el espacio
se repetirÃ­a con ellos. PodrÃ­amos interpretar que vivimos en un cubo en el
que estÃ¡n identificadas cada una de las caras con la opuesta. NÃ³tese que
segurÃ­a siendo un universo orientable.

En un segundo ejemplo, podrÃ­a ocurrir que tuviÃ©ramos un universo similar
al anterior pero en el que una de las paredes del cubo cambiara la 
orientaciÃ³n. Al pasar por ella volverÃ­amos al mismo punto pero habrÃ­amos
intercambiado derecha e izquierda.

En un tercer ejemplo, podrÃ­a ocurrir que el universo fuera infinito en
una dimensiÃ³n pero en las otras dos se comportara como un toro plano.
Dependiendo de la direcciÃ³n que tomÃ¡ramos, volverÃ­amos o no al punto
de partida.

***** Ejercicio 2
#+begin_statement
1. Â¿CuÃ¡les de las siguientes superficies tienen la misma topologÃ­a?
   
   [[./img/formasuniverso1.png]]

2. En Planilandia CP descubriÃ³ que dos caminos cerrados partiendo del mismo
   punto en direcciones opuestas no tienen por quÃ© volver a cruzarse en un
   punto diferente. Â¿Es esta propiedad geomÃ©trica o topolÃ³gica?
3. Describir superficies con la misma topologÃ­a pero diferente geometrÃ­a.
#+end_statement

1) Tienen la misma topologÃ­a:
   * la esfera y el objeto justo debajo.
   * el toro y la taza.
   * el resto de objetos.
2) Esta es una propiedad topolÃ³gica. El hecho seguirÃ¡ siendo cierto al
   aplicar deformaciones continuas al espacio.
3) Por ejemplo de un ejercicio anterior tomamos el toro y la taza, que
   tenÃ­an la misma topologÃ­a pero se puede observar que al hacer la
   deformaciÃ³n del toro en la taza han cambiado propiedades como Ã¡rea
   que es diferente para ambas figuras.

***** Ejercicio 3
#+begin_statement
Distingue segÃºn su topologÃ­a intrÃ­nseca y extrÃ­nseca.

[[./img/formasuniverso2.png]]

 * Â¿Puedes forrar un cilidro con parte de una hoja de papel sin deformarla?,
   Â¿y un cono?, Â¿y un trozo de esfera?
 * Â¿Coá¸¿o pueden los planilandeses que vivan en mundos como los de la figura
   conocer que sus geometrÃ­as intrÃ­nsecas son diferentes? Â¿QuÃ© pueden decir
   acerca de sus topologÃ­as extrÃ­nsecas e intrÃ­nsecas?

   [[./img/formasuniverso3.png]]

$\quad$
#+end_statement

Todas las figuras tienen misma topologÃ­a intrÃ­nseca. En el caso de la
topologÃ­a extrÃ­nseca podemos distinguir tres grupos:

 * las figuras azules junto a la morada adyacente a estas.
 * las figuras amarillas.
 * la morada que nos queda.

Podemos forrar el cilindro y el cono, por el contrario se crearÃ­an
pliegues en el papel al intentar cubrir el trozo de esfera.

En el trozo de esfera al medir los Ã¡ngulos de un triÃ¡ngulo el
resultado serÃ­a mayor que PI, en el hiperboloide serÃ­a menor que PI, y
en el plano serÃ­a igual a PI.

***** Ejercicio 4
#+begin_statement
  * Justifica que el toro llano y la superficie de un donut son
    topolÃ³gicamente equivalentes.

  * Consigue en la siguiente figura un de un toro llano tres =X=
    en lÃ­nea.

    #+attr_latex: :width 5cm 
    [[./img/formasuniverso4.png]]

  * CuÃ¡les de las posiciones siguientes serÃ­an equivalentes al jugar
    unas "tres en lÃ­nea" sobre un toro llano.

    [[./img/formasuniverso5.png]]

  * En el siguiente tablero de ajedrez sobre un toro llano, Â¿quÃ© figuras
    estÃ¡n amenazadas por el caballo blanco?

    #+attr_latex: :width 5cm 
    [[./img/formasuniverso6.png]]

  * Â¿QuÃ© figuras estÃ¡n amenazadas por el caballo y la reina blancos?

    #+attr_latex: :width 5cm 
    [[./img/formasuniverso7.png]]

  * Las siguientes figuras muestran un toro llano de tres dimensiones.
    Explica cÃ³mo se construye e imagina quÃ© verÃ­as al mirar en una direcciÃ³n
    concreta.

    #+attr_latex: :width 5cm 
    [[./img/formasuniverso8.png]]

$\quad$
#+end_statement

 * Triangulando el toro llano y el donut llegamos fÃ¡cilmente a la misma
   triangulaciÃ³n.
 * Escribimos X en la casilla inferior central.
 * Todos los celestes por un lado; todos los amarillos excepto los de la
   primera columna y el Ãºltimo de la tercera columna, y todos los demÃ¡s.
 * Rey, afil, y dos caballos negros.
 * Todas.
 * Identificando las caras, verÃ­amos nuestra espalda delante, nuestros pies
   hacia arriba y nuestra cabeza hacia abajo.

***** Ejercicio 5
#+begin_statement
Jugando en la botella de Klein:

  * Â¿CuÃ¡les de las siguientes posiciones ganan en el juego del tres en
    raya dentro de una botella de Klein?

    [[./img/formasuniverso10.png]]

  * Analiza cÃ³mo hacer tres en lÃ­nea en la siguiente figura.

    [[./img/formasuniverso11.png]]
#+end_statement

De las tres posiciones del tres en raya

 * la primera gana.
 * la segunda gana.
 * la tercera gana.

Si escribimos coordenadas del tres en raya como

| 0,0 | 0,1 | 0,2 |
| 1,0 | 1,1 | 1,2 |
| 2,0 | 2,1 | 2,2 |

se ganan las partidas con la posiciÃ³n

 * primera: (2,2)
 * segunda: (0,0)
 * tercera: (0,2)
 * cuarta: (2,2)
 * quinta: (1,1)

***** Ejercicio 6
#+begin_statement
Actividades:

 * Si un planilandÃ©s viviendo en un plano proyectivo cruza el ecuador,
   Â¿vuelve como su imagen especular?
 * Una familia de planilandeses vive en un plano proyectivo. Planean
   edificar dos gasolineras separadas cuanto mÃ¡s mejor. Â¿DÃ³nde deberÃ­an
   construirlas?
 * CP conoce que vive en una esfera o en un plano proyectivo. Â¿CÃ³mo
   podrÃ­a saber cuÃ¡l de los dos es su mundo?
 * Un segundo planilandÃ©s sabe que su Universo es un plano proyectivo o
   una botella de Klein, Â¿quÃ© podrÃ­a hacer para conocer de cuÃ¡l de los dos
   se trata?

$\quad$
#+end_statement

 1) SÃ­, el plano no es orientable.
 2) Una en el centro y otra en el ecuador.
 3) Cruzando la frontera y al volver, comparando su orientaciÃ³n.
 4) Ampliar primero una zona segura en la que pudiera volver a cada punto
    sin haber cambiado la orientaciÃ³n y luego comprobar si el resto del
    universo ha quedado dividido en dos (suma de planos) o en uno (plano).

***** Ejercicio 7
#+begin_statement
Haciendo sumas conexas:

 * Deducir que si a una cinta de MÃ¶bius le pagamos un disco por el borde,
   obtenemos un plano proyectivo.
 * Corrobora las palabras de Klein: "La cinta de MÃ¶bius es divina, si pegas
   dos por su borde obtienes mi botella".
 * Construye usando papel la suma conexa de una cinta de MÃ¶bius a un toro
   y a una botella de Klein.
 * Muestra que la suma conexa de un toro con un plano proyectivo es 
   topolÃ³gicamente equivalente a la suma conexa de una botella de Klein con
   un plano proyectivo.
 * Establece una correspondencia por equivalencia topolÃ³gica entre las
   superficies de los conjuntos A y B:

   \[
   A = \{
   \mathbb{T}^2\#\mathbb{S}^2,
   \mathbb{P}^2\#\mathbb{P}^2\#\mathbb{P}^2,
   \mathbb{B}^2,
   \mathbb{S}^2\#\mathbb{S}^2\#\mathbb{S}^2,
   \mathbb{P}^2\#\mathbb{T}^2,
   \mathbb{B}^2\#\mathbb{T}^2\#\mathbb{P}^2
   \}
   \]

   \[
   B = \{
   \mathbb{P}^2\#\mathbb{P}^2,
   \mathbb{B}^2\#\mathbb{P}^2,
   \mathbb{S}^2\#\mathbb{S}^2,
   \mathbb{P}^2\#\mathbb{P}^2\#\mathbb{P}^2\#\mathbb{B}^2,
   \mathbb{T}^2,
   \mathbb{T}^2\#\mathbb{P}^2
   \}
   \]
#+end_statement

1) Un plano proyectivo menos un disco es una banda de MÃ¶bius porque
   podemos dibujar el plano proyectivo en un disco y quitarle un disco
   que corte la frontera del disco.
2) NÃ³tese que si partimos la botella de Klein por la mitad, lo que queda
   en cada una de las mitades es una banda de MÃ¶bius.
3) SerÃ­an al final 5 planos proyectivos.
4) Usando la clasificaciÃ³n de superficies compactas sabemos que no
   necesitamos mezclar asas con gorros cruzados $\times \circ = \times^3$.
5) Usaremos clasificaciÃ³n de superficies compacatas para escribir cada
   una como suma de planos proyectivos o toros. La $\mathbb{S}^2$ es neutra bajo la
   suma conexa.

   Nos quedan

   * $\mathbb{B}^2 \cong \mathbb{P}\#\mathbb{P}$,
   * $\mathbb{B}^2 \#\mathbb{P}^2 \cong \mathbb{P}^2 \# \mathbb{P}^2 \# \mathbb{P}^2$,
   * $\mathbb{S}^2\# \mathbb{S}^2 \cong \mathbb{S}^2 \# \mathbb{S}^2 \# \mathbb{S}^2$,
   * $\mathbb{P}^2\#\mathbb{P}^2\#\mathbb{P}^2\#\mathbb{B}^2 \cong \mathbb{B}^2\#\mathbb{T}^2\#\mathbb{P}^2$,
   * $\mathbb{T}^2 \cong \mathbb{T}^2\#\mathbb{S}^2$,
   * $\mathbb{T}^2\#\mathbb{P}^2 \cong \mathbb{P}^2\#\mathbb{T}^2$.


Las geodÃ©sicas del modelo vienen dadas por intersecciones del hiperboloide
con subespacios lineales bidimensionales, que nunca serÃ¡n vacÃ­os
** Desarrollo y sistemas de informaciÃ³n
*** 1. Sistemas de informaciÃ³n
**** DefiniciÃ³n
***** Sistema
Conjunto de elementos ordenadamente relacionados contribuyendo a
una determinada funciÃ³n

***** Datos
RepresentaciÃ³n simbÃ³lica de variables cuantitativas o cualitativas.

***** InformaciÃ³n
Conjunto organizado de datos procesados, formando un mensaje que
cambia el estado de conocimiento del sistema que lo procesa.

***** Sistema de informaciÃ³n
Un sistema de informaciÃ³n es

 * un sistema automatizado o manual para recopilar, procesar y
   transmitir datos representando informaciÃ³n.

 * la infraestructura para la recopilaciÃ³n, procesamiento y transmisiÃ³n
   de informaciÃ³n.

**** Sistemas de informaciÃ³n empresarial: gestiÃ³n de recursos
La *gestiÃ³n de recursos* de una empresa es una utilidad de sistemas de
informaciÃ³n. Sirven para

 * *comunicaciÃ³n*
   * intranets/extranets
   * value added networks (VANs)

 * *resoluciÃ³n de problemas*
   * decision support systems (DSSs)
   * knowledge-based systems (KBSs)

debido a la complejidad de los sistemas actuales y al ahorro econÃ³mico
que proporcionan.

Gestionan recursos fÃ­sicos y conceptuales.

**** Sistemas de informaciÃ³n empresarial: funciones de un gerente
Las funciones de un gerente son

 - funciones de Fayol :: planificar, organizar, apoyar, dirigir y
      controlar.
 - papeles de Mintzberg :: interpersonales, informaciÃ³n y toma de
      decisiones.

a distintos niveles gerenciales

 - planificaciÃ³n estratÃ©gica :: ejecutivos,
 - control gerencial :: directores de producto, jefes de secciÃ³n,
 - control operativo :: jefes de proyecto, jefes de departamento,

a distintos niveles varÃ­a el origen y nivel de detalle de la informaciÃ³n
requerida.

**** TODO Sistemas de informaciÃ³n empresarial: CBIS

*** 2. DiseÃ±o conceptual de sistemas de informaciÃ³n
**** Modelo Entidad-RelaciÃ³n
El *modelo entidad-relaciÃ³n* es el mÃ¡s extendido para el diseÃ±o
conceptual; debe reflejar fielmente las necesidades de informaciÃ³n y
ofrecer un diseÃ±o independiente. Representa y manipula informaciÃ³n
de forma general y sistemÃ¡tica.

 - Datos :: tratamiento de un recurso conceptual.
 - Convenciones :: actuaciÃ³n sistemÃ¡tica y rigurosa.
 - Redundancia mÃ­nima :: modelado Ãºnico de cualquier objeto.

**** Elementos del modelo
# Pasar a folio los diagramas
Los elementos del modelo son

 - entidades :: objetos delimitados y distinguibles de los demÃ¡s;
 - conjuntos de entidades :: grupo de entidades con las mismas
      cualidades; tambiÃ©n llamados *tipos*;
 - atributos :: propiedades caracterizando un conjunto de entidades.

***** Atributos
Sobre los atributos, se consideran 

 - dominio :: valores permitidos para un atributo,
 - identificadores :: atributos que identifican unÃ­vocamente a
      una entidad (se seÃ±alan rellenando el cÃ­rculo),
 - atributo compuesto :: formado por varios (se seÃ±ala como un
      cÃ­rculo con atributos a su vez).

***** Entidades fuertes y dÃ©biles
Decimos que B *depende existencialmente* de A si cada B tiene
un A asociado y es imposible identificar a un b sin identificar
su a. La B es una entidad dÃ©bil respecto de A.

# TODO: Cardinalidad en entidades dÃ©biles!

***** Asociaciones
Relaciones semÃ¡nticas entre dos o mÃ¡s conjuntos de entidades.
Pueden tener *cardinalidad*

 * n:m - muchos a muchos
 * m:1 - uno a muchos
 * 1:m - muchos a uno
 * 1:1 - uno a uno,

y *participaciÃ³n*

 * 0 si es parcial u opcional,
 * 1 si es total.

Se leen fijando un objeto de la relaciÃ³n. Las relaciones pueden
tener atributos en algunos casos.

***** EspecializaciÃ³n
A es una especializaciÃ³n de B si estÃ¡ completamente incluÃ­do en Ã©l.
Todo a es un b.

 * *(p) Obligatoriedad parcial:* si hay entidades que no pertenezcan a
   ninguna especialidad.
 * *(t) Obligatoriedad total:* si toda entidad tiene que pertenecer a
   algÃºn conjunto especializado.
 * *(e) Exclusividad*: si sÃ³lo se pertenece a una especializaciÃ³n.
 * *(s) Solapada*: si puede pertenecerse a varias especializaciones.

***** AgregaciÃ³n
Entidad genÃ©rica de la que no se especifica estructura interna.

**** HeurÃ­sticas de modelado

 1. El grado de las relaciones suele ser binario. La cardinalidad
    n-aria hay que analizarla por partes.

 2. La apariciÃ³n de ciclos es normal, pero pueden estar dando lugar
    a informaciÃ³n redundante.

 3. No se debe abusar de las agregaciones. QuizÃ¡ un conjunto nuevo
    solucione la agregaciÃ³n.

**** Refinamiento
El refinamiento transforma diagramas E/R.

***** TODO Primitivas descendentes
Parten de versiÃ³n general y llegan a versiÃ³n especÃ­fica. No todos
los esquemas son producibles descendentemente, sÃ³lo los basados en
conexiones en serie y paralelo.

# TODO: Tipos de primitivas
***** TODO Primitivas ascendentes
Van de una visiÃ³n especÃ­fica a la versiÃ³n conectada. Todos los
esquemas son producibles ascendentemente.

# TODO: Tipos de primitivas
***** TODO DiseÃ±o centrÃ­fugo
VarÃ­a el orden de aplicaciÃ³n en los refinamientos.

***** DiseÃ±o mixto
Se produce un armazÃ³n y se modela cada parte con primitivas descendentes,
luego se conecta con primitivas ascendentes. Permite mÃ¡s flexibilidad.

*** Seminario 4: Modelado de datos
**** Esquemas de navegaciÃ³n

 - I :: inserciÃ³n,
 - D :: borrado,
 - U :: actualizaciÃ³n,
 - R :: consulta.

Las flechas marcan los atributos de entrada. Los atributos que aparecen
sin flecha son de salida. El sentido en el que se obtienen lo marcan las
flechas.

***** Errores
Un esquema de navegaciÃ³n sÃ³lo puede tener una consulta de inserciÃ³n,
borrado o actualizaciÃ³n; la Ãºnica excepciÃ³n son las entidades dÃ©biles.

*** 4. NormalizaciÃ³n
**** Algoritmos
***** SimplificaciÃ³n de conjunto de dependencias funcionales
Dado un conjunto de dependencias funcionales, lo simplifica a uno
equivalente. Usos:

 * facilitar la normalizaciÃ³n posterior,
 * facilitar el cÃ¡lculo de claves candidatas.

****** Algoritmo
1. Simplificar partes *derechas* AâBC como AâB y AâC.
2. Simplificar partes izquierdas eliminando *atributos raros*, esto
   es, si ABâC y AâB, tenemos directamente AâC.
3. *Eliminar* las dependencias que se deducen de otras.

***** CÃ¡lculo de claves candidatas
Obtener las claves candidatas de una relaciÃ³n. Se resume en tener cuidado
con independientes y equivalentes y luego simplemente formar claves con la
izquierda y con ambas.

****** Algoritmo
Clasificamos en

 * atributos independientes de las relaciones,
 * parejas de atributos equivalentes
 * atributos solo a izquierda,
 * atributos en ambos lados,
 * atributos solo a derecha.

Y ejecutamos

1. Elimina atributos *independientes* del algoritmo, formarÃ¡n
   siempre parte de la clave.
2. Elimina *equivalencias*, puede considerar uno y olvidar el otro,
   podrÃ­a formar parte de la clave cualquiera de los dos.
3. Comprueba si con los de izquierda se forma clave.
4. Comprueba en caso de que no haya funcionado aÃ±adiendo atributos
   de ambos lados.
5. Incorpora independientes.
6. Incorpora equivalentes.

**** Repaso de formas normales
Se definen

 * 1NF: los dominios de atributo tienen valores indivisibles.
 * 2NF: todo atributo no-primo depende completamente de cualquier
   clave candidata.
 * 3NF: todo atributo no-primo depende de forma no transitiva de cada
   clave candidata de R.
 * BCNF: cada dependencia no trivial (subconjunto) tiene a la derecha
   una clave candidata.

Se resuelven

 * 1NF: partiendo los atributos.
 * 2NF: partiendo en dos tablas distintas.
 * 3NF: partiendo en dos tablas distintas por la transitividad.
 * BCNF: rediseÃ±ando en tablas distintas.

***** Pledge
Every [non-key](FNBC) attribute must provide a fact about the key (1NF), the
whole key (2NF) and nothing but the key (3NF). For each candidate key.

***** Non-solvable problems
Beeri and Bernstein showed in 1979 that, for example, a set of
functional dependencies {AB â C, C â B} cannot be represented by a
BCNF schema.

**** 4.1. NormalizaciÃ³n
***** Dependencias
****** Dependencia funcional
Sean relaciones R(Aâ,Aâ â¯ Aâ), Î± â R, Î² â R entre atributos.
Decimos que Î± â Î² ssi â t,s â r: t[Î±] = s[Î±] â t[Î²] = s[Î²].

Los valores para el subconjunto Î± determinan de forma Ãºnica
los valores para el subconjunto Î².

****** Dependencia funcional completa
Una dependencia funcional es completa, Î± ââ Î² cuando se tiene
que ningÃºn subconjunto de atributos Î³ â Î± tiene una dependencia
Î³ â Î². Este Î± es minimal en ese sentido.

****** Atributo primo
Forma parte de una clave candidata

***** Axiomas de Armstrong sobre dependencias
1. Reflexividad. Î² â Î± nos da Î± â Î².
2. AmpliaciÃ³n. Î± â Î² nos da Î±Î³ â Î²Î³.
3. Transitividad. Î± â Î² y Î² â Î³ nos dan Î± â Î³.
4. UniÃ³n. Î± â Î² y Î± â Î³ nos dan Î± â Î²Î³.
5. DescomposiciÃ³n. Î± â Î²Î³ nos da Î± â Î² y Î± â Î³.
6. Pseudotransitividad. Î± â Î² y Î²Î³ â Î´ nos dan Î±Î³ â Î´.

****** Cierre de un conjunto de dependencias
F+ se obtiene como clausura de las dependencias de F aplicando
axiomas de Armstrong. Llamamos Î±+ a todos los atributos que se
obtienen como dependencias desde Î±.

****** Recubrimiento minimal o canÃ³nico
Es el mÃ­nimo F' tal que (F')+ = F+.

***** ObtenciÃ³n del recubrimiento minimal

1. *Partimos con la regla de descomposiciÃ³n* todas las partes derechas
   compuestas: AâBC parte en AâB y AâC.

2. *Simplificamos los atributos raros* de la izquierda, aquellos que
   dependen de los que le acompaÃ±an: si AB â C y sabemos A â B,
   podemos dejar A â C.

3. *Eliminamos dependencias redundantes* que puedan obtenerse de
   otras.

La ventaja de trabajar con recubrimiento minimal es que, al no
tener dependencias redundantes, podemos comprobar directamente si
cualquiera de ellas se ha perdido al aplicar Teorema de Heath.

***** 1FN
El dominio de cada atributo es atÃ³mico y cada valor del atributo
sÃ³lo contiene un valor para ese dominio.

***** 2FN
La 2FN es

 * estar en 1FN,
 * todos los atributos no primos dependen de forma completa de las
   claves candidatas.


****** DescomposiciÃ³n sin pÃ©rdidas
Una relaciÃ³n (R,r) se descompone *sin pÃ©rdidas* en (R1,r1) y (R2,r2)
cuando R1 âª R2 = R y ademÃ¡s (r1 join r2) = r.

****** Teorema de Heath
Si tenemos grupos Î±, Î², Î³ con Î² â Î³, podemos separar la relaciÃ³n
sin pÃ©rdidas en

 * R1(Î±,Î²)
 * R2(Î²,Î³)

***** 3FN
La 3FN es

 * estar en 2FN,
 * no tener dependencias transitivas problemÃ¡ticas, a travÃ©s de
   atributos no primos.

****** Dependencia transitiva
CK â Î², para Î² formado por algÃºn atributo no primo, es transitiva
cuando âÎ± â R, (Î± â R) â F+, tal que

(CK â Î±) â F;  y ademÃ¡s,  (Î± â Î²) â F

***** FNBC
La definiciÃ³n original de 3FN tiene deficiencias si hay varias claves
candidatas. La FNBC considera estos casos, es

 * estar en 3FN,
 * â Î±âÎ² â F se cumple que
   * Î± es clave candidata y Î²âÎ±, o
   * Î² â Î±.

Es decir, todo determinante es una clave candidata.

***** CÃ¡lculo de llaves candidatas
1. EliminaciÃ³n de atributos independientes: de ellos no se deduce
   ningÃºn otro, tendrÃ¡n que estar en la clave candidata en cualquier
   caso.

2. ConstrucciÃ³n de atributos equivalentes: para cada pareja de
   equivalentes se usa sÃ³lo uno de ellos.

3. Clave sin determinantes determinados: se selecciona un candidato a
   clave candidata sin determinantes no determinados.

4. Si hay mÃ¡s claves sin determinantes determinados se necesitarÃ¡
   cubrir todas las claves posibles, aÃ±adiendo a cada paso n nuevo
   determinante que sea determinado.

5. AÃ±adimos atributos independientes a las claves obtenidas

6. Replicamos las claves con las equivalencias del paso 2.

***** Proceso de normalizaciÃ³n
1) Localizar las claves candidatas.

   1) Descomponer dependencias a la derecha.
   2) Eliminar atributos raros a la izquierda.
   3) Eliminar dependencias redundantes.
   4) Localizar claves candidatas

2) Comprobar 2FN, usar Teorema de Heath.

3) Comprobar 3FN, eliminar transitividades problemÃ¡ticas.

**** 4.2. DiseÃ±o fÃ­sico
**** Ejercicios
***** Soluciones normalizaciÃ³n
****** Ejercicio 1
Simplificando dependencias queda

| Tabla        | ABCDE        |
| Claves       | AD, DBC, DBE |
|--------------+--------------|
| Dependencias | A â B        |
|              | A â C        |
|              | BC â A       |
|              | BCD â E      |
|              | E â C        |

No existiendo atributos primos, estÃ¡ en 3FN. Normalizamos
a FNBC como

| Tabla        | ABC    | BDE     | EC    |
| Claves       | A, BC  | BDE     | E     |
|--------------+--------+---------+-------|
| Dependencias | A â B  |         | E â C |
|              | A â C  |         |       |
|              | BC â A |         |       |

se ha perdido la dependencia BCDâE.

****** Ejercicio 2
Simplificando dependencias queda

| Tabla        | ABCD   |
| Claves       | AB, AC |
|--------------+--------|
| Dependencias | C â D  |
|              | AB â C |
|              | C â B  |

La tabla no estÃ¡ en 2FN por CâD. Partimos como sigue para llegar a 3FN

| Tabla        | ABC    | CD    |
| Claves       | AB, AC | C     |
|--------------+--------+-------|
| Dependencias | C â B  | C â D |
|              | AB â C |       |


Y partimos de nuevo para llegar a FNBC

| Tabla        | AC | CB    | CD    |
| Claves       | AC |       | C     |
|--------------+----+-------+-------|
| Dependencias |    | C â B | C â D |
|              |    |       |       |
|              |    |       |       |

Hemos perdido la dependencia ABâC.

****** Ejercicio 3
Simplificando dependencias queda

| Tabla        | AOIVND |
| Claves       | VI     |
|--------------+--------|
| Dependencias | V â D  |
|              | I â A  |
|              | IV â N |
|              | A â O  |

No estamos en 2FN por VâD y IâA, partimos en

| Tabla        | IVN    | IA    | AO    | VD    |
| Claves       | VI     | I     | A     | V     |
|--------------+--------+-------+-------+-------|
| Dependencias | IV â N | I â A | A â O | V â D |

Que estÃ¡ en FNBC.

****** Ejercicio 5
Simplificando dependencias queda

| Tabla        | SIDBQO |
| Claves       | S      |
|--------------+--------|
| Dependencias | S â I  |
|              | S â D  |
|              | I â B  |
|              | S â Q  |
|              | B â O  |

Estamos en 2FN porque todas dependen completamente por transitividad
de S, pero no estamos en 3FN. Corregimos partiendo en

| Tabla        | SIDQ  | IB    | BO    |
| Claves       | S     | I     | B     |
|--------------+-------+-------+-------|
| Dependencias | S â I | I â B | B â O |
|              | S â D |       |       |
|              | S â Q |       |       |

Que estÃ¡n en FNBC.

****** Ejercicio 6
Simplificando dependencias queda

| Tabla        | ABCDE  |
| Claves       | BE     |
|--------------+--------|
| Dependencias | A â C  |
|              | B â C  |
|              | C â D  |
|              | DE â C |
|              | CE â A |

No estÃ¡ en 2FN porque BâC, por ejemplo. Vamos a partir la tabla para
llegar a 3FN sin pÃ©rdidas. A cada paso, partiremos de forma que los
atributos compartidos entre las dos partes sean una clave en alguna de
ellas.

| Tabla        | BE | BC    | ECD    | ECA    |
| Claves       | BE | B     | EC, ED | EC, EA |
|--------------+----+-------+--------+--------|
| Dependencias |    | B â C | DE â C | CE â A |
|              |    |       | C â D  | A â C  |

Esta separaciÃ³n no estÃ¡ en FNBC por C â D y A â C, partimos de nuevo
para llegar a FBNC,
 
| Tabla        | BE | BC    | EC | CD    | EA | AC    |
| Claves       | BE | B     | EC | C     | EA | A     |
|--------------+----+-------+----+-------+----+-------|
| Dependencias |    | B â C |    | C â D |    | A â C |

hemos perdido DE â C y CE â A como dependencias.

****** Ejercicio 7
Simplificando dependencias

| Tabla        | ABCDE  |
| Claves       | AB     |
|--------------+--------|
| Dependencias | AB â C |
|              | C â E  |
|              | E â C  |
|              | C â D  |

Estamos en 2FN porque todos dependen de AB de forma completa; pero no
estamos en 3FN. Partimos para llegar a 3FN.

| Tabla        | ABC    | CE    | CD    |
| Claves       | AB     | C, E  | C     |
|--------------+--------+-------+-------|
| Dependencias | AB â C | E â C | C â D |
|              |        | C â E |       |

Que estÃ¡n en FNBC.

***** Ejercicio 1
#+begin_statement
Normalizar,

A â BC,  BC â A, BCD â E, E â C.
#+end_statement

Las dependencias estÃ¡n ya descompuestas a la derecha.

Eliminamos atributos raros a izquierda, BC â E no tiene
atributos raros; para BCD â E podemos comprobar que ni
BC genera D, ni BD genera C ni CD genera B.

Eliminamos dependencias redundantes, la A â BC no lo es
porque no se sigue de las demÃ¡s ninguna dependencia de A.
La BC â A tampoco lo es, asÃ­ como la BCD â E ni E â C.
***** Ejercicio 2
#+begin_statement
Normalizar R(A,B,C,D) con

AB â D,  C â D,  AB â C,  C â B.
#+end_statement

****** Recubrimiento minimal
Descomponemos a derecha, ya hecho. simplificamos atributos
raros, ya hecho. Eliminamos dependencias redundantes, como
la AB â D; nos queda

C â D,  AB â C,  C â B.

****** Clave candidata
Localizamos claves candidatas

| Izquierda | Ambos | Derecha |
|-----------+-------+---------|
| A         | BC    | D       |

tenemos A+={A}, luego aÃ±adimos otra, AB+={ABCD} es clave
candidata.

****** 2FN
Los atributos C, D deberÃ­an depender de forma completa.
Tenemos que no dependen de A sola ni de B sola, luego lo
hacen de forma completa.

****** 3FN
Tenemos una transitividad AB â C y C â D. Queremos que todo
determinante sea una clave candidata, asÃ­ que partimos

R1(_A,B_,C) y R2(_C_,D)

donde R1 tiene AB â C y C â B; mientras que R2 tiene C â D.

****** FNBC
Tenemos un determinante en R1 que no es clave candidata.
Solucionamos partiendo

R2(_C_,D),
R3(_A,C_),
R4(_C_,B),

el problema aquÃ­ es que se pierde el hecho de que AB â C.

***** Ejercicio 3
#+begin_statement
Normalizar R(A,O,I,V,N,D) con,

V â D,  I â A,  IV â N,  A â O.
#+end_statement

****** Clave candidata
Las dependencias estÃ¡n descompuestas a la derecha.
Eliminamos atributos raros a izquierda, nÃ³tese que en IV â N, tenemos
Iâº={IAO} y Vâº={VD}, luego no hay atributos raros.
No hay ninguna dependencia redundante, ya que cada
dependencia genera un atributo distinto.

Localizamos claves candidatas,

| Izquierda | Ambos | Derecha |
|-----------+-------+---------|
| VI        | A     | NDO     |

Tenemos que VI forman parte de cualquier clave candidata.
VIâº={VIANDO}, luego es clave candidata.

****** Segunda forma normal
Comprobamos ahora que estÃ© en segunda forma normal.

 * Los atributos AOD no primo no dependen de forma completa de las
   claves candidatas. Descomponemos

   R(V,I,N)
   R(I,A,O)
   R(V,D)

   tal que cada una estÃ¡ en forma normal. 

****** Tercera forma normal
La Ãºnica transitividad estÃ¡ en I â A y A â O. Descomponemos
como

 R($V,$I,N)
 R($V,D)
 R($I,A)
 R($A,O)

y todas estas tablas estÃ¡n en FNBC; ya que sÃ³lo la primera tiene
una clave candidata compuesta y el Ãºnico atributo depende de la
clave de forma completa.
***** Ejercicio 4 (es el 2!?)
#+begin_statement
Normalizar R(A,B,C,D)

AB â C,  AB â D,  C â D,  C â B.
#+end_statement

****** Recubrimiento minimal
Partimos a la derecha, ya hecho. Simplificamos atributos raros

***** Ejercicio 5
#+begin_statement
Normalizar

S â ID,  I â B,  IS â Q,  B â O
#+end_statement

****** Recubrimiento minimal
Partimos a la derecha S â I, S â D. Simplificamos atributos raros, con
S â Q, nos queda

 SâI, SâD, IâB, SâQ, BâO.

****** Clave candidata

| Izq | Amb | Der |
|-----+-----+-----|
| S   | IB  | DQO |

Y tenemos que S+={SIDBQO}, luego es clave candidata.

****** 2FN
DeberÃ­an depender todos de forma completa de S y lo hacen
porque estÃ¡ formada por un sÃ³lo atributo.

****** 3FN
Tenemos una transitividad SâI, IâB, BâO; la quitamos como

R1(_S_,I,D,Q)
R2(_I_,B)
R3(_B_,O)

****** FNBC
Todas las relaciones con una Ãºnica llave candidata en 3FN estÃ¡n en
FNBC.  Todo determinante es clave candidata.

***** Ejercicio 6
Sea el esquema R(A,B,C,D,E) con dependencias

 AâC, BâC, CâD, DEâC, CEâA.

****** Recubrimiento minimal
Empezamos trabajando con un recubrimiento minimal de las
dependencias, simplificamos a derecha y eliminamos atributos
raros. Eliminamos dependencias redundantes. Ya hecho.

****** Clave candidata

| Izq   | Ambos | Der |
|-------+-------+-----|
| BE    | ACD   |     |

Tenemos {BE}+={BECAD}, luego es clave candidata.

R(_B,E_,A,C,D)

****** 2FN
Todos deberÃ­an depender de forma completa de BE, pero no lo hacen;
tenemos BâC, BâD, luego partimos

R1(_B,E_,A)
R2(_B_,C,D)

****** 3FN
No queremos ahora dependencias transitivas completas, luego

R1(_B,E_,A)
R2(_B_,C)
R3(_C_,D)

hemos perdido que CEâA (??).

****** FNBC
???

***** Ejercicio 7
#+begin_statement
Sea el esquema R(A,B,C,D,E) con dependencias

AB â C,  C â E,  E â C,  C â D,  AB â E.
#+end_statement

****** Recubrimiento minimal
Descomponemos a derecha, quitamos atributos raros y eliminamos
redundancia.

Quitamos AB â E.

****** Clave candidata
Quitamos independientes, que no hay. Reducimos equivalentes,
donde tenemos CâE y EâC. Nos queda

ABâC, CâD

| Izq | Amb | Der |
|-----+-----+-----|
| AB  | C   | D   |

Como {AB}+={ABCDE}, ya tenemos clave candidata Ãºnica.

****** 2FN
Dependencia completa se tiene siempre.

****** 3FN
Transitividades que se pueden quitar estÃ¡n CâE y CâD

R1(_A,B_,C)
R2(_C_,D,*E*)

Perdemos que EâC.

****** FNBC
En R2 tenemos dos llaves candidatas, queremos que todo determinante
sea clave candidata.

R1(A,B,C) con clave A,B
R2(C,D,E) con clave C o con clave E.

***** Ejercicio 8
#+begin_statement
Dado

AâB, AâC, AâB, BâC, BâA, BâD, DâC
#+end_statement

****** Claves candidatas

| Izq | Med | Der |
|-----+-----+-----|
|     | ABD | C   |

A+={A,B,C,D}
B+={B,A,C,D}
D+={C}

La A y la B son claves candidatas. Hay dependencias transitivas
pasando por la D.

****** 3FN
No queremos transitividad a travÃ©s de no primos, asÃ­ que

R(A,B) candidatas A y B
R(A,D) clave A
R(D,C) clave D

****** FNBC
Todo determinante debe ser clave candidata. Lo es.
** Apuntes de Ã¡lgebra homolÃ³gica de Pascual Jara
*** HomologÃ­a de Hochschild
**** R;R mÃ³dulos
***** R;R mÃ³dulo
Sea $R$ una $K\text{-Ã¡lgebra}$; un $(R;R)$ *mÃ³dulo* es un $R$ mÃ³dulo a
izquierda y derecha verificando la *relaciÃ³n de compatibilidad*:

\[r_1(mr_2) = (r_1m)r_2\]

En particular, se tiene,

\[km = mk \quad \forall k \in K\]

Un *homomorfismo de R;R-mÃ³dulos* es un homomorfismo de R-mÃ³dulos a izquierda y
R-mÃ³dulos a derecha. Forman la categorÃ­a $(R;R)\mathtt{-Mod}$.

***** Ãlgebra envolvente
Sea $R$ una $K\text{-Ã¡lgebra}$, llamamos *Ã¡lgebra envolvente* a $R^e = R \otimes R^{op}$.
Con el producto:

\[ (r_1 \otimes s_1)(r_2 \otimes s_2) = (r_1r_2) \otimes (s_2s_1)\]

***** CaracterizaciÃ³n de R;R-mÃ³dulos
Para cada $K\text{-Ã¡lgebra}$, $R$, las categorÃ­as siguientes son isomorfas:

  - $(R;R)\text{-Mod}$
  - $R^e\text{-Mod}$
  - $\text{Mod-}R^e$

****** DemostraciÃ³n
NÃ³tese primero que a $R^e$ le damos estructura de Ã¡lgebra con el
producto:

\[
(r_1\otimes s_1)(r_2\otimes s_2) = (r_1r_2\otimes s_2s_1)
\]

******* Primera implicaciÃ³n
Si tenemos $M$ un $(R;R)$ mÃ³dulo, podemos darle estructura de $R^e$ mÃ³dulo
con: $(r\otimes s)m = rms$. Que esta estructura es compatible con el producto
se comprueba trivialmente con:

\[
(a\otimes b)(c \otimes d)m = acmdb = (ac \otimes bd)m
\]

******* Segunda implicaciÃ³n
Si tenemos $M$ un $R^e$ mÃ³dulo, podemos darle estructura de $R;R$ mÃ³dulo
tomando: $rms = (r\otimes s) m$. La relaciÃ³n de compatibilidad se tiene por
el mismo proceso anterior.

**** CohomologÃ­a de Hochschild
***** DefiniciÃ³n
Sea $R$ una $K\text{-Ã¡lgebra}$ y $M$ un $(R;R)\text{-mÃ³dulo}$, llamamos:

  - *CohomologÃ­a de Hochschild* de $R$ en $M$ a $HH^{\bullet}(R,M) = \operatorname{Ext}^\bullet_{R^e}(R,M)$.
  - *HomologÃ­a de Hochschild* de $R$ en $M$ a $HH_{\bullet}(R,M) = \operatorname{Tor}_\bullet^{R^e}(R,M)$.

***** Notas
1. [[https://sbseminar.wordpress.com/2007/07/22/hochschild-homology/][Hochschild homology | Secret Blogging Seminar]]

***** ResoluciÃ³n estÃ¡ndar
Para cada k-Ã¡lgebra $R$ se tiene $(P_{\bullet},d_{\bullet})$ resoluciÃ³n proyectiva de $R$ en 
$(R;R)\mathrm{-mod}$. Se llama *resoluciÃ³n estÃ¡ndar* de $R$, definiendo

 - $P = R \otimes (R^{\otimes n}) \otimes R$
 - $d_{n} = \sum_{i=0}^n (-1)^id^i_n$

donde tomamos

\[
d_n^i(a_0 \otimes \dots \otimes a_{n+1}) =
a_0 \otimes \dots \otimes a_ia_{i+1}\otimes \dots \otimes a_{n+1}.
\]

***** Complejo de cocadenas
Para el cÃ¡lculo de la cohomologÃ­a tenemos un complejo de cocadenas

\[
\mathrm{Hom}_K(K,M) \overset{b^0} \longrightarrow
\mathrm{Hom}_K(R,M) \overset{b^1} \longrightarrow
\mathrm{Hom}_K(R^{\otimes 2},M) \overset{b^2} \longrightarrow
\dots.
\]

Donde estÃ¡n definidas las $b^{n}$ como

 - $b^0(m)(a) = am-ma$
 - $b^n = \sum^{n+1}_{i=0}(-1)^ib_i^n$

para unas aplicaciones auxiliares $b_i^n$ definidas como

\[
b^n_i(f)(a_1\otimes \dots \otimes a_{n+1}) =
\left\{\begin{array}{ll} 
a_1f(a_2\otimes\dots\otimes a_{n+1})& \mbox{if } i=0  \\
f(a_1\otimes\dots\otimes a_ia_{i+1}\otimes\dots\otimes a_{n+1}}& \mbox{if } i=1,\dots,n \\
f(a_1\otimes\dots\otimes a_n)a_{n+1}& \mbox{if } i=n+1
\end{array} 
\right.
\]

**** Desde Weibel
***** Identidades simpliciales
Las identidades simpliciales son las relaciones entre morfismos cara y
morfismos degenerados en un objeto simplicial.

****** Objeto simplicial
Un conjunto simplicial con morfismos:

 - *Morfismos cara*, $\partial_i : S_n \to S_{n-1}$, que eliminan el vÃ©rtice $i$.
 - *Morfismos degenerados*, $\sigma_i : S_n \to S_{n+1}$, que duplican el vÃ©rtice $i$.

****** Identidades simpliciales
Se cumplen:

 1. $\partial_i\circ\partial_j = \partial_{j-1}\partial_i$ cuando $i<j$
 2. $\sigma_i\comp\sigma_j = \sigma_j\circ\sigma_{i-1}$ cuando $i > j$
 3. Se componen como:

    \[
    \partial_i\circ s_j = \left\{\begin{array}{ll} 
    s_{j-1}\circ\partial_i & \mbox{if } i<j \\
    id & \mbox{if } i=j \mbox{ or } i=j+1 \\
    s_j\circ \partial_{i-1}& \mbox{if } i>j+1
    \end{array} 
    \right.
    \]

****** Diferencial alternado
Definimos el morfismo cara alternado como la suma alternada de morfismos
cara:

\[
\partial(x) =
\sum_{k=0}^n (-1)^k \partial_k(x)
\]

Esto nos da un operador de frontera cumpliendo $\partial\circ\partial=0$.

******* DemostraciÃ³n
Usando distributividad de la composiciÃ³n y la primera identidad 
simplicial:

\[\begin{aligned}
\partial\circ\partial
&=
\left(
\sum (-1)^k \partial_k
\right)
\left(
\sum (-1)^t \partial_t
\right)
\\&= 
\sum_{k,t} (-1)^{k+t} \partial_k\partial_t
\\&=
\sum_{k<t} 
\left(
(-1)^{k+t} \partial_k\partial_t +
(-1)^{k+t-1} \partial_{t-1}\partial_k
\right)
\\&= 0
\end{aligned}
\]
***** MÃ³dulo simplicial sobre R;R mÃ³dulo
Sea $R$ una k-Ã¡lgebra y $M$ un $R;R$ mÃ³dulo. Tenemos un k-mÃ³dulo simplicial
en $M\otimes R^{\otimes\ast}$, dado por:

\[
\dots \longrightarrow
M\otimes R\otimes R \longrightarrow
M\otimes R \longrightarrow
M \longrightarrow 
0
\]

Con fronteras:

\[
\partial_i(m\otimes r_1 \otimes \dots \otimes r_n) =
\left\{\begin{array}{ll} 
mr_1 \otimes r_2 \otimes \dots \otimes r_n& \mbox{if } i = 0 \\
m \otimes r_1 \otimes r_2 \otimes \dots \otimes r_ir_{i+1} \otimes \dots \otimes r_n& \mbox{if } 0 < i < n \\
r_nm \otimes r_1 \otimes r_2 \otimes \dots \otimes r_{n-1} & \mbox{if } i = n
\end{array}.
\right.
\]

Y con degeneraciÃ³n

\[
\sigma_i(m \otimes r_1 \otimes \dots \otimes r_n) =
m \otimes \dots \otimes r_i \otimes 1 \otimes r_{i+1} \otimes \dots \otimes r_n.
\]

***** HomologÃ­a de Hochschild
La homologÃ­a de Hochschild se define como la de los k-mÃ³dulos:

\[
HH_n(R,M) = H_nC(M\otimes R^{\otimes\ast})
\]

**** Ejemplos de cÃ¡lculo de cohomologÃ­a de Hoschild
***** TODO Derivaciones y derivaciones externas
*** Ãlgebras separables
**** Ãlgebras separables
***** DimensiÃ³n de Hochschild
Definimos $\mathrm{Hdim}(R)$ como el menor $n$ tal que $HH^{n+1}(R,M) = 0$ para cualquier
mÃ³dulo $M$. Si no existe, decimos que hay dimensiÃ³n de Hochschild infinita.

***** Ãlgebras separables
Las Ã¡lgebras de dimensiÃ³n de Hochschild $0$ se llaman *separables*. Se
tiene $R$ un $R^e-\mathrm{mod}$ izquierda proyectivo.

***** CaracterizaciÃ³n de Ã¡lgebras separables
Para una k-Ã¡lgebra R equivalen:

  1) Existen $a_i,b_i \in R$ con $\sum_{i=1}^na_ib_i=1$ y $\sum_{i=1}^n ra_i\otimes b_i = \sum_{i=1}^n a_i \otimes b_ir$.
  2) DimensiÃ³n de Hochschild de $R$ cero.
  3) $\mathrm{Der(R,M)} = \mathrm{Der}_{int}(R,M)$ para cada $M$ y $n \geq 1$.

Llamamos $e = \sum_{i=1}^n a_i\otimes b_i \in R^e$ *idempotente de separabilidad*.

***** Teorema de Zelinsky
Toda k-Ã¡lgebra separable es un k-espacio vectorial de dimensiÃ³n finita.

**** Ãlgebras semisimples
***** Ãlgebra semisimple
Una k-Ã¡lgebra es semisimple si cada mÃ³dulo a la izquierda suyo es proyectivo.

***** Separable es semisimple
Toda k-Ã¡lgebra separable es semisimple.

****** TODO DemostraciÃ³n

***** Ãlgebras separables sobre cuerpos algebraicamente cerrados
Una k-Ã¡lgebra separable en un cuerpo de caracterÃ­stica cero es de la forma:

\[
R \cong
M_{n_1}(K) \times \dots \times M_{n_t}(K)
\]

***** Traza
Llamamos *traza* de $\varphi \in \mathrm{End}(R)$ a la suma de su diagonal. Tenemos

 - una aplicaciÃ³n $\tau(r) = \mathrm{Tr}(\lambda(r))$ k-lineal.
 - una aplicaciÃ³n $\sigma(r,s) = \tau(rs)$ k-bilineal simÃ©trica.

Cuando $R$ es k-Ã¡lgebra separable, $\sigma$ es no degenerada.

***** CaracterizaciÃ³n de Ã¡lgebras separables
Para una k-Ã¡lgebra $R$ equivalen

 1) $R$ una k-Ã¡lgebra separable.
 2) $\mathrm{dim}_{k}(R) < \infty$ y $\sigma$ es forma bilineal simÃ©trica no degenerada.
 3) Existe un Ãºnico idempotente de separabilidad simÃ©trico.

**** Ãlgebras formalmente lisas
***** Extensiones de Hochschild
Para $\pi : S \longrightarrow R$ homomorfismo sobreyectivo con $\ker(\pi)^2=0$,

\[
0 \longrightarrow 
\mathsf{a}= \mathrm{ker}(\pi) \longrightarrow
S \longrightarrow
R \longrightarrow
0
\]

es una *extensiÃ³n de Hochschild* de $R$ por $\mathsf{a}$.

***** Estructura de R;R-mÃ³dulo de la extensiÃ³n
Si $0 \longrightarrow \mathsf{a} \overset{\pi}\longrightarrow S \longrightarrow R \longrightarrow 0$ es una extensiÃ³n de Hochschild, entonces
$\mathsf{a}$ es un R;R-mÃ³dulo.

****** DemostraciÃ³n
Consideramos una inversa del homomorfismo sobreyectivo $\pi \circ \psi = id$.
Definimos el producto como

 - $ax = \psi(a)x$
 - $xa = x\psi(a)$

y comprobamos que $a(bx) = (ab)x$. Se tiene en efecto que

\[
\psi(a)\psi(b) - \psi(ab) \in \mathrm{ker}(\pi) = \mathsf{a}
\]

***** Extensiones equivalentes
Dos extensiones de Hochschild son equivalentes si existe un diagrama
conmutativo

\[\begin{tikzcd}
0 \rar & M \arrow[d,equal] \rar & S_1\dar{\varphi}\rar{\pi_{1}} & R \rar\dar[equal] & 0 \\
0 \rar & M \rar & S_2\rar{\pi_{2}} & R \rar & 0
\end{tikzcd}\]

para $\varphi$ un homomorfismo de k-Ã¡lgebras.

***** Ãlgebra formalmente lisa
Un Ã¡lgebra $R$ es formalmente lisa si su dimensiÃ³n de Hochschild es menor o
igual que 1. Equivalentemente, $HH^2(R,M) = 0$ para cualquier $M$. Como
consecuencia, toda extensiÃ³n de Hochschild es trivial.

***** Producto de separable y formalmente lisa
Si $S$ es separable y $R$ es formalmente lisa, $R \otimes S$ es formalmente lisa.

*** Anillos graduados
**** Ãlgebra aumentada
Una $K\text{-Ã¡lgebra}$ $R$ es *aumentada* si existe un $R\text{-mÃ³dulo}$ $M$ con un 
epimorfismo $\varepsilon\colon R \to M$.

***** Ãlgebra aumentada tensor
Si $\varepsilon\colon R \to M$ es un Ã¡lgebra aumentada, $\varepsilon\otimes S\colon R\otimes_K S \longrightarrow M \otimes_K S$ es
un Ã¡lgebra aumentada.

**** MÃ³dulos y Ã¡lgebras graduadas
***** MÃ³dulo graduado
Un $K\text{-mÃ³dulo}$ graduado es aquel que se escribe como suma directa,

\[
M = \bigoplus_{n \in \mathbb{N}} M_n.
\]

****** Elemento homogÃ©neo
Un elemento es homogÃ©neo de grado $n$ si pertenece a $M_n$.

****** Homomorfismo graduado
Un homomorfismo $f\colon M \to N$ tal que $f(M_n) \subseteq f(N_n)$.

****** Ãlgebra graduada
Un $K\text{-mÃ³dulo}$ graduado $R = \bigoplus R_i$ cumpliendo

\[
R_rR_t \subseteq R_{r+t}.
\]

***** Ejemplos de Ã¡lgebras graduadas
****** GraduaciÃ³n trivial
****** GraduaciÃ³n del anillo de polinomios
****** GraduaciÃ³n del anillo de endomorfismos
***** Homomorfismos graduados
Un homomorfismo graduado de grado $t \in \mathbb{N}$ es aquel homomorfismo $f\colon M \to N$
cumpliendo $f(M_r) \subseteq f(M_{r+t})$.

*** MÃ³dulos diferenciales
**** Diferencial
Un homomorfismo graduado de grado uno, $\delta\colon M \to M$ se llama *diferencial*
si $\delta\circ\delta = 0$.

**** DG-mÃ³dulo
Un par $(M,\delta)$ formado por un $K\text{-mÃ³dulo}$ graduado $M$ y un diferencial $\delta$.

***** Homomorfismo de DG-mÃ³dulos
Un homomorfismo de DG-mÃ³dulos $f \colon (M,\delta_1) \to (N,\delta_2)$ es un homomorfismo
de mÃ³dulos graduados cumpliendo

\[
f\delta_1 = \delta_2 f.
\]

**** Ãlgebra graduada diferencial
Una $K\text{-Ã¡lgebra}$ graduada $R$ con un diferencial $\delta\colon R\to R$ que verifica:

  * $(R,\delta)$ es un DG-mÃ³dulo.
  * $\delta(ab) = \delta(a)b + (-1)^{|b|}a\delta(b)$.

Es lo que conocemos como *Ã¡lgebra graduada diferencial*, o DG-Ã¡lgebra.

***** AritmÃ©tica en Ã¡lgebras graduadas I
Dada una DG-Ã¡lgebra $(R,\delta)$,

 1) Para elementos homogÃ©neos $h_1,\dots,h_n$ se verifica

    \[ \delta(h_1\dots h_n) =
    \sum_{i=1}^t (-1)^{|h_1|+\dots+|h_{i-1}|}h_1\dots \delta(h_i)\dots h_t.
    \]

 2) Para elementos $a_0,\dots,a_n \in R_0$ se verifica

    \[\delta(a_0 \delta(a_1)\dots \delta(a_n)) =
    \delta(a_0)\delta(a_1)\dots\delta(a_n).
    \]

***** TODO AritmÃ©tica en Ã¡lgebras graduadas II

**** Complejo de Hochschild
Definimos el *complejo de Hochschild* como

\[
C_{\bullet}(R,M)\colon \dots 
\longrightarrow M \otimes R^{\otimes n}
\overset{d}\longrightarrow M \otimes R^{\otimes n-1}
\overset{d}\longrightarrow \cdots
\]

donde las diferenciales se definen como $d = \sum (-1)^id_i$ para

\[\begin{aligned}
d_0(m\otimes a_1\otimes\dots\otimes a_n) &= (ma_1)\otimes \dots \otimes a_n \\
d_i(m\otimes a_1\otimes\dots\otimes a_n) &= m \otimes a_1\otimes \dots \otimes a_ia_{i+1} \otimes \dots \otimes a_n \\
d_i(m\otimes a_1\otimes\dots\otimes a_n) &= a_nm \otimes a_1\otimes \dots \otimes \dots \otimes a_{n-1} \\
\end{aligned}
\]

***** Es un diferencial
Se tiene que $d\circ d = 0$.

***** Complejo bar
En el caso particular de $R=M$ tenemos

\[C_{\bullet}(R) = C_{\bullet}(R,R) = 
(\cdots \to R^{\otimes n+1} \to \cdots \to R)\]

Llamamos *complejo bar* a

\[ C^{bar}_{\ast}(R) = (\cdots \to R^{\otimes n} \to \cdots \to R^{\otimes 2}).
\]

***** Complejo bar normalizado
Llamamos $D_n$ al generado por elementos $m\otimes a_1\otimes \dots\otimes a_n$ donde algÃºn $a_i = 1$.
Podemos crear el complejo bar normalizado con mÃ³dulos de la forma

\[\frac{M\otimes R^{\otimes n}}{D_n} \cong M \otimes \overline{R}^{\otimes n}.
\]

Lo representamos por $\overline{C}(R,M)$.

*** La DG-Ã¡lgebra Omega
**** DG-Ã¡lgebra omega
Definimos

\[
\Omega_S^nR = 
R \otimes_S \overline{R}^{\otimes n}
\quad\text{ y, en total, }\quad
\Omega_SR = \bigoplus_{n\in \mathbb{N}} \Omega_S^n R.
\]

La diferencial la definimos mediante

\[
\delta(a_0\oplus \overline{a_1} \oplus\dots\oplus \overline{a_r})
=
1 \oplus \overline{a_0} \oplus \dots \oplus \overline{a_r}.
\]

** Ãlgebra conmutativa de Carlos Ivorra
*** I. Funtores Derivados
**** 1.1. Haces
***** Prehaces
Un *prehaz* sobre un espacio topolÃ³gico $X$ es un par $({\cal F},\rho)$, donde cada abierto $U$
tiene un grupo asociado ${\cal F}(U)$ y cada inclusiÃ³n $U \subset V$ tiene asociado un homomorfismo
llamado *restricciÃ³n*, $\rho_U^V : {\cal F}(V) \longrightarrow {\cal F}(U)$ cumpliendo:

  - ${\cal F}(\varnothing) = 0$
  - $\rho_U^U$ es la identidad
  - Si $U\subset V\subset W$, entonces $\rho_V^W \circ \rho_U^V = \rho_U^W$

Cuando los grupos ${\cal F}(U)$ son anillos o mÃ³dulos tenemos un *prehaz de anillos* o un
*prehax de mÃ³dulos*.

# CategÃ³ricamente, un funtor contravariante desde los conjuntos del espacio
# topolÃ³gico con la inclusiÃ³n a los grupos, o mÃ³dulos, o Ã¡lgebras...

***** NotaciÃ³n de restricciÃ³n
Normalmente escribiremos $f|_{U}$ para llamar a la restricciÃ³n de $f$ a $U$, esto 
es $\rho_U^V(f)$.

***** Haces
Un *haz* es un prehaz tal que si $U = \bigcup U_i$ es el recubrimiento de un abierto:

  - Si $f|_{U_i} = 0$ para todos los $i$, entonces $f = 0$.
  - Para una familia de elementos $f_i \in {\cal F}(U_i)$ cumpliendo que 
    $f_i|_{U_i \cap U_j} = f_j|_{U_i \cap U_j}$, se tiene que hay un $f \in {\cal F}(U)$ tal que $f|_{U_i} = f_i$.

***** Grupo de gÃ©rmenes o grupo local
Dado un prehaz ${\cal F}$ sobre $X$, con $P \in X$, llamamos *grupo de gÃ©rmenes* en $P$ al grupo
${\cal F}_P$, formado por las clases de equivalencia de pares $(U,f)$ con $P\in U$, $f \in {\cal F}(U)$;
respecto de la relaciÃ³n dada por $(U,f) \sim (V,g)$ ssi hay un abierto $W \subset U \cap V$
tal que $P \in W$ y ademÃ¡s $f|_W = g|_W$. Teniendo como operaciÃ³n de grupo a:

\[ [(U,f)]+[(V,g)] = [(U\cap V, f|_{U\cap V} + g|_{U\cap V})] \]

***** Homomorfismo de prehaces
Un *homomorfismo de prehaces* $\alpha : {\cal F} \longrightarrow {\cal G}$, asigna a cada abierto $U$ un homomorfismo
de grupos $\alpha_U : {\cal F}(U) \longrightarrow {\cal G}(U)$, tal que:

\[ \begin{tikzcd}
{\cal F}(V) \rar{\alpha_V} \dar[swap]{\rho_U^V} & {\cal G}(V) \dar{\rho_U^V} \\
{\cal F}(U) \rar{\alpha_U} & {\cal G}(U)
\end{tikzcd} \]

# CategÃ³ricamente son transformaciones naturales.

** Koszul pairs
# Parte de la memoria de beca de colaboraciÃ³n con Pascual Jara.
*** Introduction
*Koszul algebras* have numerous applications in diverse fields of
Mathematics such as Algebraic Topology, Combinatorics, Representation
Theory or Algebraic Geometry, as it is showed in cite:polishchuk05,
and many of its fundamental properties still hold in *Koszul rings*,
a particular case of graded rings.

*Almost-Koszul pairs* are a tool for the study of Koszul rings; to
every strongly graded ring corresponds a canonical almost-Koszul pair.
Every almost-Koszul pair has three associate chain complexes and three
cochain complexes. If any of this six complexes is exact, all the others
are exact too; in this case, we call the pair a *Koszul pair*.

*** Area of interest
In order to define Koszul pairs, we need to introduce some homological
algebra prerequsites. We will define abelian categories in general,
even if we are going to use later only the particular case of module
categories, where we will define projective, injective and flat
modules.

In particular, we will need to define *Hoschschild comohology*.

**** Abelian categories
The theory of abelian categories was introduced by Buchsbaum and Grothendieck
in cite:grothendieck57 to unify the multiple cohomology theories at the time.

***** Additive category
The original motivation for additive categories is the category of abelian
groups, and, more generally, the category of momdules over a fixed ring $R$.
In these categories, morphisms between two objects form an abelian group;
and we can define functors preserving this group structure.

#+begin_definition
${\cal C}$ is an *additive category* if:

 - $\mathrm{Hom}(A,B)$ is an /abelian group/.
 - /Distributivity/ holds: $b \circ (f+g) = b\circ f + b \circ g$ and $(f+g)\circ a = f\circ a + g\circ a$.
 - Has a /zero object/.
 - Has finite /products/ and /coproducts/.

A functor $T$ between two additive categories is and *additive functor* 
if $T(f+g) = Tf+Tg$. cite:rotman08_setting
#+end_definition

***** Abelian category
Our interest is specifically on abelian categories. We will need to
assume that every morphism has a kernel and a cokernel in order to
prove results on homological algebra.

#+begin_definition
An *abelian category* is an /additive category/ such that

  * every morphism has a kernel and cokernel.
  * every monomorphism is a kernel.
  * every epimorphism is a cokernel.
#+end_definition

The category of $R\text{-modules}$ is an abelian category, but also
the category of chain complexes of an arbitrary abelian category,
$\mathtt{Ch}({\cal A})$, is an abelian category.

**** Chain complexes and homology
/Homology/ was originally defined in algebraic topology as a rigorous
method allowing the topological distinction of manifolds with arbitrary
dimensional holes. The same construction can be translated into multiple
different homology theories.

$\quad$

In abelian categories, the homology provides a formal description of
the failure of a functor to be exact.

***** Chain complexes
/Chain complexes/ were initially a representation the relationships
between cycles and boundaries on a topological space; we will study
chain complexes in the abstract setting of module categories, devoided
of any explicit relation to its motivating example.

#+begin_definition
A *chain complex* is a family of $R\text{-modules}$ $\left\{ C_n \right\}$ and homomorphisms
$d_n \colon C_n \to C_{n-1}$ called /differentials/, such that each composite of
consecutive differentials is zero, i.e. $d_{n-1} \circ d_n = 0$.
#+end_definition

#+begin_theorem
Given an abelian category ${\cal A}$, the category $\mathtt{Ch}({\cal A})$ is an abelian category.
cite:weibel94_introd
#+end_theorem

***** Exact sequences
/Exact sequences/ provide a convenient framework for homological questions
such as the completion of the middle term of a particular sequence in such
a way that the homology groups are exactly zero. This kind of problems, in
particular in the category of groups, have been proven to be useful to the
resolution of problems such as the classification of finite simple groups.

#+begin_definition
A pair of composable morphisms is *exact* in the object where they can be
composed when $\mathrm{img}(f) = \mathrm{ker}(g)$. Equivalently, when $\mathrm{coker}(f) = \mathrm{coimg}(g)$.
#+end_definition

#+begin_definition
A *short exact sequence* is a diagram

\[
0 \longrightarrow
a \overset{f}\longrightarrow
b \overset{g}\longrightarrow
c \longrightarrow
0
\]

exact on $a$,$b$ and $c$.
#+end_definition

#+begin_definition
A *morphism of short exact sequences* is defined by three morphisms
$f,g,h$ making the following diagram commute

\[\begin{tikzcd}
0 \rar& 
\cdot \rar{m}\dar{f}& 
\cdot \rar{e}\dar{g}& 
\cdot \rar\dar{h}& 
0 \\
0 \rar& 
\cdot \rar{m'}& 
\cdot \rar{e'}& 
\cdot \rar& 
0 & .\\
\end{tikzcd}\]

The short exact sequences of an abelian category $A$ define a category with
these morphisms called $\mathtt{Ses}(A)$, which is preadditive with the component by 
component sum.
#+end_definition

***** Snake lemma
The /snake lemma/ will provide us with a tool to construct long exact sequences,
which will be used in the definition of derived functors. This is a first result
on algebraic homology theory.

#+begin_theorem
Given a morphism of short exact sequences $f,g,h$; there exists a morphism
$\delta \colon \operatorname{ker} h \to \operatorname{coker} f$ such that the following sequence is exact

\[\begin{tikzcd}
0 \rar &
\mathrm{ker}(f) \rar{m} &
\mathrm{ker}(g) \rar{e} &
\mathrm{ker}(h) \arrow[out = 0,in =180,swap]{dll}{\delta} \\&
\mathrm{coker}(f) \rar{m'} &
\mathrm{coker}(g) \rar{e'} &
\mathrm{coker}(h) \rar &
0
\end{tikzcd}\]
#+end_theorem
#+begin_proof
In this extended diagram

 \[ \begin{tikzcd}
	& 0 \dar              & 0 \dar            & 0 \dar           &   \\
 0 \rar & ker(f) \rar \dar  & ker(g) \rar \dar    & ker(h) \dar \ar[out=355, in=175,looseness=1, overlay, swap]{dddll}{\delta}       &   \\
 0 \rar & a \rar{m} \dar{f}  & b \rar{e} \dar{g} & c \rar \dar{h}        & 0 \\
 0 \rar & a' \rar{m'} \dar & b' \rar{e'} \dar & c' \rar \dar        & 0 \\
	& coker(f) \rar \dar & coker(g) \rar \dar  & coker(h) \rar \dar & 0 \\
	& 0                   & 0                 & 0                &
 \end{tikzcd} \]

we can first define the morphism $\delta$ using the properties of abelian categories
to prove that it exists and then prove that it is exact using again the
properties of monomorphisms and epimorphisms.
#+end_proof

***** Homology
The definition of /homology/ tries to capture the failure of the complex to be
exact as the quotient of the kernel and the image of the sucessive differential
maps.

#+begin_definition
Given a chain complex $\left\{ C_n \right\}$ with differentials $d_n$, we define the nth
homology group as

\[ H_n(C) \cong \mathrm{ker}(d_n) / \mathrm{im}(d_{n+1}).
\]
#+end_definition

It is important to notice that a sequence will be exact if and only if
all their homology groups are zero.

**** Projective, injective and flat resolutions
Resolutions of injective modules are needed to define Hochschild
homology.

***** Definitions
#+begin_definition
An R-module $D$ is:

 1. *Projective* if $\mathrm{Hom}(D, -)$ is exact.
 2. *Injective* if $\mathrm{Hom}(-,D)$ is exact.
 3. *Flat* if $D \otimes -$ is exact.
#+end_definition

We know that $\mathrm{Hom}(D,-)$ and $\mathrm{Hom}(-,D)$ are left-exact and that
$D\otimes -$ is right-exact; so for them to be exact, we only need:

 - A module $D$ is *projective* when $B \longrightarrow C$ epimorphism induces
   $\mathrm{Hom}(D,B) \longrightarrow \mathrm{Hom}(D,C)$ epimorphism.

   \[ \begin{tikzcd}
               & B \dar[two heads] \\
   D \rar\urar[dashed]{\exists} & C
   \end{tikzcd} \]

 - A module $D$ is *injective* when $A \longrightarrow B$ epimorphism induces
   $\mathrm{Hom}(B,D) \longrightarrow \mathrm{Hom}(A,D)$ epimorphism.

   \[ \begin{tikzcd}
     & A \dar[two heads]\dlar \\
   D & B \lar[dashed]{\exists}
   \end{tikzcd} \]

 - A module $D$ is *flat* when $A \longrightarrow B$ monomorphism induces 
   $D\otimes A \longrightarrow D \otimes B$ monomorphism.

***** Resolutions
#+begin_definition
A *projective resolution* is a resolution

\[\dots\longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

where every $P_i$ is projective.
#+end_definition

#+begin_definition
An *injective resolution* is a resolution

\[0 \longrightarrow M \longrightarrow E^0\longrightarrow E^1
\longrightarrow E^2 \longrightarrow \dots\]

where every $E^i$ is injective.
#+end_definition

#+begin_definition
A *flat resolution* is a resolution

\[\dots\longrightarrow F_2\longrightarrow F_1\longrightarrow F_0
\longrightarrow M \longrightarrow 0\]

where $F_i$ is flat.
#+end_definition

****** Explicit construction
Notice that, given a module $M$, we can always find
a surjection from a proyective module (if we have /enough
projectives/). So we can construct a projective resolution as

\[ \begin{tikzcd}[column sep=tiny]
&\ker f_2 \drar&&&&\ker \pi\drar &&& \\
\dots&&P_2 \drar[two heads]{f_2}&&P_1 \urar[two heads]{f_1} && P_0 \ar[two heads,rr]{\pi} && M \rar & 0\\
&&&\ker f_1 \urar&&&&
\end{tikzcd} \]

We can also reverse the arrows to obtain an injective resolution.

***** Derived functors
/Derived functors/ provide a canonical way to extend the image of an
exact sequence by a non two-sided exact functor.  We need an abelian
category $A$ with enough injectives to construct left-derived
functors; and we need enough projectives to construct right-derived
functors.

****** Construction of the right derived functor
Let $F$ be additive, covariant and left-exact. Let $0 \longrightarrow M \longrightarrow E^\bullet$ be an 
injective resolution with $M$ deleted; then $F(E^\bullet)$ is a complex, and we define:

\[R^i F(M) = H^i(F(E^\bullet)) = 
\frac{\ker \{F(E^i) \longrightarrow F(E^{i+1})\}}
{\im\{ F(E^{i-1}) \longrightarrow F(E^i)\}}\]

That is, if we take the /injective resolution/

\[ 0 \longrightarrow M \longrightarrow E^0 \longrightarrow E^1 
\longrightarrow \dots\]

we can delete $M$ and apply $F$ to get a (non neccesarily exact) complex where 
we can compute the homology

\[ 0 \longrightarrow F(E^0) \longrightarrow F(E^1)
\longrightarrow F(E^2) \longrightarrow \dots.\]

****** Construction of the left derived functor
Let $F$ be additive, contravariant and left-exact. Let 
$P^\bullet \longrightarrow M \longrightarrow 0$ be a projective resolution with $M$ deleted; 
then $F(P^\bullet)$ is a complex, and we define

\[R^i F(M) = H^i(F(P^\bullet)) = 
\frac{\ker \{F(P_i) \longrightarrow F(P_{i+1})\}}
{\im\{ F(P_{i-1}) \longrightarrow F(P_i)\}}\]

That is, if we take the /projective resolution/

\[\dots \longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

Delete $M$ and apply $F$ to get a (non neccesarily exact) complex 
where we can compute the homology:

\[ 0 \longrightarrow F(P_0) \longrightarrow F(P_1)
\longrightarrow F(P_2) \longrightarrow \dots\]

**** Hochschild homology
***** Preliminaries
Our main interest will be on a particular kind of homology and comology called
/Hochschild homology/. This section defines the preliminary necessary concepts
to develop the notion of Hochschild homology.

****** Opposite algebra.
The definition of an /opposite algebra/ is a trvial notion which will be crucial
to create the definition of a standard resolution of an algebra over a field.

#+begin_definition
If $A$ is an $R\text{-algebra}$, the *opposite algebra* of $A$, with a multiplication
given by yuxtaposition is $A^\ast$; an algebra with the same set of elements and where
the multiplication $\circ$ is defined as $x \circ y = yx$.
#+end_definition

****** Enveloping algebra.
#+begin_definition
Let $R$ be a $k\text{-algebra}$, the *enveloping algebra* of $R$ is the tensor
product $R^e = R \otimes R^{op}$, where the product is defined as

\[ (r_1 \otimes s_1)(r_2 \otimes s_2) = (r_1r_2) \otimes (s_2s_1).\]
#+end_definition

#+begin_theorem
Given any $k\text{-algebra}$, $R$, the following categories are isomorphic:

  - $(R;R)\text{-Mod}$
  - $R^e\text{-Mod}$
  - $\text{Mod-}R^e$
#+end_theorem
#+begin_proof
If $M$ is an $(R;R)\text{-module}$, we can provide it with $R^e\text{-module}$ structure
by defining $(r\otimes s)m = rms$. It is trivial to check that this structure is
compatible with our previously defined product, as

\[
(a\otimes b)(c \otimes d)m = acmdb = (ac \otimes bd)m.
\]

If $M$ is an $R^e\text{-module}$, we can provide it with $R;R\text{-module}$ structure
taking $rms = (r\otimes s) m$. Compatibility relation can be checked by the same
reasoning.
#+end_proof

****** Standard resolution.
#+begin_definition
Given $R$, a $k\text{-algebra}$ we define the *standard resolution* $(P_{\bullet},d_{\bullet})$ of $R$ in
$(R;R)\mathrm{-Mod}$ as

 - $P_n = R \otimes (R^{\otimes n}) \otimes R$
 - $d_{n} = \sum_{i=0}^n (-1)^id^i_n$

where

\[
d_n^i(a_0 \otimes \dots \otimes a_{n+1}) =
a_0 \otimes \dots \otimes a_ia_{i+1}\otimes \dots \otimes a_{n+1}
\]
#+end_definition

***** Hochschild homology
#+begin_definition
Given $R$, a $K\text{-algebra}$, and $M$, an $(R;R)\text{-module}$, we define:

  - The *Hochschild cohomology* of $R$ in $M$ as $HH^{\bullet}(R,M) = \operatorname{Ext}^\bullet_{R^e}(R,M)$.
  - The *Hochschild homology* of $R$ in $M$ as $HH_{\bullet}(R,M) = \operatorname{Tor}_\bullet^{R^e}(R,M)$.
#+end_definition

In order to compute the cohomology, we can take the following cochain
complex

\[
\mathrm{Hom}_K(K,M) \overset{b^0} \longrightarrow
\mathrm{Hom}_K(R,M) \overset{b^1} \longrightarrow
\mathrm{Hom}_K(R^{\otimes 2},M) \overset{b^2} \longrightarrow
\dots.
\]

where the $b^n$ are defined as

 - $b^0(m)(a) = am-ma$
 - $b^n = \sum^{n+1}_{i=0}(-1)^ib_i^n$

and the auxiliary morphisms $b_i^n$ are defined as

\[
b^n_i(f)(a_1\otimes \dots \otimes a_{n+1}) =
\left\{\begin{array}{ll} 
a_1f(a_2\otimes\dots\otimes a_{n+1})& \mbox{if } i=0  \\
f(a_1\otimes\dots\otimes a_ia_{i+1}\otimes\dots\otimes a_{n+1}}& \mbox{if } i=1,\dots,n \\
f(a_1\otimes\dots\otimes a_n)a_{n+1}& \mbox{if } i=n+1
\end{array}.
\right.
\]

*** Methods
Our methodology is based on the review of the basic bibliography of the
subject. This article is an attempt to collect all the needed
prerequisites to work with homology and cohomology theory and
ultimately to work with Hochschild homology and Koszul pairs.

Apart from the bibliographic review, we have developed materials for
future students interested in the subject.

***** Wikipedia articles
In order to achieve a higher level of understanding of the topic and
to provide future students with accesible resources, we have written
the following articles for the Spanish wikipedia; they are published
using a Creative Commons license.

 * [[https://es.wikipedia.org/wiki/Compleci%C3%B3n_(%C3%A1lgebra)][CompleciÃ³n (Ã¡lgebra)]]
 * [[https://es.wikipedia.org/wiki/Lema_de_escisi%C3%B3n][Lema de escisiÃ³n]]
 * [[https://es.wikipedia.org/wiki/Lema_de_la_serpiente][Lema de la serpiente]]
 * [[https://es.wikipedia.org/wiki/Funtor_Tor][Funtor Tor]]
 * [[https://es.wikipedia.org/wiki/Homolog%25C3%25ADa_de_Hochschild][HomologÃ­a de Hochschild]]
 * [[https://es.wikipedia.org/wiki/Categor%25C3%25ADa_coma][CategorÃ­a coma]]
 
***** Student-organized seminars
Two student-organized seminars have been held in the university. In
those seminars, the author has lectured about category theory; giving
a basic introduction to mathematics and computer science students.

*** Discussion
**** Koszul algebra
/Koszul rings/ will be a generalization of /Koszul algebras/. A pair of a
ring and its categorical dual, a coring; toghether with certain coherence
relations, will constitute our definition of a Koszul pair.

#+begin_definition
A graded algebra $A$ is a *Koszul algebra* over a field $k$ if
every graded module has a graded projective resolution
$P_{\bullet}$ where the projective module $P_j$ is generated by homogeneous
elements of degree $j$.
#+end_definition

***** Quadratic algebras
Koszul algebras are a particular case of quadratic algebras. We can describe
them in full generality with the following definition.

#+begin_definition
A graded algebra $A$ is a *quadratic algebra* if the natural
application from its tensor algebra $T(A) \to A$ is surjective
and its kernel $J_A$ is generated from $J_{A} \cap (A^{1} \otimes A^1)$. cite:polishchuk05
#+end_definition

In other words, a graded quadratic algebra is determined as the
quotient of a vector space $A_{1}$ by a subspace of homogeneous
quadratic relations $S \subset V \otimes V$ as

\[
A = T(V) / \left\langle S \right\rangle.
\]

#+begin_theorem
Every Koszul $R\text{-ring}$ is a quadratic algebra.
#+end_theorem

**** Almost-koszul pairs
Previous to the definition of Koszul pairs, we are going to define
/almost-koszul pairs/. Those will provide us with a weaker set of
requirements and we will be able to obtain a definition of Koszul
pairs suitable to every almost-koszul pair.

***** Graded rings
#+begin_definition 
A *graded ring* is a ring that can be written as a direct sum of
abelian groups

\[ A = \bigoplus_{n \in \mathbb{N}} A_n\]

such that $A_iA_j \subset A_{i+j}$.
#+end_definition

A *homogeneous element* is an element of any submodule $A_i$ of the
decomposition.

***** Koszul rings
#+begin_definition
A graded ring $A$ is a *Koszul ring* if $A^0$ is a semisimple ring 
and it has a resolution $P_\ast$ by projective graded left A-modules such 
that each $P_n$ is generated by homogeneous elements of degree $n$.
cite:jarastefan10
#+end_definition

***** R-rings
#+begin_definition
An $R\text{-ring}$ is an associative and unital algebra. It is an associative and
unital ring $A$ together with a morphism $u : R \longrightarrow A$.
#+end_definition

A R-ring is *graded* if it is equipped with a decomposition

\[A = \bigoplus_{n \in \mathbb{N}} A^n \]

such that multiplicaton $m^{p,q}$ maps $A^p \otimes A^q$ into $A^{p+q}$. It is *connected* 
when $A_0 = R$. It is *strongly graded* when $m^{1,p}$ is surjective. We 
call $\pi^n_A$ to the projection of $A$ onto $A^n$.

***** R-coring
Corings will be the categorical dual of rings. In order to define them, we
proceed by giving defitions of coalgebra and certain properties of this kind
of algebras. Those are also the dual notions to the preliminary definitions
we described at the start of this chapter.

#+begin_definition
A *coalgebra* over a field $K$ is a *vector space* $V$ together with linear
maps $\Delta : V \longrightarrow V \otimes V$ and $\varepsilon : V \longrightarrow K$ such that:

 1. $(id \otimes \Delta) \circ \Delta = (\Delta \otimes id) \circ \Delta$
 2. $(id \otimes \varepsilon) \circ \Delta = id 
    = (\varepsilon \otimes id) \circ \Delta$
#+end_definition

When writting in coalgebras, we will follow the *Sweedler
notation*. cite:underwood15_hopf

#+begin_definition
An $\mathbf{R\text{-coring}}$ is a /coassociative/ and /counital/ /coalgebra/. It is an 
$R\text{-bimodule}$ with a /comultiplication/ $\Delta : C \longrightarrow C \otimes C$ and  a /counit/
$\epsilon : C \longrightarrow R$.
#+end_definition

A $R\text{-coring}$ is *graded* if it is equipped with a decomposition 
$C = \bigoplus_{n \in \mathbb{N}} C_n$, such that

\[\Delta(C_n) \subset \bigoplus_{p=0}^n C_p \otimes C_{n-p}.\]

***** Almost-koszul pair
#+begin_definition
An *almost-Koszul pair* is a connected $R\text{-ring}$ and $R\text{-coring}$ $(A,C)$ 
with an isomorphism $\theta_{C,A} : C_1 \longrightarrow A^1$ that satisfies the relation

\[ m^{1,1} \circ (\theta_{C,A} \otimes \theta_{C,A}) \circ \Delta_{1,1}
= 0.\]
#+end_definition

Using Sweedler notation we can rewrite the condition as follows 


\[ \sum \theta_{C,A}(c_{(1,1)}) \theta_{C,A}(c_{(2,1)}) = 0,\]

for any $c \in C_2$.

**** Almost-koszul pair complexes
Six complexes will be associated to any given Koszul pair. Its
exactness will be related, i.e., if any one of them is exact, the six
complexes will be exact. This provides a definition of Koszul
pairs relying only on this kind of complexes.

#+begin_definition
Let $(A,C)$ be an almost-Koszul pair, if we define

\[
K^{-1}_l(A,C) = R
\quad\text{ and }\quad
K^n_l(A,C) = C \otimes A^n,
\]

and the differential maps

\[
d^n_l(c \otimes a) = \sum c_{(1,p-1)} \otimes \theta_{C,A}(c_{(2,1)})a,
\]

with the exceptional case $n = -1$, where we take $d^n_l$ to be the canonical
bimodule morphisms $R \to C \otimes A^0$, mapping $1 \mapsto 1 \otimes 1 \in C_0 \otimes A^0$. We will
also define the same notion on the opposite pair as

\[
K_r^{\ast}(A,C) = K^{\ast}_l(A^{op},C^{op}).
\]
#+end_definition

#+begin_definition
Let $(A,C)$ be an almost-Koszul pair, we define

\[
K^{-1}(A,C) = C
\quad\text{ and }\quad
K^n(A,C) = C \otimes A^n\otimes C,
\]

and the differential relations given by $d^{-1} = \Delta$ and

\[
d^n = d^n_l \otimes I_C + (-1)^{n+1}I_C \otimes d^n_r.
\]
#+end_definition

#+begin_definition
Let $(A,C)$ be an almost-Koszul pair, if we define

\[
K^r_{-1}(A,C) = R
\quad\text{ and }\quad
K^r_n(A,C) = C_n \otimes A,
\]

and the differential maps

\[
d_n^r(c \otimes a) = \sum c_{(1,n-1)} \otimes \theta_{C,A}(c_{(2,1)})a.
\]

Appliying the same construction to the opposite almost-Koszul pair
gives us the complex $K^l_{\ast}$.
#+end_definition

#+begin_definition
Let $(A,C)$ be an almost-Koszul pair, we define

\[
K_{-1}(A,C) = A
\quad\text{ and }\quad
K_n(A,C) = A \otimes C_n \otimes A,
\]

and the differential relations given by $d_0$, induced by multiplication,
and

\[
d_n^{l}(a \otimes c) = \sum a\theta_{C,A}(c_{(1,1)}) \otimes c_{(2,n-1)};
\]

defining $d_n = d_n^l \otimes I_A + (-1)^nI_A\otimes d_n^r$.
#+end_definition

**** Koszul pairs
#+begin_theorem
Given an almost-Koszul pair $(A,C)$, if one of these six complexes is
exact, all of them are exact, as it is showed in cite:jarastefan10.

 * $K^{l}_{\ast}(A,C)$.
 * $K^r_{\ast}(A,C)$.
 * $K_{\ast}(A,C)$.
 * $K^{\ast}_l(A,C)$.
 * $K_r^{\ast}(A,C)$.
 * $K^{\ast}(A,C)$.
#+end_theorem

#+begin_definition
An almost-Koszul pair $(A,C)$ is said to be *Koszul* if and only if the
previously discussed complexes are exact.
#+end_definition

*** Conclusions
Starting from a very basic undergraduate mathematical level, all the
necessary definitions of categories, chain complexes and homology have
been developed in this work. This constitutes a reference for students
interested on the specific field of Koszul pairs and sets the ground
for future developments and undergraduate and graduate-level research.

$\quad$

In particular, future work should be able to find new
characterizations of Koszul pairs in terms of homology and cohomology
apart from the known characterizations found on cite:jarastefan10.

*** References
bibliographystyle:unsrt
bibliography:math.bib
** TFG I: links and resources
*** Journal
**** June 2017
***** <2017-05-31 Wed>
****** created this file.
****** read MacLane chapter III.2.
***** <2017-06-01 Thu>
****** read https://golem.ph.utexas.edu/category/2006/08/cartesian_closed_categories_an_1.html
****** started reading chapter 4 on Lecture Notes on the lambda calculus by Selinger
****** superficial complete lecture of Selinger
***** <2017-06-02 Fri>
****** read https://bartoszmilewski.com/2016/11/21/monads-programmers-definition/
****** read multiple sections of MacLane
****** wrote a mikrokosmos section
***** <2017-06-03 Sat>
****** read MacLane. Yoneda Lemma
****** exercises from MacLane
***** <2017-06-04 Sun>
****** read I.1.1 to I.1.6 of Hott book
****** installed agda on emacs
***** <2017-06-05 Mon>
****** installed hott-library on agda
****** wrote simple proofs in agda
****** read I.1.10 and I.1.11 of Hott book
***** <2017-06-06 Tue>
****** installed [[https://proofgeneral.github.io/download/][Proofgeneral]] and [[https://coq.inria.fr/][Coq]]
****** completed first chapter exercises of Software Foundations
****** added 'tasty-hunit' tests to mikrokosmos
****** added travis-ci to mikrokosmos
***** <2017-06-07 Wed>
****** read MacLane: monads and algebras
***** <2017-06-08 Thu>
****** exercises from MacLane
***** <2017-06-09 Fri>
****** read Sheaves in geometry and logic
***** <2017-06-10 Sat>
****** applied to EUTypes 2017
***** <2017-06-11 Sun>
****** exercises from chapters 1 and 2 of software foundations
***** <2017-06-12 Mon>
****** created a specific ctlc.org file
****** rewrote tfg.org
Mikrokosmos won't solve the IO problem. If IO is needed,
it can be achieved via scripts.
***** <2017-06-13 Tue>
****** installed Why3
***** <2017-06-14 Wed>
****** installed docker container for Why3 and solvers
****** installed Coq-hott library
***** <2017-06-15 Thu>
****** wrote a jupyter kernel for mikrokosmos
***** <2017-06-16 Fri>
****** minor changes on mikrokosmos
****** wrote mikrokosmos user's guide
***** <2017-06-17 Sat>
****** wrote highlighter for Jupyter Notebook
****** solved minor problems on Jupyter output formatting
****** saw [[https://www.youtube.com/watch?v=21qPOReu4FI][Five Stages of Accepting Constructive Mathematics, by Andrej Bauer]]
****** wrote a section on simply typed lambda calculus
***** <2017-06-18 Sun>
****** started reading Types and programming languages
****** started following the agda tutorial
***** <2017-06-19 Mon>
****** wrote notes on the first lecture on HoTT.
****** wrote notes on the second lecture on HoTT.
***** <2017-06-20 Tue>
****** done homework 1 of the course on HoTT.
****** wrote notes on the third lecture on HoTT.
****** wrote notes on the fourth lecture on HoTT.
***** <2017-06-21 Wed>
****** wrote notes on the fifth lecture on HoTT.
***** <2017-06-22 Thu>
****** wrote notes on the sixth lecture on HoTT.
****** wrote notes on the seventh lecture on HoTT.
****** wrote notes on the eighth lecture on HoTT.
***** <2017-06-23 Fri>
****** wrote notes on the ninth lecture on HoTT.
****** wrote notes on the tenth lecture on HoTT.
****** wrote an org-mode template for the thesis.
***** <2017-06-24 Sat>
****** [[http://orgmode.org/cgit.cgi/org-mode.git/commit/?id=e903288e5080775cbd4d87c69deeba3268cda5c1][fixed bug]] in org-mode
***** <2017-06-25 Sun>
****** wrote notes on the eleventh lecture on HoTT.
****** wrote notes on the twelfth lecture on HoTT.
****** wrote notes on the thirteenth lecture on HoTT.
****** wrote notes on the fourteenth lecture on HoTT.
***** <2017-06-26 Mon>
****** wrote notes on the fiveteenth lecture on HoTT.
***** <2017-06-27 Tue>
****** [[https://math.stackexchange.com/questions/2337093/does-homotopy-type-theory-have-a-computational-interpretation][has HoTT a computational interpretation?]]
***** <2017-06-28 Wed>
***** <2017-06-29 Thu>
****** wrote hott code in agda
***** <2017-06-30 Fri>
****** wrote hott code in agda
**** July 2017
***** <2017-07-01 Sat>
***** <2017-07-02 Sun>
****** wrote hott code in agda
***** <2017-07-04 Tue>
****** wrote notes on the 16th lecture on HoTT.
***** <2017-07-05 Wed>
****** wrote notes on the 17th lecture on HoTT.
****** wrote notes on the 18th lecture on HoTT.
***** <2017-07-06 Thu>
****** wrote notes on the 19th lecture on HoTT.
****** wrote notes on the 20th lecture on HoTT.
***** <2017-07-07 Fri>
****** wrote notes on the 21th lecture on HoTT.
****** wrote notes on the 22th lecture on HoTT.
****** wrote notes on the 23th lecture on HoTT.
****** finished course on HoTT.
***** <2017-07-08 Sat>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-09 Sun>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-10 Mon>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-11 Tue>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-12 Wed>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-13 Thu>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-14 Fri>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-15 Sat>
****** EUTypes 2017. Ohrid, Macedonia.
***** <2017-07-16 Sun>
****** read about the continuum hypothesis
***** <2017-07-17 Mon>
****** solved a bug in the stack tool by updating.
****** mikrokosmos starts using GHC 8.0.2.
****** color and verbose options for mikrokosmos.
****** multiple line format on mikrokosmos.
***** <2017-07-18 Tue>
****** Ski abstraction on mikrokosmos.
***** <2017-07-19 Wed>
****** Ski option for the user.
****** pip-installable jupyter kernel.
***** <2017-07-20 Thu>
****** Closed multiple minor issues on mikrokosmos
***** <2017-07-21 Fri>
****** Read about algebraic Lawvere theories
****** Rewrote installation
****** Rewrote user's guide
****** Mikrokosmos tutorial: part 1
***** <2017-07-22 Sat>
****** Published mikrokosmos v0.3.0
****** Mikrokosmos tutorial: part 2
***** <2017-07-23 Sun>
****** Untyped lambda calculus
***** <2017-07-24 Mon>
****** Wrote: Church-Rosser theorem
****** Wrote: Programming in the untyped \lambda-calculus
***** <2017-07-25 Tue>
****** Types on mikrokosmos
****** Type unification algorithm
****** Type inference algorithm
***** <2017-07-26 Wed>
****** Type normalization algorithm
****** Update tutorial and user's guide
***** <2017-07-29 Sat>
****** Read about Topoi
***** <2017-07-30 Sun>
****** Rewrote STCL using only the implication
****** Wrote a section on Curry typing
***** <2017-07-31 Mon>
****** Wrote about the curry-howard correspondence
****** Implementing generalized STLC types on mikrokosmos
**** August 2017
***** <2017-08-01 Tue>
****** Wrote about the curry-howard correspondence
****** Wrote about natural deduction
***** <2017-08-02 Wed>
****** Refactored mikrokosmos code
****** Updated jupyter-kernel
****** Added unicode and agda output to the latex template
***** <2017-08-03 Thu>
****** updated user's guide
****** updated tutorial
****** wrote programming on STLC
***** <2017-08-04 Fri>
****** wrote about normalization and evaluation strategies
***** <2017-08-05 Sat>
****** wrote about categories
***** <2017-08-06 Sun>
****** tried (and failed) to create a debian package for mikrokosmos
***** <2017-08-07 Mon>
****** Wrote about basic category theory
***** <2017-08-08 Tue>
****** Heyting algebras
****** Minor changes to Mikrokosmos
***** <2017-08-09 Wed>
****** Russell's paradox in agda
***** <2017-08-12 Sat>
****** Wrote about Heyting algebras
***** <2017-08-13 Sun>
****** Talk on Categorical foundations by Awodey
***** <2017-08-14 Mon>
****** Notes on categorical foundations
****** Fix minor bugs on mikrokosmos
****** Use macros on latex
***** <2017-08-15 Tue>
****** Created server on Digital ocean
****** Created domain name on Namecheap
****** Created SSL certificate
****** Installed mikrokosmos in the server
****** Installed JupyterHub
****** GitHub OAuth
****** trymikrokosmos.me
***** <2017-08-16 Wed>
****** Wrote 7 tutorials on mikrokosmos
****** Updated libraries on trymikrokosmos.me
****** Minor additions to the category theory chapter
****** Wrote about SKI combinators
****** Wrote about evaluation and output in mikrokosmos
***** <2017-08-17 Thu>
****** Tested mikrokosmos under NixOS
***** <2017-08-18 Fri>
****** Wrote about pure type systems and the lambda cube
***** <2017-08-19 Sat>
****** Wrote about Haskell
****** Wrote about the lambda cube
***** <2017-08-20 Sun>
****** Algebraic theories
****** Models as functors
****** Closed multiple issues on Mikrokosmos
***** <2017-08-21 Mon>
****** Introduction to topos theory
****** Derivatives of regular types and zippers
***** <2017-08-23 Wed>
****** Read An introduction to topos theory.
***** <2017-08-24 Thu>
****** Wrote about dinatural transformations.
***** <2017-09-25 Mon>
***** <2017-09-26 Tue>
****** Experimenting with GHCJS
***** <2017-09-27 Wed>
****** MikrokosmosJS
***** <2017-09-28 Thu>
****** Codemirror pads for MikrokosmosJS
***** <2017-09-29 Fri>
****** Tutorials for mikrokosmos
****** Gentzen deduction trees
***** <2017-09-30 Sat>
****** Mikrokosmos 0.7.0
****** Update libraries
***** <2017-08-31 Thu>
****** Wrote tutorials for Mikrokosmos
**** September 2017
***** <2017-09-01 Fri>
****** JupyterHub server on iemath
****** Testing Mikrokosmos on NixOS
***** <2017-09-02 Sat>
****** Lawvere algebraic theories
***** <2017-09-03 Sun>
****** From sets to types to categories to sets
***** <2017-09-04 Mon>
****** Cartesian closed categories and lambda theories
***** <2017-09-05 Tue>
****** Meeting
****** Rewrote first sections on untyped lambda calculus
***** <2017-09-06 Wed>
****** Rewrote untyped lambda calculus
****** Added a section on SKI combinators
***** <2017-09-07 Thu>
***** <2017-09-08 Fri>
***** <2017-09-09 Sat>
***** <2017-09-10 Sun>
***** <2017-09-11 Mon>
****** Ends
****** 2-categories
***** <2017-09-16 Sat>

*** Notes
**** [[file:ctlc.org][Category theory and lambda-calculus]]
**** [[file:categoriesfortheworking.org][Categories for the working mathematician - MacLane]]
**** [[file:lecturesonthelambdacalculus.org][Lecture notes on the lambda calculus - Selinger]]
**** [[file:homotopytypetheory.org][Homotopy type theory - Univalent foundations]]
**** [[file:sheavesgeometrylogic.org][Sheaves in geometry and logic - MacLane]]
**** [[file:typesandprogramminglanguages.org][Types and programming languages - Pierce]]
**** DONE [[file:courseonhott.org][Course on Homotopy Type Theory - Robert Harper]]
**** [[file:introcategoricallogic.org][Introduction to categorical logic - Bauer, Awodey]]
*** Tasks
**** Write
***** Ideas
****** Syntax and semantics
****** Variable capture and deBruijn indices.
***** Mikrokosmos section
***** Untyped lambda calculus section
***** Type theory introduction
***** Category theory introduction
***** Wikipedia
****** Agda
**** Read
***** From sets to categories to types to sets
***** Elienberg categorical foundations
***** Extensionality on categories
Commutativity and efficiency

**** [#A] Mikrokosmos
Mikrokosmos is a lambda calculus interpreter.

***** TODO Implement type theory http://www.cse.chalmers.se/%7Ebengt/papers/GKminiTT.pdf
***** TODO Implement simply-typed lambda calculus with products and a terminal type
It is the language of CCCs.
It can be showed that it is strongly normalizable.

***** TODO Implement Hindley-Milner or System F
System F has no decidable inference, you have to write more.
Hindley-Milner does not allow for types on quantifiers.

System F and the Girard-Reynolds isomorphism seem like great ideas.

***** DONE [#C] Multiple lines on modules
Multiple line notation should be allowed by joining multiple lines whenever
they start with a <TAB> or a space.
***** DONE Write tests
***** DONE Should Multibimap be a package on its own?
***** Compile to categories
***** CHECK Seminars about mikrokosmos
***** Dependent types
http://math.andrej.com/2012/11/08/how-to-implement-dependent-type-theory-i/
***** Write a pygments lexer
http://pygments.org/docs/lexerdevelopment/

***** Emacs theme
***** DONE Logo background should be a "cosmos" background
***** DONE [#A] Module system
The module system is being written.

I should first find every module I have to load and, only then,
load all of them in a single command.

Every module should have a list of DEPENDENCY directives.
***** DONE [#B] Jupyter kernel
[[https://ipython.org/ipython-doc/3/development/kernels.html][Making kernels for IPython]]
[[http://andrew.gibiansky.com/blog/ipython/ipython-kernels/]]
***** DONE Change notation
In order to avoid confusions with types

#+BEGIN_SRC haskell
:load -- !load
qwer != asdf -- qwer ?= asdf
#+END_SRC

***** DONE Use a .mikrokosmos config file
Is it really necessary?

***** DONE Config files
A simply typed mikroskosmos would be great for writing configuration files.

***** TODO compiling mikrokosmos
It could be compiled to C.

***** TODO Write a compiler
Maybe a compiler would work better than an interpreter.

***** TODO Optimizing mikrokosmos in Agda
It seems very difficult to write a mikrokosmos interpreter on Agda.

***** TODO [#C] The IO problem
How to declare an imperative - Wadler

****** syscall proposal
******* Output
#+BEGIN_SRC 
#printNum (\s.\z.(s (s z)))
#+END_SRC

printNum.hs
#+BEGIN_SRC haskell
num :: Lambda -> Int
num Nil = 0
num (Lambda x s) = 1 + printnum s

main :: IO ()
main = do
  l <- read :: Lambda
  print $ printnum l
  return ()
#+END_SRC

******* Input
#+BEGIN_SRC
#readNum ()
#+END_SRC

****** syscall with literals
Add a "literal" type.

******* Output
#+BEGIN_SRC mikrokosmos
churchLiteral n = n (\k . syscall ("increment.sh" k)) "0"
printLiteral l = syscall2 ""
#+END_SRC

******* Input
It would use =$= as an antiliteral. Any string started on =$= would be interpreted as
a lambda term.

#+BEGIN_SRC mikrokosmos
readChurch = syscall "readChurch"
#+END_SRC

=readChurch= would return something as ="@(\s.\z.s (s (s z)))"=. Two literals applied one
over thw other would be reinterpreted as their concatenation.
**** [#C] Write articles
***** TODO Adjoint functors post
***** TODO Church-Rosser post
***** TODO Wikipedia article Church-Rosser
**** Ask for a type theory book on SO
**** Ask for a category theory + lambda calculus book on SO
*** Resources
**** Books on categories and types for the TFG
***** Books on category theory
****** [[http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf][Categories for the working mathematician - Saunders Mac Lane]]
A complete course on category theory.

  * Category theory.
  * Monoidal categories.
  * 2-categories.

****** [[https://github.com/Mzk-Levi/texts/blob/master/Lambek%2520J.,%2520Scott%2520P.J.%2520Introduction%2520to%2520Higher%2520Order%2520Categorical%2520Logic.pdf][Introduction to Higher order categorical logic - Lambek]]
A course in categorical logic.

  * Cartesian closed categories.
  * Type theory and toposes.

****** [[https://s3.amazonaws.com/arena-attachments/325201/2ff932bf546d8985eb613fccf02b69c7.pdf][Conceptual Mathematics: a first introduction to categories - Lawvere]]
A course on category theory, oriented towards categorical logic.

  * Basic cateogory theory
  * Categorical logic, toposes

****** [[http://paultaylor.eu/prafm/][Practical foundations of Mathematics - Paul Taylor]]
A type-oriented foundation of mathematics.

  * Type theory.
  * Cartesian closed categories.
  * Algebra of dependent types.

****** [[http://tocs.ulb.tu-darmstadt.de/35821485.pdf][Sheaves in Geometry and Logic - MacLane, Moerdijk]]
A course on sheaves, topoi and logic. It assumes a categorical background.

  * Grothendieck topologies and sheaves.
  * Topoi and logic.
  * Classifying topoi.

****** [[http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf][Introduction to category theory and categorical logic - Thomas Streicher]]
Basic notions of category theory and a bit on \lambda-calculus,
cartesian-closed categories and toposes.

  * Basic category theory
  * Cartesian closed categories and \lambda-calculus
  * Logic of toposes
  
****** An introduction to topos theory - Kostecki
****** [[http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf][Toposes, Triples and Theories - Barr & Wells]]
***** Books on \lambda-calculus
****** [[http://pages.di.unipi.it/ferrari/CORSI/PR2/HarperBook.pdf][Practical foundations for Programming Languages - Robert Harper]]
A complete course on types, syntax and programming languages.

  * Judgements and rules.
  * Levels of syntax.
  * Data types.
  * System F.
  * Types and propositions.
  * Laziness, paralellism and concurrency.

****** Types and programming languages - Benjamin C. Pierce
****** [[http://www.mscs.dal.ca/~selinger/papers/lambdanotes.pdf][Lecture notes on the lambda calculus - Peter Selinger]]
****** [[https://homotopytypetheory.org/book/][Homotopy Type Theory Book - The univalent foundations program]]
The foundational book on homotopy type theory.
****** [[http://pds14.egloos.com/pds/200901/16/93/Lambda-Calculus_and_Combinators.pdf][Lambda Calculus and Combinators. An Introduction - Hindley, Seldin]]
Basic introduction to the \lambda-calculus.

  * Combinatory logic.
  * Formal theories of \lambda-calculus.
  * Typing.
  * Models of \lambda-calculus.

****** [[http://www.cse.chalmers.se/research/group/logic/book/book.pdf][Programming in Martin-LÃ¶f's Type Theory - NordstrÃ¶m, Petersson]]
Martin-LÃ¶f type theory and how to write languages based on it.
Based on sets.

****** [[https://github.com/pigworker/CS410-14][CS410 Advanced Functional Programming - Connor McBride]]
A course on Agda.

  * Propositions as types.
  * Dependent types.
****** Software foundations - Benjamin C. 
**** Blog articles and web pages
***** [[https://hottheory.files.wordpress.com/2012/08/hott2.pdf][hott2.pdf]] - Master thesis on HoTT
***** [[https://en.wikipedia.org/wiki/Typed_lambda_calculus][Typed lambda calculus - Wikipedia]]
The kinds of typed lambda calculi.
http://homepages.inf.ed.ac.uk/wadler/papers/esslli/esslli-1.pdf

A typed lambda calculus in which the user can set the base types
and its constants. If we add products and a terminal type, this is
the language of cartesian closed categories; and a fragment of
intuitionistic logic called minimal logic.

More precisely, there exist functors between the category of Cartesian
closed categories, and the category of simply-typed lambda theories.

Not making assumptions about the type gives us something that behaves
like type variables.

***** https://github.com/Zepheus/SystemF
A SystemF implementation on Haskell following the B. Pierce's book.
***** [[https://ncatlab.org/homotopytypetheory/files/Joyal.pdf][Categorical Homotopy Type Theory - Joyal.pdf]]
***** [[https://homotopytypetheory.org/2011/04/23/running-circles-around-in-your-proof-assistant/][Running Circles Around (In) Your Proof Assistant; or, Quotients that Compute | Homotopy Type Theory]]
How to implement HIT on Agda using Licata's trick.
***** [[http://sweet.ua.pt/dirk/ct2015/slides/Guallart.pdf][Guallart.pdf]] A comparison between ITT and COC
It uses a categorical formulation of STLC similar to H-M.
***** [[http://www.cse.chalmers.se/research/group/logic/Types/tutorials.html][The Types Project]]
***** [[https://en.wikipedia.org/wiki/Intuitionistic_type_theory#Categorical_models_of_type_theory][Intuitionistic type theory - Wikipedia]] - Categorical models
***** [[https://www.microsoft.com/en-us/research/wp-content/uploads/1997/01/henk.pdf][The Henk intermediate language. core.dvi - henk.pdf]]
Based on the lambda cube with a single syntax for terms, types and kinds.
***** [[https://stackoverflow.com/questions/23995736/example-of-type-in-system-f-that-is-not-available-in-hindley-milner-type-inferen][Example of type in System F that is not available in Hindley Milner type inference - Stack Overflow]]
***** [[http://typessummerschool07.cs.unibo.it/courses/coquand-1.pdf][Models of type theory - Coquand]]
***** https://en.wikipedia.org/wiki/Pure_type_system
***** [[http://strictlypositive.org/calculus/][Differential Calculus with Datatypes]]
***** TODO [[http://people.inf.elte.hu/divip/AgdaTutorial/Index.html][Agda Tutorial]]
***** [[https://cs.stackexchange.com/questions/14674/intro-to-martin-l%C3%B6f-type-theory][logic - Intro to Martin-LÃ¶f type theory - Computer Science Stack Exchange]]

***** [[https://plato.stanford.edu/entries/type-theory-intuitionistic/][Intuitionistic Type Theory (Stanford Encyclopedia of Philosophy)]]
Intuitionistic type theory is thus a typed functional programming
language with the unusual property that all programs terminate.
***** DONE [[https://www.youtube.com/watch?v=21qPOReu4FI][Five Stages of Accepting Constructive Mathematics - Andrej Bauer - YouTube]]
"Taking the Principle of the Excluded Middle from the mathematician... 
is the same as ... prohibiting the boxer the use of his fists."
- /David Hilbert/

 * [[https://en.wikipedia.org/wiki/Brouwer%E2%80%93Hilbert_controversy][BrouwerâHilbert controversy - Wikipedia]]

***** [[https://www.quora.com/What-is-the-best-textbook-for-Category-theory][What is the best textbook for category theory - Edward Kmett]]
***** DONE [[https://golem.ph.utexas.edu/category/2006/08/cartesian_closed_categories_an_1.html][CCCs and the Î»-calculus | The n-Category CafÃ©]] :math:
***** [[https://github.com/mattearnshaw/lawvere][mattearnshaw/lawvere: The collected works of F. W. Lawvere]] :logic:math:
***** [[https://www.cs.cmu.edu/~rwh/][Robert Harper's Home Page]]                                      :math:logic:
***** [[http://www.jonmsterling.com/index.html][Notes from Jon Sterling Thought]]           :math:logic:philosophy:
***** [[http://www2.tcs.ifi.lmu.de/~abel/MscThesisJoakimOhman.pdf][MscThesisJoakimOhman.pdf]] A logical relation for dependent type theory :mikrokosmos:types:logic:
Talks about identity types.
***** [[https://mathoverflow.net/questions/152497/formalizations-of-category-theory-in-proof-assistants][Formalizations of category theory in proof assistants - MathOverflow]] 
***** [[http://strictlypositive.org/Easy.pdf][Easy.pdf]] Simply Easy! - Conor McBride            :mikrokosmos:math:
***** [[https://github.com/lambda-pi-plus/lambda-pi-plus][lambda-pi-plus: A simple Depdently-Typed Language for Research&Learning]] :mikrokosmos:math:
***** [[http://strictlypositive.org/Easy.pdf][Easy.pdf]] Implementation of dependent lambda calculus :math:logic:mikrokosmos:
***** [[https://www2.eecs.berkeley.edu/Pubs/TechRpts/2007/EECS-2007-113.pdf][EECS-2007-113.pdf]] Adam Chipala - Implementing dependent types :types:math:logic:
***** [[http://math.andrej.com/2012/11/08/how-to-implement-dependent-type-theory-i/][How to implement dependent type theory I | Mathematics and Computation]] :math:types:
***** [[http://www.hedonisticlearning.com/posts/category-theory-syntactically.html][Category Theory, Syntactically ]] :math:logic:
***** [[https://github.com/Mzk-Levi/texts][Mzk-Levi/texts]] - Type theory and categorical logic texts :math:types:categories:
***** [[http://www.cs.ru.nl/B.Jacobs/CLT/bookinfo.html][Categorical Logic and Type Theory]] :math:categories:logic:
***** [[https://www.newton.ac.uk/event/bprw01][Computer-aided mathematical proofs]]              :types:logic:math:
***** [[http://math.ucr.edu/home/baez/topos.html][Topos theory in a nutshell]]
***** [[http://www.logicmatters.net/tyl/][Logic matters]]
***** [[https://mathoverflow.net/questions/69251/is-mac-lane-still-the-best-place-to-learn-category-theory/70891#70891][Is MacLane still the best place to learn category theory?]]
***** Homotopy type theory
****** [[http://dlicata.web.wesleyan.edu/pubs/lb14cubical/lb14cubes-oxford.pdf][A cubical type theory]]
****** [[https://ncatlab.org/homotopytypetheory/files/AwodeyDMVrev.pdf][AwodeyDMVrev.pdf]] Cubical type theory
****** [[http://www.helsinki.fi/lc2015/materials/slides_awodey.pdf][slides_awodey.pdf]] Cubical Type theory
****** [[http://www.cse.chalmers.se/%7Ecoquand/cubicaltt.pdf][Cubical Type Theory - Coquand, Cohen]]
****** [[http://neil-strickland.staff.shef.ac.uk/formal/][Formalised mathematics]] in Agda
****** https://mathoverflow.net/questions/156238/function-extensionality-does-it-make-a-difference-why-would-one-keep-it-out-of/156295#156295
The conversation here between Sterling and Bauer is very interesting.
****** [[http://math.andrej.com/2013/08/28/the-elements-of-an-inductive-type/][The elements of an inductive type | Mathematics and Computation]]
****** [[https://github.com/HoTT/book/issues/460][Path induction again (sorry) Â· Issue #460 Â· HoTT/book]]
****** [[https://www.youtube.com/watch?v=fJJ7NhkySXM][Homotopy Group - (1)Dan Licata, (2)Guillaume Brunerie, (3)Peter Lumsdaine - YouTube]]
****** [[https://homotopytypetheory.org/links/][Links | Homotopy Type Theory]]
****** [[https://agda.readthedocs.io/en/latest/language/cubical.html][Cubical Type Theory in Agda â Agda 2.6.0 documentation]]
****** [[http://www-sop.inria.fr/members/Anders.Mortberg/slides/TTT-cubicaltt.pdf][Cubical Type Theory: a constructive interpretation of the univalence axiom - TTT-cubicaltt.pdf]]
**** Articles
***** [[http://www.ams.org/notices/201309/rnoti-p1164.pdf][The Univalence Axiom in Homotopy Type Theory]] - Awodey
***** [[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.207.4309][CiteSeerX â Observational Equality, Now!]] - McBride, Altenkirch
***** [[http://www.cse.chalmers.se/~ulfn/papers/tphols09/tutorial.pdf][A brief overview of Agda]] - Ulf Norell
***** [[http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf][propositions-as-types.pdf]] Philip Wadler
***** TODO [[http://imps.mcmaster.ca/doc/seven-virtues.pdf][seven-virtues]] Seven virtues of type theory
***** TODO [[http://conal.net/papers/compiling-to-categories/compiling-to-categories.pdf][Compiling to categories]]
**** Mikrokosmos resources
***** [[http://www.madore.org/~david/computers/callcc.html][A page about call/cc]]
***** [[http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3FF1F113BC01E491476F4F1930F838FE?doi=10.1.1.44.1497&rep=rep1&type=pdf][An untyped lambda calculus with IO]]
** Miscellany
*** CARDS
**** Partitions of a set                                                                                     :drill:
:PROPERTIES:
:ID:       6e56efc2-7a11-42dd-b8cb-815b65f26021
:END:
Number of partitions of a set of $n$ elements.
***** Answer
In combinatorial mathematics, *Bell numbers* count the partitions of a set.
https://en.wikipedia.org/wiki/Bell_number

** Notas sueltas y borradores
*** Teorema de aproximaciÃ³n de Weierstrass                                                                    :extra:
# La generalizaciÃ³n de Stone es mucho mÃ¡s elegante, simple y general.

**** ConvoluciÃ³n
La convoluciÃ³n de dos funciones $f,g \colon \mathbb{R} \to \mathbb{R}$ es la funciÃ³n

\[
(f \ast g)(x) := \int_{\mathbb{R}}f(x-t)g(t)\,dt,
\]

que no tiene por quÃ© estar bien definida.

**** Propiedades de la convoluciÃ³n
La convoluciÃ³n es

 * conmutativa, $f \ast g = g \ast f$,
 * asociativa, $f \ast (g \ast h) = (f \ast g) \ast h$
 
***** Proof
# EstarÃ­a bien una demostraciÃ³n que fuera usando conjuntos de integraciÃ³n.
# Asociatividad por Fubini.

**** Identidades aproximadas
Una *identidad aproximada* es una sucesiÃ³n de funciones $g_n$ tales que

 * $\int_{\mathbb{R}} g_n = 1$,

 * $\int_{\mathbb{R}}\abs{g_n} < M$ acotadas por una constante uniforme,

 * $\forall \delta,\varepsilon > 0\colon \exists n_0 \colon \forall n \geq n_0\colon \forall x \geq \delta\colon |g_n(x)| < \varepsilon$,

 * $g_n(x) = 0$ para $|x|>1$.

**** AproximaciÃ³n por identidades
Para $f$ continua y nula fuera de $[0,1]$ y para $g_n$ identidad aproximada
se tiene $g_n\ast f \overset{c.u.}\longrightarrow f$.

***** Proof
Fijado un $\varepsilon > 0$ probaremos $\abs{(f \ast g_n)(x) - f(x)} < \varepsilon$. Al ser $f$ continua
en el compacto $[0,1]$, es uniformemente continua y existe $\delta$ tal que para
cualquier $x \in \mathbb{R}$ y $t \in (x-\delta,x+\delta)$ se tiene
\[
\abs{f(x) - f(x-t)} < \frac{\varepsilon}{2M},
\]
donde $M$ es la constante que acota las integrales en valor absoluto de $g_n$.
Por otro lado, por Teorema de Weierstrass, existe una acotaciÃ³n $\abs{f(x)} \leq C$,
y podemos usar la propiedad de la identidad aproximada $g_n$ para
tener que existe un $N$ tal que para todo $n \geq N$ y $|t| > \delta$ se tiene $\abs{g_n(t)} < \frac{\varepsilon}{8C}$.
Ahora, podemos usar lo que acabamos de probar y las propiedades de
la identidad aproximada para tener
\[\begin{aligned}
\abs{(f \ast g_n)(x) - f(x)} &= 
\abs{\int_{-\delta}^{\delta} f(x-t)g_n(t)\,dt - f(x)\int_{-\delta}^{\delta} g_n(t)\,dt} \\&=
\abs{\int_{-\delta}^{\delta} \Big(f(x-t) - f(x)\Big)g_n(t)\,dt} \\&\leq
\int_{-\delta}^{\delta} \abs{f(x-t)-f(x)}\abs{g_n(t)}\,dt +
\int_{|t| > \delta} \Big(\abs{f(x-t)}+\abs{f(x)}\Big)\abs{g_n(t)}\,dt \\&\leq
\frac{\varepsilon}{2M}\int_{-\delta}^{\delta} \abs{g_n(t)}\,dt +
2C\int_{|t| > \delta} \abs{g_n(t)}\,dt \\&\leq
\frac{\varepsilon}{2} + 2C\frac{2\varepsilon}{8C} < \varepsilon.
\end{aligned}\]

**** ConvoluciÃ³n de polinomios
Sea $f \colon [0,1] \to \mathbb{R}$ continua y con $f(0)=f(1)=0$, y sea $P$ un polinomio
restringido a $[-1,1]$; entonces $f \ast P$ es una funciÃ³n polinomial.

***** Proof
NÃ³tese que al ser $P$ un polinomio, $P(x-t)f(t)$ puede escribirse
como una suma de tÃ©rminos de la forma $a_ix^jt^kf(t)$ por lo que podemos
escribir.
\[
\int_{\mathbb{R}}P(x-t)f(t)\,dt =
\sum a_ix^j \left(\int_{\mathbb{R}} t^kf(t)\,dt\right).
\]

**** Polinomios para aproximaciÃ³n
La familia de polinomios $Q_n(x) = C_n(1-x^2)^n$ restringidos a $[-1,1]$
y con 
\[
C_n = \sum_{k=0}^n (-1)^k\frac{2}{2k+1}{n \choose k},
\]
entonces $Q_n$ es una identidad aproximada.

# TODO: IntegraciÃ³n por partes
***** TODO Proof

**** TODO Teorema de aproximaciÃ³n de Weierstrass
**** TODO Teorema de aproximaciÃ³n en localmente compactos
* Local variables                                                                                              :ignore:
# Local Variables:
# eval: (rainbow-delimiters-mode -1)
# org-tags-column: -119
# org-latex-inputenc-alist: (("utf8" . "utf8x"))
# eval: (setq org-latex-default-packages-alist (cons '("mathletters" "ucs" nil) org-latex-default-packages-alist))
# End:
